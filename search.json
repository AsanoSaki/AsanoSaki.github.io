[{"title":"Django Channels、WS协议及同步与异步详解","date":"2023-10-11T08:28:00.000Z","url":"/posts/45720.html","tags":[["Django","/tags/Django/"]],"categories":[["Django","/categories/Django/"]],"content":" 介绍同步与异步、WebSocket 协议以及 Django Channels 三者的概念及其联系，并结合代码示例进行讲解。 1. 同步与异步 在 Django 中，同步和异步主要涉及到请求处理的方式。这两种方式的主要区别在于它们如何处理多个并发请求： 同步（Synchronous）：在同步模式下，Django 会为每个请求创建一个单独的线程或进程。这意味着，如果一个请求正在等待响应（例如，等待数据库查询返回结果），那么整个线程或进程将被阻塞，直到响应返回。这可能会导致资源的浪费，因为在等待期间，线程或进程不能做其他任何事情。 异步（Asynchronous）：与同步模式不同，异步模式允许单个线程或进程同时处理多个请求。当一个请求需要等待响应时（例如，等待数据库查询返回结果），线程或进程可以切换到另一个请求，继续执行其他任务，而不是被阻塞。这样可以更有效地利用系统资源，提高并发处理能力。 Django 3.1 版本开始引入了对异步视图和中间件的支持，这意味着你可以编写异步的视图函数，这些函数可以使用 Python 的 async 和 await 关键字进行定义。这使得 Django 可以更好地处理 I/O 密集型任务，如 HTTP 请求、数据库操作和文件读写等。 然而需要注意的是，并非所有的 Django 组件都支持异步操作。例如，Django 的 ORM（对象关系映射）目前仍然是同步的，这意味着你不能在异步视图或中间件中直接使用它。如果你需要在异步代码中执行数据库操作，你需要使用 Django 提供的 sync_to_async 或 async_to_sync 函数来确保数据库操作在同步环境中执行。 总的来说，同步和异步各有优势和适用场景。对于 CPU 密集型任务，同步模式可能更合适；而对于 I/O 密集型任务，异步模式可能会带来更好的性能。在实际开发中，你可能需要根据应用的具体需求和性能要求来选择使用同步还是异步。 （1）同步代码样例 现在我们来看一些同步与异步的样例，首先是同步视图，在下面这个例子中，当请求到达 sync_view 时，Django 将等待视图函数完成后才会处理下一个请求： 下面这个例子在视图中同步地从数据库获取所有的博客对象，然后将它们传递给模板： （2）异步代码样例 在下面这个例子中，async_view 是一个异步视图。当请求到达这个视图时，Django 可以在等待 asyncio.sleep(1) 完成时处理其他请求： 请注意，要使用异步视图，你需要确保你的 Django 项目正在运行在支持异步的 ASGI 服务器上，而不是传统的 WSGI 服务器。此外，你的中间件和任何你在视图中调用的代码也必须支持异步。否则，你可能会遇到问题。如果你的代码库主要是同步的，那么最好坚持使用同步视图。如果你正在编写新的、主要使用 Python 的异步库的代码，那么异步视图可能会很有用。请记住，混合使用同步和异步代码可以很复杂，需要谨慎对待。 再来看下面的例子，我们创建了一个新的 get_blogs 异步函数，它使用 run_in_executor 方法在一个单独的线程中运行数据库查询。这允许 Django 在等待数据库查询完成时处理其他请求： 请注意，虽然这个示例展示了如何在 Django 视图中使用异步代码操作数据库，但是 Django 的数据库层目前还不支持原生的异步操作。因此，在实践中，你可能需要使用像 asgiref.sync_to_async 这样的工具来安全地在异步视图中执行同步数据库操作。同时，你也需要确保你的数据库驱动程序和数据库服务器能够处理并发连接。否则，你可能会遇到性能问题或错误。如果你不确定如何正确地使用异步代码，那么最好使用同步视图和同步数据库操作。 2. WebSocket WebSocket 是一种用于在 Web 和移动应用程序之间进行实时通信的新标准。WebSocket 设计为在 Web 浏览器和 Web 服务器之间实现，但也可以由客户端或服务器应用程序使用。WebSocket 是一种提供单个 TCP 连接上的全双工通信通道的协议，可以实现服务器和客户端之间的实时交互。 WebSocket 与 HTTP 不同，其主要区别如下： 通信方式：HTTP 是单向的，客户端发送请求，服务器发送响应。而 WebSocket 是双向的，在客户端-服务器通信的场景中使用的全双工协议，即客户端和服务器可以同时发送和接收数据。 连接：HTTP 每次请求都需要重新建立连接，而 WebSocket 使用长连接实现数据实时推送。一旦通信链接建立和连接打开后，消息交换将以双向模式进行，客户端-服务器之间的连接会持久存在。 数据传输：HTTP 协议中的数据传输是文本格式的，而 WebSocket 可以传输文本和二进制数据。 性能：由于 HTTP 的每次请求都需要建立连接和断开连接，而 WebSocket 可以在一次连接上进行多次通信，因此 WebSocket 在性能上比 HTTP 有优势。 应用场景：HTTP 主要用于客户端和服务器之间的请求和响应，如浏览器请求网页和服务器返回网页的 HTML 文件。WebSocket 可以实现双向通信，常常用于实时通信场景。 协议头：HTTP 协议头的大小从200字节到2KB不等，常见大小是700-800字节。而 WebSocket 协议头相对较小，这使得其在高频率、小数据量的通信场景下更有优势。 状态：HTTP 是无状态协议，而 WebSocket 是有状态协议。这意味着客户端和服务器之间的连接将保持活动状态，直到被任何一方（客户端或服务器）终止。 两种协议都位于 OSI 模型的第七层，并依赖于第四层的 TCP。尽管它们是不同的，但 RFC 6455 指出，WebSocket 旨在通过 HTTP 端口443和80工作，并支持 HTTP 代理和中介，从而使其与 HTTP 兼容。为了实现兼容性，WebSocket 握手使用 HTTP Upgrade 头从 HTTP 协议切换到 WebSocket 协议。 WebSocket 协议使得 Web 浏览器（或其他客户端应用程序）和 Web 服务器之间可以在不需要客户端请求的情况下发送内容，以及在保持连接打开的同时传递消息。这样，客户端和服务器之间可以进行双向持续的对话。通信通常是通过 TCP 端口号443（或80，如果是非安全连接）进行的，这对于使用防火墙阻止非 Web Internet 连接的环境是有利的。 在 Django 中实现 WebSocket，你可以选择使用 channels 或者 dwebsocket。但是，channels 被更广泛地使用，因为它可以完美地集成到 Django 的生态系统中。 3. 在JavaScript中使用WebSocket （1）创建 WebSocket 对象 （2）连接成功时的回调函数 当 WebSocket 连接成功时，onopen 事件会被触发。你可以在这个函数中发送消息到服务器： （3）从服务器接收信息时的回调函数 当从服务器接收到信息时，onmessage 事件会被触发。你可以在这个函数中处理接收到的数据： （4）连接关闭时的回调函数 当连接关闭后，onclose 事件会被触发。你可以在这个函数中处理连接关闭后的逻辑： （5）连接失败时的回调函数 （6）监听窗口关闭事件 当窗口关闭时，主动去关闭 WebSocket 连接，防止连接还没断开就关闭窗口，这样服务端会抛异常： 4. Django Channels Django Channels 是一个开源框架，它扩展了 Django 的功能，使得 Django 不仅可以处理 HTTP，还可以处理需要长时间连接的协议，如 WebSocket、MQTT（消息队列遥测传输）、聊天协议、广播等实时应用。 Channels 允许 Django 项目支持“长连接”，它用 ASGI 替换了 Django 的默认 WSGI。ASGI（Asynchronous Server Gateway Interface）为异步 Python Web 服务器和应用程序提供了一个接口，同时支持 WSGI 提供的所有功能。 Channels 保留了 Django 的同步行为，并添加了一层异步协议，允许用户编写完全同步、异步或两者混合的视图（Views）。Django Channels 提供了一种通信系统，叫做 Channel Layer，它可以让多个 Consumer 实例之间互相通信，以及与外部 Django 程序实现互通。 Channel Layer 主要包括两种抽象概念：Channel 和 Group。Channel 是一个发送消息的通道，每个 Channel 都有一个名称，拥有这个名称的人都可以往 Channel 里面发送消息。Group 是多个 Channel 的集合，每个 Group 都有一个名称，拥有这个名称的人都可以往这个 Group 里添加/删除 Channel，也可以往 Group 里发送消息。Group 内的所有 Channel 都可以收到，但是不能给 Group 内的具体某个 Channel 发送消息。使用 Django Channels 可以实现一些实时通讯的功能，如在线聊天室、游戏、通知等。 Consumer 是 Channels 的基本单位，相当于 Django 的视图，它是一个事件驱动的类，可以处理不同类型的事件，如连接、断开、接收消息等，支持同步和异步应用程序。 现在我们来看一下 Django Channels 的样例，首先需要安装 Channels 和 Channels Redis： 然后需要在你的项目设置中（settings.py 文件）配置 CHANNEL_LAYERS 需要添加 channels 到你的 INSTALLED_APPS 列表，并设置 ASGI_APPLICATION 和 CHANNEL_LAYERS。例如： 在这个例子中，我们假设你正在本地运行一个 Redis 服务器，它监听在6379端口。如果你的 Redis 服务器在其他地方或者使用了不同的端口，你需要更新 hosts 设置。 接下来你需要在 Django App 的目录下创建一个路由文件 routing.py，作用相当于 HTTP 的 urls，并在其中定义你的 WebSocket 路由。我们先创建出来： 此外，你还需要运行一个兼容的 ASGI 服务器，如 Daphne 或 Uvicorn。我们安装 Daphne： 输入 daphne 命令查看是否可用，如果不可用说明应该是没有配置环境变量，按如下方式修改环境变量（需要重启系统）： 为了在 Django 项目中使用 Daphne，你需要确保你的项目已经配置为使用 ASGI 而不是 WSGI。这通常意味着你需要在你的项目中创建一个 asgi.py 文件，并在你的设置文件中设置 ASGI_APPLICATION 变量（之前已经设置好了）。 现在我们配置 djangoapp/djangoapp/asgi.py 文件： 现在我们在 Django App 目录下创建 consumers.py： 在这个例子中，我们创建了一个 ChatConsumer 类，它是一个异步的 WebSocket Consumer，这个类继承自 AsyncWebsocketConsumer，这是 Django Channels 提供的一个基础类。 其中的主要函数说明如下： async def connect(self)：当一个 WebSocket 连接打开时，这个方法会被调用。在这个方法中，我们从 URL 路由中获取房间名，并将其保存在 self.room_name 中。然后，我们创建一个房间组名，并将其保存在 self.room_group_name 中。然后，我们将当前的 Channel 添加到房间组中。最后，我们接受 WebSocket 连接。 async def disconnect(self, close_code)：当一个 WebSocket 连接关闭时，这个方法会被调用。在这个方法中，我们将当前的 Channel 从房间组中移除。 async def receive(self, text_data)：当从 WebSocket 接收到消息时，这个方法会被调用。在这个方法中，我们首先将接收到的文本数据解析为 JSON，然后我们从 JSON 数据中获取消息，并将其发送到房间组中。 async def chat_message(self, event)：这是一个自定义的事件处理器方法，当从房间组接收到类型为 chat_message 的事件时，这个方法会被调用。在这个方法中，我们首先从事件中获取消息，然后我们将消息发送回 WebSocket。 总的来说，这段代码的功能为：当用户连接到 WebSocket 时，他们会被添加到一个名为 chat_&#123;room_name&#125; 的组中。当他们发送消息时，这个消息会被广播到他们所在的组中的所有其他用户。当他们接收到组中的消息时，这个消息会被发送回他们的 WebSocket，即实现了一个简单的聊天室功能。 最后我们定义一下 Consumer 的路由，以下代码将 URL 路径 ws/chat/&#123;room_name&#125;/ 映射到我们的 ChatConsumer。这意味着当用户连接到这个路径的 WebSocket 时，他们会被连接到聊天室： "},{"title":"Django使用SMTP发送邮件教程","date":"2023-10-08T07:37:00.000Z","url":"/posts/60606.html","tags":[["Django","/tags/Django/"]],"categories":[["Django","/categories/Django/"]],"content":" 介绍 Django 中如何使用 SMTP 协议发送邮件。 1. SMTP介绍 SMTP（Simple Mail Transfer Protocol）即简单邮件传输协议，它是一组用于由源地址到目的地址传送邮件的规则，由它来控制信件的中转方式。SMTP 协议属于 TCP/IP 协议簇，它帮助每台计算机在发送或中转信件时找到下一个目的地。通过 SMTP 协议所指定的服务器，就可以把 E-mail 寄到收信人的服务器上了，整个过程只要几分钟。SMTP 服务器则是遵循 SMTP 协议的发送邮件服务器，用来发送或中转发出的电子邮件。 一个 SMTP 服务器总是有一个独特的地址，和一个用于发送邮件的特定端口，在大多数情况下是587。本文以使用 QQ 邮箱为例，因此我们将使用的地址是 smtp.qq.com，端口号是587。 2. 申请邮箱授权码 首先我们需要启用 QQ 邮箱的 SMTP 服务，并申请一个授权码，打开邮箱网页，然后在“设置-账号”选项卡中找到 SMTP 服务部分，点击开启服务，然后完成相关的身份验证后会收到一串授权码。 3. Django发送邮件 首先在项目的 settings.py 文件中添加配置信息： 配置参数说明如下： EMAIL_BACKEND：声明了我们的 Django 项目将用于连接 SMTP 服务器的后端。这个变量指向 smtp.EmailBackend 类，该类接收发送邮件所需的所有参数。虽然这个类是默认的 EMAIL_BACKEND，但在 Django 的设置中明确定义被认为是一个好的做法。 EMAIL_HOST：将要使用的 SMTP 服务器域，取决于你的电子邮件提供商。常用的有：smtp.qq.com、smtp.163.com、smtp.gmail.com 等。 EMAIL_PORT：SMTP 服务器端口号，587是大多数 SMTP 服务器的默认端口，对于个人电子邮件提供商来说，这一点仍然适用。 EMAIL_USE_TLS：TLS（Transport Layer Security，传输层安全协议），用于在两个应用程序之间提供保密性和数据完整性。在此处用于加密网络应用程序（Django）和服务器（SMTP 服务器）之间的通信。 EMAIL_HOST_USER：主机用户（发件人邮箱），设置是你的个人电子邮件地址。 EMAIL_HOST_PASSWORD：发件人的授权码，即从你的电子邮件帐户获得的应用程序密码。 RECIPIENT_ADDRESS：收件人邮箱列表。 现在假设我们的前端发来如下请求： 然后我们需要在后端接收 message，并将其通过邮件（django.core.mail.send_mail 函数）发送给自己（即 settings 中的 EMAIL_HOST_USER 为自己的邮箱，RECIPIENT_ADDRESS 列表中也仅有一个自己的邮箱）： "},{"title":"Django学习笔记-实现聊天系统","date":"2023-10-04T07:08:00.000Z","url":"/posts/54938.html","tags":[["Django","/tags/Django/"]],"categories":[["Django","/categories/Django/"]],"content":" 本节内容是通过 WebSocket 实现多人模式中的在线聊天系统。 1. 实现聊天系统前端界面 聊天系统整体可以分为两部分：输入框与历史记录。 我们需要先修改一下之前代码中的一个小 BUG，当在一个窗口中按 Q 时，另一个窗口中点击鼠标左键也能攻击，因为按下按键的事件被所有窗口都捕捉到了，这是不合理的。 我们之前监听的对象是 window，每个地图是一个 canvas 元素，因此我们可以绑定到 canvas 对象上。由于不是所有对象都能添加绑定事件的，因此我们还需要对 canvas 做一个修改，需要添加 tabindex 参数并将其聚焦后才能监听事件，首先在 GameMap 类中修改一下 canvas 对象： 在 Player 类中修改： 聊天的前端界面需要创建一个新的文件，我们在 ~/djangoapp/game/static/js/src/playground 目录下创建一个 chat_field 目录，并进入该目录创建 zbase.js 文件： 然后在 AcGamePlayground 类中创建出来： 现在在 Player 类中即可监听事件： 然后我们还需要实现一下聊天区的 CSS 样式（在 ~/djangoapp/game/static/css 目录的 game.css 文件中）： 现在我们实现在历史记录区域里添加新消息的功能： 2. 实现后端同步函数 我们先在 WebSocket 前端实现发送和接收消息的函数： 然后实现后端代码： 最后在前端的 ChatField 类中调用一下发送消息的函数即可： 上一章：Django学习笔记-实现联机对战(下)。 下一章：无。"},{"title":"Django、Nginx、uWSGI详解及配置示例","date":"2023-10-03T02:25:00.000Z","url":"/posts/65222.html","tags":[["Django","/tags/Django/"]],"categories":[["Django","/categories/Django/"]],"content":" 介绍 Django、Nginx 以及 uWSGI 三者的概念及其联系与区别，并讲解如何配置。 一、Django、Nginx、uWSGI的概念、联系与区别 Django、Nginx 和 uWSGI 都是用于构建和运行 Web 应用程序的软件，这三个软件的概念如下： Django：Django 是一个基于 Python 的开源 Web 框架，它提供了一套完整的工具和组件，可以帮助开发人员快速构建 Web 应用程序。Django 遵循了 MVC（模型-视图-控制器）的设计模式，将业务逻辑、数据模型和用户界面分离，提高了代码的可读性和可维护性。Django 还支持多种数据库、缓存、模板引擎、表单验证、国际化、安全性等特性，使得开发者可以专注于业务需求，而不用担心底层的细节。Django 框架主要负责处理业务逻辑和生成结果给 Web 服务器，再由 Web 服务器返回给浏览器。而 Web 框架和 Web 服务器之间的通信需要遵循一套规范，这个规范就是 WSGI。 Nginx：Nginx 是一个高性能的 HTTP 和反向代理 Web 服务器，它可以处理来自客户端（例如浏览器）的 HTTP 请求，并将其转发给后端的 Web 应用程序或其他服务器。Nginx 具有高并发、低内存占用、负载均衡、静态文件缓存等优点，可以提高 Web 应用程序的响应速度和可靠性。如果你有多个 Web 服务器，你可以使用 Nginx 来做负载均衡，根据某些规则将不同的请求分发到不同的 Web 服务器上去。 uWSGI：uWSGI 是一个实现了 WSGI 协议、uwsgi 协议和 HTTP 协议的 Web 服务器接口，它可以在 Web 服务器和 Web 应用程序之间提供接口，使得它们可以相互通信。WSGI（Web Server Gateway Interface）是一种 Python 用于 Web 开发的标准接口，它定义了 Web 服务器如何调用 Web 应用程序，并将结果返回给客户端，在生产环境中使用 WSGI 作为 Python Web 的服务器。uwsgi 是 uWSGI 程序实现的一个私有协议，它采用二进制格式传输数据，比 HTTP 协议更高效。uWSGI 是一个应用服务器，它可以将客户端请求转发给 Django 等 Web 应用程序进行处理。 Django、Nginx、uWSGI 之间的联系和区别主要体现在以下几个方面： 角色：Django 是一个 Web 框架，负责处理业务逻辑和生成响应内容；Nginx 是一个 Web 服务器，负责接收和转发 HTTP 请求；uWSGI 是一个 Web 服务器接口，负责将 HTTP 请求转换为 WSGI 请求，并调用 Django 处理。 协议：Django 遵循 WSGI 协议，与 uWSGI 进行通信；Nginx 遵循 HTTP 协议，与客户端和 uWSGI 进行通信；uWSGI 支持多种协议，包括 HTTP、uwsgi 和 WSGI。 性能：Django 本身不是一个高性能的 Web 框架，它需要借助其他软件来提高效率；Nginx 是一个高性能的 Web 服务器，它可以处理大量的并发请求，并缓存静态文件；uWSGI 是一个高效的 Web 服务器接口，它可以利用 uwsgi 协议减少数据传输的开销。 配置：Django 需要在 settings.py 文件中配置数据库、中间件、应用等信息；Nginx 需要在 nginx.conf 文件中配置监听端口、反向代理规则、静态文件路径等信息；uWSGI 需要在 uwsgi.ini 文件中配置项目路径、端口号、进程数等信息。 总结一下： Nginx：HTTP 服务器，反向代理服务器。 uWSGI：应用服务器，或者更精确地说是 WSGI 应用容器。 Django：WSGI 应用程序（框架）。 二、Nginx正向代理和反向代理的区别 正向代理和反向代理是两种不同的代理模式，它们的区别主要在于代理的对象和目的不同： 正向代理：指客户端（如浏览器）通过代理服务器来访问目标服务器，目的是为了隐藏客户端的真实身份或者突破访问限制。正向代理的特点是客户端知道目标服务器的地址，而目标服务器不知道客户端的地址。例如，如果你想访问某个国外的网站，但是由于网络封锁或者速度慢，你可以通过一个正向代理服务器来转发你的请求，这样就可以提高访问效率或者绕过限制。 反向代理：指客户端（如浏览器）直接访问代理服务器，然后代理服务器再转发请求给目标服务器，目的是为了提高目标服务器的性能或者安全性。反向代理的特点是客户端不知道目标服务器的地址，而目标服务器知道代理服务器的地址。例如，如果你想访问某个网站，但是这个网站有多台后端服务器提供服务，你可以通过一个反向代理服务器来分发你的请求，这样就可以实现负载均衡或者缓存等功能。 总之，正向代理和反向代理的区别就是看你是站在客户端的角度还是目标服务器的角度。正向代理是为了满足客户端的需求，而反向代理是为了满足目标服务器的需求。 三、Nginx与uWSGI的配置文件示例 要在 Django 应用程序中使用 Nginx 和 uWSGI，你需要做以下几个步骤： 安装 Nginx 和 uWSGI，你可以使用 apt 或 pip 命令来安装它们： 配置 uWSGI，你需要创建一个 ini 文件，指定你的项目目录、模块、端口、进程、日志等信息。 配置 Nginx，你需要创建一个 conf 文件，指定你的监听端口、服务器名、静态文件路径、反向代理规则等信息。 启动 uWSGI 和 Nginx，你可以使用 systemctl/service 或 uwsgi 命令来启动它们。 Nginx 配置文件的位置一般是在 /etc/nginx/nginx.conf，它用来定义 Nginx 服务器的基本参数。Nginx 配置文件的语法格式是由多个块组成，每个块用花括号 &#123;&#125; 包围，每个指令用分号 ; 结束。例如，一个简单的 Nginx 配置文件可以写成这样： uWSGI 配置文件的位置可以自己指定，一般放在项目目录下，假设我们在项目根目录下的 scripts/uwsgi.ini 文件中。uWSGI 配置文件的语法格式是由多个节组成，每个节用方括号 [] 包围，每个指令用等号 = 赋值。例如，一个简单的 uWSGI 配置文件可以写成这样： "},{"title":"Django学习笔记-实现联机对战(下)","date":"2023-09-25T12:19:00.000Z","url":"/posts/60167.html","tags":[["Django","/tags/Django/"]],"categories":[["Django","/categories/Django/"]],"content":" 本节内容是通过 Django Channels 框架使用 WebSocket 协议实现多人模式中的同步移动，攻击以及被击中判定函数。 此外实现房间内玩家数提示板、技能冷却时间以及闪现技能。 1. 编写移动同步函数move_to 与上一章中的 create_player 同步函数相似，移动函数的同步也需要在前端实现 send_move_to 和 receive_move_to 函数。我们修改 MultiPlayerSocket 类（在目录 ~/djangoapp/game/static/js/src/playground/socket/multiplayer 中）： 然后修改一下后端通信代码（~/djangoapp/game/consumers/multiplayer 目录中的 index.py 文件）： 最后我们还需要调用函数，首先我们需要在 AcGamePlayground 类中记录下游戏模式 mode： 然后在 Player 类中进行修改，当为多人模式时，需要广播发送 move_to 信号： 现在即可实现多名玩家的同步移动。当 A 窗口中的玩家移动时，首先该窗口（Player 类）的监听函数会控制该玩家自身进行移动，接着判定为多人模式，因此再调用 MultiPlayerSocket 类中的 send_move_to 函数向服务器发送信息（通过 WebSocket 向服务器发送一个事件），接着服务器端（~/djangoapp/game/consumers/multiplayer/index.py 文件中）的 receive 函数会接收到信息，发现事件 event 为 move_to，就会调用 move_to 函数，该函数会向这个房间中的其他所有玩家群发消息，每个窗口都会在前端（MultiPlayerSocket 类中）的 receive 函数接收到信息，通过事件路由到 receive_move_to 函数，该函数就会通过 uuid 调用每名玩家的 move_to 函数。 2. 编写攻击同步函数shoot_fireball 由于发射的火球是会消失的，因此需要先将每名玩家发射的火球存下来，此外我们实现一个根据火球的 uuid 删除火球的函数，在 Player 类中进行修改： 由于火球在 Player 中存了一份，因此我们在删除火球前需要将它从 Player 的 fire_balls 中删掉。且由于 FireBall 类中的 update 函数过于臃肿，可以先将其分成 update_move 以及 update_attack，我们修改 FireBall 类： 然后我们在 MultiPlayerSocket 类中实现 send_shoot_fireball 和 receive_shoot_fireball 函数： 现在我们需要实现后端函数： 最后是在 Player 类中调用函数： 3. 编写击中判定同步函数attack 我们需要统一攻击这个动作，由一个窗口来唯一判断是否击中，若击中则广播给其他窗口，因此窗口中看到其他玩家发射的火球仅为动画，不应该有击中判定。我们先在 FireBall 类中进行修改： 每名玩家还需要有一个函数 receive_attack 表示接收到被攻击的信息： 我们假设发射火球的玩家为 attacker，被击中的玩家为 attackee，被击中者的位置也是由攻击者的窗口决定的，且火球在击中其他玩家后在其他玩家的窗口也应该消失，因此还需要传火球的 uuid。我们在 MultiPlayerSocket 类中实现 send_attack 与 receive_attack 函数： 然后实现后端函数如下： 最后需要在火球 FireBall 类中调用攻击判定的同步函数： 4. 优化改进（玩家提示板、技能CD） 我们限制在房间人数还没到3个时玩家不能移动，需要在 AcGamePlayground 类中添加一个状态机 state，一共有三种状态：waiting、fighting、over，且每个窗口的状态是独立的，提示板会在之后进行实现： 接下来我们实现一个提示板，显示当前房间有多少名玩家在等待，在 ~/djangoapp/game/static/js/src/playground 目录下新建 notice_board 目录，然后进入该目录创建 zbase.js 文件如下： 每次有玩家创建时就将 player_count 的数量加一，当玩家数量大于等于3时将游戏状态转换成 Fighting，且设置除了在 Fighting 状态下点击鼠标或按下按键才有效果，否则无效。在 Player 类中进行修改： 现在对局一开始就能攻击，显然不太合适，因此还需要设定在游戏刚开始的前若干秒无法攻击，即技能冷却。每个窗口只有自己才有技能冷却，也就是只能看到自己的冷却时间。现在我们给火球技能设置一秒的冷却时间，在 Player 类中进行修改： 我们还不知道技能什么时候冷却好，因此还需要加上一个技能图标与 CD 提示，可以模仿其他 MOBA 类游戏，在技能图标上添加一层 CD 涂层即可。假设我们的技能图标资源存放在 ~/djangoapp/game/static/image/playground 目录下，那么我们在 Player 类中渲染技能图标： 5. 闪现技能 闪现技能的实现很简单，整体参考之前的火球技能即可，我们先实现单机模式下的闪现技能，在 Player 类中实现： 然后我们还需要将闪现技能在多人模式中进行同步，原理和移动的同步是一样的，先在 MultiPlayerSocket 类中实现前端函数： 然后实现一下后端，在 ~/djangoapp/game/consumers/multiplayer/index.py 文件中实现： 最后在 Player 类中调用一下广播闪现技能的函数即可： 上一章：Django学习笔记-实现联机对战(上)。 下一章：Django学习笔记-实现聊天系统。"},{"title":"Web学习笔记-React（Redux）","date":"2023-09-11T02:56:00.000Z","url":"/posts/41844.html","tags":[["Web","/tags/Web/"]],"categories":[["Web","/categories/Web/"]],"content":" 本文记录 React 的学习过程，内容为 Redux。 之前我们提到过一个问题，就是如果两个兄弟组件要访问对方的数据，需要将数据存放到最近公共祖先上，这样当 DOM 树很复杂时就很麻烦。Redux 就是在整个 DOM 树之外拿出一个地方，用来存储一些全局的值。 1. Redux基本概念 Redux 会将 state 维护成一个树结构，每个节点存一个值，使用一个函数 reducer 维护每个值。Redux 用字典存储子节点，一般被称为 store。 我们如果想修改树里面的某一个值，会将整个树重新计算一遍。我们会使用 dispatch 函数，他会递归调用每个 reducer 函数。此外还需要传入一个对象参数，表示需要对哪个节点进行操作，其中有个属性 type，我们会给每个节点定义一个唯一的 type。 Redux 的基本概念总结如下： store：存储树结构。 state：维护的数据，一般维护成树的结构。 reducer：对 state 进行更新的函数，每个 state 绑定一个 reducer。传入两个参数：当前 state 和 action，返回新 state。 action：一个普通对象，存储 reducer 的传入参数，一般描述对 state 的更新类型，其中的 type 属性表示要修改的节点。 dispatch：传入一个参数 action，对整棵 state 树操作一遍，即递归调用整棵树的所有 reducer 函数。 我们先创建一个 Redux 项目 redux-app： 进入项目根目录，安装相关的模块： 假设现在我们只维护一个 state，我们构建一个最简单的朴素版 Redux（和 React 无关）： 现在来看看如何维护两个节点： 控制台输出如下： f_all 也可以不用自己手写实现，可以使用 combineReducers 实现： 2. React-Redux基本概念 现在我们来看一下 Redux 如何与 React 组合起来。我们需要用 Provider 将我们的整个项目包含起来。React-Redux基本概念如下： Provider 组件：用来包裹整个项目，其 store 属性用来存储 Redux 的 store 对象。 connect(mapStateToProps, mapDispatchToProps) 函数：用来将 store 与组件关联起来，该函数会返回一个函数，返回的函数可以将组件作为输入参数，然后返回一个新的组件，这个新的组件会将 state 的值绑定到组件的 props 属性上。 mapStateToProps：每次 store 中的状态更新后调用一次，用来更新组件中的值，即将 store 中 state 的值绑定到组件的 props 属性上。 mapDispatchToProps：组件创建时调用一次，用来将 store 的 dispatch 函数传入组件，即将 dispatch 函数映射到组件的 props 属性上。 为了方便展示，我们定义三个组件：App、Number（state 从0开始）、String（state 从空串开始）。 App 代码如下： 然后我们在 index.js 中实现 React-Redux： 现在我们来看一下如何在 Number 和 String 组件中调用他们的 state 值，需要用到一个 API：connect，以 Number 为例： 现在我们再来看一下如何修改 state 的值，需要定义 mapDispatchToProps 对象将 dispatch 映射到 props 里，假设我们要在 Number 中操作 String 里的 state： 同理我们实现在 String 中操作 Number 中的 state： 上一章：Web学习笔记-React（路由）。 下一章：无。"},{"title":"Web学习笔记-React（路由）","date":"2023-09-10T10:22:00.000Z","url":"/posts/25510.html","tags":[["Web","/tags/Web/"]],"categories":[["Web","/categories/Web/"]],"content":" 本文记录 React 的学习过程，内容为路由。 本节内容是如何将页面和 URL 一一对应起来，并实现前端渲染。 1. Web分类 Web 页面可以分为两大类： 静态页面：页面里的数据是写死的，即整个文件存放在服务器上，当用户访问 URL 时，服务器原封不动地将页面信息传给前端。 动态页面：页面里的数据是动态填充的，即服务器上存的是页面的模板，数据是存到数据库里的，当用户打开页面时，会动态将这个页面拼接起来。现在一般都是动态页面。 后端渲染：数据在后端填充，即模板与数据的拼接操作是在服务器端进行的。客户端向服务器端发送 URL，服务器端返回拼接好的页面。 前端渲染：数据在前端填充，即模板与数据的拼接操作是在用户的浏览器进行的。第一次打开页面时，客户端向服务器端发送 URL，服务器端返回所有页面的模板，渲染的时候根据当前需要哪些数据再向服务器端请求数据；第二次打开页面时，直接用 JS 刷新当前页面，不一定会向后端发送请求。 2. Route组件 Route 组件可以让我们的前端页面也可以和 URL 唯一对应起来，使得前端渲染的模式看起来假装和后端渲染是一样的。 我们创建一个新的项目 route-app，然后用 VS Code 打开项目： 配置一下环境： VS Code 安装插件：Auto Import - ES6, TS, JSX, TSX 安装 Route 组件（在项目根目录下安装，安装好后重启一下 VS Code）：npm i react-router-dom 安装 Bootstrap：npm i bootstrap Route 组件介绍： BrowserRouter：所有需要路由的组件，都要包裹在 BrowserRouter 组件内； Link：跳转到某个链接（但是没有向后端发请求），to 属性表示跳转到的链接； Routes：类似于 C++ 中的 switch，但是只匹配第一个路径，即从前往后看每个 Route，判断当前链接是否等于 Route 中的链接，如果是则渲染 Route 中的组件，之后的就不继续往下判断了； Route：路由，path 属性表示路径，element 属性表示路由到的内容（组件）。 我们先创建好我们项目的根组件 App、导航栏 NavBar，以及多个子页面的组件：Home、Linux、Django、Web、NotFound。 NavBar 代码如下： App 代码如下： Home、Linux、Django、Web、NotFound 代码类似，只展示一个： 现在我们根据 URL 来渲染页面，注意此时还是属于后端渲染，每次都会重新加载页面，我们修改 App： 现在我们用 Link 替换 NavBar 中的链接标签 a，这样就变为了前端渲染： 3. URL中传递参数 当网站的页面数量很多的时候，我们肯定不可能去写那么多个 Route。 假设我们现在有几篇 Web 讲义，第 $i$ 篇的路由链接为：/web/content/i： 我们先实现一下讲义内容的组件 WebContent： 然后在 App 中写一下路由（我们不能写多个 &lt;Route path='/web/content/i' element=&#123;&lt;WebContent /&gt;&#125; /&gt;，而是用 :xxx）： 现在我们如何在 WebContent 中获取 :chapter 参数呢？先看一下函数组件获取参数的方式，可以直接用 useParams 函数获取参数： 如果是类组件的话就需要先套一层函数组件，然后把 useParams 函数作为参数传给自己： 4. Search Params传递参数 如果网站链接形式为：/web/content?chapter=3，这样的链接也可以获取参数。 我们先改一下 Web 中的链接形式： 然后在 WebContent 中获取链接的参数： 函数组件的写法如下： 5. 重定向 当打开一个不存在的链接时应该重定向到 404 Not Found，我们先将这个路由定义出来：&lt;Route path='/404' element=&#123;&lt;NotFound /&gt;&#125; /&gt;。 使用 Navigate 组件可以重定向，我们可以使用通配符 * 匹配其余的所有路径，然后将其重定向到 /404 页面即可： 6. 嵌套路由 假设 Linux 组件中有两个子模块 Homework 和 Terminal，我们可以在 App 中创建嵌套路由： 但是现在执行网页 /linux/homework 时不会渲染出子路由的内容，我们需要在父组件中添加 &lt;Outlet /&gt; 组件，用来填充子组件的内容： 上一章：Web学习笔记-React（组合Components）。 下一章：Web学习笔记-React（Redux）。"},{"title":"Web学习笔记-React（组合Components）","date":"2023-09-09T09:38:00.000Z","url":"/posts/18290.html","tags":[["Web","/tags/Web/"]],"categories":[["Web","/categories/Web/"]],"content":" 本文记录 React 的学习过程，内容为组合 Components。 本节内容是组件与组件之间的组合，例如用不同组件构成 DOM 树，以及给不同的组件传递数据或者调用不同组件的方法，还有不同组件的生命周期。 1. 创建父组件 我们还是继续在之前的 Box 组件上进行操作，首先创建一个 Boxes 组件，其中包含一系列 Box 组件。 在 components 目录中创建 boxes.jsx： 然后修改一下 index.js： 现在我们在 Boxes 中加入多个 Box，当一个组件中包含多个并列元素的时候，需要用一个标签将他们括起来，可以使用 React 中的一个虚拟标签 &lt;React.Fragment&gt;： 为了方便也可以用一个数组来表示，将 Box 的信息存到 state 里，由于 React 组件如果有若干个儿子那么他们的 key 需要不一样，因此还需要存一个唯一的 id： 2. 从上往下传递数据 通过 this.props 属性可以从上到下传递数据。例如我们在 Boxes 中传递 x： 可以在 Box 中输出信息 console.log(this.props); 查看内容。 修改 Box 中的 x： 3. 传递子节点 可以将标签写成 &lt;Box&gt;&lt;/Box&gt; 的形式，然后在标签中添加子标签： 这样 this.props 中会多一个属性 children，可以使用 [] 单独访问某个子标签。我们可以将这个传过来的值定义在任何地方，例如可以放到每个 Box 组件的最上方： 4. 从下往上调用函数 父元素可以通过 this.props 向子元素传递信息，子元素也可以使用函数向父元素传递信息。假设我们需要实现通过点击删除按钮删除某个 Box，其信息保存在 Boxes 的 state 中，但是我们点击触发事件是在 Box 中（注意：每个组件的 this.state 只能在组件内部修改，不能在其他组件内修改）。 我们可以在父元素中定义好函数，然后将函数传给子元素： 这样子元素就能调用函数对父元素进行操作了： 现在我们在 Boxes 中实现一个 Reset 按钮实现清空所有 Box 的 x： 在控制台观察时可以发现点击 Reset 按钮后 x 确实置零了，但是 Box 显示出来的 x 并没有改变，这是因为 state 值不能在外部修改，因此我们可以将 Box 中的 state 删掉，需要在该组件中渲染外面的 state 的值。 每个维护的数据仅能保存在一个 this.state 中，不要直接修改 this.state 的值，因为 setState 函数可能会将修改覆盖掉。 修改 Boxes，将之前 Box 中操作 state 的函数转移过来： 然后修改 Box，将 this.state 替换成父组件传递过来的 props： 5. 兄弟组件间传递消息 如果组件的结构关系更为复杂，那么就需要将多个组件共用的数据存放到最近公共祖先的 this.state 中。 我们创建一个 App 组件，其包含两个子组件 NavBar（导航栏）和 Boxes，这两个组件为兄弟组件。 首先是 navbar.jsx： 然后是 app.jsx： 现在假设我们要在 NavBar 中存放 Boxes 中有几个 Box 的信息，那么只能把信息放到这两个组件的最近公共祖先 App 中。 我们将 Boxes 中与 state 有关的内容都移到 App 中： 移动后的 App 如下： 现在即可在 NavBar 中读取 Boxes 的长度信息了： 6. 无状态函数组件 当组件中没有用到 this.state 时，可以简写为无状态的函数组件。类相对于函数最大的好处就是可以很方便地维护状态（局部变量）。 无状态函数组件（Stateless Funtion Component），输入 sfc 即可自动补全出来。函数组件相当于只有 render 函数的类组件。注意：函数的传入参数为 props 对象： 7. 组件的生命周期 Mount 周期（挂载，表示对象被创建出来），执行顺序（按顺序执行三个函数）：constructor() -&gt; render() -&gt; componentDidMount() Update 周期（修改，例如点击按钮），执行顺序：render() -&gt; componentDidUpdate() Unmount 周期（删除），执行顺序：componentWillUnmount() 其中，componentDidUpdate 函数有两个参数，分别表示更新前的 props 和 state： 输出的 state 内容如下： 上一章：Web学习笔记-React（配置环境、ES6语法补充、Components）。 下一章：Web学习笔记-React（路由）。"},{"title":"Django学习笔记-实现联机对战(上)","date":"2023-08-27T09:42:00.000Z","url":"/posts/31494.html","tags":[["Django","/tags/Django/"]],"categories":[["Django","/categories/Django/"]],"content":" 本节内容是通过 Django Channels 框架使用 WebSocket 协议实现多人模式中的同步创建玩家函数。 1. 统一长度单位 多人模式中每个玩家所看到的地图相对来说应该是一样的，因此需要固定地图的长宽比，一般固定为16:9。我们需要在游戏窗口的长宽中取最小值，然后将地图渲染为16:9的大小。 我们在 AcGamePlayground 类中实现一个 resize 函数用于将长宽比调整为16:9并且达到最大： 现在需要将窗口大小的修改效果作用到黑色背景上，因此我们在 GameMap 类中也实现一个 resize 函数用于修改背景大小： 我们修改一下 game.css 文件，添加以下内容，实现将地图居中： 现在我们还需要修改地图里面的目标，一共有三种分别是玩家、火球、被击中的粒子效果。 首先修改一下 AcGamePlayground 类中的玩家初始化代码： 然后我们修改 Player 类，将所有绝对变量替换为相对变量： 然后修改 FireBall 类，只需要修改 eps 以及 render 函数即可： 最后修改 Particle 类，同样也是只需要修改 eps 以及 render 函数即可： 2. 增加联机对战模式 我们先修改 AcGameMenu 类，实现多人模式按钮的逻辑： 然后修改 AcGamePlayground 类，区分两种模式，且需要进一步区分玩家类别，之前使用 True/False 表示是否是玩家本人，现在可以用字符串区分玩家本人（me）、其他玩家（enemy）以及人机（robot）： 然后还需要修改一下 Player 类，将原本的 this.is_me 判断进行修改： 3. 配置Django Channels 假设有三名玩家编号为1、2、3进行多人游戏，那么每个玩家都有自己的一个窗口，且窗口中都能看到三名玩家。如果当前玩家1、2在进行游戏，3加入了游戏，那么需要告诉1、2两名玩家3来了，且还要告诉3当前已经有玩家1、2了。 要实现这一点，可以通过一个中心服务器 Server（可以就是自己租的云服务器），即3向服务器发送他来了，服务器给1、2发送消息，且服务器给3发送消息说之前已经有1、2两名玩家了。因此服务器中需要存储每个地图中的玩家信息，用于完成第一个同步事件：生成玩家事件。 我们之后一共需要实现四个同步函数：create_player、move_to、shoot_fireball、attack。前三个函数顾名思义，最后的 attack 函数是因为服务器存在延迟，比如3发射一个火球在本地看打中了1，但是由于延迟在1那边可能是没被打中的。 攻击判断是一个权衡问题，一般的游戏都是选择在本地进行攻击判断，而不是云服务器，即以发起攻击的玩家窗口进行判断，如果击中了则通过 attack 函数在服务器上广播信息。 在此之前我们使用的是 HTTP 协议，该协议为单向的，即客户端需要先向服务器请求信息后服务器才会返回信息，而服务器是不会主动向客户端发送信息的。 因此此处我们需要使用 WebSocket 协议（WS），同理该协议也有对应的加密协议 WSS，Django Channels 即为 Django 支持 WSS 协议的一种实现方式。 （1）安装 channels_redis： （2）配置 djangoapp/djangoapp/asgi.py 文件： 文件内容如下（注意 djangoapp 需要改成自己项目的名称）： （3）配置 djangoapp/djangoapp/settings.py 文件： 在 INSTALLED_APPS 中添加 channels，添加后如下所示（注意 djangoapp 需要改成自己项目的名称）： 然后在文件末尾添加： （4）配置 game/routing.py 文件： 这一部分的作用相当于 HTTP 的 urls，文件内容如下： （5）编写 game/consumers，这一部分的作用相当于 HTTP 的 views。 在 game 目录下创建 consumers 目录，然后进入该目录，先创建好 __init__.py 文件。由于我们未来会使用 WSS 协议支持联机对战和聊天室，因此我们需要再创建两个目录，先创建 multiplayer 目录，进入该目录创建 __init__.py 文件，然后编写 index.py： （6）启动 django_channels： 首先安装 daphne： 输入 daphne 查看是否可用，如果不可用说明应该是没有配置环境变量，按如下方式修改环境变量（需要重启系统）： HTTP 有 uwsgi 启动服务，WS 同样也需要启动，使用的是 asgi，在 ~/djangoapp 目录下执行（注意 djangoapp 需要改成自己项目的名称）： 项目进行到这里已经需要启动多个服务了，顺序是： 4. 前端创建连接 前端（Playground）需要跟后端（WS）连接，我们需要在每个客户端中建立一个和服务器的连接，一般是使用 Web Socket 连接。 首先配置一下 /djangoapp/game 目录下的路由 routing.py： 我们在 /djangoapp/game/static/js/src/playground 目录下创建 socket 目录，Socket 也分为两个模块，分别为联机模式和聊天室，还是先实现联机模式，在 socket 目录中创建 multiplayer 目录，进入该目录后创建 zbase.js： 当前端创建连接，即执行 new WebSocket 时，会调用 consumers 的 MultiPlayer 类（之后直接称为 MultiPlayer）中的 connect 函数；当前端断开连接（刷新或者关闭页面）时，会调用 disconnect 函数；receive 函数用于接收前端向后端发送的请求。 由于服务器需要向多个客户端群发消息，Django Channels 中有一个概念叫做组（group），可以将多个不同的连接放到同一个组里，可以使用相关函数统一操作组里的连接，例如 group_send 群发消息。 现在在 AcGamePlayground 类中添加我们刚实现的 MultiPlayerSocket： 然后我们打包静态文件并同步到发行版本，复习一下： 重启一下服务，现在我们每次都需要重启两个服务，即 HTTPS 和 WSS： 打开游戏进入多人模式，即可在后端的 WSS 服务看到输出信息。 5. 前端发送请求 有玩家进来时，需要两个函数，一个是实现客户端向服务器发送 create_player 请求，另一个是实现服务器从客户端接收请求的功能。 先实现发送请求功能，在 MultiPlayerSocket 类中编写 send_create_player 函数： 然后在 AcGamePlayground 类中调用： 没有修改过后端代码，因此不需要重启服务，直接重新打包一下静态文件即可，然后进入多人模式即可看到后端的输出信息，说明现在连接就已经创建成功了。 6. 编写同步函数create_player 我们每一个地图的所有信息都会有一个备份，比如1号玩家在2、3号窗口都会有一个备份，在不同的地图里我们需要能够判断出来谁是谁，比如在1号窗口中玩家1击中了玩家2，那么把信息发送到第二个窗口后，需要知道谁是2。 因此我们需要给所有信息一个唯一的编号（可以使用一个随机的八位数，如果怕重复可以使用更多位数），使得我们未来知道需要同步哪些东西。在 AcGameObject 类中进行修改： 现在又有一个问题，每次有一名新玩家进入时都会创建若干个编号，但是和之前其它窗口中的编号不一致，我们需要用通信的方式将他们保持一致，原则为谁创建的对象就用谁那边产生的编号，比如1号窗口创建了1号玩家，那么其它玩家窗口中的1号玩家的编号就由1号窗口发送过来。 由于某个客户端（假设是1号窗口）向服务器发送消息后服务器会转播给所有客户端，也就是会发给自己（1号窗口），这种情况1号窗口应该要 Pass 掉这条信息，因此我们需要判断信息是哪个客户端发的。这边我们就可以用每个玩家的唯一编号，这样就可以保证每个窗口不一样，我们在 AcGamePlayground 中修改： 现在客户端向服务器端发送消息的时候就要带上自己的 uuid： 现在再测试一下即可在后端看到服务器接收到的客户端信息。 接下来我们需要同步 create_player 这个事件，即当有新玩家来的时候，在所有窗口里创建这个玩家，同时将已有的玩家渲染到当前窗口里。 首先我们需要在服务器端存下每一个游戏房间的信息，可以存在 Redis 里，我们可以用房间（room）的概念。每个房间有人数上限，这是一个通用配置，可以写在 settings.py 里，我们往该文件里添加一行： 然后修改后端文件（/djangoapp/game/consumers/multiplayer 目录中的 index.py）： 现在我们需要在 AcGamePlayground 中传送用户名和头像参数： 然后在 MultiPlayerSocket 中接收信息并向服务器后端发送请求： 现在编写后端接收到信息后的处理逻辑，接收到创建玩家的信息后需要将该玩家添加到房间中，并给房间中其他连接群发消息，组内各个连接接收到消息后再发送给前端即可： 现在我们需要在前端处理接收 WSS 协议的信息，修改一下 MultiPlayerSocket 类： 现在我们多开窗口即可看到多名玩家同步到一个地图上了。 上一章：Django学习笔记-AcApp端授权AcWing一键登录。 下一章：Django学习笔记-实现联机对战(下)。"},{"title":"Django学习笔记-AcApp端授权AcWing一键登录","date":"2023-08-26T11:59:00.000Z","url":"/posts/57210.html","tags":[["Django","/tags/Django/"]],"categories":[["Django","/categories/Django/"]],"content":" 本节内容是通过 OAuth2 实现 AcApp 端的 AcWing 一键登录功能。 AcApp 端使用 AcWing 一键授权登录的流程与之前网页端的流程一样，只有申请授权码这一步有一点细微的差别。 我们在打开 AcApp 应用之后会自动向 AcWing 请求账号登录，客户端会向后端服务器请求一些参数，然后后端服务器向 AcWing 请求授权码，然后 AcWing 在接到请求之后会询问用户是否要授权登录，如果用户同意了那么 AcWing 会给客户端发送一个授权码，客户端可以通过授权码加上自己的身份信息向 AcWing 服务器请求自己的授权令牌 access_token 和用户的 openid，最后客户端在拿到令牌和 ID 后即可向 AcWing 服务器请求用户的用户名和头像等信息。 在网页端授权登录时我们使用的方法是通过 URL 的方式重定向到某一个链接里申请授权码，而这次的 AcApp 不是通过链接，而是通过 AcWing 的一个 API 申请，请求授权码的 API： 参数说明： appid：应用的唯一 ID，可以在 AcWing 编辑 AcApp 的界面里看到； redirect_uri：接收授权码的地址，表示 AcWing 端要将授权码返回到哪个链接，需要对链接进行编码：Python3 中使用 urllib.parse.quote；Java 中使用 URLEncoder.encode； scope：申请授权的范围，目前只需填 userinfo； state：用于判断请求和回调的一致性，授权成功后原样返回该参数值，即接收授权码的地址需要判断是否是 AcWing 发来的请求（判断收到的 state 与发送出去的 state 是否相同），如果不是直接 Pass。该参数可用于防止 CSRF 攻击（跨站请求伪造攻击），建议第三方带上该参数，可设置为简单的随机数（如果是将第三方授权登录绑定到现有账号上，那么推荐用 随机数 + user_id 作为 state 的值，可以有效防止CSRF攻击）。此处 state 可以存到 Redis 中，设置两小时有效期； callback：redirect_uri 返回后的回调函数，即接受 receive_code 函数向前端返回的信息。 用户同意授权后，会将 code 和 state 传递给 redirect_uri。 如果用户拒绝授权，则将会收到如下错误码： 我们在 game/views/settings/acwing/acapp 目录中将之前网页端的 apply_code.py 与 receive_code.py 复制过来，然后对 apply_code.py 进行一点小修改，这次不是返回一个链接，而是返回四个参数： 进入 game/urls/settings/acwing 修改一下路由： 现在访问  即可看到返回内容。 然后我们修改一下 receive_code.py： 接着我们修改前端文件，也就是 game/static/js/src/settings 目录中的 Settings 类： 注意，如果遇到跨域问题：Access to XMLHttpRequest at 'XXX'，大概率是某个文件的内容写错了，可以检查 uWSGI 启动后的报错内容修改代码。 上一章：Django学习笔记-VS Code本地运行项目。 下一章：Django学习笔记-实现联机对战(上)。"},{"title":"DeepLabV3Plus核心代码详解","date":"2023-08-19T07:49:00.000Z","url":"/posts/21781.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 本文记录 DeepLabV3+ 论文（Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation）的阅读笔记。 1. Atrous Spatial Pyramid Pooling ASPP（Atrous Spatial Pyramid Pooling），空洞空间金字塔池化。简单理解就是个至尊版池化层，其目的与普通的池化层一致，尽可能地去提取特征。 ASPP 本质上由一个 1×1 的卷积层、三个 3×3 的空洞卷积层 ASPP Conv 以及一个全局池化层 ASPP Pooling。五个模块输出的特征图尺寸都与输入相同，因此最后将它们在通道维度上 Concat 起来然后再通过一个 1×1 的卷积层降维得到 ASPP 的输出。 1.1 ASPP Conv 空洞卷积层与一般卷积层之间的差别在于膨胀率（dilation rate），膨胀率控制的是卷积时的 padding 以及 dilation。ASPP 应用了多个不同膨胀率的并行空洞卷积。通过不同的填充与膨胀，可以在不改变输出图像尺寸的情况下获取不同尺度的感受野，提取多尺度的信息。注意卷积核尺寸始终保持 3×3 不变： 1.2 ASPP Pooling ASPP Pooling 首先是一个 AdaptiveAvgPool2d 层。所谓自适应均值池化，其自适应的地方在于不需要指定 kernel size 和 stride，只需指定最后的输出尺寸（此处为 1×1）。通过将各通道的特征图尺寸分别压缩至 1×1，从而提取各通道的特征，进而获取全局的特征。然后是一个 1×1 的卷积层，对上一步获取的特征进行进一步的提取，并降维。需要注意的是，在 ASPP Pooling 的网络结构部分，只是对特征进行了提取；而在 forward 方法中，除了顺序执行网络的各层外，最终还将特征图从 1×1 上采样回原来的尺寸： 1.3 ASPP 我们将以上模块进行组合即可构建完整的 ASPP 模块： 2. 空洞卷积ResNet 本文以 ResNet50、101、152 为例，构建加入了空洞卷积的 ResNet 网络。ResNet 网络一共有四大层，我们记为 layer1、layer2、layer3 以及 layer4，默认情况下输出的 Feature Map 宽高尺寸比原图像小32倍，我们可以在第2~4层使用空洞卷积，假设在最后一层使用了空洞卷积，那么最后输出的 Feature Map 宽高尺寸比原图像小16倍： 3. IntermediateLayerGetter 我们需要用到 ResNet 特征提取层提取出的特征，其中有浅层特征（layer1 的输出）与深层特征（layer4 的输出），两者的通道维度我们分别记作 low_level_channels 与 in_channels，越深层的特征蕴含的语义信息越丰富，但是目标的位置信息更为模糊，小目标的特征可能还会丢失。因此我们需要记录下这些特征，实现 IntermediateLayerGetter： 4. DeepLabV3Plus ResNet 与 ASPP 均为 DeepLabV3 的 Encoder 部分，现在先介绍一下 Decoder 部分 DeepLabHeadV3Plus。 ResNet 提取出的浅层特征与深层特征的通道维度我们分别记作 low_level_channels 与 in_channels。先对浅层特征做一个投影，降低通道维度，将投影后的特征记作 low_level_feature；接着将深层特征通过 ASPP，并上采样到与 low_level_feature 相同的尺寸，将该特征记作 output_feature；最后将这两个特征在通道维度上 Concat 起来后通过分类头将通道维度映射为分类数量： 空间金字塔池化模块能够通过使用滤波器（卷积）或池化操作以多种比率（rate）和多个有效感受野（fields-of-view）探测传入特征，对多尺度的上下文信息进行编码；Encoder-Decoder 结构的模块能够通过逐渐恢复空间信息来捕获更清晰的对象边界。DeepLabV3Plus 结合了这两者的优点，通过添加了一个简单而有效的解码器模块来扩展 DeepLabv3，以细化分割结果，尤其是沿对象边界的分割结果。 此外还可将深度可分离卷积应用于 ASPP 和解码器模块，从而形成更快、更强的编码器-解码器网络，该部分代码在下一节中介绍。 最后我们组合所有的模块即可构建完整的 DeepLabV3Plus，注意最后需要对 Decoder 的分类头输出进行上采样，恢复到原始的输入图像大小： 5. AtrousSeparableConvolution 深度可分离卷积，将标准卷积分解为 Depth-wise 卷积与 Point-wise 卷积，大大降低了计算复杂度。我们在其中结合空洞卷积即可构建空洞可分离卷积： "},{"title":"KMeans聚类与PCA主成分分析","date":"2023-08-09T02:03:00.000Z","url":"/posts/23991.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 介绍二维数据与高维数据的 K-Means 聚类算法以及高维数据的 PCA 主成分分析方法。 1. 二维数据K-Means聚类 2. 主成分分析PCA PCA（Principal Component Analysis）是一种常见的数据分析方式，常用于高维数据的降维，可用于提取数据的主要特征分量。PCA 通常用于降低大型数据集的维数，使数据集中的指标数量变少，并且保留原数据集中指标的大部分信息。总而言之：减少数据指标数量，保留尽可能多的信息。 PCA 优点在于数据降维，便于提取数据的主要特征，使得数据更容易使用，减少计算开销，去除噪音等；PCA 适用于结构化数据，不仅能将数据压缩，也使得降维之后的数据特征相互独立。PCA 缺点在于不一定需要，有可能损失有用信息，只针对训练集保留主要信息，可能造成过拟合。 PCA 的步骤主要分为五步：标准化连续初始变量的范围、计算协方差矩阵以识别相关性、计算协方差矩阵的特征向量和特征值以识别主成分、创建特征向量来决定保留那些主成分、沿主成分轴重铸数据。 我们随机生成具有 y = 0.5x 的二维数据，并在 Y 轴方向上添加少量随机噪音。通过使用 PCA 法拟合数据，其中最重要的是分量和解释方差，对于这些数字的概念，让我们将其可视化为输入数据上的向量，使用分量来定义向量的方向，使用解释方差来定义向量的平方长度。 这些向量代表数据的主轴，向量的长度表明该轴在描述数据分布方面的重要性，更准确的说，它是投影时数据方差的度量到那个轴。每个数据点在主轴上的投影是数据的主成分。这种从数据轴到主轴的变换被称为 Affine transformation，基本上由平移，旋转和均匀缩放组成。 代码如下： 3. 高维数据PCA后聚类 4. 高维数据聚类并计算与中心的相似度 "},{"title":"分割图像的着色与相似度匹配","date":"2023-08-08T06:14:00.000Z","url":"/posts/21944.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 介绍图像分割后产生的 Mask 灰度图的着色以及相似度匹配计算。 1. 分割图像着色 以 SAM 分割为例，我们分割出来产生的 masks 为一个 List，长度为分割出来的类别数，List 中的每个元素为一个 Dict，记录了分割目标的面积、边界框等信息，其中的 segmentation 字段为分割出来的二值图，宽高与原图一致，目标像素点为 True，否则为 False。 我们实现两种方式分别对分割出来的 masks 以及保存下来的若干张分割图像进行合并与上色。灰度图像和伪彩色图像都对应一个索引表，这个索引表又叫调色板。图像的像素值就是索引，灰度图的索引表为： 像素值 R G B 0 0 0 0 1 1 1 1 2 2 2 2 3 3 3 3 ... ... ... ... 255 255 255 255 索引表不同的像素值对应的 RGB 值就是该像素的颜色，灰度图像的索引表中的 RGB 值都与像素值相同。同理，只要修改这些 RGB 数值，就可以显示伪彩色图像了。注意调色板的索引从0-255，因此，调色板的每个索引对应的 RGB 值都要进行设置。 代码如下： 2. 分割图相似度匹配 区域相似度（Region Similarity）：为了测量基于区域的分割相似度，即错误像素的数量，我们使用 Jaccard 索引 𝒥 表示， 𝒥 定义为预测的分割输出 Mask 和真值 Mask 之间的交并比 IoU（Intersection over Union），Jaccard 索引提供了关于错误分类像素的直观的信息。 边沿精度（Contour Accuracy）：边沿精度即计算 F-score，F-score 评估的是预测 Mask 的边界是否与真值 Mask 的边界对应。首先应提取预测 Mask 和真值 Mask 的边界元素坐标，将边界上的元素置为 True，非边界的元素置为 False。F-score 被定义为精度和召回率的调和平均数。 精度（Precision，P，也称查准率）：分母应是预测 Mask 的边界元素总数，分子则是在预测 Mask 为边界的那些元素中真正属于真值的。换句话说，预测 Mask 假设有100个元素为边界元素，但实际上可能只有70个存在于真值图的对应位置上，即70个真值的正样本被正确（True）预测为 Positive，属于 True Positive（TP），所以此时的查准率为70%，剩下的30个元素是错误（False）预测为 Positive，属于 False Positive（FP）。 召回率（Recall，R，也称查全率）：分母是真值 Mask 的边界元素总数，分子表示多少个本质的正样本被预测出来。例如真值 Mask 的边界有140个元素，但实际的预测 Mask 中只有70个真值的正样本被正确（True）预测为 Positive（TP），还有70个被错误（False）预测为 Negative（False Negative），那么此时的 Recall 为50%。 J &amp; F 指标的计算代码如下： "},{"title":"DeAOT视频追踪论文阅读笔记","date":"2023-08-08T05:58:00.000Z","url":"/posts/6828.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 本文记录 DeAOT 视频追踪论文的阅读笔记。 涉及的相关知识点为：AOT（Associating Objects with Transformers for Video Object Segmentation）、DeAOT（Decoupling Features in Hierarchical Propagation for Video Object Segmentation）、FPN（Feature Pyramid Networks for Object Detection）、Depth-wise Convolution、DropPath、GroupNorm。 1. 相关知识 1.1 深度可分离卷积 Depth-wise（DW）卷积与 Point-wise（PW）卷积，合起来被称作 Depth-wise Separable Convolution（深度可分离卷积），该结构和常规卷积操作类似，可用来提取特征，但相比于常规卷积操作，其参数量和运算成本较低。所以在一些轻量级网络中会碰到这种结构，如 MobileNet。 Depth-wise Convolution 的一个卷积核负责一个通道，即一个通道只被一个单通道的卷积核卷积，而常规卷积每个卷积核是同时操作输入图片的每个通道，即每个卷积核的通道数与图片的通道数相同。 Depth-wise Convolution 完成后的 Feature Map 数量与输入层的通道数相同，无法扩展 Feature Map。而且这种运算对输入层的每个通道独立进行卷积运算，没有有效地利用不同通道在相同空间位置上的特征信息。因此需要Point-wise Convolution 来将这些 Feature Map 进行组合生成新的 Feature Map。 Depth-wise Convolution 代码如下： Point-wise Convolution 的运算与常规卷积运算非常相似，它的卷积核的尺寸为 1 × 1 × M，M 为输入图像的通道数。所以这里的卷积运算会将上一步的 Feature Map 在深度方向上进行加权组合，生成新的 Feature Map。有几个卷积核就有几个输出 Feature Map。 Point-wise Convolution 代码如下： Depth-wise 卷积对每个输入通道独立执行空间卷积，而 Point-wise 卷积用于组合 Depth-wise 卷积的输出。将两者组合起来即为深度可分离卷积神经网络： 1.2 DropPath DropPath 是一种针对分支网络而提出的网络正则化方法，作用是将深度学习网络中的多分支结构随机删除。DropPath 作用的是网络分支，而 DropOut 作用的是 Feature Map，DropConnect 作用的是参数。 简单来说，DropPath 的输出是随机将一个 batch 中所有的神经元均设置为0；而在 DropOut 中，是在每个 batch 中随机选择神经元设置为0。 DropPath 代码如下： 1.3 Group Normalization BN 全名是 Batch Normalization，见名知意，其是一种归一化方式，而且是以 batch 的维度做归一化，那么问题就来了，此归一化方式如果使用过小的 batch size 会导致其性能下降，一般来说每个 GPU 上的 batch size 设为32最合适，但是对于一些其他深度学习任务 batch size 往往只有1或2，比如目标检测，图像分割，视频分类上，输入的图像数据很大，较大的 batch size 显存吃不消。 另外，Batch Normalization 是在 batch 这个维度上做 Normalization，但是这个维度并不是固定不变的，比如训练和测试时一般不一样，一般都是训练的时候在训练集上通过滑动平均预先计算好平均（mean），和方差（variance）参数，在测试的时候，不再计算这些值，而是直接调用这些预计算好的参数来用。但是，当训练数据和测试数据分布有差别时，训练机上预计算好的数据并不能代表测试数据，这就导致在训练、验证、测试这三个阶段存在 inconsistency（不一致性）。 Group Normalization（GN）首先将 channel 分为许多组（group），对每一组做归一化，即先将 feature 的维度由 [N, C, H, W] reshape 为 [N, G, C/G, H, W]，归一化的维度为 [C/G, H, W]。事实上，GN 的极端情况就是 LN（Layer Normalization）和 IN（Instance Normalization），分别对应 G = 1 和 G = C，作者在论文中给出 G 设为32较好。 Group Normalization 代码如下： 1.4 FPN特征金字塔网络 目标的多尺度一直是目标检测算法极为棘手的问题。像 Fast R-CNN，YOLO 这些只是利用深层网络进行检测的算法，是很难把小目标物体检测好的。因为小目标物体本身的像素就比较少，随着下采样的累积，它的特征更容易被丢失。 特征金字塔网络（Feature Pyramid Network，FPN）是一个在特征尺度的金字塔操作，它是通过将自底向上（Bottom-up）和自顶向下（Top-down）的特征图进行融合来实现特征金字塔操作的。FPN 提供的是一个特征融合的机制，并没有引入太多的参数，实现了以增加极小计算代价的情况下提升对多尺度目标的检测能力。 自底向上即是卷积网络的前向过程，我们可以选择不同的骨干网络，例如 ResNet-50 或者 ResNet-101。前向网络的返回值依次是 C2、C3、C4、C5，是每次池化之后得到的 Feature Map。 通过自底向上路径，FPN 得到了四组 Feature Map。浅层的 Feature Map，例如 C2 含有更多的底层信息（纹理，颜色等），而深层的 Feature Map 如 C5 含有更多的语义信息。为了将这四组倾向不同特征的 Feature Map 组合起来，FPN 使用了自顶向下及横向连接的策略，最终得到 P2、P3、P4、P5 四个输出。 最后，FPN 在 P2、P3、P4、P5 之后均接了一个 3*3 Conv 操作，该卷积操作是为了减轻上采样的混叠效应（aliasing effect）。 FPN 和 U-Net 最大的不同是它的多个层级的都会有各自的输出层，而每个输出层都有不同尺度的感受野。一个比较粗暴的方式是每一层都预测所有的样本，而另一个更好的选择是根据一些可能存在的先验知识选择一个最好的层。 FPN 代码如下： 2. 半监督VOS与AOT模型 2.1 VOS与AOT简介 视频对象分割（VOS）旨在识别和分割给定视频中的一个或多个感兴趣的对象，半监督 VOS 需要算法在给定一帧或多帧的对象注释掩码的情况下跟踪和分割整个视频序列中的对象。 此前最先进的方法学习用单个正目标解码特征，因此必须在多目标场景下单独匹配和分割每个目标，消耗多倍的计算资源。我们提出 Associating Objects with Transformers（AOT）方法来统一匹配和解码多个对象。AOT 采用 Identification 机制将多个目标关联到同一高维嵌入空间中。因此可以像处理单个对象一样高效地同时处理多个对象的匹配和分割解码。 AOT 方法将分层传播引入到 VOS 中。分层传播可以逐渐将 ID 信息从过去的帧传播到当前帧，并将当前帧的特征从 object-agnostic（对象不可知）转移到 object-specific（对象特定）。 2.2 ID 机制 ID 机制为每个目标分配唯一的 ID 信息，并将任意数量（要求小于预定义的大量）目标的 mask 嵌入到同一高维空间中。因此，网络可以学习所有目标之间的关联或相关性。此外，可以利用分配的 ID 信息直接解码多对象分割。 我们初始化一个身份库（ID Bank），其中存储 M 个具有 C 维的识别向量。为了嵌入多个不同的目标掩码，每个目标将被随机分配一个不同的识别向量。 2.3 Long Short-Term Transformer（LSTT） 本文设计长短期 Transformer（LSTT）用于构建分层对象匹配和传播。每个 LSTT 块都利用长期注意力来匹配第一帧的嵌入，并利用短期注意力来匹配多个附近帧的嵌入。与仅利用一个注意力层的方法相比，我们发现分层注意力结构在关联多个对象方面更有效。 LSTT 首先采用自注意力层，负责学习当前帧内目标之间的关联或相关性。此外，LSTT 还引入了长期注意力和短期注意力，前者用于聚合来自长期记忆帧的目标信息，后者能够从邻近的短期帧学习时间平滑性。所有注意力模块都是以多头注意力的形式实现的，即多个注意力模块后跟串联和线性投影。 长期注意力负责将目标的信息从过去的记忆帧（包含参考帧和存储的预测帧）聚合到当前帧。由于当前帧和过去帧之间的时间间隔是可变的并且可以是长期的，因此时间平滑性难以保证。因此，长期注意力采用非局部注意力。 短期注意力用于聚合每个当前帧位置的时空邻域中的信息。直观上，多个连续视频帧之间的图像变化始终是平滑且连续的。因此，连续帧中的目标匹配和传播可以限制在小的时空邻域内，从而比非局部过程具有更好的效率。 三、DeAOT 本文重点是为半监督视频对象分割（VOS）开发一种更有效的分层传播方法。在 AOT 方法中 object-specific 信息的增加将不可避免地导致深层传播层中 object-agnostic 的视觉信息的丢失。为了解决这样的问题并进一步促进视觉嵌入的学习，本文提出了一种分层传播中的解耦特征（DeAOT）方法。DeAOT 通过在两个独立的分支中处理 object-agnostic 和 object-specific 的嵌入来解耦它们的分层传播。其次，为了补偿双分支传播的额外计算，设计了一种用于构造分层传播的有效模块 GPM（门控传播模块），它是通过单头注意力精心设计的。 3.1 分层双分支传播 DeAOT 在两个并行分支中传播对象的视觉特征（visual features）和掩码（mask features）特征。具体来说，视觉分支负责匹配对象、收集过去的视觉信息并提炼对象特征。为了重新识别对象，ID 分支重用视觉分支计算的匹配图（注意力图），将 ID 嵌入（由 AOT 中的 ID 机制编码）从过去的帧传播到当前帧。两个分支共享相同的具有 L 个传播层的层次结构。 3.2 门控传播模块GPM 门控传播函数首先通过使用条件门来增强基于注意力的传播，此外，我们利用 Depth-wise 卷积以轻量级方式增强局部空间上下文的建模。 门控传播模块由三种门控传播组成：自传播、长期传播、短期传播。与 LSTT 相比，GPM 去掉了前馈模块，进一步节省了计算量和参数。所有传播过程都采用门控传播函数。"},{"title":"Django学习笔记-VS Code本地运行项目","date":"2023-07-06T05:19:00.000Z","url":"/posts/56617.html","tags":[["Django","/tags/Django/"]],"categories":[["Django","/categories/Django/"]],"content":" 截止到上一章节，我们的项目一直是部署在云服务器上，包括编写代码以及调试运行也是在云服务器上，现在我们尝试将其放回 Windows 本地环境运行。 1. 将项目传到本地 这一步可以使用 Git 也可以使用 SCP，由于之前项目上传在 AcGit 上，只能在 AC Terminal 中拉取，因此使用 SCP 远程传输： 2. 虚拟环境配置 我们需要先配置一个和云服务上相同的虚拟环境，首先在 VS Code 中打开该项目并且打开终端，在项目根目录下创建虚拟环境： 虚拟环境创建成功之后，一般不会自动启用，所以需要启用它，进入 venv/Scripts 目录运行脚本 Activate.ps1： 此时在 VS Code 中可以看到命令行首部多了 (venv)，说明已进入虚拟环境，然后在右下角选择虚拟环境中的解释器即可。 现在我们在虚拟环境中安装所需的环境： 3. 修改项目相关文件 首先修改一下我们的打包脚本 compress_game_js.sh： 然后打开 Git Bash，进入 scripts 文件夹即可运行脚本： 然后将 djangoapp/settings.py 中的 Redis 配置删除，即删除以下这段话： 同样在该文件中需要将 localhost 添加到 ALLOWED_HOSTS 中： 然后将 AcWing 一键授权登录的相关文件夹删除：game/views/settings/acwing、game/urls/settings/acwing，然后修改 urls/settings/index.py 中的路由： 修改 AcGame 类，去掉 AcWingOS API，且改为非模块化引入 JS 方式，即去掉 export 关键字： 同时前端的 web.html 文件也需要修改： 最后修改 Settings 类，去掉 AcWing 一键登录功能，且修改 ajax 请求的地址： 顺带还需要修改一下 game.css： 到此已经可以在本地运行该项目了，我们也可以根据 Django 学习笔记-实现联机对战（上）和 Django 学习笔记-实现联机对战（下）统一长度单位以及添加闪现技能与技能冷却，因为这些部分与云端内容无关。 最后我们将 .git 文件夹删除，重新上传至 Github（先创建一个仓库 Small_Ball_Fight）： 上一章：Django学习笔记-Web端授权AcWing一键登录。 下一章：Django学习笔记-AcApp端授权AcWing一键登录。"},{"title":"Django学习笔记-Web端授权AcWing一键登录","date":"2023-06-27T15:19:00.000Z","url":"/posts/54821.html","tags":[["Django","/tags/Django/"]],"categories":[["Django","/categories/Django/"]],"content":" 本节内容是通过 OAuth2 实现网页端的 AcWing 一键登录功能。 1. 在Django中集成Redis Redis 为内存数据库，目前我们使用的是 Django 自带的数据库 SQLite，且能够很容易地迁移到 MySQL，这些数据库的效率不如 Redis，其特点为： Redis 存的内容为 key, value 对，而其它数据库存的是若干张表，每张表为若干条目； Redis 为单线程的，不会出现读写冲突。 首先我们需要先安装 Redis： 接着配置一下 Django 的缓存机制，将下面这段代码复制到 settings.py 中： 然后启动 redis-server，启动后可以使用 top 命令查看 redis-server 是否运行： 此时在项目根目录执行 python3 manage.py shell 进入 IPython 交互界面，输入以下代码测试一下 Redis： 注意如果出现报错：ConnectionError: Error 111 connecting to 127.0.0.1:6379. Connection refused. 说明 redis-server 没有启动。 2. 申请授权码 一键授权登录的流程是用户点击一键登录后弹出确认授权的页面，用户确认后就自动创建一个新的账号，且登录到网站里。 具体交互流程为：用户点击按钮后向网站服务器端（Web）发起申请，请求用 AcWing 账号登录，然后 Web 将自己的 AppID 报给 AcWing，AcWing 给用户返回一个页面询问用户是否要授权给刚刚的网站，如果用户同意，那么 AcWing 会将一个授权码 code（两小时有效期）发给 Web，Web 接到授权码后再加上自己的身份信息 AppSecret 以及 AppID 向 AcWing 申请一个授权令牌 access-token（两小时有效期）和用户的 openid（唯一辨别用户），Web 拿到令牌和用户 ID 后即可向 AcWing 申请用户的用户名和头像。 由于我们需要记录每个用户的 openid，因此我们需要在数据库（game/models/player/）的 Player 类中添加一个信息： 在根目录下更新一下数据库： 申请授权码的请求地址：。 请求方法为 GET，参考示例： 参数说明： appid：应用的唯一 ID，可以在 AcWing 编辑 AcApp 的界面里看到； redirect_uri：接收授权码的地址，表示 AcWing 端要将授权码返回到哪个链接，需要对链接进行编码：Python3 中使用 urllib.parse.quote；Java 中使用 URLEncoder.encode； scope：申请授权的范围，目前只需填 userinfo； state：用于判断请求和回调的一致性，授权成功后原样返回该参数值，即接收授权码的地址需要判断是否是 AcWing 发来的请求（判断收到的 state 与发送出去的 state 是否相同），如果不是直接 Pass。该参数可用于防止 CSRF 攻击（跨站请求伪造攻击），建议第三方带上该参数，可设置为简单的随机数（如果是将第三方授权登录绑定到现有账号上，那么推荐用 随机数 + user_id 作为 state 的值，可以有效防止 CSRF 攻击）。此处 state 可以存到 Redis 中，设置两小时有效期。 用户同意授权后会重定向到 redirect_uri，返回参数为 code 和 state。链接格式如下： 如果用户拒绝授权，则不会发生重定向。 我们在 game/views/settings 目录中创建一个 acwing 目录表示 AcWing 授权登录，然后在该目录中先创建一个 __init__.py，再创建两个子目录 web 和 acapp 分别表示 Web 端的 AcWing 一键登录以及 AcApp 端的 AcWing 一键登录，最后同样在这两个子目录中创建 __init__.py 文件。 在 web 目录下创建申请授权码的 API apply_code.py： 接着创建接收授权码的 API receive_code.py： 然后在 game/urls/settings 目录中也创建一个 acwing 目录，在该目录中创建 __init__.py 和 index.py，index.py 内容如下： 然后需要将该目录 include 进 settings 目录的 index.py 中： 重启项目后即可访问 ;公网IP&gt;/settings/acwing/web/apply_code/ 以及 ;公网IP&gt;/settings/acwing/web/receive_code/ 测试效果。 现在来看看 urllib.parse.quote 的作用，它能够将链接重新编码，替换掉原本的部分特殊字符防止出现 BUG： 现在我们完善一下 apply_code.py 的内容： 然后修改一下前端代码（Settings 类）： 3. 申请授权令牌和用户ID 请求地址：。 请求方法为 GET，参考示例： 参数说明： appid：应用的唯一 ID，可以在 AcWing 编辑 AcApp 的界面里看到； secret：应用的秘钥，可以在 AcWing 编辑 AcApp 的界面里看到； code：上一步中获取的授权码。 申请成功的返回内容示例： 申请失败的返回内容示例： 返回参数说明： access_token：授权令牌，有效期2小时； expires_in：授权令牌还有多久过期，单位（秒）； refresh_token：用于刷新 access_token 的令牌，有效期30天； openid：用户的 ID。每个 AcWing 用户在每个 AcApp 中授权的 openid 是唯一的，可用于识别用户； scope：用户授权的范围。目前范围为 userinfo，包括用户名、头像。 现在我们点击 AcWing 一键登录按钮即可跳出请求授权的页面，AcWing 会向 receive_code 函数发送 code 以及 state，现在我们完善 receive_code.py 接到授权后的操作： 现在我们点击一键登录按钮即可在后台看到接收到的授权令牌内容。 4. 申请用户信息 请求地址：。 请求方法为 GET，参考示例： 参数说明： access_token：上一步中获取的授权令牌； openid：上一步中获取的用户 openid。 申请成功的返回内容示例： 申请失败的返回内容示例： 现在我们用授权令牌向 AcWing 申请用户的用户名和头像，进一步完善 receive_code.py： 至此 Web 端的 AcWing 一键登录已经实现。 上一章：Django学习笔记-用户名密码登录。 下一章：Django学习笔记-VS Code本地运行项目。"},{"title":"Django学习笔记-用户名密码登录","date":"2023-06-27T07:48:00.000Z","url":"/posts/6653.html","tags":[["Django","/tags/Django/"]],"categories":[["Django","/categories/Django/"]],"content":" 本节内容是使用 Django 实现用户的注册与登录等后端功能。 1. 扩充Django数据库 首先我们先在 settings.py 中修改：DEBUG = True，否则如果服务器端的代码报错时前端不会显示报错详细信息。 Django 自带一个账号系统，在网址后面添加后缀 /admin 即可访问管理员界面，登录之前的超级管理员账号，进入管理员界面后其中的 Users 界面就是自带的账号系统。 其中可以填写每个用户的姓名和邮箱，Active 表示用户是否被封，不勾选即无法登录；Staff status 表示用户是否能进入到后台管理页面；Superuser status 表示用户是否具有超级管理员的权限。 这个自带的账号系统不能满足我们的需求，例如需要存储用户的头像，该数据库就需要进行扩充。 我们要在 djangoapp/game/models 目录下创建我们所需的表，即创建类似之前提到的 Users，我们在该目录中新建一个 player 目录，然后进入该目录，记得需要先创建一个 __init__.py 文件。 接着我们创建 player.py 用来存储 player 这个数据表的信息，创建类时需要继承基类 django.db.models.Model（开发时有个小 Tips，如果忘记一些关键字怎么写可以在项目根目录执行 python3 manage.py shell，进入 IPython 交互功能，然后输入代码即具有补全功能）： 创建好数据表后如果希望让我们的数据表出现在管理员界面，需要将它注册到管理员页面，在 game 目录下可以看到一个 admin.py 文件，对其进行修改： 每次对数据表的定义更新后都需要执行以下两句指令（在项目根目录）： 然后我们重启一下项目：uwsgi --ini scripts/uwsgi.ini，即可看到新创建的 Player 表，在右上角的 ADD PLAYER 选项中即可添加 Player。 从这个例子即可看出数据库中的 Table 对应 Django 中的 Class，Table 中的每一条数据就对应 Class 中的每一个对象。 2. 实现获取用户信息 假设我们将网页刷新后，Clinet 先向后台服务器（Server）发送一个请求获得当前玩家的信息 getinfo，后台一种是返回用户名和头像，还有一种是返回未登录，现在我们先默认每次都返回玩家的用户名和头像，每次写一个函数时需要写三个部分：views 表示调用数据库的逻辑、urls 表示路由、js 实现调用。 由于有多个前端，因此后端在接收请求的时候需要知道是哪个前端，需要对 AcGame 类进行修改，让其多传入一个参数 acwingos，如果在 AcWing 打开该项目则会传入该参数： 进入 views 目录，我们将所有用户的信息全部放到 settings 目录中，在 settings 目录创建一个 getinfo.py 文件，内容如下： 然后进入 urls/settings 目录，修改 index.py 文件： 现在我们即可访问 ;项目IP地址&gt;/settings/getinfo/ 查看效果。 最后我们需要在游戏的菜单界面之前添加一个界面判断用户是否登录，先将 AcGameMenu 类在初始化操作中 hide，然后进入 /djangoapp/game/static/js/src/settings 目录中创建 zbase.js： 然后在 AcGame 类中创建 Settings 对象： 此时访问网站即可看到控制台的输出信息。 此时项目的执行顺序为：创建 Settings 类的对象时先执行构造函数，接着会执行 start 中的 getinfo 函数，会向后端（）发一个请求，然后路由会找到 views.settings.getinfo 文件中的 getinfo 函数，然后判断 platform 为 WEB，最后通过 getinfo_web 返回用户名和用户头像到 resp 中。 现在我们来判断用户是否登录，修改 views/settings 目录中的 getinfo.py： 然后我们需要将用户的信息存下来，将头像渲染到小球里，首先在 js/src/settings 目录中修改 zbase.js： 然后在 Player 类中渲染用户的头像： 3. 渲染登录与注册界面 首先我们下载 AcWing 一键登录所需要的图标资源，进入 static/image/settings 目录执行以下指令： 然后完善 Settings 类渲染登录与注册界面并且实现两个界面的跳转功能： game.css 内容如下： 4. 实现登录与登出功能 实现登录与登出功能我们同样要编写 views、urls 以及 js 文件，首先在 game/views/settings 目录下创建 login.py 文件： 然后进入 game/urls/settings 目录修改 index.py： 此时重启一下项目后访问 ;公网IP&gt;/settings/login/ 即可看到返回结果。 现在我们实现一下登出函数，在 game/views/settings 目录下创建 logout.py 文件： 然后进入 game/urls/settings 目录修改 index.py： 此时重启一下项目后在登录状态下访问 ;公网IP&gt;/settings/logout/ 即可登出。 然后我们在 Settings 类中实现登录登出： 其中登出函数 logout_on_remote 需要在 AcGameMenu 类中调用： 5. 实现注册功能 首先在 game/views/settings 目录下创建 register.py 文件： 然后进入 game/urls/settings 目录修改 index.py： 此时重启一下项目后访问 ;公网IP&gt;/settings/register/ 即可看到返回结果。 最后修改前端 Settings 类： 6. 修改获取用户信息 之前我们是默认返回第一个用户的信息，我们对 game/views/settings 目录下的 getinfo.py 文件进行修改： 上一章：Django学习笔记-部署Nginx与对接AcApp。 下一章：Django学习笔记-Web端授权AcWing一键登录。"},{"title":"Django学习笔记-部署Nginx与对接AcApp","date":"2023-06-25T15:10:00.000Z","url":"/posts/13510.html","tags":[["Django","/tags/Django/"]],"categories":[["Django","/categories/Django/"]],"content":" 本节内容是通过 Nginx 与 uWSGI 将项目部署至 AcWing 平台上，同时修复在 AcApp 小窗口上存在的部分 BUG。 现在我们需要将之前在网页上运行的项目部署至 AcWing 上，让其前后端分离，一个后端可以对应多个前端。 如果要将网站修改为 HTTPS 协议，需要先购买一个域名，然后申请证书，还需要进行备案，非常麻烦。在 AcWing 上线 App 已具备域名和证书。 1. 增加容器的映射端口80与443 由于创建后的容器不方便增加新的端口映射，因此我们先将原容器保存成镜像后再生成一个新的容器。 首先需要将容器中正在运行的任务全部关闭，然后登录运行容器的服务器，执行以下命令： 此时如果无法远程连接容器可以按照 Django 学习笔记-配置 Docker、Git 环境与项目创建文章创建一个 hosts.allow 文件。 接着去云服务器的控制台，在安全组配置中开放80和443端口。 2. 安装Nginx （1）安装依赖包（安装均在根用户 root 下进行）： （2）安装 Nginx： （3）查看版本号： （4）启动 Nginx 以及查看是否在运行中： 如果报错：System has not been booted with systemd as init system (PID 1). Can't operate. Failed to connect to bus: Host is down，可以参考这篇文章：System has not been booted with systemd as init system (PID 1). Can‘t operate. 问题解决方法，简而言之就是用以下指令代替： 3. 写入AcWing配置信息 配置信息在 AcWing 平台上可以查看，按以下步骤将信息复制到项目的配置文件中即可： 将 nginx.conf 中的内容写入服务器 /etc/nginx/nginx.conf 文件中。如果 Django 项目路径与配置文件中不同，注意修改路径。 将 acapp.key 中的内容写入服务器 /etc/nginx/cert/acapp.key 文件中（cert 目录需要自己创建）。 将 acapp.pem 中的内容写入服务器 /etc/nginx/cert/acapp.pem 文件中。 然后启动 Nginx 服务： 如果启动不成功可以重新加载一下 Nginx 的配置文件即可看到错误在哪： 4. 修改Django项目的配置 打开 settings.py 文件： 将分配的域名添加到 ALLOWED_HOSTS 列表中。注意只需要添加 https:// 后面的部分。 令 DEBUG = False。 归档 static 文件： 在项目根目录下执行：python3 manage.py collectstatic，执行完后可以看到生成了一份 game 中的 static 目录。 5. 配置uWSGI WSGI（Web Server Gateway Interface）是为 Python 语言定义的 Web 服务器和 Web 应用程序或框架之间的一种简单而通用的接口。网关（Gateway）的作用就是在协议之间进行转换。很多框架都自带了 WSGI Server，比如 Flask、Django 等。当然性能都不好，自带的 Web Server 更多的是测试用途，发布时则使用生产环境的 WSGI Server 或者是联合 Nginx 做 uWSGI。 uWSGI 是一个 Web 服务器，它实现了 WSGI、uwsgi、HTTP 等协议。Nginx 中 HttpUwsgiModule 的作用是与 uWSGI 服务器进行交换。 要注意 WSGI、uwsgi、uWSGI 这三个概念的区分： WSGI 的内容之前已经讲过了，是一种通信协议。 uwsgi 同 WSGI 一样是一种通信协议。 uWSGI 则是实现了 uwsgi 和 WSGI 两种协议的 Web 服务器。 uwsgi 协议是一个 uWSGI 服务器自有的协议，它用于定义传输信息的类型（type of information），每一个 uwsgi packet 前 4byte 为传输信息类型描述，它与 WSGI 相比是两样东西。 为什么有了 uWSGI 还需要 Nginx？因为 Nginx 具备优秀的静态内容处理能力，然后将动态内容转发给 uWSGI 服务器，这样可以达到很好的客户端响应。 现在用户通过80/443端口访问 Nginx，而 Nginx 与 Django 项目之间还需要一个桥梁就是 uWSGI。 在 Django 项目中添加 uWSGI 的配置文件：scripts/uwsgi.ini，内容如下： 由于使用 uwsgi 命令启动项目后原来的公网 IP 就无法访问了，因此需要对 web.html 中 CSS、JS 的链接地址进行修改： 然后启动 uwsgi 服务（代替 python3 manage.py runserver 0.0.0.0:8000）： 在 AcWing 填写完应用的剩余信息即可发布。 6. 部分BUG修复 在 AcWing 中打开应用时可以发现鼠标右键点击的位置不对，这是因为之前我们默认游戏的画布在左上角，而开启小窗口后画布就不在左上角了，我们移动小球时用的 e.clientX/Y 表示的是整个屏幕的坐标，而小球的位置坐标本身是画布中的相对坐标。 Canvas 中的 getBoundingClientRect 函数可以获得当前视窗在浏览器中的位置以及自身占据的空间的大小，left 表示窗口左侧边框距离浏览器视窗左侧的距离，top 表示窗口顶侧边框距离浏览器视窗顶侧的距离，right 表示窗口右侧边框距离浏览器视窗左侧的距离，bottom 表示窗口底侧边框距离浏览器视窗顶侧的距离： 因此我们将鼠标点击的坐标分别减去 left 和 top 即可映射到当前视窗内，在 Player 类中进行修改： 注意现在我们的项目为发行版本，修改完静态文件且打包完后还需要去项目根目录执行：python3 manage.py collectstatic，此时会问你是否要覆盖，输入 yes 即可。我们同样也可以修改之前的打包脚本 compress_game_js.sh，在其后面添加一行：echo yes | python3 manage.py collectstatic。 由于游戏有菜单界面，应该在从菜单进入游戏后才开始计算窗口大小，因此我们需要把 AcGamePlayground 的初始化放在 show 函数中执行： 最后我们需要对菜单页面进行修改，在小窗口中的菜单页面不太美观，需要让其适应窗口大小，应该用小窗口的相对距离来表示，对 game.css 进行修改： 上一章：Django学习笔记-创建游戏界面。 下一章：Django学习笔记-用户名密码登录。"},{"title":"Django学习笔记-创建游戏界面","date":"2023-06-14T12:50:00.000Z","url":"/posts/4217.html","tags":[["Django","/tags/Django/"]],"categories":[["Django","/categories/Django/"]],"content":" 本节内容是游戏界面的设计，包括各个目标的绘制、角色的移动与攻击、AI 敌人的设计等部分。 1. 模块化引入JS变量 首先我们需要对之前的代码进行一点小修改，在 web.html 中使用 &lt;script&gt; 会导致定义的所有 Class（例如 AcGame）都会变成网页的全局变量，当引入多个 JS 文件后网页可能会出现重名变量导致冲突，我们最好做一个模块化，当我们需要某个名称的时候，我们只将这一个名称引入进来： 这时候我们刷新网页会看到报错：Uncaught SyntaxError: The requested module '/static/js/dist/game.js' does not provide an export named 'AcGame'，表示如果我们想加载 AcGame 的话需要在这个类前面加一个关键字 export： 此时再次刷新网页即可看到报错消失。 2. 实现物体运动基类 在游戏中物体的运动效果是通过不断刷新界面实现的，浏览器每秒刷新60次（即60帧），每一帧都是一张图片，因此我们需要先实现一个能够每一帧都调用对象的刷新函数的基类（简易游戏引擎）。 我们在 static/js/src/playground/ 目录下创建一个 ac_game_object 目录，然后进入该目录创建 zbase.js。 这个类一般有三个函数，函数 start 在开始时执行一次，用于创建对象时初始化对象的颜色、分值、昵称等信息（从服务器端加载出来），函数 update 每一帧都会执行一次，函数 destroy 表示删除当前对象： 然后我们需要实现每一帧循环渲染一遍全局数组中的对象，JS 提供了一个 API：requestAnimationFrame()，该函数会在一秒钟调用60次，也就是将一秒钟分成60等份，在下一帧时执行一遍函数，我们可以定义一个函数 AC_GAME_ANIMATION 表示每帧需要执行的操作： 3. Canvas绘制游戏画面 接下来我们需要创建游戏画面，在 playground 目录下创建一个 game_map 目录，然后创建 zbase.js： 在 playground 目录下的 zbase.js 中将游戏画面对象创建出来即可： 现在浏览器中的 Canvas 是没有颜色的，我们需要将它渲染出来，即在 GameMap 类中添加一个渲染函数 render： 4. 创建游戏角色及实现移动效果 在 playground 目录下创建一个 player 目录，然后创建 zbase.js（角色也是一个游戏对象，因此也要从 AcGameObject 类中扩展出来），角色需要传入中心坐标 x, y、半径 radius、颜色 color、每秒移动的距离百分比 speed、是否为自己 is_me（因为未来在联机的时候自己和敌人的操作方式是不一样的，敌人的操作是通过网络传过来的）： 然后我们修改 AcGamePlayground 类将自己创建出来试试效果： 接下来我们实现小球的移动，我们需要给每个小球设置一个 X 轴方向的速度和 Y 轴方向的速度，在每次刷新对象的时候更新一下小球的坐标即可： 现在我们还需要实现小球移动到鼠标右键点击的位置，如下图所示，假设从 (x, y) 移动到 (tx, ty)，移动的距离即为两点间的欧几里得距离，atan2(y, x) 函数可以求出方向角 θ，我们将其移动方向归一化视为一个单位圆，那么 X 轴方向的速度即为 1 * cos(θ)，Y 轴方向的速度即为 1 * sin(θ)。 5. 创建角色技能 在 playground 目录下创建一个 skill 目录，我们先实现火球技能，在 skill 目录下再创建 fireball 目录，然后创建 zbase.js： 然后我们要实现玩家选中某个技能后点击鼠标能够使用该技能，获取按键信息时不能用 Canvas，因为不能聚焦，可以用 Window 来获取，每个按键的编号可以在网上查找对照表获得： 6. 创建敌人及实现简单AI 首先我们在 AcGamePlayground 类中添加5名敌人： 然后我们需要让敌人移动起来，可以设定一个随机的目的地，然后移动到该目的地时再随机一个新的目的地，在 Player 中进行相应的修改： 7. 火球碰撞检测 两个圆的相交检测很简单，只需要判断两个圆的圆心距离是否小于两圆的半径之和即可，我们在 FireBall 类中进行碰撞检测： 接着我们需要实现玩家被攻击时的效果，被攻击时有向后的击退效果，且击退过程中玩家无法移动，被攻击后血量减少（使用小球半径表示血量，即被攻击后小球半径变小），血量越少移动速度越快，我们在 Player 类中进行修改： 8. 实现被攻击时的粒子效果 我们可以设计一个当被攻击时向前爆出若干小球的粒子效果，在 playground 目录下创建一个 particle 目录，然后创建 zbase.js： 然后在 Player 类的被击中函数中创建若干粒子： 9. 敌人随机颜色 最后我们随机生成每个敌人的颜色，在 Playground 类中进行修改： 10. 敌人自动攻击 我们可以设置敌人随机向玩家射击，但是开局前几秒限制敌人射击，修改 Player 类如下： 上一章：Django学习笔记-创建菜单界面。 下一章：Django学习笔记-部署Nginx与对接AcApp。"},{"title":"Django学习笔记-创建菜单界面","date":"2023-06-10T12:04:00.000Z","url":"/posts/602.html","tags":[["Django","/tags/Django/"]],"categories":[["Django","/categories/Django/"]],"content":" 本节内容是项目结构的划分与游戏菜单界面的设计。 1. 项目总体设计 （1）系统设计 menu：菜单页面； playground：游戏界面； settings：设置界面。 （2）文件结构 templates：管理 HTML 文件； urls：管理路由，即链接与函数的对应关系； views：管理 HTTP 函数； models：管理数据库数据； static：管理静态文件，比如： css：对象的格式，比如位置、长宽、颜色、背景、字体大小等； js：对象的逻辑，比如对象的创建与销毁、事件函数、移动、变色等； image：图片； audio：声音； …… consumers：管理 WebSocket 函数。 （3）素材地址 背景图片： 下载方式：wget --output-document=自定义图片名称 图片地址； 本地上传：scp -P 20000 .\\xxx.jpeg asanosaki@&lt;公网IP&gt;:。 jQuery 库： &lt;link rel=&quot;stylesheet&quot; href=&quot;;公网IP&gt;:8000/static/css/jquery-ui.min.css&quot;&gt;； &lt;script src=&quot;;公网IP&gt;:8000/static/js/jquery-3.6.1.min.js&quot;&gt;&lt;/script&gt;。 2. 全局设置与项目结构创建 为了后期方便项目的维护管理，首先先将 game 中的 urls.py、models.py、views.py 删除，将其创建为一个目录，并在这三个目录下创建好 __init__.py 文件，注释掉上一节中在总 URL 文件中的配置信息，然后创建好 static 目录。 接着进行一些项目的全局设置，首先设置一下项目的时区，打开 ~/djangoapp/djangoapp/settings.py，修改 TIME_ZONE 和 USE_TZ： 如果 USE_TZ 设置为 True 时，Django 会使用系统默认设置的时区，即 America/Chicago，此时的 TIME_ZONE 不管有没有设置都不起作用。 注意：由于我们是在云服务器上开发的，如果是 Windows 则设置 TIME_ZONE 是无效的，Django 会使用本机的时间。 然后将自己创建的 App 加载进来，找到 INSTALLED_APPS，将 game/apps.py 添加进来： 找到 STATIC_URL = 'static/'，在其附近添加几行： 我们在 game/static/ 中创建 image/menu/ 目录用来存放菜单界面的背景，将图片 background.png 放入该目录中即可在自己的网址上访问这张图片：;公网IP&gt;:8000/static/image/menu/background.png。 一般 CSS 文件只需要一个就行，因此只需要在 game/static/css/ 中创建一个 game.css 文件即可。JS 文件最后一般会有很多，因此在 game/static/js/ 中创建两个目录：dist 和 src，分别表示最终合并在一起生成的 js 文件以及许多 js 源文件，我们可以写一个脚本完成合并操作，在 ~/djangoapp/ 目录下创建一个 scripts 目录，然后在该目录中编写一个 compress_game_js.sh 文件： 添加可执行权限： 我们执行一下该脚本：./compress_game_js.sh，可以看到 ~/djangoapp/game/static/js/dist/ 中多了一个 game.js 文件。 此时我们先将代码上传至 Git： 3. 创建HTML 我们先在 templates 目录下创建三个目录：menu、playground、settings。由于项目是前后端分离的，最后可以放在多种不同的终端上运行，因此我们再建一个 multiends 目录。 在 static/src 目录下也创建三个目录：menu、playground、settings，然后再创建一个文件 zbase.js，表示总文件，由于打包工具是按照字典序打包的，该文件中为总的 Class，会调用前面三个目录中的 Class，JS 在调用之前必须定义好，因此在打包时需要保证前面三个目录中的文件在 zbase.js 之前被打包，因此加一个字典序最大的字母在首部。 在 zbase.js 中定义好总类： 在 templates/multiends/ 中创建 web.html，文件内容如下： 4. 创建Views与更新URL 我们先在 views 目录下创建三个目录：menu、playground、settings，这三个目录都是存放 Python 文件的，因此每个目录下都需要一个 __init__.py 文件。 然后再在 views 目录下创建一个 index.py 作为总函数，该函数只会在 Web 端被调用，主要作用是用来返回上一节中的 HTML 文件的： 现在我们开始写路由，先在 urls 目录下创建三个目录：menu、playground、settings，在每个目录下都创建一个 __init__.py 文件。接着同样创建一个 index.py，用来将所有该路径下其它目录中的路径 Include 进来，可以参考总 URL 文件编写。 先在三个目录中也创建好 index.py，内容如下： 此前在 views 目录中实现的 index.py 为总函数，即整个项目只有一个主链接，因此 urls 目录中的 index.py 也要将其 Include 进来： 最后修改一下总 URL 文件（~/djangoapp/djangoapp/ 中的 urls.py）： 此时梳理一下路由顺序：首先会进入到 ~/djangoapp/djangoapp/ 中的 urls.py 中，然后发现浏览器链接中没有带后缀（例如 admin/），所以会进到 game.urls.index 中，由于没有后缀因此又调用第一个路由，也就是直接调用 game.views.index 中的 index 函数，该函数会渲染 multiends/web.html 里面的内容。 Tips：浏览器中按 F12 打开控制台后如果看到报错：The Cross-Origin-Opener-Policy header has been ignored, because the URL's origin was untrustworthy. It was defined either in the final response or a redirect.，表示出现了跨域问题，在 settings.py 中添加如下代码即可： 最后将代码上传至 Git： 5. 创建菜单 我们需要创建一个菜单对象，首先在 static/js/src/menu/ 目录中创建一个 zbase.js 文件，内容如下： 然后在 game.css 中定义相应的样式： 最后需要更新 static/js/src/ 中的 zbase.js 文件： 此时即可在页面中看到自己的背景图片（注意更新完 JS 文件后都需要执行一下打包脚本 compress_game_js.sh）。 Tips：如果修改完 CSS 文件后网页没有变化说明页面把 CSS 文件缓存下来了，可以按 F12 打开控制台，在 Network 选项卡中勾选上 Disable cache。 现在我们在菜单界面添加几个按钮，首先修改菜单目录中的 zbase.js 文件（修改完记得运行打包脚本）： 然后修改 game.css 设置一下按钮的样式： 现在我们实现点击按钮切换页面的功能，需要给每个按钮绑定一个函数，我们可以定义一个 start 函数放在构造函数中，表示对象被创建出来时需要执行的一些初始化操作，即在 start 中可以绑定一些事件。 首先我们先把游戏界面的简易版本写出来，在 static/js/src/playground/ 目录中创建一个 zbase.js 文件，内容如下： 然后更新一下总的 zbase.js 文件： 最后在菜单界面中设置一下按钮点击的响应操作，点击单人模式按钮即可跳转到 AcGamePlayground 界面： 最后将代码上传至 Git： 上一章：Django学习笔记-概述与项目环境配置。 下一章：Django学习笔记-创建游戏界面。"},{"title":"Django学习笔记-概述与项目环境配置","date":"2023-06-08T09:16:00.000Z","url":"/posts/26150.html","tags":[["Django","/tags/Django/"]],"categories":[["Django","/categories/Django/"]],"content":" 本文记录 Django 的学习过程。 Django 官方文档：Django Web Docs。 本节内容是 Django 框架的介绍以及本次开发的小游戏项目的环境配置与初始化。 1. Django概述 Django 是后起之秀，近些年越来越流行，Youtube（月活20亿+）、Instagram（月活10亿+）等公司采用了 Django 框架。可以作为 Web、App、小程序、AcWing 云端 App（AC APP）等各种项目的后端。 Django 优势： 开发效率高，生态完善，有官方社区长期支持。 运行效率高（常见误区：Python 运行效率低，所以 Python 写的应用运行效率低）。 项目运行效率瓶颈有很多，比如：数据库查询、网络带宽/延迟、硬盘读写速度等，这些与框架关系不大。 计算密集型的模块可以用 C/C++ 实现，然后编译成动态链接库再 import 进来。 计算密集型的微服务可以通过 thrift 等工具对接，微服务的 Server 端代码可以用 C/C++ 语言实现。 有很多工具可以将 Python 代码翻译成 C/C++，比如 Cython、Pypy。AcWing 题库中的不少题目，会发现 Python3 比 Java 还快一些。 既适合大公司，也适合个人开发者，平均开发一个 Web/AC App 只需要半个月至一个月。 2. 开发环境介绍 完全无需配置本地环境。利用 AC Terminal 直接在云端开发，使用工具：vim、tmux 等。不推荐在本地开发。 本项目会涉及多台服务器间的网络通信，如果在本地开发，未来不方便调试和部署。 在本地开发无法统一开发环境，部分 Python 包在 Windows 系统上安装困难。 需要租一台具有公网 IP 的云服务器，并安装 Docker。 服务器配置无要求。 后期可以利用 Docker 随意迁移。 在 AC Terminal 的 /var/lib/acwing/docker/images/ 目录下给大家提供统一的课程 Docker 镜像（也会讲解如何自己配置 Django 开发环境）。 标准化开发环境，避免未来出现软件版本不兼容。 省去配环境的环节。 使用 AC Git 管理项目代码。 方便回滚代码。 3. 配置Docker环境 首先拉取一个 Ubuntu 镜像： 创建容器后进入容器配置基本环境并创建用户： 然后去云服务器官网放行20000端口和8000端口，以阿里云为例，协议类型选 TCP，端口范围分别为 目的: 8000/8000 与 目的: 20000/20000，授权对象都为 源: 0.0.0.0/0。 通过 SSH 远程连接容器： 如果出现 kex_exchange_identification: read: Connection reset by peer 报错，就去进入容器的根用户下编辑 hosts.allow 文件： 在文件中添加一行：sshd: ALL，然后重启 SSH 服务：service ssh restart。 进入容器根用户安装剩余的环境： 在命令行查看 Django 版本： 4. Django项目创建 通过以下命令创建一个 Django 项目： 将项目上传至 Git（注意需要先在容器中生成公钥，并在 Git 中添加公钥）： 尝试启动一下项目： 这时候访问一下网址：;云服务器的公网IP&gt;:8000/，会看到提示：Invalid HTTP_HOST header: '&lt;云服务器的公网IP&gt;:8000'. You may need to add '&lt;云服务器的公网IP&gt;' to ALLOWED_HOSTS.，这是因为 Django 是个很安全的框架，会自动屏蔽很多可疑的访问，我们需要将公网 IP 添加到 settings.py 文件的 ALLOWED_HOSTS 中： Tips：如果找不到 ALLOWED_HOSTS 可以使用 ag 命令查找： 现在即可成功访问网址。 此时会看到项目文件夹下出现了一个 __pycache__ 目录，这个是预编译好的一些文件，用于加速 Python 运行，我们在往 Git 上传代码时最好不要上传这些中间文件，我们可以在仓库的根目录下添加一个 .gitignore 文件，文件内容如下： 最后上传至 Git： 5. Django App创建 在上一节中启动的页面为 Django 的默认页面，我们在开发时需要创建一个新的 App 写自己的页面，首先通过以下指令创建一个名为 game 的 App，创建好后当前目录下会生成一个名为 game 的目录： 此时启动 runserver 指令时会发现出现了报错：You have 18 unapplied migration(s). Your project may not work properly ......，原因是有一部分的数据库修改还没有同步到数据库里，运行以下指令同步数据库的修改： 打开 ;公网IP&gt;:8000/admin/ 页面可以看到 Django 自带的管理员页面，我们可以创建管理员用户（假设用户名和密码都为 admin）： 然后即可登录管理员账户进入管理员界面。 我们再回过头来进入之前创建的 App 中，能看到有几个比较重要的文件：models.py、views.py，我们先手动创建剩余的比较重要的文件： 接下来整个项目需要操作的就这四个文件，其中 models.py 存储各种数据结构，views.py 存储视图（函数），例如每点一次按钮都要调用一次服务器端的函数，urls.py 是一个路由，用户访问某个功能（页面）时传的是一个地址（URL），服务器端拿到这个地址后需要做一个路由，查看调用的是哪个函数，templates 存储 HTML 文件。 首先在 views.py 中写一个简单的函数： 然后参考 ~/djangoapp/djangoapp/urls.py（总 URL 文件）编写 game 中的 urls.py 文件： 然后我们需要更新一下总 URL 文件（~/djangoapp/djangoapp/urls.py）： 此时我们在项目根目录下启动项目：python3 manage.py runserver 0.0.0.0:8000，然后打开网址 ;公网IP&gt;:8000/game/ 即可看到我们的网页，最后更新一下 Git 即可。 6. 更改Django项目名称 我们以 settings.py 文件为例，假设项目的目录结构为：OLD_NAME/OLD_NAME/settings.py，项目根目录的名称可以随意改变，即我们可以先改为 NEW_NAME/OLD_NAME/...，然后我们再将第二级 OLD_NAME 目录改为 NEW_NAME，修改完第二级目录的名字后我们还需要改以下几个文件： settings.py manage.py asgi.py wsgi.py 上一章：无。 下一章：Django学习笔记-创建菜单界面。"},{"title":"Kaggle项目实战","date":"2023-05-30T01:45:00.000Z","url":"/posts/6211.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 记录 Kaggle 中的一些经典竞赛，也当做自己的练手小项目。 1. 实战Kaggle比赛：预测房价 "},{"title":"Python遥感常用模块Rasterio与Rioxarray教程","date":"2023-05-29T07:52:00.000Z","url":"/posts/9681.html","tags":[["Python","/tags/Python/"]],"categories":[["Python","/categories/Python/"]],"content":" rasterio 是一个很多模块是基于 GDAL 的 Python 包，可用于处理地理空间栅格数据，例如 GeoTIFF 文件。xarray 是一个为数组提供标签，例如尺寸、坐标和其他特定属性的 Python 包，它使大维数组的工作更加直观。rioxarray 结合了 rasterio 的功能和 xarray 的所有优点。 1. Rasterio与Rioxarray安装 首先安装 Rasterio 模块，（本人使用 conda 安装时遇到过报错 ImportError: cannot import name 'CRS' from 'pyproj' (unknown location)，是由于 pyproj 模块安装不全，因此建议采用后面的离线安装方式或者之后遇到问题时删除 pyproj 模块后再离线安装该模块）： 如果安装失败可以采用离线安装的方式，Rasterio 依赖很多第三方库，所以比较麻烦，按下面的顺序依次安装即可，可以尝试使用 pip 安装或者下载 .whl 文件离线安装（注意对上 Python 版本）： 各个模块的链接：Pyproj、Shapely、GDAL、Fiona、Rasterio。 离线安装指令： 在 Python 中使用 Anaconda 安装 rioxarray 包时，首先需要安装 GDAL 和 rasterio，然后再安装 rioxarray： 2. 使用教程 （1）使用 Rioxarray 读取并展示图像： 也可以用另一种形式展示（注意如果使用 Rasterio 读取图像则无法使用该方式展示图像）： （2）使用 Rasterio 读取图像： （3）转换为 Tensor 类型： （4）将 TIFF 图像逐像素提取出数据构建 CSV 文件： "},{"title":"Python绘图模块Plotly教程","date":"2023-05-29T07:42:00.000Z","url":"/posts/31783.html","tags":[["Python","/tags/Python/"]],"categories":[["Python","/categories/Python/"]],"content":" Plotly 是一个快速完善并崛起的交互式的、开源的绘图库库，Python 库则是它的一个重要分支。现已支持超过40种独特的图表类型，涵盖了广泛的统计、金融、地理、科学和三维用例。 1. Plotly安装 Python 中可以使用 pip 或者 conda 安装 Plotly： 2. Plotly绘图教程 2.1 折线图与散点图 折线图不仅可以表示数量的多少，而且可以反映同一事物在不同时间里的发展变化的情况，易于显示数据变化趋势，可以直观地反映这种变化以及各组之间的差别。 2.2 饼图 饼图用于强调各项数据占总体的占比，强调个体和整体的比较。 2.3 直方图 直方图虽然也和条形图一样通过矩形的长度表示数值，但他的宽度一般用于表示各组的组距，因此其高度与宽度均有意义，适合展示大量数据集的统计结果，直方图的表示的数据通常是连续排列，而柱状图则是分开排列。 可设置 barmode 参数实现多个直方图覆盖的效果： 2.4 条形图 条形图用于比较各组数据的差异性，强调进行个体间的比较。 2.5 热力图 热力图是一种特殊的图表，它是一种通过对色块着色来显示数据的统计图表，在绘图时，需要指定每个颜色映射的规则（一般以颜色的强度或色调为标准）；比如颜色越深的表示数值越大、程度越深或者颜色越浅的数值越大、程度越深。热力图适合用于查看总体的情况、观察特殊值或者显示多个变量之间的差异性、检测它们之间是否存在相关性等等。 2.6 导出图像到本地 首先我们需要安装两个依赖项：orca 和 psutil，orca 在 PyPi 存储库中不可用，因此需要使用 conda 安装： 或者直接安装 kaleido 模块： 安装完成后即可使用 Plotly 的 io 库导出图像（格式可以是 SVG、JPG、PNG等）： "},{"title":"动手学深度学习笔记(李沐)-注意力机制","date":"2023-05-21T09:57:00.000Z","url":"/posts/39408.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 李沐动手学深度学习（PyTorch）课程学习笔记第十章：注意力机制。 1. 注意力提示 注意力机制（Attention Mechanism）是人们在机器学习模型中嵌入的一种特殊结构，用来自动学习和计算输入数据对输出数据的贡献大小。 非自主性提示是基于环境中物体的突出性和易见性。想象一下，假如我们面前有五个物品：一份报纸、一篇研究论文、一杯咖啡、一本笔记本和一本书，所有纸制品都是黑白印刷的，但咖啡杯是红色的。换句话说，这个咖啡杯在这种视觉环境中是突出和显眼的，不由自主地引起人们的注意，所以我们会把视力最敏锐的地方放到咖啡上。喝咖啡后，我们会变得兴奋并想读书，所以转过头，重新聚焦眼睛，然后看看书，与咖啡杯是由于突出性导致的选择不同，此时选择书是受到了认知和意识的控制，因此注意力在基于自主性提示去辅助选择时将更为谨慎。受试者的主观意愿推动，选择的力量也就更强大。自主性的与非自主性的注意力提示解释了人类的注意力的方式，下面来看看如何通过这两种注意力提示，用神经网络来设计注意力机制的框架。 首先，考虑一个相对简单的状况，即只使用非自主性提示。要想将选择偏向于感官输入，则可以简单地使用参数化的全连接层，甚至是非参数化的最大汇聚层或平均汇聚层。 因此，“是否包含自主性提示”将注意力机制与全连接层或汇聚层区别开来。在注意力机制的背景下，自主性提示被称为查询（query）。给定任何查询，注意力机制通过注意力汇聚（attention pooling）将选择引导至感官输入（sensory inputs，例如中间特征表示）。在注意力机制中，这些感官输入被称为值（value）。更通俗的解释，每个值都与一个键（key）配对，这可以想象为感官输入的非自主提示。可以通过设计注意力汇聚的方式，便于给定的查询（自主性提示）与键（非自主性提示）进行匹配，这将引导得出最匹配的值（感官输入）。 平均汇聚层可以被视为输入的加权平均值，其中各输入的权重是一样的。实际上，注意力汇聚得到的是加权平均的总和值，其中权重是在给定的查询和不同的键之间计算得出的。为了可视化注意力权重，需要定义一个 show_heatmaps 函数，其输入 matrices 的形状是 (要显示的行数, 要显示的列数, 查询的数目, 键的数目)。下面使用一个简单的例子进行演示，在本例子中，仅当查询和键相同时，注意力权重为1，否则为0： 此外可以使用 Plotly 绘制热力图： 2. 注意力汇聚：Nadaraya-Watson核回归 上节介绍了框架下的注意力机制的主要成分：查询（自主提示）和键（非自主提示）之间的交互形成了注意力汇聚；注意力汇聚有选择地聚合了值（感官输入）以生成最终的输出。本节将介绍注意力汇聚的更多细节，以便从宏观上了解注意力机制在实践中的运作方式。具体来说，1964年提出的 Nadaraya-Watson 核回归模型是一个简单但完整的例子，可以用于演示具有注意力机制的机器学习，其理论介绍可见：注意力汇聚：Nadaraya-Watson核回归。 首先生成一个非线性函数的人工数据集： 函数 plot_kernel_reg 将绘制所有的训练样本（样本由圆圈表示），不带噪声项的真实数据生成函数（标记为 Truth），以及学习得到的预测函数（标记为 Pred）。先使用最简单的估计器来解决回归问题，即基于平均汇聚来计算所有训练样本输出值的平均值： 显然，平均汇聚忽略了输入，Nadaraya-Watson 核回归根据输入的位置对输出进行加权，是一个非参数模型。接下来，我们将基于这个非参数的注意力汇聚模型来绘制预测结果。从绘制的结果会发现新的模型预测线是平滑的，并且比平均汇聚的预测更接近真实。 非参数的 Nadaraya-Watson 核回归具有一致性（consistency）的优点：如果有足够的数据，此模型会收敛到最优结果。尽管如此，我们还是可以轻松地将可学习的参数集成到注意力汇聚中。 为了更有效地计算小批量数据的注意力，我们可以利用深度学习开发框架中提供的批量矩阵乘法： 在注意力机制的背景中，我们可以使用小批量矩阵乘法来计算小批量数据中的加权平均值： 定义 Nadaraya-Watson 核回归的带参数版本为： 接下来，将训练数据集变换为键和值用于训练注意力模型。在带参数的注意力汇聚模型中，任何一个训练样本的输入都会和除自己以外的所有训练样本的“键-值”对进行计算，从而得到其对应的预测输出： 训练带参数的注意力汇聚模型时，使用平方损失函数和随机梯度下降： 训练完带参数的注意力汇聚模型后可以发现：在尝试拟合带噪声的训练数据时，预测结果绘制的线不如之前非参数模型的平滑，因为与非参数的注意力汇聚模型相比，带参数的模型加入可学习的参数后，曲线在注意力权重较大的区域变得更不平滑。 3. 注意力评分函数 在上一节中使用了高斯核来对查询和键之间的关系建模。高斯核的指数部分可以视为注意力评分函数（attention scoring function），简称评分函数（scoring function），然后把这个函数的输出结果输入到 Softmax 函数中进行运算。通过上述步骤，将得到与键对应的值的概率分布（即注意力权重）。最后，注意力汇聚的输出就是基于这些注意力权重的值的加权和。 选择不同的注意力评分函数会导致不同的注意力汇聚操作。本节将介绍两个流行的评分函数，稍后将用他们来实现更复杂的注意力机制。 正如上面提到的，Softmax 操作用于输出一个概率分布作为注意力权重。在某些情况下，并非所有的值都应该被纳入到注意力汇聚中。例如，为了在机器翻译中高效处理小批量数据集，某些文本序列被填充了没有意义的特殊词元。为了仅将有意义的词元作为值来获取注意力汇聚，可以指定一个有效序列长度（即词元的个数），以便在计算 Softmax 时过滤掉超出指定范围的位置。下面的 masked_softmax 函数实现了这样的掩蔽 Softmax 操作（masked softmax operation），其中任何超出有效长度的位置都被掩蔽并置为0。 为了演示此函数是如何工作的，考虑由两个2*4矩阵表示的样本，这两个样本的有效长度分别为2和3。经过掩蔽 Softmax 操作，超出有效长度的值都被掩蔽为0： 同样，也可以使用二维张量，为矩阵样本中的每一行指定有效长度： 接下来将介绍加性注意力与缩放点积注意力，其理论分析可见：注意力评分函数。 一般来说，当查询和键是不同长度的矢量时，可以使用加性注意力作为评分函数。将查询和键连结起来后输入到一个多层感知机（MLP）中，感知机包含一个隐藏层，其隐藏单元数是一个超参数。通过使用 tanh 作为激活函数，并且禁用偏置项： 尽管加性注意力包含了可学习的参数，但由于本例子中每个键都是相同的，所以注意力权重是均匀的，由指定的有效长度决定： 使用点积可以得到计算效率更高的评分函数，但是点积操作要求查询和键具有相同的长度，为了演示 DotProductAttention 类，我们使用与先前加性注意力例子中相同的键、值和有效长度。对于点积操作，我们令查询的特征维度与键的特征维度大小相同： 与加性注意力演示相同，由于键包含的是相同的元素，而这些元素无法通过任何查询进行区分，因此获得了均匀的注意力权重。 4. Bahdanau注意力（使用注意力的seq2seq） Bahdanau 注意力模型的原理可见：Bahdanau 注意力。 下面看看如何定义 Bahdanau 注意力，实现循环神经网络编码器-解码器。其实，我们只需重新定义解码器即可。为了更方便地显示学习的注意力权重，以下 AttentionDecoder 类定义了带有注意力机制解码器的基本接口： 接下来，让我们在接下来的 Seq2SeqAttentionDecoder 类中实现带有 Bahdanau 注意力的循环神经网络解码器。首先，初始化解码器的状态，需要下面的输入： 编码器在所有时间步的最终层隐状态，将作为注意力的键和值； 上一时间步的编码器全层隐状态，将作为初始化解码器的隐状态； 编码器有效长度（排除在注意力池中填充词元）。 在每个解码时间步骤中，解码器上一个时间步的最终层隐状态将用作查询。因此，注意力输出和输入嵌入都连结为循环神经网络解码器的输入。 接下来，使用包含7个时间步的4个序列输入的小批量测试 Bahdanau 注意力解码器： 我们在这里指定超参数，实例化一个带有 Bahdanau 注意力的编码器和解码器，并对这个模型进行机器翻译训练： 模型训练后，我们用它将几个英语句子翻译成法语并计算它们的 BLEU 分数： 训练结束后，下面通过可视化注意力权重会发现每个查询都会在键值对上分配不同的权重，这说明在每个解码步中，输入序列的不同部分被选择性地聚集在注意力池中： 5. 多头注意力 在实践中，当给定相同的查询、键和值的集合时，我们希望模型可以基于相同的注意力机制学习到不同的行为，然后将不同的行为作为知识组合起来，捕获序列内各种范围的依赖关系（例如，短距离依赖和长距离依赖关系）。因此，允许注意力机制组合使用查询、键和值的不同子空间表示（representation subspaces）可能是有益的。 为此，与其只使用单独一个注意力汇聚，我们可以用独立学习得到的 h 组不同的线性投影（linear projections）来变换查询、键和值。然后，这 h 组变换后的查询、键和值将并行地送到注意力汇聚中。最后，将这 h 个注意力汇聚的输出拼接在一起，并且通过另一个可以学习的线性投影进行变换，以产生最终输出。这种设计被称为多头注意力（multihead attention）。对于 h 个注意力汇聚输出，每一个注意力汇聚都被称作一个头（head）。基于这种设计，每个头都可能会关注输入的不同部分，可以表示比简单加权平均值更复杂的函数。 多头注意力模型的原理可见：多头注意力。 在实现过程中通常选择缩放点积注意力作为每一个注意力头： 为了能够使多个头并行计算，上面的 MultiHeadAttention 类将使用下面定义的两个转置函数。具体来说，transpose_output 函数反转了 transpose_qkv 函数的操作： 下面使用键和值相同的小例子来测试我们编写的 MultiHeadAttention 类。多头注意力输出的形状是 (batch_size, num_queries, num_hiddens)： 6. 自注意力和位置编码 在深度学习中，经常使用卷积神经网络（CNN）或循环神经网络（RNN）对序列进行编码。想象一下，有了注意力机制之后，我们将词元序列输入注意力池化中，以便同一组词元同时充当查询、键和值。具体来说，每个查询都会关注所有的键-值对并生成一个注意力输出。当查询、键和值来自同一组输入时被称为自注意力（self-attention），也被称为内部注意力（intra-attention）。本节将使用自注意力进行序列编码，以及如何使用序列的顺序作为补充信息。 自注意力模型的原理可见：自注意力和位置编码。 下面的代码片段是基于多头注意力对一个张量完成自注意力的计算，张量的形状为 (批量大小, 时间步的数目或词元序列的长度, h)，输出与输入的张量形状相同： 在处理词元序列时，循环神经网络是逐个的重复地处理词元的，而自注意力则因为并行计算而放弃了顺序操作。为了使用序列的顺序信息，通过在输入表示中添加位置编码（positional encoding）来注入绝对的或相对的位置信息。位置编码可以通过学习得到也可以直接固定得到。接下来描述的是基于正弦函数和余弦函数的固定位置编码。 在位置嵌入矩阵 P 中，行代表词元在序列中的位置，列代表位置编码的不同维度。从下面的例子中可以看到位置嵌入矩阵的第6列和第7列的频率高于第8列和第9列。第6列和第7列之间的偏移量（第8列和第9列相同）是由于正弦函数和余弦函数的交替： 通过绘制热力图可以看到，位置编码通过使用三角函数在编码维度上降低频率： 7. Transformer"},{"title":"动手学深度学习笔记(李沐)-现代循环神经网络","date":"2023-04-11T04:58:00.000Z","url":"/posts/7592.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 李沐动手学深度学习（PyTorch）课程学习笔记第九章：现代循环神经网络。 1. 门控循环单元（GRU） 在通过时间反向传播中，我们讨论了如何在循环神经网络中计算梯度，以及矩阵连续乘积可以导致梯度消失或梯度爆炸的问题。下面我们简单思考一下这种梯度异常在实践中的意义，我们可能会遇到以下的情况： 早期观测值对预测所有未来观测值具有非常重要的意义。考虑一个极端情况，其中第一个观测值包含一个校验和，目标是在序列的末尾辨别校验和是否正确。在这种情况下，第一个词元的影响至关重要。我们希望有某些机制能够在一个记忆元里存储重要的早期信息。如果没有这样的机制，我们将不得不给这个观测值指定一个非常大的梯度，因为它会影响所有后续的观测值。 一些词元没有相关的观测值。例如，在对网页内容进行情感分析时，可能有一些辅助 HTML 代码与网页传达的情绪无关。我们希望有一些机制来跳过隐状态表示中的此类词元。 序列的各个部分之间存在逻辑中断。例如，书的章节之间可能会有过渡存在，或者证券的熊市和牛市之间可能会有过渡存在。在这种情况下，最好有一种方法来重置我们的内部状态表示。 在学术界已经提出了许多方法来解决这类问题。其中最早的方法是长短期记忆（long-short-term memory，LSTM），我们将在下一节中讨论。门控循环单元（gated recurrent unit，GRU）是一个稍微简化的变体，通常能够提供同等的效果，并且计算的速度明显更快。由于门控循环单元更简单，我们从它开始解读。 门控循环单元与普通的循环神经网络之间的关键区别在于：前者支持隐状态的门控。这意味着模型有专门的机制来确定应该何时更新隐状态，以及应该何时重置隐状态。这些机制是可学习的，并且能够解决了上面列出的问题。例如，如果第一个词元非常重要，模型将学会在第一次观测之后不更新隐状态。同样，模型也可以学会跳过不相关的临时观测。最后，模型还将学会在需要的时候重置隐状态。下面我们将详细讨论各类门控。 我们首先介绍重置门（reset gate）和更新门（update gate）。我们把它们设计成 (0, 1) 区间中的向量，这样我们就可以进行凸组合。重置门允许我们控制可能还想记住的过去状态的数量；更新门将允许我们控制新状态中有多少个是旧状态的副本。 我们从构造这些门控开始。重置门和更新门的输入是由当前时间步的输入和前一时间步的隐状态给出。两个门的输出是由使用 Sigmoid 激活函数的两个全连接层给出。门控循环单元的数学表达详见：门控循环单元（GRU）。 1.1 门控循环单元的从零开始实现 为了更好地理解门控循环单元模型，我们从零开始实现它。首先，我们读取上一章中使用的时间机器数据集： 下一步是初始化模型参数。我们从标准差为0.01的高斯分布中提取权重，并将偏置项设为0，超参数 num_hiddens 定义隐藏单元的数量，实例化与更新门、重置门、候选隐状态和输出层相关的所有权重和偏置： 现在我们将定义隐状态的初始化函数 init_gru_state。与从零开始实现 RNN 中定义的 init_rnn_state 函数一样，此函数返回一个形状为 (批量大小, 隐藏单元个数) 的张量，张量的值全部为零： 现在我们准备定义门控循环单元模型，模型的架构与基本的循环神经网络单元是相同的，只是权重更新公式更为复杂： 训练和预测的工作方式与从零开始实现 RNN 完全相同。训练结束后，我们分别打印输出训练集的困惑度，以及前缀 time traveler 和 traveler 的预测序列上的困惑度： 1.2 门控循环单元的简洁实现 高级 API 包含了前文介绍的所有配置细节，所以我们可以直接实例化门控循环单元模型。这段代码的运行速度要快得多，因为它使用的是编译好的运算符而不是 Python 来处理之前阐述的许多细节： 2. 长短期记忆网络（LSTM） 长期以来，隐变量模型存在着长期信息保存和短期输入缺失的问题。解决这一问题的最早方法之一是长短期存储器（long short-term memory，LSTM）。它有许多与门控循环单元一样的属性。 可以说，长短期记忆网络的设计灵感来自于计算机的逻辑门。长短期记忆网络引入了记忆元（memory cell），或简称为单元（cell）。有些文献认为记忆元是隐状态的一种特殊类型，它们与隐状态具有相同的形状，其设计目的是用于记录附加的信息。为了控制记忆元，我们需要许多门。其中一个门用来从单元中输出条目，我们将其称为输出门（output gate）。另外一个门用来决定何时将数据读入单元，我们将其称为输入门（input gate）。我们还需要一种机制来重置单元的内容，由遗忘门（forget gate）来管理，这种设计的动机与门控循环单元相同，能够通过专用机制决定什么时候记忆或忽略隐状态中的输入。 长短期记忆网络的数学表达详见：长短期记忆网络（LSTM）。 2.1 长短期记忆网络的从零开始实现 现在，我们从零开始实现长短期记忆网络，我们首先加载时光机器数据集： 接下来，我们需要定义和初始化模型参数。如前所述，超参数 num_hiddens 定义隐藏单元的数量。我们按照标准差0.01的高斯分布初始化权重，并将偏置项设为0： 在初始化函数中，长短期记忆网络的隐状态需要返回一个额外的记忆元，单元的值为0，形状为 (批量大小, 隐藏单元数)。因此，我们得到以下的状态初始化： 实际模型的定义与我们前面讨论的一样：提供三个门和一个额外的记忆元。请注意，只有隐状态 H 才会传递到输出层，而记忆元 C 不直接参与输出计算： 让我们通过实例化从零实现 RNN 章节中引入的 RNNModelScratch 类来训练一个长短期记忆网络，就如我们在上一节中所做的一样： 2.2 长短期记忆网络的简洁实现 使用高级 API，我们可以直接实例化 LSTM 模型： 3. 深度循环神经网络 到目前为止，我们只讨论了具有一个单向隐藏层的循环神经网络。其中，隐变量和观测值与具体的函数形式的交互方式是相当随意的。只要交互类型建模具有足够的灵活性，这就不是一个大问题。然而，对一个单层来说，这可能具有相当的挑战性。之前在线性模型中，我们通过添加更多的层来解决这个问题。而在循环神经网络中，我们首先需要确定如何添加更多的层，以及在哪里添加额外的非线性，因此这个问题有点棘手。 事实上，我们可以将多层循环神经网络堆叠在一起，通过对几个简单层的组合，产生了一个灵活的机制。特别是，数据可能与不同层的堆叠有关。例如，我们可能希望保持有关金融市场状况（熊市或牛市）的宏观数据可用，而微观数据只记录较短期的时间动态。 深度循环神经网络的数学表达详见：深度循环神经网络。 实现多层循环神经网络所需的许多逻辑细节在高级 API 中都是现成的。简单起见，我们仅示范使用此类内置函数的实现方式。以长短期记忆网络模型为例，该代码与上一节中使用的代码非常相似，实际上唯一的区别是我们指定了层的数量，而不是使用单一层这个默认值。像往常一样，我们从加载数据集开始： 像选择超参数这类架构决策也跟上一节中的决策非常相似。因为我们有不同的词元，所以输入和输出都选择相同数量，即 vocab_size。隐藏单元的数量仍然是256。唯一的区别是，我们现在通过 num_layers 的值来设定隐藏层数： 最后和上一节一样训练模型看看效果： 4. 双向循环神经网络 在双向循环神经网络中，每个时间步的隐状态由当前时间步的前后数据同时决定，通过反向更新的隐藏层来利用反向时间信息，通常用来对序列抽取特征、填空，而不是预测未来。 双向循环神经网络的数学表达详见：双向循环神经网络。 由于双向循环神经网络使用了过去的和未来的数据，所以我们不能盲目地将这一语言模型应用于任何预测任务。尽管模型产出的困惑度是合理的，该模型预测未来词元的能力却可能存在严重缺陷。我们用下面的示例代码引以为戒，以防在错误的环境中使用它们： 5. 机器翻译与数据集 语言模型是自然语言处理的关键，而机器翻译是语言模型最成功的基准测试。因为机器翻译正是将输入序列转换成输出序列的序列转换模型（sequence transduction）的核心问题。 与语言模型那一节中的语料库是单一语言的语言模型问题存在不同，机器翻译的数据集是由源语言和目标语言的文本序列对组成的。因此，我们需要一种完全不同的方法来预处理机器翻译数据集，而不是复用语言模型的预处理程序。 首先，下载一个由双语句子对组成的“英-法”数据集，数据集中的每一行都是制表符分隔的文本序列对，序列对由英文文本序列和翻译后的法语文本序列组成。请注意，每个文本序列可以是一个句子，也可以是包含多个句子的一个段落。在这个将英语翻译成法语的机器翻译问题中，英语是源语言（source language），法语是目标语言（target language）。 下载数据集后，原始文本数据需要经过几个预处理步骤。例如，我们用空格代替不间断空格（non-breaking space），使用小写字母替换大写字母，并在单词和标点符号之间插入空格： 与之前的字符级词元化不同，在机器翻译中，我们更喜欢单词级词元化（最先进的模型可能使用更高级的词元化技术）。下面的 tokenize_nmt 函数对前 num_examples 个文本序列对进行词元化，其中每个词元要么是一个词，要么是一个标点符号。此函数返回两个词元列表：source 和 target，source[i] 是源语言（这里是英语）第 i 个文本序列的词元列表，target[i] 是目标语言（这里是法语）第 i 个文本序列的词元列表。 让我们绘制每个文本序列所包含的词元数量的直方图。在这个简单的“英-法”数据集中，大多数文本序列的词元数量少于20个： 由于机器翻译数据集由语言对组成，因此我们可以分别为源语言和目标语言构建两个词表。使用单词级词元化时，词表大小将明显大于使用字符级词元化时的词表大小。为了缓解这一问题，这里我们将出现次数少于2次的低频率词元视为相同的未知（&lt;unk&gt;）词元。除此之外，我们还指定了额外的特定词元，例如在小批量时用于将序列填充到相同长度的填充词元（&lt;pad&gt;），以及序列的开始词元（&lt;bos&gt;）和结束词元（&lt;eos&gt;）。这些特殊词元在自然语言处理任务中比较常用。 回想一下，语言模型中的序列样本都有一个固定的长度，无论这个样本是一个句子的一部分还是跨越了多个句子的一个片断。这个固定长度是由语言模型中的 num_steps（时间步数或词元数量）参数指定的。在机器翻译中，每个样本都是由源和目标组成的文本序列对，其中的每个文本序列可能具有不同的长度。 为了提高计算效率，我们仍然可以通过截断（truncation）和填充（padding）方式实现一次只处理一个小批量的文本序列。假设同一个小批量中的每个序列都应该具有相同的长度 num_steps，那么如果文本序列的词元数目少于 num_steps 时，我们将继续在其末尾添加特定的 &lt;pad&gt; 词元，直到其长度达到 num_steps；反之，我们将截断文本序列时，只取其前 num_steps 个词元，并且丢弃剩余的词元。这样，每个文本序列将具有相同的长度，以便以相同形状的小批量进行加载。下面的 truncate_pad 函数将截断或填充文本序列： 现在我们定义一个函数，可以将文本序列转换成小批量数据集用于训练。我们将特定的 &lt;eos&gt; 词元添加到所有序列的末尾，用于表示序列的结束。当模型通过一个词元接一个词元地生成序列进行预测时，生成的 &lt;eos&gt; 词元说明完成了序列输出工作。此外，我们还记录了每个文本序列的长度，统计长度时排除了填充词元，在稍后将要介绍的一些模型会需要这个长度信息。 最后，我们定义 load_data_nmt 函数来返回数据迭代器，以及源语言和目标语言的两种词表： 6. 编码器-解码器架构 正如我们在上一节中所讨论的，机器翻译是序列转换模型的一个核心问题，其输入和输出都是长度可变的序列。为了处理这种类型的输入和输出，我们可以设计一个包含两个主要组件的架构：第一个组件是一个编码器（encoder）：它接受一个长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。第二个组件是解码器（decoder）：它将固定形状的编码状态映射到长度可变的序列。这被称为编码器-解码器（encoder-decoder）架构，示意图可见：编码器-解码器架构。 由于“编码器-解码器”架构是形成后续章节中不同序列转换模型的基础，因此本节将把这个架构转换为接口方便后面的代码实现。 在编码器接口中，我们只指定长度可变的序列作为编码器的输入 X。任何继承这个 Encoder 基类的模型将完成代码实现： 在下面的解码器接口中，我们新增一个 init_state 函数，用于将编码器的输出（enc_outputs）转换为编码后的状态。注意，此步骤可能需要额外的输入，例如输入序列的有效长度。为了逐个地生成长度可变的词元序列，解码器在每个时间步都会将输入（例如在前一时间步生成的词元）和编码后的状态映射成当前时间步的输出词元： 总而言之，“编码器-解码器”架构包含了一个编码器和一个解码器，并且还拥有可选的额外的参数。在前向传播中，编码器的输出用于生成编码状态，这个状态又被解码器作为其输入的一部分： 7. 序列到序列学习（seq2seq） 遵循编码器-解码器架构的设计原则，循环神经网络编码器使用长度可变的序列作为输入，将其转换为固定形状的隐状态。换言之，输入序列的信息被编码到循环神经网络编码器的隐状态中。为了连续生成输出序列的词元，独立的循环神经网络解码器是基于输入序列的编码信息和输出序列已经看见的或者生成的词元来预测下一个词元。在机器翻译中使用两个循环神经网络进行序列到序列学习的图示以及理论介绍可见：序列到序列学习（seq2seq）。 在序列到序列学习中，特定的 &lt;eos&gt; 表示序列结束词元。一旦输出序列生成此词元，模型就会停止预测。在循环神经网络解码器的初始化时间步，有两个特定的设计决定：首先，特定的 &lt;bos&gt; 表示序列开始词元，它是解码器的输入序列的第一个词元。其次，使用循环神经网络编码器最终的隐状态来初始化解码器的隐状态。 从技术上讲，编码器将长度可变的输入序列转换成形状固定的上下文变量，并且将输入序列的信息在该上下文变量中进行编码。 现在，让我们实现循环神经网络编码器。注意，我们使用了嵌入层（embedding layer）来获得输入序列中每个词元的特征向量。嵌入层的权重是一个矩阵，其行数等于输入词表的大小（vocab_size），其列数等于特征向量的维度（embed_size）。另外，本文选择了一个多层门控循环单元来实现编码器。 下面，我们实例化上述编码器的实现：我们使用一个两层门控循环单元编码器，其隐藏单元数为16。给定一小批量的输入序列 X（批量大小为4，时间步为7）。在完成所有时间步后，最后一层的隐状态的输出是一个张量（output 由编码器的循环层返回），其形状为 (时间步数, 批量大小, 隐藏单元数)。 由于这里使用的是门控循环单元，所以在最后一个时间步的多层隐状态的形状是 (隐藏层的数量, 批量大小, 隐藏单元的数量)。如果使用长短期记忆网络，state 中还将包含记忆单元信息。 当实现解码器时，我们直接使用编码器最后一个时间步的隐状态来初始化解码器的隐状态。这就要求使用循环神经网络实现的编码器和解码器具有相同数量的层和隐藏单元。为了进一步包含经过编码的输入序列的信息，上下文变量在所有的时间步与解码器的输入进行拼接（concatenate）。为了预测输出词元的概率分布，在循环神经网络解码器的最后一层使用全连接层来变换隐状态。 下面，我们用与前面提到的编码器中相同的超参数来实例化解码器。如我们所见，解码器的输出形状变为 (批量大小, 时间步数, 词表大小)，其中张量的最后一个维度存储预测的词元分布。 在每个时间步，解码器预测了输出词元的概率分布。类似于语言模型，可以使用 Softmax 来获得分布，并通过计算交叉熵损失函数来进行优化。回想一下我们将特定的填充词元添加到序列的末尾，因此不同长度的序列可以以相同形状的小批量加载。但是，我们应该将填充词元的预测排除在损失函数的计算之外。 为此，我们可以使用下面的 sequence_mask 函数通过零值化屏蔽不相关的项，以便后面任何不相关预测的计算都是与零的乘积，结果都等于零。例如，如果两个序列的有效长度（不包括填充词元）分别为1和2，则第一个序列的第一项和第二个序列的前两项之后的剩余项将被清除为零： 现在，我们可以通过扩展 Softmax 交叉熵损失函数来遮蔽不相关的预测。最初，所有预测词元的掩码都设置为1。一旦给定了有效长度，与填充词元对应的掩码将被设置为0。最后，将所有词元的损失乘以掩码，以过滤掉损失中填充词元产生的不相关预测。 我们可以创建三个相同的序列来进行代码健全性检查，然后分别指定这些序列的有效长度为4、2和0。结果就是，第一个序列的损失应为第二个序列的两倍，而第三个序列的损失应为零。 在下面的循环训练过程中，特定的序列开始词元（&lt;bos&gt;）和原始的输出序列（不包括序列结束词元 &lt;eos&gt;）拼接在一起作为解码器的输入。这被称为强制教学（teacher forcing），因为原始的输出序列（词元的标签）被送入解码器。或者，将来自上一个时间步的预测得到的词元作为解码器的当前输入。 为了采用一个接着一个词元的方式预测输出序列，每个解码器当前时间步的输入都将来自于前一时间步的预测词元。与训练类似，序列开始词元（&lt;bos&gt;）在初始时间步被输入到解码器中，当输出序列的预测遇到序列结束词元（&lt;eos&gt;）时，预测就结束了。 我们可以通过与真实的标签序列进行比较来评估预测序列。虽然BLEU（bilingual evaluation understudy）最先是用于评估机器翻译的结果，但现在它已经被广泛用于测量许多应用的输出序列的质量。BLEU的详细介绍可见：序列到序列学习（seq2seq）。 8. 束搜索 束搜索为预测输出序列的一个算法，详细介绍可见：束搜索。"},{"title":"动手学深度学习笔记(李沐)-循环神经网络","date":"2023-04-05T06:04:00.000Z","url":"/posts/11559.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 李沐动手学深度学习（PyTorch）课程学习笔记第八章：循环神经网络。 1. 序列模型 由于涉及较多数学公式，序列模型的讲解可以转至：序列模型。 首先，我们生成一些数据：使用正弦函数和一些可加性噪声来生成序列数据，时间步为 1, 2, ..., 1000： 接下来，我们将这个序列转换为模型的特征-标签（feature-label）对。基于嵌入维度 𝜏，我们将数据映射为数据对 𝑦_𝑡 = 𝑥_𝑡 和 𝐱_𝑡 = [𝑥_&#123;𝑡 - 𝜏&#125;, ...,𝑥_&#123;𝑡 - 1&#125;]，这比我们提供的数据样本少了 𝜏 个，因为我们没有足够的历史记录来描述前 𝜏 个数据样本。一个简单的解决办法是：如果拥有足够长的序列就丢弃这几项；另一个方法是用零填充序列。在这里，我们仅使用前600个特征-标签对进行训练： 在这里，我们使用一个相当简单的架构训练模型：一个拥有两个全连接层的多层感知机，ReLU 激活函数和平方损失： 现在准备训练模型，实现下面的训练代码的方式与前面几章中的循环训练基本相同。因此，我们不会深入探讨太多细节： 由于训练损失很小，因此我们期望模型能有很好的工作效果。让我们看看这在实践中意味着什么。首先是检查模型预测下一个时间步的能力，也就是单步预测（one-step-ahead prediction）： 正如我们所料，单步预测效果不错。即使这些预测的时间步超过了604（n_train + tau），其结果看起来仍然是可信的。然而有一个小问题：如果数据观察序列的时间步只到604，我们需要一步一步地向前迈进，换句话说，我们必须使用我们自己的预测（而不是原始数据）来进行多步预测。让我们看看效果如何： 如上面的例子所示，绿线的预测显然并不理想。经过几个预测步骤之后，预测的结果很快就会衰减到一个常数。为什么这个算法效果这么差呢？事实是由于误差的累积：假设在步骤1之后，我们积累了一些误差，于是步骤2的输入被扰动了，后面的预测误差依此类推。因此误差可能会相当快地偏离真实的观测结果。例如，未来24小时的天气预报往往相当准确，但超过这一点，精度就会迅速下降。我们将在本章及后续章节中讨论如何改进这一点。 基于 k = 1, 4, 16, 64，通过对整个序列预测的计算，让我们更仔细地看一下 k 步预测的困难： 2. 文本预处理 对于序列数据处理问题，我们在上一节中评估了所需的统计工具和预测时面临的挑战。这样的数据存在许多种形式，文本是最常见例子之一。例如，一篇文章可以被简单地看作一串单词序列，甚至是一串字符序列。本节中，我们将解析文本的常见预处理步骤。这些步骤通常包括： 将文本作为字符串加载到内存中。 将字符串拆分为词元（如单词和字符）。 建立一个词表，将拆分的词元映射到数字索引。 将文本转换为数字索引序列，方便模型操作。 首先，我们从 H.G.Well 的时光机器中加载文本。这是一个相当小的语料库，只有30000多个单词，但足够我们小试牛刀，而现实中的文档集合可能会包含数十亿个单词。下面的函数将数据集读取到由多条文本行组成的列表中，其中每条文本行都是一个字符串。为简单起见，我们在这里忽略了标点符号和字母大写： 下面的 tokenize 函数将文本行列表（lines）作为输入，列表中的每个元素是一个文本序列（如一条文本行）。每个文本序列又被拆分成一个词元列表，词元（token）是文本的基本单位。最后，返回一个由词元列表组成的列表，其中的每个词元都是一个字符串（string）： 词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。现在，让我们构建一个字典，通常也叫做词表（vocabulary），用来将字符串类型的词元映射到从0开始的数字索引中。我们先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计，得到的统计结果称之为语料（corpus）。然后根据每个唯一词元的出现频率，为其分配一个数字索引。很少出现的词元通常被移除，这可以降低复杂性。另外，语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元 &lt;unk&gt;。我们可以选择增加一个列表，用于保存那些被保留的词元，例如：填充词元（&lt;pad&gt;）、序列开始词元（&lt;bos&gt;）、序列结束词元（&lt;eos&gt;）： 我们首先使用时光机器数据集作为语料库来构建词表，然后打印前几个高频词元及其索引： 现在，我们可以将每一条文本行转换成一个数字索引列表： 在使用上述函数时，我们将所有功能打包到 load_corpus_time_machine 函数中，该函数返回 corpus（词元索引列表）和 vocab（时光机器语料库的词表）。我们在这里所做的改变是： 为了简化后面章节中的训练，我们使用字符（而不是单词）实现文本词元化； 时光机器数据集中的每个文本行不一定是一个句子或一个段落，还可能是一个单词，因此返回的 corpus 仅处理为单个列表，而不是使用多词元列表构成的一个列表。 3. 语言模型和数据集 由于涉及较多数学公式，语言模型的讲解可以转至：语言模型和数据集。 根据上一节中介绍的时光机器数据集构建词表，并打印前10个最常用的（频率最高的）单词： 正如我们所看到的，最流行的词看起来很无聊，这些词通常被称为停用词（stop words），因此可以被过滤掉。尽管如此，它们本身仍然是有意义的，我们仍然会在模型中使用它们。此外，还有个明显的问题是词频衰减的速度相当地快。例如，最常用单词的词频对比，第10个还不到第1个的1/5。为了更好地理解，我们可以画出的词频图： 通过词频图我们可以发现：词频以一种明确的方式迅速衰减。将前几个单词作为例外消除后，剩余的所有单词大致遵循双对数坐标图上的一条直线。这意味着单词的频率满足齐普夫定律（Zipf’s law）。这告诉我们想要通过计数统计和平滑来建模单词是不可行的，因为这样建模的结果会大大高估尾部单词的频率，也就是所谓的不常用单词。那么其他的词元组合，比如二元语法、三元语法等等，又会如何呢？我们来看看二元语法的频率是否与一元语法的频率表现出相同的行为方式： 这里值得注意：在十个最频繁的词对中，有九个是由两个停用词组成的，只有一个与 the time 有关。我们再进一步看看三元语法的频率是否表现出相同的行为方式： 最后，我们直观地对比三种模型中的词元频率：一元语法、二元语法和三元语法： 由于序列数据本质上是连续的，因此我们在处理数据时需要解决这个问题。在第一节中我们以一种相当特别的方式做到了这一点：当序列变得太长而不能被模型一次性全部处理时，我们可能希望拆分这样的序列方便模型读取。 在介绍该模型之前，我们看一下总体策略。假设我们将使用神经网络来训练语言模型，模型中的网络一次处理具有预定义长度（例如 𝑛 个时间步）的一个小批量序列。现在的问题是如何随机生成一个小批量数据的特征和标签以供读取。 首先，由于文本序列可以是任意长的，例如整本《时光机器》（The Time Machine），于是任意长的序列可以被我们划分为具有相同时间步数的子序列。当训练我们的神经网络时，这样的小批量子序列将被输入到模型中。假设网络一次只处理具有 𝑛 个时间步的子序列，那么可以从指定的起始位置开始截取连续的长度为 𝑛 的子序列，因为我们可以选择任意偏移量来指示初始位置，所以我们有相当大的自由度。 如果我们只选择一个偏移量，那么用于训练网络的、所有可能的子序列的覆盖范围将是有限的。因此，我们可以从随机偏移量开始划分序列，以同时获得覆盖性（coverage）和随机性（randomness）。下面，我们将描述如何实现随机采样（random sampling）和顺序分区（sequential partitioning）策略。 在随机采样中，每个样本都是在原始的长序列上任意捕获的子序列。在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元，因此标签是移位了一个词元的原始序列。 下面的代码每次可以从数据中随机生成一个小批量。在这里，参数 batch_size 指定了每个小批量中子序列样本的数目，参数 num_steps 是每个子序列中预定义的时间步数： 下面我们生成一个从0到34的序列。假设批量大小为2，时间步数为5，这意味着可以生成6个特征-标签子序列对。如果设置小批量大小为2，我们只能得到3个小批量： 在迭代过程中，除了对原始序列可以随机抽样外，我们还可以保证两个相邻的小批量中的子序列在原始序列上也是相邻的。这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为顺序分区： 基于相同的设置，通过顺序分区读取每个小批量的子序列的特征 X 和标签 Y。通过将它们打印出来可以发现：迭代期间来自两个相邻的小批量中的子序列在原始序列中确实是相邻的： 现在，我们将上面的两个采样函数包装到一个类中，以便稍后可以将其用作数据迭代器： 最后，我们定义了一个函数 load_data_time_machine，它同时返回数据迭代器和词表，因此可以与其他带有 load_data 前缀的函数（如 d2l.load_data_fashion_mnist）类似地使用： 4. 循环神经网络 由于涉及较多数学公式，循环神经网络的理论部分可以转至：循环神经网络。 4.1 循环神经网络的从零开始实现 本节将从头开始基于循环神经网络实现字符级语言模型。这样的模型将在 H.G.Wells 的时光机器数据集上训练。和前面上一节中介绍过的一样，我们先读取数据集： 回想一下，在 train_iter 中，每个词元都表示为一个数字索引，将这些索引直接输入神经网络可能会使学习变得困难。我们通常将每个词元表示为更具表现力的特征向量。最简单的表示称为独热编码（one-hot encoding）。 简言之，将每个索引映射为相互不同的单位向量：假设词表中不同词元的数目为N（即 len(vocab)），词元索引的范围为0~N-1。如果词元的索引是整数 i，那么我们将创建一个长度为N的全0向量，并将第 i 处的元素设置为1。此向量是原始词元的一个独热向量。索引为0和2的独热向量如下所示： 我们每次采样的小批量数据形状是二维张量：(批量大小, 时间步数)。one_hot 函数将这样一个小批量数据转换成三维张量，张量的最后一个维度等于词表大小（len(vocab)）。我们经常转换输入的维度，以便获得形状为 (时间步数, 批量大小, 词表大小) 的输出。这将使我们能够更方便地通过最外层的维度，一步一步地更新小批量数据的隐状态： 接下来，我们初始化循环神经网络模型的模型参数。隐藏单元数 num_hiddens 是一个可调的超参数。当训练语言模型时，输入和输出来自相同的词表（输出可以看成多分类问题，即输出表示对每个词元的预测概率）。因此，它们具有相同的维度，即词表的大小： 为了定义循环神经网络模型，我们首先需要一个 init_rnn_state 函数在初始化时返回隐状态。这个函数的返回是一个张量，张量全用0填充，形状为 (批量大小, 隐藏单元数)。在后面的章节中我们将会遇到隐状态包含多个变量的情况，而使用元组可以更容易地处理些： 下面的 rnn 函数定义了如何在一个时间步内计算隐状态和输出。循环神经网络模型通过 inputs 最外层的维度实现循环，以便逐时间步更新小批量数据的隐状态H。此外，这里使用 tanh 函数作为激活函数，当元素在实数上满足均匀分布时，tanh 函数的平均值为0： 定义了所有需要的函数之后，接下来我们创建一个类来包装这些函数，并存储从零开始实现的循环神经网络模型的参数： 让我们检查输出是否具有正确的形状。例如，隐状态的维数是否保持不变： 我们可以看到输出形状是 (时间步数 * 批量大小, 词表大小)，而隐状态形状保持不变，即 (批量大小, 隐藏单元数)。 让我们首先定义预测函数来生成 prefix 之后的新字符，其中的 prefix 是一个用户提供的包含多个字符的字符串。在循环遍历 prefix 中的开始字符时，我们不断地将隐状态传递到下一个时间步，但是不生成任何输出。这被称为预热（warm-up）期，因为在此期间模型会自我更新（例如，更新隐状态），但不会进行预测。预热期结束后，隐状态的值通常比刚开始的初始值更适合预测，从而预测字符并输出它们： 现在我们可以测试 predict 函数。我们将前缀指定为 time traveller，并基于这个前缀生成10个后续字符。鉴于我们还没有训练网络，它会生成荒谬的预测结果： 梯度裁剪的理论可转至：循环神经网络的从零开始实现。 下面我们定义一个函数来裁剪模型的梯度，模型是从零开始实现的模型或由高级 API 构建的模型。我们在此计算了所有模型参数的梯度的范数： 在训练模型之前，让我们定义一个函数在一个迭代周期内训练模型。它与我们训练 Softmax 模型的方式有三个不同之处： 序列数据的不同采样方法（随机采样和顺序分区）将导致隐状态初始化的差异。 我们在更新模型参数之前裁剪梯度。这样的操作的目的是，即使训练过程中某个点上发生了梯度爆炸，也能保证模型不会发散。 我们用困惑度来评价模型。这样的度量确保了不同长度的序列具有可比性。 具体来说，当使用顺序分区时，我们只在每个迭代周期的开始位置初始化隐状态。由于下一个小批量数据中的第 i 个子序列样本与当前第 i 个子序列样本相邻，因此当前小批量数据最后一个样本的隐状态，将用于初始化下一个小批量数据第一个样本的隐状态。这样，存储在隐状态中的序列的历史信息可以在一个迭代周期内流经相邻的子序列。然而，在任何一点隐状态的计算，都依赖于同一迭代周期中前面所有的小批量数据，这使得梯度计算变得复杂。为了降低计算量，在处理任何一个小批量数据之前，我们先分离梯度，使得隐状态的梯度计算总是限制在一个小批量数据的时间步内。 当使用随机抽样时，因为每个样本都是在一个随机位置抽样的，因此需要为每个迭代周期重新初始化隐状态。 循环神经网络模型的训练函数既支持从零开始实现，也可以使用高级 API 来实现。 现在，我们训练循环神经网络模型。因为我们在数据集中只使用了10000个词元，所以模型需要更多的迭代周期来更好地收敛： 4.2 循环神经网络的简洁实现 虽然从零开始实现循环神经网络对了解网络的实现方式具有指导意义，但并不方便。本节将展示如何使用深度学习框架的高级 API 提供的函数更有效地实现相同的语言模型。我们仍然从读取时光机器数据集开始： 高级 API 提供了循环神经网络的实现。我们构造一个具有256个隐藏单元的单隐藏层的循环神经网络层 rnn_layer。事实上，我们还没有讨论多层循环神经网络的意义（这将在深度循环神经网络中介绍）。现在仅需要将多层理解为一层循环神经网络的输出被用作下一层循环神经网络的输入就足够了： 我们使用张量来初始化隐状态，它的形状是 (隐藏层数, 批量大小, 隐藏单元数)： 通过一个隐状态和一个输入，我们就可以用更新后的隐状态计算输出。需要强调的是，rnn_layer 的输出（Y）不涉及输出层的计算：它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入： 我们为一个完整的循环神经网络模型定义了一个 RNNModel 类。注意，rnn_layer 只包含隐藏的循环层，我们还需要创建一个单独的输出层： 在训练模型之前，让我们基于一个具有随机权重的模型进行预测，d2l.predict_ch8 函数与上一节中的 predict 函数相同： 很明显，这种模型根本不能输出好的结果。接下来，我们使用上一节中定义的超参数训练模型： "},{"title":"Python路径操作模块pathlib教程","date":"2023-03-31T02:47:00.000Z","url":"/posts/55.html","tags":[["Python","/tags/Python/"]],"categories":[["Python","/categories/Python/"]],"content":" Python 路径操作新标准：pathlib 模块相较于老式的 os.path 更为简洁易用，本文为该模块的使用教程。 pathlib 库从 Python 3.4 开始，到 Python 3.6 已经比较成熟。如果你的新项目可以直接用 3.6 及以上，建议用 pathlib。相比于老式的 os.path 有几个优势： 老的路径操作函数管理比较混乱，有的是导入 os，有的又是在 os.path 当中，而新的用法统一可以用 pathlib 管理。 老用法在处理不同操作系统 Win、Mac 以及 Linux 之间很吃力。换了操作系统常常要改代码，还经常需要进行一些额外操作。 老用法主要是函数形式，返回的数据类型通常是字符串。但是路径和字符串并不等价，所以在使用 os 操作路径的时候常常还要引入其他类库协助操作。新用法是面向对象，处理起来更灵活方便。 pathlib 简化了很多操作，用起来更轻松。 1. 路径获取 （1）获取当前工作目录 注意：工作目录是在哪个目录下运行你的程序，不是项目目录。 虽然在这里打印出来的很像一个字符串，但实际上得到的是一个 WindowsPath('D:\\Dive into Deep Learning\\src') 对象，如果只想得到字符串表示，不想要 WindowsPath 对象，可以用 str() 转化。 （2）获取用户 Home 目录 （3）获取当前文件路径 （4）获取任意字符串路径 （5）获取绝对路径 （6）获取文件属性 文件属性比如文件大小、创建时间、修改时间等。 2. 路径组成部分 获取路径的组成部分非常方便： .name：文件名，包含后缀名，如果是目录则获取目录名。 .stem：文件名，不包含后缀。 .suffix：后缀，比如 .txt、.png。 .parent：父级目录，相当于 cd ..。 .anchor：锚，目录前面的部分 C:\\ 或者 /。 3. 子路径扫描 （1）扫描某个目录下的所有路径 （2）使用模式匹配（正则表达式）查找目录下的指定文件 （3）检查路径是否符合规则 4. 路径拼接 pathlib 支持用 / 拼接路径，如果用不惯 /，也可以用类似 os.path.join 的方法： 5. 路径测试（判断） 6. 文件操作 （1）创建文件 exist_ok 表示当文件已经存在时，程序的反应。如果为 True，文件存在时，不进行任何操作；如果为 False，则会报 FileExistsError 错误。 （2）创建目录 用 os 创建目录分为两个函数：mkdir() 和 makedirs()。mkdir() 一次只能创建一级目录，makedirs() 可以同时创建多级目录。使用 pathlib 只需要用 path.mkdir() 函数就可以。它提供了 parents 参数，设置为 True 可以创建多级目录，不设置则只能创建一层： （3）删除目录 删除目录非常危险，并且没有提示，一定要谨慎操作。一次只删除一级目录，且当前目录必须为空： （4）删除文件 "},{"title":"动手学深度学习笔记(李沐)-计算机视觉","date":"2023-03-10T02:24:00.000Z","url":"/posts/24840.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 李沐动手学深度学习（PyTorch）课程学习笔记第七章：计算机视觉。 1. 图像增广 图像增广在对训练图像进行一系列的随机变化之后，生成相似但不同的训练样本，从而扩大了训练集的规模。此外，应用图像增广的原因是，随机改变训练样本可以减少模型对某些属性的依赖，从而提高模型的泛化能力。例如，我们可以以不同的方式裁剪图像，使感兴趣的对象出现在不同的位置，减少模型对于对象出现位置的依赖。我们还可以调整亮度、颜色等因素来降低模型对颜色的敏感度。 下面的代码有50%的几率使图像向左或向右翻转： 有50%的几率向上或向下翻转，注意，上下翻转图像不如左右图像翻转那样常用，需要根据数据集的特征考虑是否可以将图像上下翻转： 随机裁剪一个面积为原始面积10%到100%的区域，该区域的宽高比从0.5~2之间随机取值。然后，区域的宽度和高度都被缩放到200像素： 我们可以改变图像颜色的四个方面：亮度、对比度、饱和度和色调。在下面的示例中，我们随机更改图像的亮度，随机值为原始图像的50%(1 - 0.5)到150%(1 + 0.5)之间： 在实践中，我们将结合多种图像增广方法。我们可以通过使用一个 Compose 实例来综合上面定义的不同的图像增广方法，并将它们应用到每个图像： 图像增广可以直接作用在图像数据上，也可以在使用 torchvision.datasets 导入数据集的时候通过 transform 参数指定： 2. 微调 微调（fine-tuning）是迁移学习（transfer learning）中的常见技巧，微调包括以下四个步骤： 在源数据集（例如 ImageNet 数据集）上预训练神经网络模型，即源模型。 创建一个新的神经网络模型，即目标模型。这将复制源模型上的所有模型设计及其参数（输出层除外）。我们假定这些模型参数包含从源数据集中学到的知识，这些知识也将适用于目标数据集。我们还假设源模型的输出层与源数据集的标签密切相关；因此不在目标模型中使用该层。 向目标模型添加输出层，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。 在目标数据集（如椅子数据集）上训练目标模型。输出层将从头开始进行训练，而所有其他层的参数将根据源模型的参数进行微调。 当目标数据集比源数据集小得多时，微调有助于提高模型的泛化能力。 我们将在一个 CIFAR10 数据集上微调 ResNet-18 模型。该模型已在 ImageNet 数据集上进行了预训练： 3. 目标检测和边界框 在图像分类任务中，我们假设图像中只有一个主要物体对象，我们只关注如何识别其类别。然而，很多时候图像里有多个我们感兴趣的目标，我们不仅想知道它们的类别，还想得到它们在图像中的具体位置。在计算机视觉里，我们将这类任务称为目标检测（object detection）或目标识别（object recognition）。 下面加载本节将使用的示例图像。图像左边是一只狗，右边是一只猫。它们是这张图像里的两个主要目标： 在目标检测中，我们通常使用边界框（bounding box）来描述对象的空间位置。边界框是矩形的，由矩形左上角的以及右下角的 x 和 y 坐标决定。另一种常用的边界框表示方法是边界框中心的 (x, y) 轴坐标以及框的宽度和高度。 在这里，我们定义在这两种表示法之间进行转换的函数：box_corner_to_center 从两角表示法转换为中心宽度表示法，而 box_center_to_corner 反之亦然。输入参数 boxes 可以是长度为4的张量，也可以是形状为 (N, 4) 的二维张量，其中 N 是边界框的数量。 我们将根据坐标信息定义图像中狗和猫的边界框。图像中坐标的原点是图像的左上角，向右的方向为 x 轴的正方向，向下的方向为 y 轴的正方向： 我们可以将边界框在图中画出，以检查其是否准确。画之前，我们定义一个辅助函数 bbox_to_rect。它将边界框表示成 matplotlib 的边界框格式，在图像上添加边界框之后，我们可以看到两个物体的主要轮廓基本上在两个框内： 4. 目标检测数据集 目标检测领域没有像 MNIST 和 Fashion-MNIST 那样的小数据集。为了快速测试目标检测模型，我们收集并标记了一个小型数据集。首先，我们拍摄了一组香蕉的照片，并生成了1000张不同角度和大小的香蕉图像。然后，我们在一些背景图片的随机位置上放一张香蕉的图像。最后，我们在图片上为这些香蕉标记了边界框。 包含所有图像和 CSV 标签文件的香蕉检测数据集可以直接从互联网下载，通过 read_data_bananas 函数，我们读取香蕉检测数据集的图像和标签。该数据集的 CSV 文件内含目标类别标签和位于左上角和右下角的真实边界框坐标： 以下 BananasDataset 类别将允许我们创建一个自定义 Dataset 实例来加载香蕉检测数据集： 最后，我们定义 load_data_bananas 函数，来为训练集和测试集返回两个数据加载器实例。对于测试集，无须按随机顺序读取它： 让我们读取一个小批量，并打印其中的图像和标签的形状。图像的小批量的形状为：(批量大小, 通道数, 高度, 宽度)，它与我们之前图像分类任务中的相同。标签的小批量的形状为：(批量大小, M, 5)，其中 M 是数据集的任何图像中边界框可能出现的最大数量。 小批量计算虽然高效，但它要求每张图像含有相同数量的边界框，以便放在同一个批量中。通常来说，图像可能拥有不同数量个边界框；因此，在达到 M 之前，边界框少于 M 的图像将被非法边界框填充。这样，每个边界框的标签将被长度为5的数组表示。数组中的第一个元素是边界框中对象的类别，其中-1表示用于填充的非法边界框。数组的其余四个元素是边界框左上角和右下角的 (x, y) 坐标值（值域在0~1之间）。对于香蕉数据集而言，由于每张图像上只有一个边界框，因此 M = 1。 接下来让我们展示10幅带有真实边界框的图像。我们可以看到在所有这些图像中香蕉的旋转角度、大小和位置都有所不同。当然，这只是一个简单的人工数据集，实践中真实世界的数据集通常要复杂得多： 5. 锚框 由于本节难度较大，因此详细分析见：D2L-计算机视觉-锚框。 6. 多尺度目标检测 在上一节中，我们以输入图像的每个像素为中心，生成了多个锚框。基本而言，这些锚框代表了图像不同区域的样本。然而，如果为每个像素都生成的锚框，我们最终可能会得到太多需要计算的锚框。想象一个561*728的输入图像，如果以每个像素为中心生成五个形状不同的锚框，就需要在图像上标记和预测超过200万个锚框（561*728*5）。 6.1 多尺度锚框 减少图像上的锚框数量并不困难。比如，我们可以在输入图像中均匀采样一小部分像素，并以它们为中心生成锚框。此外，在不同尺度下，我们可以生成不同数量和不同大小的锚框。直观地说，比起较大的目标，较小的目标在图像上出现的可能性更多样。例如，1*1、1*2和2*2的目标可以分别以4、2和1种可能的方式出现在2*2的图像上。因此，当使用较小的锚框检测较小的物体时，我们可以采样更多的区域，而对于较大的物体，我们可以采样较少的区域。 为了演示如何在多个尺度下生成锚框，让我们先读取一张图像。它的高度和宽度分别为561和728像素： display_anchors 函数定义如下。我们在特征图（fmap）上生成锚框（anchors），每个单位（像素）作为锚框的中心。由于锚框中的 (x, y) 轴坐标值（anchors）已经被除以特征图（fmap）的宽度和高度，因此这些值介于0和1之间，表示特征图中锚框的相对位置。 由于锚框（anchors）的中心分布于特征图（fmap）上的所有单位，因此这些中心必须根据其相对空间位置在任何输入图像上均匀分布。更具体地说，给定特征图的宽度和高度 fmap_w 和 fmap_h，以下函数将均匀地对任何输入图像中 fmap_h 行和 fmap_w 列中的像素进行采样。以这些均匀采样的像素为中心，将会生成大小为 s（假设列表 s 的长度为1）且宽高比（ratios）不同的锚框： 首先，让我们考虑探测小目标。为了在显示时更容易分辨，在这里具有不同中心的锚框不会重叠：锚框的尺度设置为0.15，特征图的高度和宽度设置为4。我们可以看到，图像上4行和4列的锚框的中心是均匀分布的： 然后，我们将特征图的高度和宽度减小一半，然后使用较大的锚框来检测较大的目标。当尺度设置为0.4时，一些锚框将彼此重叠： 最后，我们进一步将特征图的高度和宽度减小一半，然后将锚框的尺度增加到0.8。此时，锚框的中心即是图像的中心： 6.2 多尺度检测 既然我们已经生成了多尺度的锚框，我们就将使用它们来检测不同尺度下各种大小的目标。下面，我们介绍一种基于 CNN 的多尺度目标检测方法，将在第8节（SSD）中实现。 在某种规模上，假设我们有 c 张形状为 h * w 的特征图。使用上一小节中的方法，我们生成了 hw 组锚框，其中每组都有 a 个中心相同的锚框。例如，在上一小节实验的第一个尺度上，给定10个（通道数量）4 * 4 的特征图，我们生成了16组锚框，每组包含3个中心相同的锚框。接下来，每个锚框都根据真实值边界框来标记了类和偏移量。在当前尺度下，目标检测模型需要预测输入图像上 hw 组锚框类别和偏移量，其中不同组锚框具有不同的中心。 假设此处的 c 张特征图是 CNN 基于输入图像的正向传播算法获得的中间输出。既然每张特征图上都有 hw 个不同的空间位置，那么相同空间位置可以看作含有 c 个单元。根据感受野的定义，特征图在相同空间位置的 c 个单元在输入图像上的感受野相同：它们表征了同一感受野内的输入图像信息。因此，我们可以将特征图在同一空间位置的 c 个单元变换为使用此空间位置生成的 a 个锚框类别和偏移量。本质上，我们用输入图像在某个感受野区域内的信息，来预测输入图像上与该区域位置相近的锚框类别和偏移量。 当不同层的特征图在输入图像上分别拥有不同大小的感受野时，它们可以用于检测不同大小的目标。例如，我们可以设计一个神经网络，其中靠近输出层的特征图单元具有更宽的感受野，这样它们就可以从输入图像中检测到较大的目标。 简言之，我们可以利用深层神经网络在多个层次上对图像进行分层表示，从而实现多尺度目标检测。在第8节我们将通过一个具体的例子来说明它是如何工作的。 7. 区域卷积神经网络（R-CNN）系列 7.1 R-CNN R-CNN 首先从输入图像中选取若干（例如2000个）提议区域（如锚框也是一种选取方法），并标注它们的类别和边界框（如偏移量）。然后，用卷积神经网络对每个提议区域进行前向传播以抽取其特征。接下来，我们用每个提议区域的特征来预测类别和边界框。具体来说，R-CNN 包括以下四个步骤： 对输入图像使用选择性搜索来选取多个高质量的提议区域。这些提议区域通常是在多个尺度下选取的，并具有不同的形状和大小。每个提议区域都将被标注类别和真实边界框； 选择一个预训练的卷积神经网络，并将其在输出层之前截断。将每个提议区域变形为网络需要的输入尺寸，并通过前向传播输出抽取的提议区域特征； 将每个提议区域的特征连同其标注的类别作为一个样本。训练多个支持向量机对目标分类，其中每个支持向量机用来判断样本是否属于某一个类别； 将每个提议区域的特征连同其标注的边界框作为一个样本，训练线性回归模型来预测真实边界框。 尽管 R-CNN 模型通过预训练的卷积神经网络有效地抽取了图像特征，但它的速度很慢。想象一下，我们可能从一张图像中选出上千个提议区域，这需要上千次的卷积神经网络的前向传播来执行目标检测。这种庞大的计算量使得 R-CNN 在现实世界中难以被广泛应用。 7.2 Fast R-CNN R-CNN 的主要性能瓶颈在于，对每个提议区域，卷积神经网络的前向传播是独立的，而没有共享计算。由于这些区域通常有重叠，独立的特征抽取会导致重复的计算。Fast R-CNN 对 R-CNN 的主要改进之一，是仅在整张图像上执行卷积神经网络的前向传播。Fast R-CNN 的主要计算如下： 与 R-CNN 相比，Fast R-CNN 用来提取特征的入卷积神经网络的输入是整个图像，而不是各个提议区域。此外，这个网络通常会参与训练。设输入为一张图像，将卷积神经网络的输出的形状记为 1 * c * h1 * w1； 假设选择性搜索生成了 n 个提议区域。这些形状各异的提议区域在卷积神经网络的输出上分别标出了形状各异的兴趣区域。然后，这些感兴趣的区域需要进一步抽取出形状相同的特征（比如指定高度 h2 和宽度 w2），以便于连结后输出。为了实现这一目标，Fast R-CNN 引入了兴趣区域汇聚层（RoI pooling）：将卷积神经网络的输出和提议区域作为输入，输出连结后的各个提议区域抽取的特征，形状为 n * c * h2 * w2； 通过全连接层将输出形状变换为 n * d，其中超参数 d 取决于模型设计； 预测 n 个提议区域中每个区域的类别和边界框。更具体地说，在预测类别和边界框时，将全连接层的输出分别转换为形状为 n * q（q 是类别的数量）的输出和形状为 n * 4 的输出。其中预测类别时使用 Softmax 回归。 下面，我们演示了兴趣区域汇聚层的计算方法。假设卷积神经网络抽取的特征 X 的高度和宽度都是4，且只有单通道： 让我们进一步假设输入图像的高度和宽度都是40像素，且选择性搜索在此图像上生成了两个提议区域。每个区域由5个元素表示：区域目标类别、左上角和右下角的 (x, y) 坐标： 由于 X 的高和宽是输入图像高和宽的1/10，因此，两个提议区域的坐标先按 spatial_scale 乘以0.1。然后，在 X 上分别标出这两个兴趣区域 X[:, :, 0:3, 0:3] 和 X[:, :, 1:4, 0:4]。最后，在 2 * 2 的兴趣区域汇聚层中，每个兴趣区域被划分为子窗口网格，并进一步抽取相同形状 2 * 2 的特征： 7.3 Faster R-CNN 为了较精确地检测目标结果，Fast R-CNN 模型通常需要在选择性搜索中生成大量的提议区域。Faster R-CNN 提出将选择性搜索替换为区域提议网络（region proposal network），从而减少提议区域的生成数量，并保证目标检测的精度。具体来说，区域提议网络的计算步骤如下： 使用填充为1的 3 * 3 的卷积层变换卷积神经网络的输出，并将输出通道数记为 c。这样，卷积神经网络为图像抽取的特征图中的每个单元均得到一个长度为 c 的新特征； 以特征图的每个像素为中心，生成多个不同大小和宽高比的锚框并标注它们； 使用锚框中心单元长度为 c 的特征，分别预测该锚框的二元类别（含目标还是背景）和边界框； 使用非极大值抑制，从预测类别为目标的预测边界框中移除相似的结果。最终输出的预测边界框即是兴趣区域汇聚层所需的提议区域。 值得一提的是，区域提议网络作为 Faster R-CNN 模型的一部分，是和整个模型一起训练得到的。换句话说，Faster R-CNN 的目标函数不仅包括目标检测中的类别和边界框预测，还包括区域提议网络中锚框的二元类别和边界框预测。作为端到端训练的结果，区域提议网络能够学习到如何生成高质量的提议区域，从而在减少了从数据中学习的提议区域的数量的情况下，仍保持目标检测的精度。 7.4 Mask R-CNN 如果在训练集中还标注了每个目标在图像上的像素级位置，那么 Mask R-CNN 能够有效地利用这些详尽的标注信息进一步提升目标检测的精度。 Mask R-CNN 是基于 Faster R-CNN 修改而来的。具体来说，Mask R-CNN 将兴趣区域汇聚层替换为了兴趣区域对齐层（RoI Align），使用双线性插值（bilinear interpolation）来保留特征图上的空间信息，从而更适于像素级预测。兴趣区域对齐层的输出包含了所有与兴趣区域的形状相同的特征图。它们不仅被用于预测每个兴趣区域的类别和边界框，还通过额外的全卷积网络预测目标的像素级位置。本章的后续章节将更详细地介绍如何使用全卷积网络预测图像中像素级的语义。 8. 单发多框检测（SSD） SSD 模型主要由基础网络组成，其后是几个多尺度特征块。基本网络用于从输入图像中提取特征，因此它可以使用深度卷积神经网络。单发多框检测论文中选用了在分类层之前截断的 VGG，现在也常用 ResNet 替代。我们可以设计基础网络，使它输出的高和宽较大。这样一来，基于该特征图生成的锚框数量较多，可以用来检测尺寸较小的目标。接下来的每个多尺度特征块将上一层提供的特征图的高和宽缩小（如减半），并使特征图中每个单元在输入图像上的感受野变得更广阔。 回想一下在第6节中，通过深度神经网络分层表示图像的多尺度目标检测的设计。由于接近顶部的多尺度特征图较小，但具有较大的感受野，它们适合检测较少但较大的物体。简而言之，通过多尺度特征块，单发多框检测生成不同大小的锚框，并通过预测边界框的类别和偏移量来检测大小不同的目标，因此这是一个多尺度目标检测模型。 8.1 类别预测层与边界框预测层 设目标类别的数量为 q。这样一来，锚框有 q + 1 个类别，其中第0类是背景。在某个尺度下，设特征图的高和宽分别为 h 和 w。如果以其中每个单元为中心生成 a 个锚框，那么我们需要对 hwa 个锚框进行分类。如果使用全连接层作为输出，很容易导致模型参数过多。回忆 NiN 一节介绍的使用卷积层的通道来输出类别预测的方法，单发多框检测采用同样的方法来降低模型复杂度。 具体来说，类别预测层使用一个保持输入高和宽的卷积层。这样一来，输出和输入在特征图宽和高上的空间坐标一一对应。考虑输出和输入同一空间坐标 (x, y)：输出特征图上 (x, y) 坐标的通道里包含了以输入特征图 (x, y) 坐标为中心生成的所有锚框的类别预测。因此输出通道数为 a * (q + 1)。 类别预测层的定义如下： 边界框预测层的设计与类别预测层的设计类似。唯一不同的是，这里需要为每个锚框预测4个偏移量，而不是 q + 1 个类别： 8.2 连结多尺度的预测 单发多框检测使用多尺度特征图来生成锚框并预测其类别和偏移量。在不同的尺度下，特征图的形状或以同一单元为中心的锚框的数量可能会有所不同。因此，不同尺度下预测输出的形状可能会有所不同。 在以下示例中，我们为同一个小批量构建两个不同比例（Y1 和 Y2）的特征图，其中 Y2 的高度和宽度是 Y1 的一半。以类别预测为例，假设 Y1 和 Y2 的每个单元分别生成了5个和3个锚框。进一步假设目标类别的数量为10，对于特征图 Y1 和 Y2，类别预测输出中的通道数分别为 5 * (10 + 1) = 55 和 3 * (10 + 1) = 33，其中任一输出的形状是 (批量大小, 通道数, 高度, 宽度)： 除了批量大小这一维度外，其他三个维度都具有不同的尺寸。为了将这两个预测输出链接起来以提高计算效率，我们将把这些张量转换为更一致的格式。 通道维包含中心相同的锚框的预测结果。我们首先将通道维移到最后一维。因为不同尺度下批量大小仍保持不变，我们可以将预测结果转成二维的 (批量大小, 高 * 宽 * 通道数) 的格式，以方便之后在维度1上的连结。这样一来，尽管 Y1 和 Y2 在通道数、高度和宽度方面具有不同的大小，我们仍然可以在同一个小批量的两个不同尺度上连接这两个预测输出： 8.3 高和宽减半块 高和宽减半块将输入特征图的高度和宽度减半，会扩大每个单元在其输出特征图中的感受野，该模块此前已在 VGG 中使用过： 8.4 基本网络块 基本网络块用于从输入图像中抽取特征。为了计算简洁，我们构造了一个小的基础网络，该网络串联3个高和宽减半块，并逐步将通道数翻倍： 8.5 完整的模型 完整的单发多框检测模型由五个模块组成，每个块生成的特征图既用于生成锚框，又用于预测这些锚框的类别和偏移量。在这五个模块中，第一个是基本网络块，第二个到第四个是高和宽减半块，最后一个模块使用全局最大池化层将高度和宽度都降到1。从技术上讲，第二到第五个区块都是 SSD 中的多尺度特征块： 现在我们为每个块定义前向传播。与图像分类任务不同，此处的输出包括：CNN 特征图 Y、在当前尺度下根据 Y 生成的锚框、预测的这些锚框的类别和偏移量（基于 Y）： 超参数的设置过程可以看：单发多框检测（SSD）。 现在，我们就可以按如下方式定义完整的模型 TinySSD 了： 8.6 训练模型 首先读取数据集和设置超参数： 然后定义损失函数和评价函数，目标检测有两种类型的损失。第一种有关锚框类别的损失：我们可以简单地复用之前图像分类问题里一直使用的交叉熵损失函数来计算；第二种有关正类锚框偏移量的损失：预测偏移量是一个回归问题。但是，对于这个回归问题，我们在这里不使用平方损失，而是使用 L1 范数损失，即预测值和真实值之差的绝对值。掩码变量 bbox_masks 令负类锚框和填充锚框不参与损失的计算。最后，我们将锚框类别和偏移量的损失相加，以获得模型的最终损失函数： 我们可以沿用准确率评价分类结果。由于偏移量使用了 L1 范数损失，我们使用平均绝对误差来（MAE）评价边界框的预测结果。这些预测结果是从生成的锚框及其预测偏移量中获得的： 最后是训练模型，在训练模型时，我们需要在模型的前向传播过程中生成多尺度锚框 anchors，并预测其类别 cls_preds 和偏移量 bbox_preds。然后，我们根据标签信息 label 为生成的锚框标记类别 cls_labels 和偏移量 bbox_labels。最后，我们根据类别和偏移量的预测和标注值计算损失函数： 8.7 预测目标 在预测阶段，我们希望能把图像里面所有我们感兴趣的目标检测出来。在下面，我们读取并调整测试图像的大小，然后将其转成卷积层需要的四维格式。使用 multibox_detection 函数，我们可以根据锚框及其预测偏移量得到预测边界框，然后通过非极大值抑制来移除相似的预测边界框。最后，我们筛选所有置信度不低于0.9的边界框，做为最终输出： 9. 语义分割和数据集 在前几节中讨论的目标检测问题中，我们一直使用方形边界框来标注和预测图像中的目标。本节将探讨语义分割（semantic segmentation）问题，它重点关注于如何将图像分割成属于不同语义类别的区域。与目标检测不同，语义分割可以识别并理解图像中每一个像素的内容：其语义区域的标注和预测是像素级的。与目标检测相比，语义分割标注的像素级的边框显然更加精细。 9.1 图像分割和实例分割 计算机视觉领域还有2个与语义分割相似的重要问题，即图像分割（image segmentation）和实例分割（instance segmentation）。我们在这里将它们同语义分割简单区分一下： 图像分割将图像划分为若干组成区域，这类问题的方法通常利用图像中像素之间的相关性。它在训练时不需要有关图像像素的标签信息，在预测时也无法保证分割出的区域具有我们希望得到的语义。以图像 catdog.jpg 作为输入，图像分割可能会将狗分为两个区域：一个覆盖以黑色为主的嘴和眼睛，另一个覆盖以黄色为主的其余部分身体。 实例分割也叫同时检测并分割（simultaneous detection and segmentation），它研究如何识别图像中各个目标实例的像素级区域。与语义分割不同，实例分割不仅需要区分语义，还要区分不同的目标实例。例如，如果图像中有两条狗，则实例分割需要区分像素属于的两条狗中的哪一条。 9.2 Pascal VOC2012 语义分割数据集 最重要的语义分割数据集之一是 Pascal VOC2012，下面我们深入了解一下这个数据集： 进入路径 ../data/VOCdevkit/VOC2012 之后，我们可以看到数据集的不同组件。ImageSets/Segmentation 路径包含用于训练和测试样本的文本文件，而 JPEGImages 和 SegmentationClass 路径分别存储着每个示例的输入图像和标签。此处的标签也采用图像格式，其尺寸和它所标注的输入图像的尺寸相同。此外，标签中颜色相同的像素属于同一个语义类别。下面将 read_voc_images 函数定义为将所有输入的图像和标签读入内存： 下面我们绘制前5个输入图像及其标签。在标签图像中，白色和黑色分别表示边框和背景，而其他颜色则对应不同的类别： 接下来，我们列举 RGB 颜色值和类名： 通过上面定义的两个常量，我们可以方便地查找标签中每个像素的类索引。我们定义了 voc_colormap2label 函数来构建从上述 RGB 颜色值到类别索引的映射，而 voc_label_indices 函数将 RGB 值映射到在 Pascal VOC2012 数据集中的类别索引： 例如，在第一张样本图像中，飞机头部区域的类别索引为1，而背景索引为0： 之前的实验我们通过再缩放图像使其符合模型的输入形状。然而在语义分割中，这样做需要将预测的像素类别重新映射回原始尺寸的输入图像。这样的映射可能不够精确，尤其在不同语义的分割区域。为了避免这个问题，我们将图像裁剪为固定尺寸，而不是再缩放。具体来说，我们使用图像增广中的随机裁剪，裁剪输入图像和标签的相同区域： 我们通过继承高级 API 提供的 Dataset 类，自定义了一个语义分割数据集类 VOCSegDataset。通过实现 __getitem__ 函数，我们可以任意访问数据集中索引为 idx 的输入图像及其每个像素的类别索引。由于数据集中有些图像的尺寸可能小于随机裁剪所指定的输出尺寸，这些样本可以通过自定义的 filter 函数移除掉。此外，我们还定义了 normalize_image 函数，从而对输入图像的 RGB 三个通道的值分别做标准化： 最后，我们定义以下 load_data_voc 函数来下载并读取 Pascal VOC2012 语义分割数据集。它返回训练集和测试集的数据迭代器： 10. 转置卷积"},{"title":"动手学深度学习笔记(李沐)-现代卷积神经网络","date":"2023-03-03T01:57:00.000Z","url":"/posts/21165.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 李沐动手学深度学习（PyTorch）课程学习笔记第六章：现代卷积神经网络。 1. 深度卷积神经网络（AlexNet） AlexNet 和 LeNet 的设计理念非常相似，但也存在显著差异： AlexNet 比相对较小的 LeNet5 要深得多。AlexNet 由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。 AlexNet 使用 ReLU 而不是 Sigmoid 作为其激活函数。 此外，AlexNet 将 Sigmoid 激活函数改为更简单的 ReLU 激活函数。一方面，ReLU 激活函数的计算更简单，它不需要如 Sigmoid 激活函数那般复杂的求幂运算。另一方面，当使用不同的参数初始化方法时，ReLU 激活函数使训练模型更加容易。当 Sigmoid 激活函数的输出非常接近于0或1时，这些区域的梯度几乎为0，因此反向传播无法继续更新一些模型参数。相反，ReLU 激活函数在正区间的梯度总是1。因此，如果模型参数没有正确初始化，Sigmoid 函数可能在正区间内得到几乎为0的梯度，从而使模型无法得到有效的训练。 尽管原文中 AlexNet 是在 ImageNet 上进行训练的，但本文在这里使用的是 Fashion-MNIST 数据集。因为即使在现代 GPU 上，训练 ImageNet 模型，同时使其收敛可能需要数小时或数天的时间。将 AlexNet 直接应用于 Fashion-MNIST 的一个问题是 Fashion-MNIST 图像的分辨率（28×28像素）低于 ImageNet 图像。为了解决这个问题，我们将它们增加到224×224像素（通常来讲这不是一个明智的做法，但在这里这样做是为了有效使用 AlexNet 架构）。 2. 使用块的网络（VGG） 虽然 AlexNet 证明深层神经网络卓有成效，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络。 经典卷积神经网络的基本组成部分是下面的这个序列： 带填充以保持分辨率的卷积层。 非线性激活函数，如 ReLU。 汇聚层，如最大汇聚层。 而一个 VGG 块与之类似，由一系列卷积层组成，后面再加上用于空间下采样的最大汇聚层。 VGG 使用可重复使用的卷积块来构建深度卷积神经网络，不同的卷积块个数和超参数可以得到不同复杂度的变种。 下面的代码中，我们定义了一个名为 vgg_block 的函数来实现一个 VGG 块，该函数有三个参数，分别对应于卷积层的数量 num_convs、输入通道的数量 in_channels 和输出通道的数量 out_channels： 原始 VGG 网络有5个卷积块，其中前两个块各有一个卷积层，后三个块各包含两个卷积层。第一个模块有64个输出通道，每个后续模块将输出通道数量翻倍，直到达到512。由于该网络使用8个卷积层和3个全连接层，因此它通常被称为 VGG-11。 由于 VGG-11 比 AlexNet 计算量更大，因此我们构建了一个通道数较少的网络，足够用于训练 Fashion-MNIST 数据集： 最后我们读取数据集并进行训练，超参数设置：lr, num_epochs = 0.02, 15，训练过程与第一节内容一样，因此不再放出代码。 PS：如果显存不够可以减小 batch_size，从128改为64或32。 3. 网络中的网络（NiN） 回想一下，卷积层的输入和输出由四维张量组成，张量的每个轴分别对应样本、通道、高度和宽度。另外，全连接层的输入和输出通常是分别对应于样本和特征的二维张量。NiN 的想法是在每个像素位置（针对每个高度和宽度）应用一个全连接层。如果我们将权重连接到每个空间位置，我们可以将其视为1×1卷积层（如第五章第四节 PS 中所述），或作为在每个像素位置上独立作用的全连接层。从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征（feature）。 NiN 块以一个普通卷积层开始，后面是两个1×1的卷积层。这两个1×1卷积层充当带有 ReLU 激活函数的逐像素全连接层，对每个像素增加了非线性特性： NiN 和 AlexNet 之间的一个显著区别是 NiN 完全取消了全连接层。相反，NiN 使用一个 NiN 块，其输出通道数等于标签类别的数量。最后放一个全局平均汇聚层（global average pooling layer），生成一个对数几率（logits）。NiN 设计的一个优点是，它显著减少了模型所需参数的数量。然而，在实践中，这种设计有时会增加训练模型的时间。 最后我们读取数据集并进行训练，超参数设置：lr, num_epochs = 0.1, 15，训练过程与第一节内容一样，因此不再放出代码。 4. 含并行连结的网络（GoogLeNet） 在 GoogLeNet 中，基本的卷积块被称为 Inception 块（Inception block）。Inception 块由四条并行路径组成。前三条路径使用窗口大小为1×1、3×3和5×5的卷积层，从不同空间大小中提取信息。中间的两条路径在输入上执行1×1卷积，以减少通道数，从而降低模型的复杂性。第四条路径使用3×3最大汇聚层，然后使用1×1卷积层来改变通道数。这四条路径都使用合适的填充来使输入与输出的高和宽一致，最后我们将每条线路的输出在通道维度上连结，并构成 Inception 块的输出。在 Inception 块中，通常调整的超参数是每层输出通道数。 GoogLeNet 一共使用9个 Inception 块和全局平均汇聚层的堆叠来生成其估计值。Inception 块之间的最大汇聚层可降低维度。第一个模块类似于 AlexNet 和 LeNet，Inception 块的组合从 VGG 继承，全局平均汇聚层避免了在最后使用全连接层。 现在，我们逐一实现 GoogLeNet 的每个模块。第一个模块使用64个通道、7×7卷积层： 第二个模块使用两个卷积层：第一个卷积层是64个通道、1×1卷积层；第二个卷积层使用将通道数量增加三倍的3×3卷积层。这对应于 Inception 块中的第二条路径： 第三个模块串联两个完整的 Inception 块。第一个 Inception 块的输出通道数为 64 + 128 + 32 + 32 = 256，四个路径之间的输出通道数量比为 2 : 4 : 1 : 1，第二个和第三个路径首先将输入通道的数量分别减少到96和16，然后连接第二个卷积层。第二个 Inception 块的输出通道数增加到 128 + 192 + 96 + 64 = 480，四个路径之间的输出通道数量比为 4 : 6 : 3 : 2，第二条和第三条路径首先将输入通道的数量分别减少到128和32： 第四模块更加复杂，它串联了5个 Inception 块，其输出通道数分别是 192 + 208 + 48 + 64 = 512、160 + 224 + 64 + 64 = 512、128 + 256 + 64 + 64 = 512、112 + 288 + 64 + 64 = 528 和 256 + 320 + 128 + 128 = 832： 第五模块包含输出通道数为 256 + 320 + 128 + 128 = 832 和 384 + 384 + 128 + 128 = 1024 的两个 Inception 块。需要注意的是，第五模块的后面紧跟输出层，该模块同 NiN 一样使用全局平均汇聚层，将每个通道的高和宽变成1。最后我们将输出变成二维数组，再接上一个输出个数为标签类别数的全连接层： 最后我们读取数据集并进行训练，超参数设置：lr, num_epochs = 0.1, 15，训练过程与第一节内容一样，因此不再放出代码。 5. 批量规范化（BN） 批量规范化（Batch Normalization）是一种流行且有效的技术，可持续加速深层网络的收敛速度。 使用真实数据时，我们的第一步是标准化输入特征，使其平均值为0，方差为1。直观地说，这种标准化可以很好地与我们的优化器配合使用，因为它可以将参数的量级进行统一。 第二，对于典型的多层感知机或卷积神经网络。当我们训练时，中间层中的变量（例如，多层感知机中的仿射变换输出）可能具有更广的变化范围：不论是沿着从输入到输出的层，跨同一层中的单元，或是随着时间的推移，模型参数随着训练更新的变幻莫测。批量规范化的发明者非正式地假设，这些变量分布中的这种偏移可能会阻碍网络的收敛。直观地说，我们可能会猜想，如果一个层的可变值是另一层的100倍，这可能需要对学习率进行补偿调整。 第三，更深层的网络很复杂，容易过拟合。这意味着正则化变得更加重要。 批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。接下来，我们应用比例系数和比例偏移。正是由于这个基于批量统计的标准化，才有了批量规范化的名称。 请注意，如果我们尝试使用大小为1的小批量应用批量规范化，我们将无法学到任何东西。这是因为在减去均值之后，每个隐藏单元将为0。所以，只有使用足够大的小批量，批量规范化这种方法才是有效且稳定的。请注意，在应用批量规范化时，批量大小的选择可能比没有批量规范化时更重要。 通常，我们将批量规范化层置于全连接层中的仿射变换和激活函数之间。同样，对于卷积层，我们可以在卷积层之后和非线性激活函数之前应用批量规范化。 下面，我们从头开始实现一个具有张量的批量规范化层： 我们现在可以创建一个正确的 BatchNorm 层。这个层将保持适当的参数：拉伸 gamma 和偏移 beta，这两个参数将在训练过程中更新。此外，我们的层将保存均值和方差的移动平均值，以便在模型预测期间随后使用。 为了更好理解如何应用 BatchNorm，下面我们将其应用于 LeNet 模型： 最后我们读取数据集并进行训练，超参数设置：lr, num_epochs = 1, 15，由于网络模型类似 LeNet，因此无需对输入图像进行 Resize 操作，训练过程与第一节内容一样，因此不再放出代码。 6. 残差网络（ResNet） 只有当较复杂的函数类包含较小的函数类时，我们才能确保提高它们的性能。对于深度神经网络，如果我们能将新添加的层训练成恒等映射（identity function）：f(x) = x，新模型和原模型将同样有效。同时，由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。 残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。 ResNet 沿用了 VGG 完整的卷积层设计。残差块里首先有2个有相同输出通道数的卷积层。每个卷积层后接一个批量规范化层和 ReLU 激活函数。然后我们通过跨层数据通路，跳过这2个卷积运算，将输入直接加在最后的 ReLU 激活函数前。这样的设计要求2个卷积层的输出与输入形状一样，从而使它们可以相加。如果想改变通道数，就需要引入一个额外的1×1卷积层来将输入变换成需要的形状后再做相加运算。残差块的实现如下： 此代码生成两种类型的网络：一种是当 use_1x1conv = False 时，应用 ReLU 非线性函数之前，将输入添加到输出。另一种是当 use_1x1conv = True 时，添加通过1×1卷积调整通道和分辨率。 ResNet 的前两层跟之前介绍的 GoogLeNet 中的一样：在输出通道数为64、步幅为2的7×7卷积层后，接步幅为2的3×3最大汇聚层。不同之处在于 ResNet 每个卷积层后增加了批量规范化层。 GoogLeNet 在后面接了4个由 Inception 块组成的模块。ResNet 则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。第一个模块的通道数同输入通道数一致。由于之前已经使用了步幅为2的最大汇聚层，所以无须减小高和宽。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。 下面我们来实现这个模块。注意，我们对第一个模块做了特别处理： 接着在 ResNet 加入所有残差块，这里每个模块使用2个残差块： 最后，与 GoogLeNet 一样，在 ResNet 中加入全局平均汇聚层，以及全连接层输出： 每个模块有4个卷积层（不包括恒等映射的卷积层）。加上第一个7×7卷积层和最后一个全连接层，共有18层。因此，这种模型通常被称为 ResNet-18。通过配置不同的通道数和模块里的残差块数可以得到不同的 ResNet 模型，例如更深的含152层的 ResNet-152。 最后我们读取数据集并进行训练，超参数设置：lr, num_epochs = 0.02, 15，由于 ResNet 性能很强，对于 FashionMNIST 数据集很容易就过拟合了，因此可以将输入图像 Resize 为 (96, 96)，训练过程与第一节内容一样，因此不再放出代码。 ResNet 其它版本的模型结构可以参考： ResNet 及其变种的结构梳理、有效性分析与代码解读。 ResNet18、50网络结构以及 PyTorch 实现代码。 ResNet50网络结构搭建（PyTorch）。 7. 稠密连接网络（DenseNet） ResNet 和 DenseNet 的关键区别在于，DenseNet 输出是连接（用 [.] 表示）而不是如 ResNet 的简单相加。 稠密网络主要由2部分构成：稠密块（dense block）和过渡层（transition layer）。前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。 DenseNet 使用了 ResNet 改良版的“批量规范化、激活和卷积”架构。我们首先实现一下这个架构： 一个稠密块由多个卷积块组成，每个卷积块使用相同数量的输出通道。然而，在前向传播中，我们将每个卷积块的输入和输出在通道维上连结： 例如我们构建一个 DenseBlock(2, 3, 10)，那么两层卷积层分别为 conv_block(3, 10)、conv_block(13, 10)。第一层卷积输出的通道维是10，与输入 X 在通道维上连结后通道维是13，因此第二层卷积输入的通道维是13，第二层卷积输出的通道维是10，与输入 X 在通道维上连结后通道维是23： 由于每个稠密块都会带来通道数的增加，使用过多则会过于复杂化模型。而过渡层可以用来控制模型复杂度。它通过1×1卷积层来减小通道数，并使用步幅为2的平均汇聚层减半高和宽，从而进一步降低模型复杂度： 对上一个例子中稠密块的输出使用通道数为10的过渡层。此时输出的通道数减为10，高和宽均减半： 我们来构造 DenseNet 模型。DenseNet 首先使用同 ResNet 一样的单卷积层和最大汇聚层： 接下来，类似于 ResNet 使用的4个残差块，DenseNet 使用的是4个稠密块。与 ResNet 类似，我们可以设置每个稠密块使用多少个卷积层。这里我们设成4，从而与之前的 ResNet-18 保持一致。稠密块里的卷积层通道数（即增长率）设为32，所以每个稠密块将增加128个通道。 在每个模块之间，ResNet 通过步幅为2的残差块减小高和宽，DenseNet 则使用过渡层来减半高和宽，并减半通道数： 与 ResNet 类似，最后接上全局汇聚层和全连接层来输出结果： 最后我们读取数据集并进行训练，超参数设置：lr, num_epochs = 0.1, 15，将输入图像 Resize 为 (96, 96)，训练过程与第一节内容一样，因此不再放出代码。 下一章：计算机视觉。"},{"title":"动手学深度学习笔记(李沐)-卷积神经网络","date":"2023-03-01T01:20:00.000Z","url":"/posts/25122.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 李沐动手学深度学习（PyTorch）课程学习笔记第五章：卷积神经网络。 1. 从全连接层到卷积 假设我们有一个足够充分的照片数据集，数据集中是拥有标注的照片，每张照片具有百万级像素，这意味着网络的每次输入都有一百万个维度。即使将隐藏层维度降低到1000，这个全连接层也将有十亿个参数。 假设我们想从一张图片中找到某个物体。合理的假设是：无论哪种方法找到这个物体，都应该和物体的位置无关。卷积神经网络正是将空间不变性（spatial invariance）的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示： 平移不变性（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。 局部性（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。 2. 图像卷积 严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是互相关运算（cross-correlation），而不是卷积运算。在卷积层中，输入张量和核张量通过互相关运算产生输出张量。 在二维互相关运算中，卷积窗口从输入张量的左上角开始，从左到右、从上到下滑动。当卷积窗口滑动到新一个位置时，包含在该窗口中的部分张量与卷积核张量进行按元素相乘，得到的张量再求和得到一个单一的标量值，由此我们得出了这一位置的输出张量值。 我们可以自己实现如上过程： 卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。 我们可以基于上面定义的 corr2d 函数实现二维卷积层： 现在来看一下卷积层的一个简单应用：通过找到像素变化的位置，来检测图像中不同颜色的边缘。我们假设0为黑色像素，1为白色像素： X 的内容如下： 接下来，我们构造一个高度为1、宽度为2的卷积核K。当进行互相关运算时，如果水平相邻的两元素相同，则输出为零，否则输出为非零： Y 的内容如下： 现在我们使用 PyTorch 的卷积层尝试通过正确结果 Y 是否能学习出我们之前自己构造出的卷积核参数： 3. 填充和步幅 在应用多层卷积时，我们常常丢失边缘像素。由于我们通常使用小卷积核，因此对于任何单个卷积，我们可能只会丢失几个像素。但随着我们应用许多连续卷积层，累积丢失的像素数就多了。解决这个问题的简单方法即为填充（padding）：在输入图像的边界填充元素（通常填充元素是0）。 在计算互相关时，卷积窗口从输入张量的左上角开始，向下、向右滑动。在前面的例子中，我们默认每次滑动一个元素。但是，有时候为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素。我们将每次滑动元素的数量称为步幅（stride）。 4. 多输入多输出通道 当输入包含多个通道时，需要构造一个与输入数据具有相同输入通道数的卷积核，以便与输入数据进行互相关运算。 为了加深理解，我们实现一下多输入通道互相关运算。简而言之，我们所做的就是对每个通道执行互相关操作，然后将结果相加： 到目前为止，不论有多少输入通道，我们还只有一个输出通道。在最流行的神经网络架构中，随着神经网络层数的加深，我们常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。直观地说，我们可以将每个通道看作对不同特征的响应。而现实可能更为复杂一些，因为每个通道不是独立学习的，而是为了共同使用而优化的。因此，多输出通道并不仅是学习多个单通道的检测器。 PS：1*1卷积层通常用于调整网络层的通道数量和控制模型复杂性，其失去了卷积层的特有能力：在高度和宽度维度上，识别相邻元素间相互作用的能力。 5. 汇聚层（池化层） 汇聚层（pooling layer）具有双重目的：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。 与卷积层类似，汇聚层运算符由一个固定形状的窗口组成，该窗口根据其步幅大小在输入的所有区域上滑动（从左至右、从上至下），为固定形状窗口（有时称为汇聚窗口）遍历的每个位置计算一个输出。然而，不同于卷积层中的输入与卷积核之间的互相关计算，汇聚层不包含参数。相反，池运算是确定性的，我们通常计算汇聚窗口中所有元素的最大值或平均值。这些操作分别称为最大汇聚层（maximum pooling）和平均汇聚层（average pooling）。 与卷积层一样，汇聚层也可以改变输出形状。和以前一样，我们可以通过填充和步幅以获得所需的输出形状。下面，我们用深度学习框架中内置的二维最大汇聚层，来演示汇聚层中填充和步幅的使用。 默认情况下，深度学习框架中的步幅与汇聚窗口的大小相同。因此，如果我们使用形状为 (3, 3) 的汇聚窗口，那么默认情况下，我们得到的步幅形状为 (3, 3)。 在处理多通道输入数据时，汇聚层在每个输入通道上单独运算，而不是像卷积层一样在通道上对输入进行汇总。这意味着汇聚层的输出通道数与输入通道数相同。下面，我们将在通道维度上连结张量 X 和 X + 1，以构建具有2个通道的输入： 6. LeNet 本节将介绍 LeNet，它是最早发布的卷积神经网络之一。总体来看，LeNet（LeNet-5）由两个部分组成： 卷积编码器：由两个卷积层组成。 全连接层密集块：由三个全连接层组成。 每个卷积块中的基本单元是一个卷积层、一个 Sigmoid 激活函数和平均汇聚层。请注意，虽然 ReLU 和最大汇聚层更有效，但它们在20世纪90年代还没有出现。每个卷积层使用5×5卷积核和一个 Sigmoid 激活函数。这些层将输入映射到多个二维特征输出，通常同时增加通道的数量。第一卷积层有6个输出通道，而第二个卷积层有16个输出通道。每个2×2池化操作（步幅2）通过空间下采样将维数减少4倍。卷积的输出形状由批量大小、通道数、高度、宽度决定。 虽然卷积神经网络的参数较少，但与深度的多层感知机相比，它们的计算成本仍然很高，因为每个参数都参与更多的乘法。通过使用 GPU，可以用它加快训练。 下一章：现代卷积神经网络。"},{"title":"动手学深度学习笔记(李沐)-PyTorch神经网络基础","date":"2023-02-28T02:34:00.000Z","url":"/posts/23526.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 李沐动手学深度学习（PyTorch）课程学习笔记第四章：PyTorch 神经网络基础。 1. 层和块 在构造自定义块之前，我们先回顾一下多层感知机（第三章第二节）的代码，下面的代码生成一个网络，其中包含一个具有256个单元和 ReLU 激活函数的全连接隐藏层，然后是一个具有10个隐藏单元且不带激活函数的全连接输出层： 在这个例子中，我们通过实例化 nn.Sequential 来构建我们的模型，层的执行顺序是作为参数传递的。简而言之，nn.Sequential 定义了一种特殊的 Module，即在 PyTorch 中表示一个块的类，它维护了一个由 Module 组成的有序列表。注意，两个全连接层都是 Linear 类的实例，Linear 类本身就是 Module 的子类。另外，到目前为止，我们一直在通过 net(X) 调用我们的模型来获得模型的输出。这实际上是 net.__call__(X) 的简写。这个前向传播函数非常简单：它将列表中的每个块连接在一起，将每个块的输出作为下一个块的输入。 在下面的代码片段中，我们从零开始编写一个块。它包含一个多层感知机，其具有256个隐藏单元的隐藏层和一个10维输出层。注意，下面的 MLP 类继承了表示块的类。我们的实现只需要提供我们自己的构造函数（Python 中的 __init__ 函数）和前向传播函数： 接着我们实例化多层感知机的层，然后在每次调用前向传播函数时调用这些层： 现在我们可以更仔细地看看 Sequential 类是如何工作的，回想一下 Sequential 的设计是为了把其他模块串起来。为了构建我们自己的简化的 MySequential，我们只需要定义两个关键函数： 一种将块逐个追加到列表中的函数； 一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。 下面的 MySequential 类提供了与默认 Sequential 类相同的功能： Sequential 类使模型构造变得简单，允许我们组合新的架构，而不必定义自己的类。然而，并不是所有的架构都是简单的顺序架构。当需要更强的灵活性时，我们需要定义自己的块。例如，我们可能希望在前向传播函数中执行 Python 的控制流。此外，我们可能希望执行任意的数学运算，而不是简单地依赖预定义的神经网络层： 我们可以混合搭配各种组合块的方法。在下面的例子中，我们以一些想到的方法嵌套块： 2. 参数管理 我们首先看一下具有单隐藏层的多层感知机： 我们从已有模型中访问参数。当通过 Sequential 类定义模型时，我们可以通过索引来访问模型的任意层。这就像模型是一个列表一样，每层的参数都在其属性中。如下所示，我们可以检查第二个全连接层的参数： 这个全连接层包含两个参数，分别是该层的权重和偏置，每个参数都表示为参数类的一个实例。要对参数执行任何操作，首先我们需要访问底层的数值： 参数是复合的对象，包含值、梯度和额外信息。这就是我们需要显式参数值的原因。除了值之外，我们还可以访问每个参数的梯度。在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态： 当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。下面，我们将通过演示来比较访问第一个全连接层的参数和访问所有层： 这为我们提供了另一种访问网络参数的方式，如下所示： 现在让我们看看如果我们将多个块相互嵌套，参数命名约定是如何工作的： 因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们： 知道了如何访问参数后，现在我们看看如何正确地初始化参数。深度学习框架提供默认随机初始化，也允许我们创建自定义初始化方法。 让我们首先调用内置的初始化器。下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量，且将偏置参数设置为0： 我们还可以将所有参数初始化为给定的常数，比如初始化为1： 我们还可以对某些块应用不同的初始化方法。例如，下面我们使用 Xavier 初始化方法初始化第一个神经网络层，然后将第三个神经网络层初始化为常量值42： 有时，深度学习框架没有提供我们需要的初始化方法。在下面的例子中，我们先初始化为-10~10的均匀分布，然后将绝对值小于5的参数置零： 有时我们希望在多个层间共享参数：我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数： 这个例子表明第三个和第五个神经网络层的参数是绑定的。它们不仅值相等，而且由相同的张量表示。因此，如果我们改变其中一个参数，另一个参数也会改变。这里有一个问题：当参数绑定时，梯度会发生什么情况？答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层（即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。 3. 自定义层 我们构造一个没有任何参数的自定义层，下面的 CenteredLayer 类要从其输入中减去均值。要构建它，我们只需继承基础层类并实现前向传播功能： 下面我们继续定义具有参数的层，这些参数可以通过训练进行调整。让我们实现自定义版本的全连接层。回想一下，该层需要两个参数，一个用于表示权重，另一个用于表示偏置项。在此实现中，我们使用 ReLU 作为激活函数。该层需要输入参数：in_units 和 units，分别表示输入数和输出数： 4. 读写文件 加载和保存张量： 将存储在文件中的数据读回内存： 存储一个张量列表，然后把它们读回内存： 我们可以写入或读取从字符串映射到张量的字典。当我们要读取或写入模型中的所有权重时，这很方便： 深度学习框架提供了内置函数来保存和加载整个网络。需要注意的一个重要细节是，这将保存模型的参数而不是保存整个模型： 5. GPU CUDA 的安装配置教程：Anaconda与PyTorch安装教程 在 PyTorch 中，CPU 和 GPU 可以用 torch.device('cpu') 和 torch.device('cuda') 表示： 我们可以查询可用 GPU 的数量： 我们可以查询张量所在的设备。默认情况下，张量是在 CPU 上创建的： 需要注意的是，无论何时我们要对多个项进行操作，它们都必须在同一个设备上。例如，如果我们对两个张量求和，我们需要确保两个张量都位于同一个设备上，否则框架将不知道在哪里存储结果，甚至不知道在哪里执行计算。 有几种方法可以在 GPU 上存储张量。例如，我们可以在创建张量时指定存储设备： 数据在同一个 GPU 上我们才可以将它们相加： 类似地，神经网络模型可以指定设备。下面的代码将模型参数放在 GPU 上： 总之，只要所有的数据和参数都在同一个设备上，我们就可以有效地学习模型。 下一章：卷积神经网络。"},{"title":"动手学深度学习笔记(李沐)-多层感知机","date":"2023-02-18T08:36:00.000Z","url":"/posts/46068.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 李沐动手学深度学习（PyTorch）课程学习笔记第三章：多层感知机。 1. 多层感知机的从零实现 FashionMNIST 数据集的读取与第二章第四节一样，此处不再放上代码。 初始化模型参数，我们将实现一个具有单隐藏层的多层感知机，它包含256个隐藏单元。注意，我们可以将这两个变量都视为超参数。通常，我们选择2的若干次幂作为层的宽度。因为内存在硬件中的分配和寻址方式，这么做往往可以在计算上更高效。 为了确保我们对模型的细节了如指掌，我们将实现 ReLU 激活函数，而不是直接调用内置的 relu 函数： 接下来定义模型： 训练函数的代码也与2.4节基本一样，只需将 net.to(device) 与 net.train() 等与 nn.Module 相关的代码去掉即可，因此不放出完整代码： 2. 多层感知机的简洁实现 与 Softmax 回归的简洁实现（第二章第四节）相比，唯一的区别是我们添加了2个全连接层（之前我们只添加了1个全连接层）。第一层是隐藏层，它包含256个隐藏单元，并使用了 ReLU 激活函数。第二层是输出层，因此我们只需要重点看一下模型即可： 3. 模型选择、欠拟合和过拟合 首先我们需要了解训练误差和泛化误差： 训练误差（training error）：模型在训练数据集上计算得到的误差。 泛化误差（generalization error）：模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。 问题是，我们永远不能准确地计算出泛化误差。这是因为无限多的数据样本是一个虚构的对象。在实际中，我们只能通过将模型应用于一个独立的测试集来估计泛化误差，该测试集由随机选取的、未曾在训练集中出现的数据样本构成。 在机器学习中，我们通常在评估几个候选模型后选择最终的模型。这个过程叫做模型选择。有时，需要进行比较的模型在本质上是完全不同的（比如，决策树与线性模型）。又有时，我们需要比较不同的超参数设置下的同一类模型。 例如，训练多层感知机模型时，我们可能希望比较具有不同数量的隐藏层、不同数量的隐藏单元以及不同的激活函数组合的模型。为了确定候选模型中的最佳模型，我们通常会使用验证集。 验证数据集：一个用来评估模型好坏的数据集，训练数据集用来训练模型参数，验证数据集用来选择模型超参数。 例如在数据集中拿出50%的训练数据作为验证数据集。 不要跟训练数据混在一起（常犯错误）。 测试数据集：只用一次的数据集。 例如未来的考试。 例如我出价的房子的实际成交价。 接下来我们需要了解一下模型容量的概念： 模型容量表示模型拟合各种函数的能力。 低容量的模型难以拟合复杂的训练数据。 高容量的模型可以记住所有的训练数据。 是否过拟合或欠拟合主要取决于模型复杂性和可用训练数据集的大小。当我们比较训练和验证误差时，我们要注意两种常见的情况： 训练误差和验证误差都很差，但它们之间仅有一点差距。如果模型不能降低训练误差，这可能意味着模型过于简单（即表达能力不足），无法捕获试图学习的模式。此外，由于我们的训练和验证误差之间的泛化误差很小，我们有理由相信可以用一个更复杂的模型降低训练误差。这种现象被称为欠拟合（underfitting）。 当我们的训练误差明显低于验证误差时要小心，这表明严重的过拟合（overfitting）。注意，过拟合并不总是一件坏事。特别是在深度学习领域，众所周知，最好的预测模型在训练数据上的表现往往比在保留（验证）数据上好得多。最终，我们通常更关心验证误差，而不是训练误差和验证误差之间的差距。 4. 权重衰减 在训练参数化机器学习模型时，权重衰减（weight decay）是最广泛使用的正则化的技术之一，它通常也被称为L2正则化。 通过限制参数值的选择范围来控制模型容量，通常不限制偏移 b。 首先生成一些数据： 定义网络模型与损失函数： 定义优化算法，我们在实例化优化器时直接通过 weight_decay 指定 weight decay 超参数。默认情况下，PyTorch 同时衰减权重和偏移。这里我们只为权重设置了 weight_decay，所以偏置参数不会衰减： 然后进行训练： 通过结果可以看到模型很快就过拟合了，可以通过修改超参数 wd 的值应用权重衰减的方式来缓解过拟合的现象。 5. 暂退法（Dropout） 一个好的模型需要对输入数据的扰动鲁棒。在训练过程中，建议在计算后续层之前向网络的每一层注入噪声，因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性。 这个想法被称为暂退法（dropout）。暂退法在前向传播过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。这种方法之所以被称为暂退法，因为我们从表面上看是在训练过程中丢弃（drop out）一些神经元。在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。 在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。换言之，每个中间活性值 h 以暂退概率 p 由随机变量替换。 Dropout 说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率 p 停止工作（将一些输出项随机置0），这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征。 我们可以将暂退法应用于每个隐藏层的输出（在激活函数之后），并且可以为每一层分别设置暂退概率：常见的技巧是在靠近输入层的地方设置较低的暂退概率。下面的模型将第一个和第二个隐藏层的暂退概率分别设置为0.2和0.5，并且暂退法只在训练期间有效。 Dropout 的从零实现核心代码如下： Dropout 的简洁实现及完整训练代码如下： 6. 数值稳定性和模型初始化 我们可能面临一些问题，要么是梯度爆炸（gradient exploding）问题：参数更新过大，破坏了模型的稳定收敛；要么是梯度消失（gradient vanishing）问题：参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。 例如当 Sigmoid 函数的输入很大或是很小时，它的梯度都会消失。此外，当反向传播通过许多层时，除非我们在刚刚好的地方，这些地方 Sigmoid 函数的输入接近于零，否则整个乘积的梯度可能会消失。 Xavier 初始化：从均值为零，方差为 2 / (n_in + n_out) 的高斯分布中采样权重（n_in 为输入的数量，n_out 为输出的数量）。 下一章：PyTorch 神经网络基础。"},{"title":"动手学深度学习笔记(李沐)-线性神经网络","date":"2023-02-02T11:25:00.000Z","url":"/posts/19931.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 李沐动手学深度学习（PyTorch）课程学习笔记第二章：线性神经网络。 1. 线性回归的从零实现 为了简单起见，我们将根据带有噪声的线性模型构造一个人造数据集。我们的任务是使用这个有限样本的数据集来恢复这个模型的参数。 首先我们生成数据集： 训练模型时要对数据集进行遍历，每次抽取一小批量样本，并使用它们来更新我们的模型。由于这个过程是训练机器学习算法的基础，所以有必要定义一个函数，该函数能打乱数据集中的样本并以小批量方式获取数据。 在下面的代码中，我们定义一个 data_iter 函数，该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为 batch_size 的小批量。每个小批量包含一组特征和标签： 在我们开始用小批量随机梯度下降优化我们的模型参数之前，我们需要先有一些参数。在下面的代码中，我们通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重，并将偏置初始化为0： 定义线性回归模型： 定义损失函数： 定义优化算法： 现在我们已经准备好了模型训练所有需要的要素，可以实现主要的训练过程部分了。理解这段代码至关重要，因为从事深度学习后，相同的训练过程几乎一遍又一遍地出现。在每次迭代中，我们读取一小批量训练样本，并通过我们的模型来获得一组预测。计算完损失后，我们开始反向传播，存储每个参数的梯度。最后，我们调用优化算法（随机梯度下降法SGD）来更新模型参数： 2. 线性回归的简洁实现 首先生成数据集： 读取数据集： 接下来我们定义模型，在 PyTorch 中，全连接层在 Linear 类中定义。值得注意的是，我们将两个参数传递到 nn.Linear 中。第一个指定输入特征形状，即2，第二个指定输出特征形状，输出特征形状为单个标量，因此为1： 定义损失函数，计算均方误差使用的是 MSELoss 类，也称平方 L2 范数，默认情况下，它返回所有样本损失的平均值： 定义优化算法： 训练过程代码与我们从零开始实现时所做的非常相似： 3. Softmax回归的从零实现 首先读入 Fashion-MNIST 数据集，原始数据集中的每个样本都是28*28的图像。本节将展平每个图像，把它们看作长度为784的向量。在后面的章节中，我们将讨论能够利用图像空间结构的特征，但现在我们暂时只把每个像素位置看作一个特征。 初始化模型参数，在 Softmax 回归中，我们的输出与类别一样多。因为我们的数据集有10个类别，所以网络输出维度为10。因此，权重将构成一个784*10的矩阵，偏置将构成一个1*10的行向量： 定义 Softmax 操作，注意，虽然这在数学上看起来是正确的，但我们在代码实现中有点草率。矩阵中的非常大或非常小的元素可能造成数值上溢或下溢，但我们没有采取措施来防止这点： 定义 Softmax 操作后，我们可以实现 Softmax 回归模型。下面的代码定义了输入如何通过网络映射到输出。注意，将数据传递到模型之前，我们使用 reshape 函数将每张原始图像展平为向量： 接下来我们定义损失函数，交叉熵采用真实标签的预测概率的负对数似然。这里我们不使用 Python 的 for 循环迭代预测（这往往是低效的），而是通过一个运算符选择所有元素。下面我们创建一个数据样本 y_hat，其中包含2个样本在3个类别的预测概率，以及它们对应的标签 y。有了 y，我们知道在第一个样本中，第一类是正确的预测；而在第二个样本中，第三类是正确的预测。然后使用 y 作为 y_hat 中概率的索引，我们选择第一个样本中第一个类的概率和第二个样本中第三个类的概率： 为了计算精度，我们执行以下操作。首先，如果 y_hat 是矩阵，那么假定第二个维度存储每个类的预测分数。我们使用 argmax 获得每行中最大元素的索引来获得预测类别。然后我们将预测类别与真实 y 元素进行比较。由于等式运算符 == 对数据类型很敏感，因此我们将 y_hat 的数据类型转换为与 y 的数据类型一致。结果是一个包含0（错）和1（对）的张量。最后，我们求和会得到正确预测的数量： 同样，对于任意数据迭代器 data_iter 可访问的数据集，我们可以评估在任意模型 net 的精度，这里定义一个实用程序类 Accumulator，用于对多个变量进行累加。在 evaluate_accuracy 函数中，我们在 Accumulator 实例中创建了2个变量，分别用于存储正确预测的数量和预测的总数量。当我们遍历数据集时，两者都将随着时间的推移而累加： 接下来可以开始进行训练： 可以在项目路径下打开 Anaconda 的 PyTorch 环境，然后使用 TensorBoard 查看训练曲线： 4. Softmax回归的简洁实现 首先读取数据集： 定义模型，我们只需在 Sequential 中添加一个带有10个输出的全连接层，这10个输出分别表示对10个类别的预测概率： 定义训练函数，由于之后很多模型的训练过程也是相似的，因此该函数可以复用： 最后设定超参数训练模型： 下一章：多层感知机。"},{"title":"动手学深度学习笔记(李沐)-预备知识","date":"2023-01-18T08:37:00.000Z","url":"/posts/15604.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 李沐动手学深度学习（PyTorch）课程学习笔记第一章：预备知识。 1. 环境安装 首先安装好 PyTorch 环境：Anaconda与PyTorch安装教程。 安装需要的包： 2. 数据操作与数据预处理 张量表示一个数值组成的数组，这个数组可能有多个维度： 我们可以通过张量的 shape 属性来访问张量的形状和张量中元素的总数： 要改变一个张量的形状而不改变元素数量和元素值，我们可以调用 reshape 函数： 使用全0、全1、其他常量或者从特定分布中随机采样的数字： 通过提供包含数值的 Python 列表（或嵌套列表）来为所需张量中的每个元素赋予确定值： 常见的标准算术运算符（+、-、*、/ 和 **）都可以被升级为按元素运算： 我们也可以把多个张量拼接在一起： 通过逻辑运算符构建二元张量： 对张量中的所有元素进行求和会产生一个只有一个元素的张量，或者指定在某一维度上求和： 即使形状不同，我们仍然可以通过调用广播机制（broadcasting mechanism）来执行按元素操作： 与 NumPy 张量相互转化： 将大小为1的张量转换为 Python 标量： 创建一个人工数据集，并存储在 CSV（逗号分隔值）文件中： 从创建的 CSV 文件中加载原始数据集： 为了处理缺失的数据，典型的方法包括插值和删除，这里，我们将考虑插值： 对于 inputs 中的类别值或离散值，我们可以将 NaN 视为一个类别： 现在 inputs 和 outputs 中的所有条目都是数值类型，它们可以转换为张量格式： 3. 线性代数 标量由只有一个元素的张量表示，你可以将向量视为标量值组成的列表： 访问张量的长度和形状： 矩阵和矩阵的转置： 给定具有相同形状的任何两个张量，任何按元素二元运算的结果都将是相同形状的张量： 求和与求平均值： 点积是相同位置的按元素乘积的和，torch.dot 只能对一维向量做点积。注意 NumPy 中的 np.dot 函数计算的是两个矩阵的矩阵乘法，而非对应元素相乘求和： 矩阵向量积： 矩阵乘法，torch.mm 与 np.dot 类似： L2 范数是向量所有元素的平方和的平方根： L1 范数为向量所有元素的绝对值之和： F 范数（弗罗贝尼乌斯范数）是矩阵所有元素的平方和的平方根： 4. 自动微分 先举一个简单的例子： 当 y 不是标量时，向量 y 关于向量 x 的导数的最自然解释是一个矩阵。对于高阶和高维的 y 和 x，求导的结果可以是一个高阶张量。 然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括深度学习中），但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。这里，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和： 有时，我们希望将某些计算移动到记录的计算图之外。例如，假设 y 是作为 x 的函数计算的，而 z 则是作为 y 和 x 的函数计算的。想象一下，我们想计算 z 关于 x 的梯度，但由于某种原因，希望将 y 视为一个常数，并且只考虑到 x 在 y 被计算后发挥的作用。 这里可以分离 y 来返回一个新变量 u，该变量与 y 具有相同的值，但丢弃计算图中如何计算 y 的任何信息。换句话说，梯度不会向后流经 u 到 x。因此，下面的反向传播函数计算 z = u * x 关于 x 的偏导数，同时将 u 作为常数处理，而不是计算 z = x * x * x 关于 x 的偏导数。 使用自动微分的一个好处是：即使构建函数的计算图需要通过 Python 控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到变量的梯度。在下面的代码中，while 循环的迭代次数和 if 语句的结果都取决于输入 a 的值。对于任何 a，存在某个常量标量 k，使得 d = f(a) = k * a，其中 k 的值取决于输入 a，因此可以用 d / a 验证梯度是否正确。 下一章：线性神经网络。"},{"title":"实用机器学习课程笔记(Stanford)","date":"2022-12-22T02:28:00.000Z","url":"/posts/33687.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" Stanford 2021 秋季实用机器学习课程学习笔记。 1. 概论 1.1 课程介绍 Challenges Formulate problem：关注最具影响力的行业问题（如自助超市、自动驾驶汽车等）。 Data：高质量数据的稀缺、隐私问题。 Train models：模型越来越复杂，需要大量数据，成本也越来越高。 Deploy models：计算量大，不适合实时推理。 Monitor：数据分布的变化、公平性问题。 Roles（不同类型的人在 ML 中的作用） Domain experts（领域专家）：具有商业洞察力，知道什么数据是重要的以及在哪可以找到它，确定一个 ML 模型真正的影响。 Data scientists：在 Data mining、Model training and deployment 方面做全栈的工作。 ML experts：定制最先进（state of the art，SOTA）的 ML models。 SDE（软件开发工程师）：开发/维护数据管道、模型训练和服务管道。 Corse topics 本课程的内容为数据科学家需要的技术，但是没有大学传统的 ML/统计/编程方面的教学。 Data 收集、处理数据。 部署的时候场景发生变化导致数据不一样，如 covariate（协变量）、concepts、label 的改变。 独立同分布之外的数据。 Train 模型验证、融合、调优。 迁移学习（Transfer learning）。 多模态（Multi-modality）：如何把不同的数据源融合起来做一个比较大的模型。 Deploy 模型怎样部署。 蒸馏（Distillation）：将比较大的模型提取出精华做的小一点。 Monitor 公平性，之后会讲模型的公平是什么含义。 可解释性，怎样理解模型在干什么。 1.2 数据获取 Flow Chart for data acquisition Discover what data is available 找已经有的数据集。 找基准数据集来检验我们的想法： 例如要做一个新的调超参数的算法，可能需要找一些数据集，且数据集不能太大，要小一点的或者中等大小的，为了客观地检验算法还要考虑找不同方面的数据集。 如果训练比较深的神经网络就需要非常大的数据集。 收集新数据（做一个应用或产品很多时候没有现成的数据集） Source of popular ML datasets MNIST：手写数字。 ImageNet：百万级别的来自搜索引擎的图片，可训练较为深度的模型。 AudioSet：YouTube 上的一些声音的切片，用于做声音的分类。 Kinetics：YouTube 上的一些视频的切片，用于人的行为分类。 KITTI：摄像头或激光雷达等各种 sensor 记录下的交通场景。 Amazon Review：Amazon 产品的用户评论。 SQuAD：维基中的一些 Q-A 对。 Librispeech：1000小时的有声读物。 数据集两大来源：各个网站上爬取，采集数据。 Where to find datasets Paperwithcodes Datasets：学术数据集与排行榜。 Kaggle Datasets：数据科学家提交的 ML datasets。 Google Dataset search：搜索网页上的数据集。 Various toolkits datasets（开源工具包）：TensorFlow、HuggingFace。 会议/公司的 ML 竞赛。 自己组织的 Data lakes（数据湖）。 Datasets comparison 数据集 好处 坏处 学术数据集 数据干净、难度适中 选择面较小、太精简、规模小 竞赛数据集 更接近真实的 ML 应用 仍较精简，且只专注在较热门的应用 原生数据 很灵活 需要很多精力去预处理 Data integration 产品数据通常存放在不同的表中，因此要涉及到表的连接。 Table 1 ID Val 1 Val 2 1 1_val1 1_val2 2 2_val1 2_val2 Table 2 ID Val 3 Val 4 1 1_val3 1_val4 3 3_val3 3_val4 Inner Join T1 & T2 ID Val 1 Val 2 Val 3 Val 4 1 1_val1 1_val2 1_val3 1_val4 Left Join T1 & T2 ID Val 1 Val 2 Val 3 Val 4 1 1_val1 1_val2 1_val3 1_val4 2 2_val1 2_val2 / / 1.3 网页数据抓取 一般不能用 curl，因为网站所有者能使用各种方法阻止。 使用无头浏览器（一个没有 GUI 的网络浏览器）： 你需要大量的新 IP，可以通过云获得很多 IP。 1.4 数据标注 半监督学习Semi-Supervised Learning（SSL） 重点关注有少量标记数据和大量未标记数据的场景。 对没有标注的数据和有标注的数据的数据分布做一些假设： 连续性假设（Continuity assumption）：如果一个样本的特征和另外一个样本相似，那么这两个样本很可能具有相同的标号。 聚类假设（Cluster assumption）：数据具有内在的聚类结构，那么假设一个类里面具有相同的标号。 流形假设（Manifold assumption）：很有可能数据在本质上是在低维的一个流形上分布的。 自学习Self-training 主动学习Active Learning 关注的场景与 SSL 相同，但有人工干预，即选择最有趣的数据（最重要的没有标号的数据）给标注工标注。 不确定性抽样（Uncertainty sampling）：选择一个最不确信的预测让人来判断。 Active Learning + Self-training 弱监督学习Weak Supervision 半自动地生成标号，通常比手动标注的准确率差，但是也是好到可以训练一个还不错的模型。 数据编程（Data programming）：用启发式的方法赋予标号： 关键字搜索、模式匹配、第三方模型。 假设判断一个 YouTube 的评论是垃圾（spam）还是有用的东西（ham）： 2. 机器学习模型 2.1 机器学习模型概览 ML 算法的种类 监督学习（Supervised）：训练有标签的数据来预测标签。 自监督学习（Self-supervised）：标签的生成来自于数据本身。 半监督学习（Semi-supervised）：在有标签和无标签的数据上进行训练，使用模型来预测无标签数据的标签。 无监督学习（Unsupervised）：在未标记的数据上进行训练。 强化学习（Reinforcement）：利用观察与环境互动的结果来采取行动以最大化收益。 本课程最多讨论的内容为监督学习。 监督学习的组成部分 模型（Model）：将输入映射到标签的参数化函数。 损失（Loss）：衡量模型在预测结果方面有多好，即衡量模型预测出来的值和真实值之间的差距，需要指导模型尽量向真实值靠近。 目标函数（Objective）：优化模型参数的目标，例如需要优化模型在训练集合上的所有预测结果的损失之和最小。 优化（Optimization）：解决 Objective 的算法，即把模型中没有指定的参数（可学习的参数）优化为合适的值，使得能够解决目标函数，也就是最小化损失。 监督学习的模型 决策树（Decision trees）：用树来做决定。 线性模型（Linear methods）：决策是根据输入特征的线性组合做出的。 核方法（Kernel machines）：使用核函数衡量两个样本的特征相似度，达到非线性的效果。 神经网络（Neural Networks）：使用神经网络学习特征表示。 2.2 决策树 优点： 可以用来解释，即训练后的模型可以看到叶子结点是什么内容，决策是怎么一步步做下来的。 能够处理数值和类别的特征。 缺点： 非常不稳定，可能数据内产生了一点噪音后整棵树构建出来的样子就不一样了。 如果数据特别复杂，会生成一个特别复杂的树，可以把整个数据里面的各种情况列出来，生成大量的节点，最后会导致过拟合。 不容易并行计算。 随机森林 训练多个决策树以提高稳定性。 树是并行地独立训练的。 对于分类问题可以用多数投票法（例如超过一半的树觉得类别是1，那么它就是1），对于回归问题可以在多棵树上取平均。 为什么叫随机呢？ Bagging：随机抽取训练样本并进行替换。例如样本本来是 [1, 2, 3, 4, 5]，做 Bagging 的时候在里面随机采样5个出来，但是采样可能是有重复的，采样到的结果为 [1, 2, 2, 3, 4]，然后拿到这个 Bagging 出来的数据集后我们就在上面训练一棵树，然后一直重复训练 N 棵树为止。 随机选择一个特征子集，即把 Bagging 出的数据拿出来之后，再从里面的特征中随机采样一些特征列出来（假设树是一个表，那么就是先随机采样出一些行，再随机采样出一些列） 2.3 线性模型 线性回归 一个简单的房价预测模型： 假设有3个特征：卧室数量 x1、浴室数量 x2、居住面积 x3； 预测价格为：y_hat = w1 * x1 + w2 * x2 + w3 * x3 + b； 权重 w1, w2, w3 和偏置 b 将从训练数据中学习。 一般来说，给定数据 x = [x1, x2, ..., xp]，线性回归的预测为：y_hat = w1 * x1 + w2 * x2 + ... + wp * xp + b = &lt;w, x&gt; + b（其中 w 和 x 为长度为 p 的向量，&lt;&gt; 表示内积运算，w 和 b 都是可学习参数）。 线性回归目标函数 假设我们收集了 n 个训练样本 X = [x1, x2, ..., xn]，其中每个 xi 均为长为 p 的向量，将其转置后即为一个 n 行 p 列的矩阵，其对应的标号为 y = [y1, ..., yn]，是一个长为 n 的向量。 目标函数是最小化均方误差（MSE），即优化 w, b 的值使得 sum((yi - &lt;xi, w&gt; - b)**2) / n 最小。 线性回归在分类问题中的应用 回归的输出是一个连续的实数，而对于分类问题，我们要输出对某个样本的类别的预测。 多类别分类： 假设标签为独热编码，即 y = [y1, y2, ..., ym]，如果该样本为第 i 类则 yi = 1，否则 yi = 0。 预测结果 y_hat = [o1, o2, ..., om]，其中 oi 表示预测该样本为第 i 类的概率。 为每个类学习一个线性模型：oi = &lt;x, wi&gt; + bi。 最小化 MSE 损失函数：(y_hat - y)**2 / m。 预测结果所表示的类为 m 个概率中最大的那个，即 argmax(y_hat)。 Mini-batch 随机梯度下降 2.4 神经网络 神经网络就是将手工特征提取的部分换成了一个神经网络。 神经网络通常需要更多的数据和更多的计算，一般都是大数个数量级。 可以选择不同的神经网络架构来更有效地抽取我们的特征： 多层感知机。 卷积神经网络。 循环神经网络。 Transformers。 设计神经网络以结合数据的先验知识。 线性模型到多层感知机（Multilayer Perceptron，MLP） 引入一种全连接层（稠密层，dense），假设输入样本数量为 n，每个样本的特征长度为 m，那么全连接层具有两个可学习参数 w, b，其中 w 是一个 n 行 m 列的实数矩阵，b 是一个长为 n 的向量。则全连接层的计算结果为：y = np.dot(w, x) + b。 线性回归可以认为是一个只有一个输出的全连接层。 Softmax 回归可以认为是一个有 C 个输出的全连接层，C 表示类别的数量。 多层感知机的目的是实现一个非线性的模型，但是如果只是简单使用多个全连接层是没用的，多个线性操作的叠加还是一个线性操作，因此还需要加入非线性函数（激活函数）。 激活函数是一个基于元素的非线性函数： sigmoid(x) = 1 / (1 + np.exp(-x))。 relu(x) = max(x, 0)。 非线性的激活函数能让我们得到非线性模型。 可以堆叠多个隐藏层（例如多个 dense 层和 activation 层堆叠），得到更深层次的模型。 超参数：隐藏层数量 hidden layers，每个隐藏层的输出大小 outputs of each hidden layer（最后一层的输出无法改变）。 代码实现： 2.5 卷积神经网络 全连接层到卷积神经网络 以一个图像识别任务为例，使用 MLP 模型学习 ImageNet（每张图像大小为300*300像素，有1000个类别），我们假设其中一个隐藏层具有10000个输出： 它会产生10亿个可学习参数，这太大了！ 因为全连接的输出是所有输入元素的加权和，而且每个输出的权重是不一样的。 识别图像中的物体： 平移不变性：无论对象在哪里，输出都是相似的。 局部性：像素与其周围像素的相关性比较高，因为图像中的物体都是连续性的。 将先验知识构建到模型结构中： 用更少的参数（#params）实现相同的模型容量。 卷积层（Convolution layer） 局部性：从 k * k 大小的输入窗口计算输出，即做局部的计算。 平移不变性：输出使用相同的 k * k 权重（核）。 卷积层的模型参数不依赖于输入/输出的大小。 一个卷积核可以被学习成去识别一个图像里面的模式，比如识别绿色通道中的某个块状物体，识别某个方向上的纹理 代码： 池化层（Pooling layer） 卷积层对输入的位置很敏感，即输入中模式的转换/旋转会导致输出中模式类似地变化，因此我们需要一定的对未知移动的鲁棒性。 池化层在大小为 k * k 的窗口中计算平均值/最大值/最小值。 代码： 卷积神经网络（Convolutional Neural Networks，CNN） 卷积神经网络的原理为叠加卷积层来提取特征。 激活函数应用于每个卷积层之后。 使用池化操作来降低位置敏感性。 现代 CNN 是具有各种超参数和层连接的深度神经网络（AlexNet, VGG, inception, ResNet, MobileNet）。 2.6 循环神经网络 全连接层到循环神经网络 语言模型：给出一个句子前面的一些词，预测下一个词是什么。例如：hello -&gt; world、hello world -&gt; !。 单纯使用 MLP 不能很好地处理序列信息，例如长度的变化和时序的变化。 循环神经网络的原理为将上一个全连接层输出的状态复制一份作为隐藏状态 H，与下一个输入状态进行拼接后再进行预测。即：h_t = RNN(W_hh * h' + W_hx * x_t + b_h)，其中 h' 为隐藏状态，x_t 为当前输入。 代码： 3. 模型评估 3.1 评估指标 损失（Loss）衡量模型在预测监督学习结果的方面有多好。 评估模型性能的其他指标： 模型相关的指标：例如分类的精度，物体检测的 mAP。 商业相关的指标：例如收益，推理延迟（如模型能在100毫秒之内返回结果）。 我们一般通过考虑多种指标来选择模型。 二分类的评估指标 Accuracy：正确的预测数量/样本总数 Precision：预测结果为类 i 且实际结果也为类 i 的数量/预测结果为类 i 的数量 Recall：预测结果为类 i 且实际结果也为类 i 的数量/实际结果为类 i 的数量 F1：平衡 Precision 和 Recall 的指标，为 Precision 和 Recall 的调和平均值：2pr / (p + r) 二分类中的 AUC 和 ROC AUC 为 ROC 曲线下的面积，大小范围为 [0.5, 1]。 衡量模型分离这两个类的能力。 选择决策阈值 x，如果输出 y_hat &gt;= x 则预测为正类，否则为负类。 展示广告的商业指标 最优化收入和客户体验。 Latency：广告应该与其他内容同时显示给用户。 ASN：平均每页显示的广告数量。 CTR：用户实际点击率。 ACP：广告商每次点击支付的平均价格。 收益 = 页面浏览量 * ASN * CTR * ACP。 "},{"title":"MySQL面试知识点总结","date":"2022-12-04T09:13:00.000Z","url":"/posts/48028.html","tags":[["MySQL","/tags/MySQL/"]],"categories":[["MySQL","/categories/MySQL/"]],"content":" MySQL 常见面试题总结，文章将不断更新。 1. 基础 1.1 数据库的三范式是什么？ 第一范式（1NF）：强调的是列的原子性，即数据库表的每一列都是不可分割的原子数据项。 第二范式（2NF）：要求实体的属性完全依赖于主关键字。所谓完全依赖是指不能存在仅依赖主关键字一部分的属性。 第三范式（3NF）：任何非主属性不依赖于其它非主属性。 1.2 MySQL支持哪些存储引擎？ MySQL 支持多种存储引擎，比如 InnoDB、MyISAM、Memory、Archive 等等。在大多数的情况下，直接选择使用 InnoDB 引擎都是最合适的，InnoDB 也是 MySQL 的默认存储引擎。 MyISAM 和 InnoDB 的区别有哪些： InnoDB 支持事务，MyISAM 不支持。 InnoDB 支持外键，MyISAM 不支持。 InnoDB 是聚集索引，数据文件是和索引绑在一起的，必须要有主键，通过主键索引效率很高；MyISAM 是非聚集索引，数据文件是分离的，索引保存的是数据文件的指针，主键索引和辅助索引是独立的。 InnoDB 不支持全文索引，MyISAM 支持全文索引，查询效率上 MyISAM 更高。 InnoDB 不保存表的具体行数，MyISAM 用一个变量保存了整个表的行数。 MyISAM 采用表级锁（table-level locking）；InnoDB 支持行级锁（row-level locking）和表级锁，默认为行级锁。 1.3 超键、候选键、主键、外键分别是什么？ 超键：在关系中能唯一标识元组的属性集称为关系模式的超键。一个属性可以为作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。 候选键：是最小超键，即没有冗余元素的超键。 主键：数据库表中对储存数据对象予以唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（NULL）。 外键：在一个表中存在的另一个表的主键称此表的外键。 1.4 SQL约束有哪几种？ NOT NULL：用于控制字段的内容一定不能为空（NULL）。 UNIQUE：控制字段内容不能重复，一个表允许有多个 UNIQUE 约束。 PRIMARY KEY：也是用于控制字段内容不能重复，但它在一个表只允许出现一个。 FOREIGN KEY：用于预防破坏表之间连接的动作，也能防止非法数据插入外键列，因为它必须是它指向的那个表中的值之一。 CHECK：用于控制字段的值范围。 1.5 MySQL中的varchar和char有什么区别？ char 是一个定长字段，假如申请了 char(10) 的空间，那么无论实际存储多少内容，该字段都占用10个字符；而 varchar 是变长的，也就是说申请的只是最大长度，占用的空间为实际字符长度 + 1，最后一个字符存储使用了多长的空间。 在检索效率上来讲，char &gt; varchar，因此在使用中，如果确定某个字段的值的长度，可以使用 char，否则应该尽量使用 varchar，例如存储用户 MD5 加密后的密码，则可以使用 char。 1.6 MySQL中in和exists区别？ MySQL 中的 in 语句是把外表和内表作 Hash 连接，而 exists 语句是对外表作 Loop 循环，每次 Loop 循环再对内表进行查询。一直大家都认为 exists 比 in 语句的效率要高，这种说法其实是不准确的。这个是要区分环境的： 如果查询的两个表大小相当，那么用 in 和 exists 差别不大。 如果两个表中一个较小，一个是大表，则子查询表大的用 exists，子查询表小的用 in。 not in 和 not exists：如果查询语句使用了 not in，那么内外表都进行全表扫描，没有用到索引；而 not extsts 的子查询依然能用到表上的索引。所以无论哪个表大，用 not exists 都比 not in 要快。 1.7 drop、delete与truncate的区别？ 三者都表示删除，但是三者有一些差别： delete truncate drop 类型 属于 DML 属于 DDL 属于 DDL 回滚 可回滚 不可回滚 不可回滚 删除内容 表结构还在，删除表的全部或者一部分数据 表结构还在，删除表中的所有数据 从数据库中删除表，所有的数据行、索引和权限也会被删除 删除速度 删除速度慢，需要逐行删除 删除速度快 删除速度最快 1.8 什么是存储过程？有哪些优缺点？ 存储过程是一些预编译的 SQL 语句。 更加直白的理解：存储过程可以说是一个记录集，它是由一些 T-SQL 语句组成的代码块，这些 T-SQL 语句代码像一个方法一样实现一些功能（对单表或多表的增删改查），然后再给这个代码块取一个名字，在用到这个功能的时候调用它就行了。 存储过程是一个预编译的代码块，执行效率比较高，一个存储过程替代大量 T-SQL 语句，可以降低网络通信量，提高通信速率，可以一定程度上确保数据安全。 但是，在互联网项目中，其实是不太推荐存储过程的，比较出名的就是阿里的《Java 开发手册》中禁止使用存储过程，我个人的理解是，在互联网项目中，迭代太快，项目的生命周期也比较短，人员流动相比于传统的项目也更加频繁，在这样的情况下，存储过程的管理确实是没有那么方便，同时，复用性也没有写在服务层那么好。 1.9 MySQL执行查询的过程？ 客户端通过 TCP 连接发送连接请求到 MySQL 连接器，连接器会对该请求进行权限验证及连接资源分配。 查缓存（当判断缓存是否命中时，MySQL 不会进行解析查询语句，而是直接使用 SQL 语句和客户端发送过来的其他原始信息。所以，任何字符上的不同，例如空格、注解等都会导致缓存的不命中）。 语法分析（SQL 语法是否写错了）：如何把语句给到预处理器，检查数据表和数据列是否存在，解析别名看是否存在歧义。 优化：是否使用索引，生成执行计划。 交给执行器，将数据保存到结果集中，同时会逐步将数据缓存到查询缓存中，最终将结果集返回给客户端。 更新语句执行会复杂一点，需要检查表是否有排它锁，写 binlog、刷盘、是否执行 commit。 2. 事务 2.1 什么是数据库事务？ 事务是一个不可分割的数据库操作序列，也是数据库并发控制的基本单位，其执行的结果必须使数据库从一种一致性状态变到另一种一致性状态。事务是逻辑上的一组操作，要么都执行，要么都不执行。 事务最经典也经常被拿出来说例子就是转账了。 假如小明要给小红转账1000元，这个转账会涉及到两个关键操作就是：将小明的余额减少1000元，将小红的余额增加1000元。万一在这两个操作之间突然出现错误比如银行系统崩溃，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。 2.2 事务具有的四个特征？ 事务就是一组原子性的操作，这些操作要么全部发生，要么全部不发生。事务把数据库从一种一致性状态转换成另一种一致性状态。 原子性（Atomicity）：事务是数据库的逻辑工作单位，事务中包含的各操作要么都做，要么都不做。 一致性（Consistency）：事务执行的结果必须是使数据库从一个一致性状态变到另一个一致性状态。因此当数据库只包含成功事务提交的结果时，就说数据库处于一致性状态。如果数据库系统运行中发生故障，有些事务尚未完成就被迫中断，这些未完成事务对数据库所做的修改有一部分已写入物理数据库，这时数据库就处于一种不正确的状态，或者说是不一致的状态。 隔离性（Isolation）：一个事务的执行不能被其它事务干扰。即一个事务内部的操作及使用的数据对其它并发事务是隔离的，并发执行的各个事务之间不能互相干扰。 持续性（Durability）：也称永久性，指一个事务一旦提交，它对数据库中的数据的改变就应该是永久性的。接下来的其它操作或故障不应该对其执行结果有任何影响。 2.3 MySQL的四种隔离级别？ Read Uncommitted（读取未提交内容） 在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为脏读（Dirty Read）。 Read Committed（读取提交内容） 这是大多数数据库系统的默认隔离级别（但不是 MySQL 默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交的事务所做的改变。这种隔离级别也支持所谓的不可重复读（Nonrepeatable Read），因为同一事务的其他实例在该实例处理其间可能会有新的 commit，所以同一 select 可能返回不同结果。 Repeatable Read（可重读） 这是 MySQL 的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，这会导致另一个棘手的问题：幻读（Phantom Read）。 Serializable（可串行化） 通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。 隔离级别 脏读 不可重复读 幻影读 Read Uncommitted 有 有 有 Read Committed 无 有 有 Repeatable Read 无 无 有 Serializable 无 无 无 MySQL 默认采用的是 REPEATABLE-READ 隔离级别，Oracle 默认采用的是 READ-COMMITTED 隔离级别。 事务隔离机制的实现基于锁机制和并发调度。其中并发调度使用的是 MVVC（多版本并发控制），通过保存修改的旧版本信息来支持并发一致性读和回滚等特性。 因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是 READ-COMMITTED（读取提交内容），但是你要知道的是 InnoDB 存储引擎默认使用 REPEATABLE-READ（可重读）并不会有任何性能损失。 InnoDB 存储引擎在分布式事务的情况下一般会用到 SERIALIZABLE（可串行化）隔离级别。 2.4 什么是脏读、不可重复读与幻读？ 脏读：事务 A 读取了事务 B 更新的数据，然后 B 回滚操作，那么 A 读取到的数据是脏数据。 不可重复读：事务 A 多次读取同一数据，事务 B 在事务 A 多次读取的过程中，对数据作了更新并提交，导致事务 A 多次读取同一数据时，结果不一致。 幻读：系统管理员 A 将数据库中所有学生的成绩从具体分数改为 ABCDE 等级，但是系统管理员 B 就在这个时候插入了一条具体分数的记录，当系统管理员 A 改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。 总结：不可重复读侧重于修改，幻读侧重于新增或删除（多了或少了行），脏读是一个事务回滚影响另外一个事务。 2.5 事务的实现原理？ 事务是基于重做日志文件（redo log）和回滚日志（undo log）实现的。 每提交一个事务必须先将该事务的所有日志写入到重做日志文件进行持久化，数据库就可以通过重做日志来保证事务的原子性和持久性。 每当有修改事务时，还会产生 undo log，如果需要回滚，则根据 undo log 的反向语句进行逻辑操作，比如 insert 一条记录就 delete 一条记录。undo log 主要实现数据库的一致性。 2.6 介绍一下MySQL事务日志？ InnoDB 事务日志包括 redo log 和 undo log。 undo log 指事务开始之前，在操作任何数据之前，首先将需操作的数据备份到一个地方。redo log 指事务中操作的任何数据，将最新的数据备份到一个地方。 事务日志的目的：实例或者介质失败，事务日志文件就能派上用场。 redu log redo log 不是随着事务的提交才写入的，而是在事务的执行过程中，便开始写入 redo log 中。具体的落盘策略可以进行配置。防止在发生故障的时间点，尚有脏页未写入磁盘，在重启 MySQL 服务的时候，根据 redo log 进行重做，从而达到事务的未入磁盘数据进行持久化这一特性。redo log 是为了实现事务的持久性而出现的产物。 undo log undo log 用来回滚行记录到某个版本。事务未提交之前，undo log 保存了未提交之前的版本数据，undo log 中的数据可作为数据旧版本快照供其他并发事务进行快照读。是为了实现事务的原子性而出现的产物，在 MySQL InnoDB 存储引擎中用来实现多版本并发控制。 2.7 什么是MySQL的binlog？ MySQL 的 binlog 是记录所有数据库表结构变更（例如 CREATE、ALTER TABLE）以及表数据修改（例如 INSERT、UPDATE、DELETE）的二进制日志。binlog 不会记录 SELECT 和 SHOW 这类操作，因为这类操作对数据本身并没有修改，但你可以通过查询通用日志来查看 MySQL 执行过的所有语句。 MySQL binlog 以事件形式记录，还包含语句执行所消耗的时间，MySQL 的二进制日志是事务安全型的。binlog 的主要目的是复制和恢复。 binlog 有三种格式，各有优缺点： statement：基于 SQL 语句的模式，某些语句和函数如 UUID、LOAD DATA INFILE 等在复制过程中可能导致数据不一致甚至出错。 row：基于行的模式，记录的是行的变化，很安全。但是 binlog 会比其他两种模式大很多，在一些大表中清除大量数据时在 binlog 中会生成很多条语句，可能导致从库延迟变大。 mixed：混合模式，根据语句来选用是 statement 还是 row 模式。 2.8 在事务中可以混合使用存储引擎吗？ 尽量不要在同一个事务中使用多种存储引擎，MySQL 服务器层不管理事务，事务是由下层的存储引擎实现的。 如果在事务中混合使用了事务型和非事务型的表（例如 InnoDB 和 MyISAM 表），在正常提交的情况下不会有什么问题。 但如果该事务需要回滚，非事务型的表上的变更就无法撤销，这会导致数据库处于不一致的状态，这种情况很难修复，事务的最终结果将无法确定。所以，为每张表选择合适的存储引擎非常重要。 2.9 什么是MVCC？ MVCC，即多版木并发控制。MVCC 的实现，是通过保存数据在某个时间点的快照来实现的。根据事务开始的时间不同，每个事务对同一张表，同一时刻看到的数据可能是不一样的。 2.10 MVCC的实现 对于 InnoDB，聚簇索引记录中包含3个隐藏的列： ROW ID：隐藏的自增 ID，如果表没有主键，InnoDB 会自动按 ROW ID 产生一个聚集索引树。 事务 ID：记录最后一次修改该记录的事务 ID。 回滚指针：指向这条记录的上一个版本。 我们举个例子，假如现在有两个事务： 事务1：insert into t1(a, b) values (1, 1); 事务2：update t1 set b = 666 where a = 1; 如图，首先 insert 语句向表 t1 中插入了一条数据，a 字段为1，b 字段为1，ROW ID 也为1，事务 ID 假设为1，回滚指针假设为 null。当执行 update t1 set b = 666 where a = 1 时，大致步骤如下： 数据库会先对满足 a = 1 的行加排他锁； 然后将原记录复制到 undo 表空间中； 修改 b 字段的值为666，修改事务 ID 为2； 并通过隐藏的回滚指针指向 undo log 中的历史记录； 事务提交，释放前面对满足 a = 1 的行所加的排他锁。 因此可以总结出 MVCC 实现的原理大致是： InnoDB 每一行数据都有一个隐藏的回滚指针，用于指向该行修改前的最后一个历史版本，这个历史版本存放在 undo log 中。如果要执行更新操作，会将原记录放入 undo log 中，并通过隐藏的回滚指针指向 undo log 中的原记录。其它事务此时需要查询时，就是查询 undo log 中这行数据的最后一个历史版本。 MVCC 最大的好处是读不加锁，读写不冲突，极大地增加了 MySQL 的并发性。通过 MVCC，保证了事务 ACID 中的隔离性。 3. 锁 3.1 为什么要加锁？ 当多个用户并发地存取数据时，在数据库中就会产生多个事务同时存取同一数据的情况。若对并发操作不加控制就可能读取和存储不正确的数据，破坏数据库的一致性。因此需要加锁使得在多用户环境下保证数据库的完整性和一致性。 3.2 按照锁的粒度分数据库锁有哪些？ 在关系型数据库中，可以按照锁的粒度把数据库锁分为行级锁（InnoDB 引擎）、表级锁(MyISAM 引擎）和页级锁（BDB引擎）。 行级锁 行级锁是 MySQL 中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁和排他锁。 开销大，加锁慢，会出现死锁，锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 表级锁 表级锁是 MySQL 中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分 MySQL 引擎支持。最常使用的 MyISAM 与 InnoDB 都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。 开销小，加锁快，不会出现死锁，锁定粒度大，发生锁冲突的概率最高，并发度最低。 页级锁 页级锁是 MySQL 中锁定粒度介于行级锁和表级锁之间的一种锁。表级锁速度快，但冲突多，行级锁冲突少，但速度慢。所以取了折衷的页级锁，一次锁定相邻的一组记录。BDB 支持页级锁。 开销和加锁时间界于表锁和行锁之间，会出现死锁，锁定粒度界于表锁和行锁之间，并发度一般。 MyISAM 和 InnoDB 存储引擎使用的锁： MyISAM 采用表级锁（table-level locking）。 InnoDB 支持行级锁（row-level locking）和表级锁，默认为行级锁。 3.3 从锁的类别上分MySQL都有哪些锁呢？ 从锁的类别上来讲，有共享锁和排他锁。 共享锁：又叫做读锁，当用户要进行数据的读取时，对数据加上共享锁。共享锁可以同时加上多个。 排他锁：又叫做写锁，当用户要进行数据的写入时，对数据加上排他锁。排他锁只可以加一个，它和其它的排他锁，共享锁都相斥。 用上面的例子来说就是用户的行为有两种，一种是来看房，多个用户一起看房是可以接受的。一种是真正的入住一晚，在这期间，无论是想入住的还是想看房的都不可以。 锁的粒度取决于具体的存储引擎，InnoDB 实现了行级锁，页级锁，表级锁。他们的加锁开销从大到小，并发能力也是从大到小。 3.4 数据库的乐观锁和悲观锁是什么？怎么实现的？ 数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。乐观并发控制（乐观锁）和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。 悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。在查询完数据的时候就把事务锁起来，直到提交事务。实现方式：使用数据库中的锁机制。 乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。在修改数据的时候把事务锁起来，通过 version 的方式来进行锁定。实现方式：乐观锁一般会使用版本号机制或 CAS 算法实现。 两种锁的使用场景： 从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。 但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行 retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。 3.5 InnoDB引擎的行锁是怎么实现的？ InnoDB 是基于索引来完成行锁的。 例如：select * from tab_with_index where id = 1 for update; for update 可以根据条件来完成行锁锁定，并且 id 是有索引键的列，如果 id 不是索引键那么 InnoDB 将完成表锁，并发将无从谈起。 3.6 什么是死锁？怎么解决？ 死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方的资源，从而导致恶性循环的现象。常见的解决死锁的方法有： 如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。 在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率。 对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率。 如果业务处理不好可以用分布式事务锁或者使用乐观锁。 3.7 隔离级别与锁的关系？ 在 Read Uncommitted 级别下，读取数据不需要加共享锁，这样就不会跟被修改的数据上的排他锁冲突。 在 Read Committed 级别下，读操作需要加共享锁，但是在语句执行完以后释放共享锁。 在 Repeatable Read 级别下，读操作需要加共享锁，但是在事务提交之前并不释放共享锁，也就是必须等待事务执行完毕以后才释放共享锁。 SERIALIZABLE 是限制性最强的隔离级别，因为该级别锁定整个范围的键，并一直持有锁，直到事务完成。 3.8 优化锁方面的意见？ 使用较低的隔离级别。 设计索引，尽量使用索引去访问数据，加锁更加精确，从而减少锁冲突。 选择合理的事务大小，给记录显示加锁时，最好一次性请求足够级别的锁。例如，修改数据的话最好申请排他锁，而不是先申请共享锁，修改时再申请排他锁，这样会导致死锁。 不同的程序访问一组表的时候，应尽量约定一个相同的顺序访问各表，对于一个表而言，尽可能固定顺序地获取表中的行，这样将大大减少死锁的机会。 尽量使用相等条件访问数据，这样可以避免间隙锁对并发插入的影响。 不要申请超过实际需要的锁级别。 数据查询的时候不是必要，不要使用加锁。MySQL 的 MVCC 可以实现事务中的查询不用加锁，优化事务性能：MVCC 只在 Read Committed（读提交）和 Repeatable Read（可重复读）两种隔离级别。 对于特定的事务，可以使用表锁来提高处理速度或者减少死锁的可能。 4. 索引 4.1 索引是什么？ 索引是一种特殊的文件（InnoDB 数据表上的索引是表空间的一个组成部分），它们包含着对数据表里所有记录的引用指针。 索引是一种数据结构。数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据。索引的实现通常使用 B 树及其变种 B+ 树。更通俗地说，索引就相当于目录。为了方便查找书中的内容，通过对内容建立索引形成目录。而且索引是一个文件，它是要占据物理空间的。 MySQL 索引的建立对于 MySQL 的高效运行是很重要的，索引可以大大提高 MySQL 的检索速度。比如我们在查字典的时候，前面都有检索的拼音和偏旁、笔画等，然后找到对应字典页码，打开字典的页数就可以知道我们要搜索的某一个 key 的全部值的信息了。 4.2 索引有哪些优缺点？ 索引的优点： 可以大大加快数据的检索速度，这也是创建索引的最主要的原因。 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。 索引的缺点： 时间方面：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，会降低增删改的执行效率。 空间方面：索引需要占用物理空间。 4.3 MySQL有哪几种索引类型？ 从存储结构上来划分：BTree 索引（B-Tree 或 B+Tree 索引）、Hash 索引、full-index 全文索引、R-Tree 索引。这里所描述的是索引存储时保存的形式。 从应用层次来分：普通索引、唯一索引、复合索引。 普通索引：即一个索引只包含单个列，一个表可以有多个单列索引。 唯一索引：索引列的值必须唯一，但允许有空值。 复合索引：多列值组成一个索引，专门用于组合搜索，其效率大于索引合并。 聚簇索引（聚集索引）：并不是一种单独的索引类型，而是一种数据存储方式。具体细节取决于不同的实现，InnoDB 的聚簇索引其实就是在同一个结构中保存了 B-Tree 索引（技术上来说是 B+Tree）和数据行。 非聚簇索引：不是聚簇索引，就是非聚簇索引。 根据表中数据的物理顺序与键值的逻辑（索引）顺序关系：聚集索引，非聚集索引。 4.4 说一说索引的底层实现？ Hash 索引：基于哈希表实现，只有精确匹配索引所有列的查询才有效，对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码（hash code），并且 Hash 索引将所有的哈希码存储在索引中，同时在索引表中保存指向每个数据行的指针。 B-Tree 索引（MySQL 使用 B+Tree）：B-Tree 能加快数据的访问速度，因为存储引擎不再需要进行全表扫描来获取数据，数据分布在各个节点之中。 B+Tree 索引：B-Tree 的改进版本，同时也是数据库索引所采用的存储结构。数据都在叶子节点上，并且增加了顺序访问指针，每个叶子节点都指向相邻的叶子节点的地址。相比 B-Tree 来说，进行范围查找时只需要查找两个节点，进行遍历即可。而 B-Tree 需要获取所有节点，相比之下 B+Tree 效率更高。 B+Tree 性质： n 棵子树的节点包含 n 个关键字，不用来保存数据而是保存数据的索引。 所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身根据关键字的大小自小而大顺序链接。 所有的非终端结点可以看成是索引部分，结点中仅含其子树中的最大（或最小）关键字。 B+ 树中，数据对象的插入和删除仅在叶节点上进行。 B+ 树有2个头指针，一个是树的根节点，一个是最小关键码的叶节点。 4.5 为什么索引结构默认使用B+Tree，而不是B-Tree，Hash，二叉树，红黑树？ B-tree：从两个方面来回答： B+ 树的磁盘读写代价更低：B+ 树的内部节点并没有指向关键字具体信息的指针，因此其内部节点相对 B/B- 树更小，如果把所有同一内部节点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多，一次性读入内存的需要查找的关键字也就越多，相对 IO 读写次数就降低了。 由于 B+ 树的数据都存储在叶子结点中，分支结点均为索引，方便扫库，只需要扫一遍叶子结点即可，但是 B 树因为其分支结点同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序来扫，所以 B+ 树更加适合区间查询的情况，所以通常 B+ 树用于数据库索引。 Hash： 虽然可以快速定位，但是没有顺序，IO 复杂度高； 基于 Hash 表实现，只有 Memory 存储引擎显式支持哈希索引； 适合等值查询，如 =、in()、&lt;=&gt;，不支持范围查询； 因为不是按照索引值顺序存储的，就不能像 B+Tree 索引一样利用索引完成排序； Hash 索引在查询等值时非常快； 因为 Hash 索引始终索引所有列的全部内容，所以不支持部分索引列的匹配查找； 如果有大量重复键值的情况下，哈希索引的效率会很低，因为存在哈希碰撞问题。 二叉树：树的高度不均匀，不能自平衡，查找效率跟数据有关（树的高度），并且 IO 代价高。 红黑树：树的高度随着数据量增加而增加，IO 代价高。 4.6 讲一讲聚簇索引与非聚簇索引？ 在 InnoDB 里，索引 B+Tree 的叶子节点存储了整行数据为主键索引，也被称之为聚簇索引，即将数据存储与索引放到了一块，找到索引也就找到了数据。 而索引 B+Tree 的叶子节点存储了主键的值为非主键索引，也被称之为非聚簇索引、二级索引。 聚簇索引与非聚簇索引的区别： 非聚簇索引与聚簇索引的区别在于非聚簇索引的叶子节点不存储表中的数据，而是存储该列对应的主键（行号）。 对于 InnoDB 来说，想要查找数据我们还需要根据主键再去聚簇索引中进行查找，这个再根据聚簇索引查找数据的过程，我们称为回表。第一次索引一般是顺序 IO，回表的操作属于随机 IO。需要回表的次数越多，即随机 IO 次数越多，我们就越倾向于使用全表扫描。 通常情况下，主键索引（聚簇索引）查询只会查一次，而非主键索引（非聚簇索引）需要回表查询多次。当然，如果是覆盖索引的话，查一次即可。 注意：MyISAM 无论主键索引还是二级索引都是非聚簇索引，而 InnoDB 的主键索引是聚簇索引，二级索引是非聚簇索引。我们自己建立的索引基本都是非聚簇索引。 4.7 非聚簇索引一定会回表查询吗？ 不一定，这涉及到查询语句所要求的字段是否全部命中了索引，如果全部命中了索引，那么就不必再进行回表查询。一个索引包含（覆盖）所有需要查询字段的值，被称之为“覆盖索引”。举个简单的例子，假设我们在员工表的年龄上建立了索引，那么当进行 select score from stuaent where score &gt; 90 的查询时，在索引的叶子节点上，已经包含了 score 信息，不会再次进行回表查询。 4.8 联合索引是什么？为什么需要注意联合索引中的顺序？ MySQL 可以使用多个字段同时建立一个索引，叫做联合索引。在联合索引中，如果想要命中索引，需要按照建立索引时的字段顺序挨个使用，否则无法命中索引。具体原因为： MySQL 使用索引时需要索引有序，假设现在建立了 name, age, school 的联合索引，那么索引的排序为：先按照 name 排序，如果 name 相同，则按照 age 排序，如果 age 的值也相等，则按照 school 进行排序。 当进行查询时，此时索引仅仅按照 name 严格有序，因此必须首先使用 name 字段进行等值查询，之后对于匹配到的列而言，其按照 age 字段严格有序，此时可以使用 age 字段用做索引查找，以此类推。因此在建立联合索引的时候应该注意索引列的顺序，一般情况下，将查询需求频繁或者字段选择性高的列放在前面。此外可以根据特例的查询或者表结构进行单独的调整。 4.9 MySQL的最左前缀原则是什么？ 最左前缀原则就是最左优先，在创建多列索引时，要根据业务需求，where 子句中使用最频繁的一列放在最左边。MySQL 会一直向右匹配直到遇到范围查询（&gt;、&lt;、between、like）就停止匹配，比如：对于 a = 1 and b = 2 and c &gt; 3 and d = 4，如果建立 (a, b, c) 顺序的索引，d 是用不到索引的，如果建立 (a, b, d, c) 的索引则都可以用到，a, b, d 的顺序可以任意调整。= 和 in 可以乱序，比如 a = 1 and b = 2 and c = 3 建立 (a, b, c) 索引可以任意顺序，MySQL 的查询优化器会帮你优化成索引可以识别的形式。 4.10 前缀索引是什么？ 因为可能我们索引的字段非常长，这既占内存空间，也不利于维护。所以我们就想，如果只把很长字段的前面的公共部分作为一个索引，就会产生超级加倍的效果。但是，我们需要注意，order by 不支持前缀索引。 创建前缀索引的流程如下： 先计算完整列的选择性：select count(distinct col_1)/count(1) from table_1； 再计算不同前缀长度的选择性：select count(distinct left(col_1, 4))/count(1) from table_1； 找到最优长度之后，创建前缀索引：create index idx_front on table_1 (col_1(4))。 4.11 如何创建索引？ 创建索引有以下三种方式： （1）在执行 CREATE TABLE 时创建索引： （2）使用 ALTER TABLE 命令添加索引： ALTER TABLE 用来创建普通索引、UNIQUE 索引或 PRIMARY KEY 索引。 其中 table_name 是要增加索引的表名，column_list 指出对哪些列进行索引，如果索引多列则各列之间用逗号分隔。 索引名 index_name 可自己命名，缺省时，MySQL 将根据第一个索引列赋一个名称。另外，ALTER TABLE 允许在单个语句中更改多个表，因此可以同时创建多个索引。 （3）使用 CREATE INDEX 命令创建索引： 4.12 创建索引时需要注意什么？ 非空字段：应该指定列为 NOT NULL，除非你想存储 NULL。在 MySQL 中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。你应该用0或者一个特殊的值或者一个空串代替空值； 取值离散（变量各个取值之间的差异程度）大的字段的列放到联合索引的前面，可以通过 count() 函数查看字段的差异值，返回值越大说明字段的唯一值越多，字段的离散程度高； 索引字段越小越好：数据库的数据存储以页为单位，一页存储的数据越多则一次 I/O 操作获取的数据越多，效率越高。 "},{"title":"英语日常学习记录","date":"2022-12-02T02:58:00.000Z","url":"/posts/4115.html","tags":[["Others","/tags/Others/"]],"categories":[["Others","/categories/Others/"]],"content":" 记录日常积累的一些英语口语句子，日积月累不断进步！ 1. 短句 我不知道 I don’t know.（中性，根据语气判断凶不凶） Sorry, I have no idea. / I haven’t got a clue.（很地道） I’m afraid I don’t know.（更礼貌） How should/would I know? / Do I look like a walking encyclopedia?（不客气） 你会说英语吗 Can you …（你会吗 / 你介不介意 / 你是不是被允许） Can you cook?（你会做饭吗 / 可以你做饭吗 / 你能做饭吗） Can you do sth.? 可能太直接、不礼貌，比如 Can you be quiet? Do you do sth.? 像是问你平时做不做某事，如果平时做，那肯定说明是会的 Do you speak English? / Can you speak English? 都表示你会说英语吗，都可以用 我在外面 Outside 指的是在房间、建筑或某个地方的外面或附近，不能指出门办事、出门玩 I’m coming, I’m just outside.（我到门口了） I’ll wait for you outside.（我在门口附近等你） Let’s eat outside.（在室外吃饭） Eat out.（下馆子） She is out with her friends.（她和朋友出去玩） They are out working.（他们外出办事） 喝咖啡 喝一般得说 have 或 want Let’s have coffee. I want a coke. Do you want a drink? 如果说 grad a drink/food 一般是比较随意的、快的、看情况的这种感觉，意思就是如果你有时间，我们可以一起喝杯咖啡，如果你没时间那也没关系 Let’s grab some coffee. Holding my coffee.（拿着咖啡） 很喜欢 英式英语中不能说 Quite like，quite 一般表示有点儿，例如我有点累：I’m quite tired. 所以在英式英语中顺序是 Don’t like、Quite like、Like、Really like 而在美式英语中顺序是 Don’t like、Like、Quite like、Really like I love shopping.（我很喜欢购物） I’m a huge fan of British food.（我非常喜欢英国料理） I’m obsessed with American accents.（我非常喜欢美式口音） 我知道了 I see. / I get it. / I got it. / I understand. 表示以前不知道，现在才知道 Got it? / Get it? / Got it. 可以不加 ‘I’，但是 I got it. 一定要加 I I know. / Of course. / Yep.（嗯哼）/ That goes without saying. 表示已经知道了，不需要你告诉我 No shit, Sherlock. 阴阳怪气表达，这么明显还用你说？ 美女/帅哥 Beautiful girl. / Pretty girl. 指真正漂亮的女生 Excuse me. / Sorry. / Hi. + (miss, mate, bro) 表示称呼陌生人（如服务员）美女或者帅哥 Sorry mate, just to check…（不好意思帅哥，想问下…） 上车/下车 In the car. 在车上 Get in the car. 上车 Get out (of) the car/taxi. 下车/下出租（从坐着的姿势直接下车） Get off the bus/train/plane. 下公交/动车/飞机（先站起来再下车） Get off my car. 表示别碰我的车 Get down from the car. 表示别站在车顶/引擎盖上 纠结/很值 Which should I buy? I’m in two minds. 我应该买哪个？我很纠结 They’re both worth the money. 他们都很值（worthy 表示值得/配得上的人或事物，不能用于价格值不值） 近视度数 What’s your prescription? / How strong are your glasses? 你眼睛多少度？ This eye is minus two, and this eye is minus two point five. 这只眼睛200度，这只250度 I’m short-sighted not long-sighted. 我是近视眼，不是远视眼 I’m wearing contact lenses. 我戴隐形眼镜 吃什么容易长胖 This steak is very fatty. 这个牛排太肥了 It’s very fattening. 太容易长胖了 Fruit is good for me. 吃水果对身体好 称呼老师 日常一般不会在老师面前直接称呼 Teacher，只会在介绍自己的职业或是间接称呼别人会用 小学、初高中比较严格，一般说：Mr/Mrs/Miss/Ms + 姓氏，也可以单独说 Sir/Miss 在大学，教书的老师可以称呼为 Professor/Doctor + 姓氏 手机没电借充电宝 My phone is dead. 我的手机没电了 Can I use your power bank? 我能用一下你的充电宝吗？ Can I also borrow that charging cable? 我能再借一下那个充电线吗？ 2. 情景对话 Day1 Hey bro!（Hello 较正式，bro 更不正式，因此 Hello bro 很奇怪） I said hello to you, but you didn’t say hello back to me. Did your parents not teach you manners?（没有家教，该句特别 savage，即太过直接、犀利、甚至无理） What shall we eat?（别说 We eat what?） Anything is OK. / Anything will do. / Whatever you want. / I don’t mind. / You choose. / I’m easy.（表示都行，不能说 Both are OK，因为没有给选择） Hot pot.（火锅，别说成 Fire pot 了） Drink some beer? / Hot pot and beer? Definitely. / Great idea. / That sounds great to me.（必须的 / 好主意 / 我看行） Day2 Why are you late?（你怎么来的这么晚） Oh, my taxi was late. / There was a lot of traffic, so I was late.（出租车来晚了 / 堵车了） It’s dark now.（天黑了） What shall we eat? What do you wanna eat? / It’s your call. / You decide.（你想吃什么 / 你决定吧）"},{"title":"PyTorch深度学习入门(CIFAR10分类)","date":"2022-12-01T10:22:00.000Z","url":"/posts/48394.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 通过 CIFAR10 数据集的分类问题初入门 Deep Learning，也是开坑 AI 系列的第一篇文章。 相关环境的搭建可以转至：Anaconda与PyTorch安装教程。 1. 常用函数 （1）路径函数 在 os 模块中常用的路径相关函数有： os.listdir(path)：将 path 目录下的内容列成一个 list。 os.path.join(path1, path2)：拼接路径：path1\\path2。 例如： （2）辅助函数 dir()：不带参数时，返回当前范围内的变量、方法和定义的类型列表；带参数时，返回参数的属性、方法列表。 help(func)：查看函数 func 的使用说明。 例如： 2. 数据加载 2.1 Dataset 数据读取和预处理是进行机器学习的首要操作，PyTorch 提供了很多方法来完成数据的读取和预处理。 其中 Dataset 表示数据集，torch.utils.data.Dataset 是代表这一数据的抽象类。你可以自己定义你的数据类，继承和重写这个抽象类，非常简单，只需要定义 __len__ 和 __getitem__ 这个两个函数即可，例如： 通过上面的方式，可以定义我们需要的数据类，可以通过迭代的方式来获取每一个数据，但这样很难实现取 batch、shuffle 或者是多线程去读取数据。 2.2 DataLoader torch.utils.data.DataLoader 构建可迭代的数据装载器，我们在训练的时候，每一个 for 循环，每一次 iteration，就是从 DataLoader 中获取一个 batch_size 大小的数据的。打个比方如果 Dataset 是一副完整的扑克牌，那么 DataLoader 就是抽取几张组成的一部分扑克牌。 DataLoader 的参数很多，但我们常用的主要有以下几个： dataset：Dataset 类，决定从哪个数据集读取数据。 batch_size：批大小。 num_works：是否多进程读取机制。 shuffle：每个 Epoch 是否乱序。 drop_last：当样本数不能被 batch_size 整除时，是否舍弃最后一批数据。 要理解这个 drop_last，首先，得先理解 Epoch、Iteration 和 Batch_size 的概念： Epoch：所有训练样本都已输入到模型中，称为一个 Epoch。 Iteration：一批样本输入到模型中，称为一个 Iteration。 Batch_size：一批样本的大小，决定一个 Epoch 有多少个 Iteration。 DataLoader 的作用就是构建一个数据装载器，根据我们提供的 batch_size 的大小，将数据样本分成一个个的 Batch 去训练模型，而这个分的过程中需要把数据取到，这个就是借助 Dataset 的 __getitem__ 方法。 例如： 接下来使用 CIFAR10 数据集再展示一次 DataLoader 的用法： PS：部分看不懂的代码可以先去学后面的 transform 以及 tensorboard。 3. TensorBoard 3.1 add_scalar TensorBoard 原本是 TensorFlow 的可视化工具，PyTorch 从1.2.0开始支持 TensorBoard。之前的版本也可以使用 TensorBoardX 代替。 先进入 Anaconda 的 PyTorch 环境，安装 TensorBoard： 在项目根目录下新建一个文件夹 logs，TensorBoard 的工作流程简单来说是将代码运行过程中的，某些你关心的数据保存在这个文件夹中（由代码中的 writer 完成），再读取这个文件夹中的数据，用浏览器显示出来（在命令行运行 TensorBoard 完成）。 我们先绘制一个 y = x 的图像，运行以下代码： add_scalar 函数主要有三个参数： tag：数据标识符，可以理解为数据图像的标题。 scalar_value：保存的值，即纵轴上的值。 global_step：记录的步长，即横轴的值，一般会设置一个不断增加的 step。 运行后会看到 logs 文件夹下生成了一个文件，然后我们在 PyCharm 终端的 PyTorch 环境中打开 TensorBoard（要在当前项目中进入 PyTorch 环境，否则 --logdir 的路径就不能用相对路径了）： 打开  即可看到绘制的图像。 如果因为某些原因导致端口冲突可以指定端口： 3.2 add_image add_image 函数主要有三个参数： tag：同 add_scalar。 img_tensor：图像数据，类型必须是 torch.Tensor、numpy.ndarry 或 string/blobname。 global_step：同 add_scalar。 可以看到传入的图片数据有类型限制，目前还没学到 torch.Tensor 类型，以 numpy.ndarry 为例，因此我们需要先安装一下 NumPy，还是在 PyTorch 环境中安装： 使用 PIL 打开一个图像，将其转换成 NumPy 数组： 可以看到图片的形状是三维的数据，前两个数据分别表示高度和宽度，第三个数据表示通道数，可以记为 (H, W, C)，简写为 HWC。 add_image 函数传入图片时格式默认为 CHW，如果格式不匹配需要设定函数中的 dataformats 参数，例如： 运行后打开 TensorBoard 即可在 IMAGES 页面下看到图片。 4. Transform 4.1 Transform的概念与基本用法 transforms 在计算机视觉工具包 torchvision 下，包含了很多种对图像数据进行变换的类，这些都是在我们进行图像数据读入步骤中必不可少的，通过图像变换可以将图片变成不同的类型，或者可以通过旋转、裁切等手段对图像数据集的图像进行变换，起到扩充数据集与数据增强的作用。 transforms 主要使用的类为：transforms.ToTensor，该类能够将 PIL.Image 或者 ndarray 类型的数据转换为 tensor，并且归一化至 [0, 1]。注意归一化至 [0, 1] 是直接除以255，若自己的 ndarray 数据尺度有变化，则需要自行修改。 为什么需要 tensor 数据类型？因为它包装了反向传播神经网络所需要的一些基础的参数，因此在神经网络中需要将图片类型转换为 tensor 类型进行训练。 例如： 4.2 Transform的常用类 transforms.Compose：Compose 能够将多种变换组合在一起。例如下面的代码可以先将 PIL.Image 中心裁切，然后再转换成 tensor： transforms.CenterCrop：需要传入参数 size，表示以 (size, size) 的大小从中心裁剪，参数也可以为 (height, width)。例如： transforms.RandomCrop：需要传入参数 size，表示以 (size, size) 的大小随机裁剪，参数也可以为 (height, width)。 transforms.Normalize(mean, std)：对数据按通道进行标准化，即先减均值 mean，再除以标准差 std，注意是 HWC 格式，处理公式为：output[channel] = (input[channel] - mean[channel]) / std[channel]，例如： transforms.Resize：需要传入参数 (height, width) 和 interpolation，表示重置图像的分辨率为 (h, w)，也可以传入一个整数 size，这样会将较短的那条边缩放至 size，另一条边按原图大小等比例缩放。interpolation 为插值方法选择，默认为 PIL.Image.BILINEAR，例如： transforms.ToPILImage：：将 tensor 或者 ndarray 的数据转换为 PIL.Image 类型数据，参数 mode 默认为 None，表示1通道， mode=3 表示3通道，默认转换为 RGB，4通道默认转换为 RGBA。 5. Torchvision数据集使用方法 Torchvision 官方文档 Torchvision 中的 torchvision.datasets 就是 Torchvision 提供的标准数据集，其中有很多已经构建和训练好的网络模型，在不同的领域下各自有着很优秀的性能。 我们以 CIFAR10 为例，该数据集包括了60000张32*32像素的图像，总共有10个类别，每个类别有6000张图像，其中有50000张图像为训练图像，10000张为测试图像。其使用说明如下图所示： root：数据集存放的路径。 train：如果为 True，创建的数据集就为训练集，否则创建的数据集就为测试集。 transform：使用 transforms 中的变换操作对数据集进行变换。 target_transform：对 target 进行 transform。 download：如果为 True，就会自动从网上下载这个数据集，否则就不会下载。 例如： 刚开始运行时可以看到正在从网上下载数据集，如果下载速度非常慢可以复制链接去迅雷之类的地方下载，下载好后自己创建设定的路径，将数据集放过来即可。 然后设置断点，用 Debug 模式运行一下代码，我们可以查看一下数据集的内容，数据集 train_data 中的 classes 表示图像的种类，classes_to_idx 表示将种类映射为整数，targets 表示每张图像对应的种类编号，试着输出一下第一张图的信息： 现在展示如何使用 transform 参数，假设我们需要将数据集的图像都转换成 tensor 类型： 6. 神经网络Torch.NN基本骨架的使用 torch.nn 能够帮助我们更优雅地训练神经网络，使神经网络代码更加简洁和灵活。官方文档：Torch.NN。 在文档中可以看到第一块内容叫做 Container（容器），这就相当于神经网络的骨架，Container 之后的东西就用于往骨架里面填充，如 Convolution Layers（卷积层）、Pooling Layers（池化层），有卷积神经网络基础的小伙伴对这些词应该都很熟悉了。 Container 中有六个模块：Module、Sequential、ModuleList、ModuleDict、ParameterList、ParameterDict，其中最常用的为 Module，这是所有神经网络的最基本的类，其基本的构造方式如下： 现在我们尝试自己创建一个简单的神经网络，并输出前向传播的结果： 我们以 Conv2d 函数为例，该函数的官方文档：TORCH.NN.FUNCTIONAL.CONV2D。 该函数有以下几个参数： input：输入的图像，size 为 (mini_batch, in_channels, height, width)。 weight：卷积核的大小，size 为 (out_channels, in_channels/groups, height, width)。 bias：偏置，默认为 None。 stride：步长，用来控制卷积核移动间隔，如果为 x 则水平和竖直方向的步长都为 x，如果为 (x, y) 则竖直方向步长为 x，水平方向步长为 y。 padding：在输入图像的边沿进行扩边操作，以保证图像输入输出前后的尺寸大小不变，在 PyTorch 的卷积层定义中，默认的 padding 为零填充，即在边缘填充0。 padding_mode：扩边的方式。 dilation：设定了取数之间的间隔。 例如： 7. Convolution Layers与Pooling Layers 由于图像是二维的，因此基本上最常用到的就是二维的卷积层和池化层：torch.nn.Conv2d、torch.nn.MaxPool2d，官方文档：torch.nn.Conv2d、Pooling Layers。 7.1 Convolution Layers 卷积运算能够提取输入图像的不同特征，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。 torch.nn.Conv2d 的主要参数有以下几个： in_channels：输入图像的通道数，彩色图像一般都是三通道。 out_channels：通过卷积后产生的输出图像的通道数。 kernel_size：可以是一个数或一个元组，表示卷积核的大小，卷积核的参数是从数据的分布中采样得到的，这些数是多少无所谓，因为在神经网络训练的过程中就是对这些参数进行不断地调整。 stride：步长。 padding：填充。 padding_mode：填充模式，有 zeros、reflect、replicate、circular，默认为 zeros。 dilation：可以是一个数或一个元组，表示卷积核各个元素间的距离，也称空洞卷积。 group：一般设置为1，基本用不到。 bias：偏置，一般设置为 True。 例如以下代码构建了一个只有一层卷积层的神经网络，该卷积层的输入和输出通道数都为三通道，卷积核大小为3*3，步长为1，无填充，然后用 CIFAR10 测试数据集进行测试： 运行后可以打开 TensorBoard 查看一下效果。 7.2 Pooling Layers Pooling Layers 中的 MaxPool 表示最大池化，也称上采样；MaxUnpool 表示最小池化，也称下采样；AvgPool 表示平均池化。其中最常用的为 MaxPool2d，官方文档：torch.nn.MaxPool2d。 最大池化的目的是保留输入数据的特征，同时减小特征的数据量。 torch.nn.MaxPool2d 的主要参数有以下几个： kernel_size：用来取最大值的窗口（池化核）大小，和之前的卷积核类似。 stride：步长，注意默认值为 kernel_size。 padding：填充，和 Conv2d 一样。 dilation：池化核中各个元素间的距离，和 Conv2d 一样。 return_indices：如果为 True，表示返回值中包含最大值位置的索引。注意这个最大值指的是在所有窗口中产生的最大值，如果窗口产生的最大值总共有5个，就会有5个返回值。 ceil_mode：如果为 True，表示在计算输出结果形状的时候，使用向上取整，否则默认向下取整。 输出图像的形状的计算公式可以在官方文档中查看。 接下来我们用代码实现这个池化层： 我们用图像来试试效果： 运行后可以打开 TensorBoard 查看一下效果。 8. Non-linear Activations与Linear Layers 8.1 Non-linear Activations 非线性激活的目的是为了在网络中引入一些非线性特征，因为非线性特征越多才能训练出符合各种曲线（特征）的模型。 非线性激活函数官方文档：Non-linear Activations。 有深度学习基础的同学应该知道最常用的非线性激活函数就是 ReLU 和 Sigmoid 函数，多分类问题会在输出层使用 Softmax 函数（如果损失函数使用的是交叉熵误差函数 CrossEntropyLoss 则会自动计算 Softmax，无需创建 Softmax 层）。这三个函数在 PyTorch 中分别为 nn.ReLU、nn.Sigmoid 和 nn.Softmax。 这两个函数的输入都是只需指明 batch_size 即可，在 PyTorch1.0 之后的版本任何形状的数据都能被计算，无需指定 batch_size。 nn.ReLU 只有一个需要设置的参数 inplace，如果为 True 表示计算结果直接替换到输入数据上，例如： 构建 ReLU 层代码如下： 由于 ReLU 对图像处理的直观效果不明显，我们使用 Sigmoid 对图像进行处理： 8.2 Linear Layers 线性层官方文档：Linear Layers。 PyTorch 的 nn.Linear 是用于设置网络中的全连接层的，需要注意的是全连接层的输入与输出都是二维张量，一般形状为：[batch_size, size]，不同于卷积层要求输入输出是四维张量，因此在将图像传入全连接层之前一般都会展开成一维的。 nn.Linear 有三个参数分别如下： in_features：指的是输入的二维张量的大小，即输入的 [batch_size, size] 中的 size。 out_features：指的是输出的二维张量的大小，即输出的二维张量的形状为 [batch_size, output_size]，当然，它也代表了该全连接层的神经元个数。从输入输出的张量的 shape 角度来理解，相当于一个输入为 [batch_size, in_features] 的张量变换成了 [batch_size, out_features] 的输出张量。 bias：偏置，相当于 y = ax + b 中的 b。 代码示例如下： 9. 神经网络模型搭建小实战 9.1 Sequential torch.nn.Sequential 是一个 Sequential 容器，能够在容器中嵌套各种实现神经网络中具体功能相关的类，来完成对神经网络模型的搭建。模块的加入一般有两种方式，一种是直接嵌套，另一种是以 OrderedDict 有序字典的方式进行传入，这两种方式的唯一区别是： 使用 OrderedDict 搭建的模型的每个模块都有我们自定义的名字。 直接嵌套默认使用从零开始的数字序列作为每个模块的名字。 （1）直接嵌套方法的代码如下： （2）使用 OrderedDict 的代码如下： 9.2 小实战 由于代码很简单，都是学过的内容进行组装，因此直接看代码： 使用 add_graph 函数可以在 TensorBoard 中生成神经网络的计算图，通过计算图可以很清晰地看到每一层计算时数据流入流出的结果，双击相应的标签可以进一步深入查看更详细的信息。 10. 损失函数与反向传播 10.1 Loss Functions 具有深度学习理论基础的同学对损失函数和反向传播一定不陌生，在此不详细展开理论介绍。损失函数是指用于计算标签值和预测值之间差异的函数，在机器学习过程中，有多种损失函数可供选择，典型的有距离向量，绝对值向量等。使用损失函数的流程概括如下： 计算实际输出和目标之间的差距。 为我们更新输出提供一定的依据（反向传播）。 损失函数的官方文档：Loss Functions。 （1）nn.L1Loss：平均绝对误差（MAE，Mean Absolute Error），计算方法很简单，取预测值和真实值的绝对误差的平均数即可。 PyTorch1.13中 nn.L1Loss 数据形状规定如下： Input：(*)，means any number of dimensions. Target：(*)，same shape as the input. Output：scalar. If reduction is none, then (*), same shape as the input. 早先的版本需要指定 batch_size 大小，现在不需要了。可以设置参数 reduction，默认为 mean，即取平均值，也可以设置为 sum，顾名思义就是取和。 测试代码如下： （2）nn.MSELoss：均方误差（MSE，Mean Squared Error），即预测值和真实值之差的平方和的平均数。 该损失函数的用法与 nn.L1Loss 相似，代码如下： （3）nn.CrossEntropyLoss：交叉熵误差，训练分类 C 个类别的模型的时候较常用这个损失函数，一般用在 Softmax 层后面，计算公式较为复杂，可以在官网中查看。 测试代码如下： 10.2 Backward 接下来以 CIFAR10 数据集为例，用上一节搭建的神经网络先设置 batch_size 为1，看一下输出结果： 现在我们来尝试解决第二个问题，即损失函数如何为我们更新输出提供一定的依据（反向传播）。 例如对于卷积层来说，其中卷积核中的每个参数就是我们需要调整的，每个参数具有一个属性 grad 表示梯度，反向传播时每一个要更新的参数都会求出对应的梯度，在优化的过程中就可以根据这个梯度对参数进行优化，最终达到降低损失函数值的目的。 PyTorch 中对损失函数计算出的结果使用 backward 函数即可计算出梯度： 我们在计算反向传播之前设置断点，然后可以在 PyCharm 下方的变量区域通过目录 network/model/Protected Attributes/_modules/'0'/weight/grad 查看到某一层参数的梯度，在反向传播之前为 None，执行反向传播的代码后可以看到 grad 处有数值了。 我们有了各个节点参数的梯度，接下来就可以选用一个合适的优化器，来对这些参数进行优化。 10.3 Optimizer 优化器 torch.optim 的官方文档：TORCH.OPTIM。 优化器主要是在模型训练阶段对模型的可学习参数进行更新，常用优化器有：SGD、RMSprop、Adam等。优化器初始化时传入传入模型的可学习参数，以及其他超参数如 lr、momentum 等，例如： 在训练过程中先调用 optimizer.zero_grad() 清空梯度，再调用 loss.backward() 反向传播，最后调用 optimizer.step() 更新模型参数，例如： 接下来我们来训练20轮神经网络，看看损失函数值的变化： 可以看到每一轮所有 batch 的损失函数值的总和确实在不断降低了。 11. 现有网络模型的使用及修改 11.1 VGG16模型的使用 我们以 VGG16 为例，该网络模型是用于大规模图像识别的超深度卷积神经网络，官方文档：VGG16。 该网络模型主要有以下参数： weights：可以设置成 torchvision.models.VGG16_Weights.DEFAULT，DEFAULT 表示自动使用最新的数据。老版本为 pretrained，如果为 True，表示使用预先训练好的权重，在官网可以看到这个权重是在 ImageNet-1K 数据集训练的，默认为不使用预先训练好的权重。 progress：如果为 True，则显示下载的进度条，默认为 True。 注意，下载网络时默认的下载路径是 C:\\Users\\&lt;username&gt;\\.cache，因此在下载模型前，我们需要修改路径：打开 D:\\Anaconda3_Environments\\envs\\PyTorch\\Lib\\site-packages\\torch 中的 hub.py 文件，搜索 load_state_dict_from_url，然后修改 model_dir 即可： 然后我们输出一下这个网络模型： 可以看到这个模型的分类结果为1000类，那么假如我们需要分类 CIFAR10 该如何应用这个网络模型呢？一种方法就是直接将最后一层 Linear 的 out_features 改为10，还有一种方法就是再添加一层 in_features=1000, out_features=10 的 Linear： 可以看到效果是比之前自己构建的网络模型好很多的。 11.2 模型的保存与读取 我们在对某些模型进行修改后可能想将其保存下来，方便以后用到时无需再构建一遍网络，可以按以下的方式将整个模型保存到路径 models/CIFAR10_VGG16.pth： 其对应的加载模型的方式为： 还有一种保存方式是将模型中的参数保存成字典的形式，官方建议使用该方式： 其对应的加载模型的方式为： 注意如果是保存自己构建的网络模型，需要在模型的类的源代码中将该类导入进来，例如在 test_save.py 中用以下代码保存自己的网络： 在 test_load.py 中导入时需要这样写： 12. 完整训练模型的方法 12.1 训练模型时的注意事项 （1）通常我们会将超参数的设置放在一起，使代码更加直观且方便修改： （2）我们在每一轮 epoch 中会先对训练集进行训练，然后使用测试集进行正确率的测试，因此一般我们会记录总共训练的次数 total_train_step 以及总共测试的次数 total_test_step，方便后续绘图使用。 （3）在开始训练之前一般需要将模型设置成训练状态，在测试之前需要设置成评估状态，这两种状态会影响少部分的层例如 Dropout 和 BatchNorm： （4）在分类问题中计算准确率一般用以下方法： （5）测试时不能对模型进行任何干扰，即在测试的时候神经网络不能产生梯度，因此在每次测试前需要加上以下代码： 12.2 使用GPU进行训练 前提：电脑有 NVIDIA 显卡，配置好了 CUDA，可以使用 torch.cuda.is_available() 来检查 CUDA 是否可用。 使用 GPU 训练的时候，需要将 Module 对象和 Tensor 类型的数据转移到 GPU 上进行计算，一般来说即为将网络模型、数据、损失函数放到 GPU 上计算。 使用 GPU 训练的方式有两种，第一种是使用 cuda() 函数，例如： 另一种是使用 to(device)，device 就是我们选择用来训练模型的设备，该方式与 cuda() 有一点细微的差别如下： 对于 Tensor 类型的数据（图像、标签等），使用 to(device) 之后，需要接收返回值，返回值才是正确设置了 device 的 Tensor。 对于 Module 对象（网络模型、损失函数），只用调用 to(device) 就可以将模型设置为指定的 device，不必接收返回值，当然接收返回值也是可以的。 例如： 注意如果加载在 GPU 上训练好的模型，然后想在 CPU 上使用，需要映射回 CPU： 12.3 CIFAR10_Net_Simple_v3 最后放上经过自己调参达到88%左右的正确率的模型和训练代码吧： 至此已经成功入门 PyTorch 啦！可以正式进入 Deep Learning 的学习啦！"},{"title":"Kratos-Rebirth主题修改部分细节教程","date":"2022-12-01T04:30:00.000Z","url":"/posts/9012.html","tags":[["Hexo","/tags/Hexo/"]],"categories":[["Hexo","/categories/Hexo/"]],"content":" 记录一下自己在使用 Kratos-Rebirth 过程中的一些样式微调。 1. 部分文本的修改 1.1 主页标题 在主题目录中的 source/css/kratosr.min.css 文件（之后也是在这个文件）中找到 .kratos-cover .desc h2，改成以下内容： 1.2 主页副标题 找到 kratos-cover .desc p,.kratos-cover .desc span，改成以下内容： 1.3 主页文章标题 找到 .kratos-entry-header a,.kratos-entry-header span，改成以下内容： 1.4 进入文章页面时的标题 找到 .kratos-entry-title，改成以下内容： 2. 主页主体部分宽度 找到 @media (min-width:1200px)，其后的 .container&#123;width:xx&#125; 即为主体部分宽度，改成以下内容： 3. 代码块 在主题目录中的 source/css/highlight/light.min.css 文件中找到 figure.highlight&#123;，将代码块顶部横条部分背景改成以下内容： 找到 figure.highlight .code&#123;，将代码块内容部分背景改成以下内容： 找到 figure.highlight .gutter pre，将代码块左侧代码行部分背景改成以下内容： "},{"title":"Anaconda与PyTorch安装教程","date":"2022-11-25T13:56:00.000Z","url":"/posts/15428.html","tags":[["Others","/tags/Others/"]],"categories":[["Others","/categories/Others/"]],"content":" 搭建 PyTorch 环境属实不容易，折腾了一整天，记录一下踩雷后的总结吧。 1. Anaconda的安装与命令介绍 1.1 安装Anaconda 首先前往 Anaconda 官网：Anaconda，下载安装文件。本文下载的为 Windows Python3.9 版本。 安装时没有需要特别注意的，设置好相应的安装路径即可，本文安装路径为：D:\\Anaconda3。 安装好后打开开始菜单能看到启动项：Anaconda Prompt，打开后如果看到命令行最左侧有 (base) 标识说明安装成功，可以查看版本号： 在 Anaconda 中我们会创建很多环境，那么我们需要先设置环境创建的路径，默认是在 C:\\Users\\XXX\\.conda 下的。 先在想要存放的地方创建文件夹 Anaconda3_Environments，本文创建在 D 盘，然后在该文件夹中再创两个文件夹：D:\\Anaconda3_Environments\\envs 和 D:\\Anaconda3_Environments\\pkgs。 在开始菜单打开 Anaconda Navigator，点击左上角的 File-Preferences-Configure Conda，修改为以下信息，然后保存即可： 1.2 Anaconda常用命令 创建名为 PyTorch 的环境，Python 版本为3.9： 删除名为 PyTorch 的环境（注意删除环境时要在 base 环境下，别在要删除的环境下）： 查看当前的所有环境： 激活 PyTorch 环境： 退出环境： 更新 conda 及 Anaconda： 如果下载速度很慢，例如下载 PyTorch 时，可以先按以下命令的方式修改镜像源： 查看相关镜像源： 将镜像源恢复成默认设置： 2. PyTorch的安装与配置 2.1 安装PyTorch 首先查看本机的 CUDA 版本，CUDA Version 即为版本号，本文的版本号为11.6： 前往 PyTorch 官网：PyTorch，在 Get Started 中选择好相应的选项：Stable、Windows、Conda、Python、CUDA 11.6，然后会生成一条安装命令（注意如果想在自己电脑上跑通代码，就选 CUDA，如果不需要在自己电脑上跑，而是在服务器上跑，或者没有独立显卡，就选 CPU。独立显卡需要 NVIDIA 显卡。这里我们一定要选择和自己版本相同或更低的 CUDA）。 本文使用离线与在线相结合的方式进行安装，也可以直接使用官网的命令安装但是速度很慢，或者修改镜像源后再安装。 前往清华大学镜像源：清华大学 PyTorch 镜像源，手动下载 pytorch、torchvision 以及 torchaudio。注意版本号要对应，本文下载的为：pytorch-1.13.0-py3.9_cuda11.6_cudnn8_0.tar.bz2、torchvision-0.14.0-py39_cu116.tar.bz2、torchaudio-0.13.0-py39_cu116.tar.bz2。py 和 cu 后面的数字分别表示 Python 和 CUDA 的版本号，找到对应版本进行下载即可。 下载好后进入 Anaconda 环境进行离线安装，本文下载路径为 D 盘根目录，在 PyTorch 环境中安装： 前往 CUDA Toolkit Archive 下载对应版本的 CUDA 套件（注意如果电脑是 Win10，Version 需要选10）。 下载好后打开程序安装 NVIDIA GPU Computing Toolkit，安装时路径需要使用默认的，即 C:\\Program Files\\NVIDIA GPU Computing Toolkit 以及 C:\\Program Files\\NVIDIA Corporation。 安装好后打开命令行检查版本： 进入 PyTorch 环境，安装剩余的包，此处还需要等待一段时间，但是最大的包已经离线安装了所以会快很多： 打开 Python，输入以下内容进行测试，没有报错即安装成功： 由于安装好 Anaconda 后顺带装了 Jupyter，但是他默认是装在 base 环境中的，因此我们还需要进入 PyTorch 环境中安装相应的包： 安装好后输入以下命令打开 Jupyter： 如果在 D 盘启动需要加上路径： 2.2 PyCharm配置PyTorch 在 PyCharm 中设置 Python 解释器，在 Conda 环境中选择现有环境，解释器选择：D:\\Anaconda3_Environments\\envs\\PyTorch\\python.exe，Conda 可执行文件选择：D:\\Anaconda3\\Scripts\\conda.exe。 设置好后即可在 Python 解释器选择菜单中找到 Python 3.9 (PyTorch) 选项，应用该环境后可以在底部导航栏打开 Python 控制台，用之前测试过的代码进行测试： 接下来需要修改 PyCharm 的终端，使其打开不是 Windows 默认的终端而是 Anaconda 的终端。先找到开始菜单中 Anaconda Prompt 的文件路径，然后查看属性，目标中有一段内容为：%windir%\\System32\\cmd.exe &quot;/K&quot; D:\\Anaconda3\\Scripts\\activate.bat D:\\Anaconda3。 将目标中的路径从 cmd.exe 开始之后的内容复制下来，进入 PyCharm，在文件-设置-工具-终端中修改 Shell 路径： 然后打开终端即可找到熟悉的感觉，即出现了 (base)。"},{"title":"计算机网络面试知识点总结","date":"2022-11-24T03:14:00.000Z","url":"/posts/54431.html","tags":[["Network","/tags/Network/"]],"categories":[["Network","/categories/Network/"]],"content":" 计算机网络常见面试题总结，文章将不断更新。 1. 概述 1.1 计算机网络的各层协议及作用？ 计算机网络体系可以大致分为三种：OSI 七层模型、TCP/IP 四层模型和五层模型。 OSI 七层模型：大而全，但是比较复杂、而且是先有了理论模型，没有实际应用。 TCP/IP 四层模型：是由实际应用发展总结出来的，从实质上讲，TCP/IP 只有最上面两层，最下面一层没有什么具体内容，TCP/IP 参考模型没有真正描述这一层的实现。 TCP/IP 五层模型：五层模型只出现在计算机网络教学过程中，这是对七层模型和四层模型的一个折中，既简洁又能将概念阐述清楚。 七层网络体系结构各层的主要功能： 应用层：为应用程序提供交互服务。在互联网中的应用层协议有很多，如域名系统 DNS，支持万维网应用的 HTTP 协议，支持电子邮件的 SMTP 协议等。 表示层：主要负责数据格式的转换，如加密解密、转换翻译、压缩解压缩等。 会话层：负责在网络中的两节点之间建立、维持和终止通信，如服务器验证用户登录便是由会话层完成的。 运输层：有时也译为传输层，向主机进程提供通用的数据传输服务。该层主要有以下两种协议： TCP：提供面向连接的、可靠的数据传输服务。 UDP：提供无连接的、尽最大努力的数据传输服务，但不保证数据传输的可靠性。 网络层：选择合适的路由和交换结点，确保数据及时传送。主要包括 IP 协议。 数据链路层：数据链路层通常简称为链路层。将网络层传下来的 IP 数据包组装成帧，并在相邻节点的链路上传送帧。 物理层：实现相邻节点间比特流的透明传输，尽可能屏蔽传输介质和通信手段的差异。 2. TCP/IP 2.1 TCP和UDP的区别？ TCP UDP 是否连接 面向连接 无连接 是否可靠 可靠传输，使用流量控制和拥塞控制 不可靠传输，不使用流量控制和拥塞控制 是否有序 有序，消息在传输过程中可能会乱序，TCP 会重新排序 无序 传输速度 慢 快 连接对象个数 只能一对一通信 支持一对一、一对多、多对一和多对多交互通信 传输方式 面向字节流 面向报文 首部开销 首部开销大，最小20字节，最大60字节 首部开销小，仅8字节 适用场景 适用于要求可靠传输的应用，例如文件传输 适用于实时应用例如 IP 电话、视频会议、直播等 总结：TCP 用于在传输层有必要实现可靠传输的情况，UDP 用于对高速传输和实时性有较高要求的通信。TCP 和 UDP 应该根据应用目的按需使用。 2.2 TCP和UDP对应的应用场景是什么？ TCP 是面向连接的，能保证数据的可靠性交付，因此经常用于： FTP 文件传输。 HTTP/HTTPS。 SMTP 简单邮件传输。 UDP 是无连接的，它可以随时发送数据，再加上 UDP 本身的处理既简单又高效，因此经常用于： 包总量较少的通信，如 DNS、SNMP 等。 视频、音频等多媒体通信。 广播通信。 2.3 TCP的三次握手机制？ 第一次握手：客户端请求建立连接，向服务端发送一个同步报文（SYN = 1），同时选择一个随机数 seq = x 作为初始序列号，并进入 SYN_SENT（同步已发送）状态，等待服务器确认。 第二次握手：服务端收到连接请求报文后，如果同意建立连接，则向客户端发送同步确认报文（SYN = 1, ACK = 1），确认号为 ack = x + 1，同时选择一个随机数 seq = y 作为初始序列号，此时服务器进入 SYN_RECV（同步收到）状态。 第三次握手：客户端收到服务端的确认后，向服务端发送一个确认报文（ACK = 1），确认号为 ack = y + 1，序列号为 seq = x + 1，客户端和服务器进入 ESTABLISHED（已建立连接）状态，完成三次握手。 理想状态下，TCP 连接一旦建立，在通信双方中的任何一方主动关闭连接之前，TCP 连接都将被一直保持下去。 2.4 为什么需要三次握手，而不是两次？ 主要有三个原因： 防止已过期的连接请求报文突然又传送到服务器，因而产生错误和资源浪费。 在双方两次握手即可建立连接的情况下，假设客户端发送报文段A请求建立连接，由于网络原因造成A暂时无法到达服务器，服务器接收不到请求报文段就不会返回确认报文段。 客户端在长时间得不到应答的情况下重新发送请求报文段B，这次B顺利到达服务器，服务器随即返回确认报文并进入 ESTABLISHED 状态，客户端在收到确认报文后也进入 ESTABLISHED 状态，双方建立连接并传输数据，之后正常断开连接。 此时姗姗来迟的报文段A才到达服务器，服务器随即返回确认报文并进入 ESTABLISHED 状态，但是已经进入 CLOSED 状态的客户端无法再接受确认报文段，更无法进入 ESTABLISHED 状态，这将导致服务器长时间单方面等待，造成资源浪费。 三次握手才能让双方均确认自己和对方的发送和接收能力都正常。 第一次握手：客户端只是发送处请求报文段，什么都无法确认，而服务器可以确认自己的接收能力和对方的发送能力正常。 第二次握手：客户端可以确认自己发送能力和接收能力正常，对方发送能力和接收能力正常。 第三次握手：服务器可以确认自己发送能力和接收能力正常，对方发送能力和接收能力正常。 可见三次握手才能让双方都确认自己和对方的发送和接收能力全部正常，这样就可以愉快地进行通信了。 告知对方自己的初始序号值，并确认收到对方的初始序号值。 TCP 实现了可靠的数据传输，原因之一就是 TCP 报文段中维护了序号字段和确认序号字段，通过这两个字段双方都可以知道在自己发出的数据中，哪些是已经被对方确认接收的。这两个字段的值会在初始序号值的基础上递增，如果是两次握手，只有发起方的初始序号可以得到确认，而另一方的初始序号则得不到确认。 2.5 为什么需要三次握手，而不是四次？ 因为三次握手已经可以确认双方的发送和接收能力正常，双方都知道彼此已经准备好，而且也可以完成对双方初始序号值的确认，也就无需第四次握手了。 第一次握手：服务端确认自己收、对方发报文功能正常。 第二次握手：客户端确认自己发、自己收、对方收、对方发报文功能正常，客户端认为连接己建立。 第三次握手：服务端确认自己发、对方收报文功能正常，此时双方均建立连接，可以正常通信。 2.6 什么是SYN洪泛攻击？如何防范？ SYN 洪泛攻击属于 DOS 攻击的一种，它利用 TCP 协议缺陷，通过发送大量的半连接请求，耗费 CPU 和内存资源。 原理： 在三次握手过程中，服务器发送 [SYN/ACK] 包（即第二个包）之后、收到客户端的 [ACK] 包（即第三个包）之前的 TCP 连接称为半连接（half-open connect），此时服务器处于 SYN_RECV（等待客户端响应）状态。如果接收到客户端的 [ACK]，则 TCP 连接成功，如果未接收到，则会不断重发请求直至成功。 SYN 攻击的攻击者在短时间内伪造大量不存在的 IP 地址，向服务器不断地发送 [SYN] 包，服务器回复 [SYN/ACK] 包，并等待客户的确认。由于源地址是不存在的，服务器需要不断的重发直至超时。 这些伪造的 [SYN] 包将长时间占用未连接队列，影响了正常的 SYN，导致目标系统运行缓慢、网络堵塞甚至系统瘫痪。 检测：当在服务器上看到大量的半连接状态时，特别是源 IP 地址是随机的，基本上可以断定这是一次 SYN 攻击。 防范： 通过防火墙、路由器等过滤网关防护。 通过加固 TCP/IP 协议栈防范，如增加最大半连接数，缩短超时时间。 SYN Cookies 技术。SYN Cookies 是对 TCP 服务器端的三次握手做一些修改，专门用来防范 SYN 洪泛攻击的一种手段。 2.7 三次握手连接阶段，如果最后一次ACK包丢失，会发生什么？ 服务端： 第三次的 ACK 包在网络中丢失，那么服务端该 TCP 连接的状态为 SYN_RECV,并且会根据 TCP 的超时重传机制，会等待3秒、6秒、12秒后重新发送 SYN + ACK 包，以便客户端重新发送 ACK 包。 如果重发指定次数之后，仍然未收到客户端的 ACK 应答，那么一段时间后，服务端自动关闭这个连接。 客户端： 客户端认为这个连接已经建立，如果客户端向服务端发送数据，服务端将以 RST 包（Reset，表示复位，用于异常的关闭连接）响应。此时，客户端便知道第三次握手失败。 2.8 TCP的四次挥手过程？ 第一次挥手：客户端向服务端发送连接释放报文（FIN = 1, ACK = 1），主动关闭连接，同时等待服务端的确认，客户端进入 FIN_WAIT_1（终止等待1）状态。序列号 seq = u，为客户端上次发送的报文的最后一个字节的序号 + 1。 第二次挥手：服务端收到连接释放报文后，立即发出确认报文（ACK = 1），序列号 seq = v，为服务端上次发送的报文的最后一个字节的序号 + 1，确认号 ack = u + 1，服务端进入 CLOSE_WAIT（关闭等待）状态。 此时 TCP 连接处于半关闭状态，即客户端到服务端的连接已经释放了，但是服务端到客户端的连接还未释放。这表示客户端已经没有数据发送了，但是服务端可能还要给客户端发送数据。 第三次挥手：客户端收到服务端的确认后进入 FIN_WAIT_2（终止等待2）状态，等待服务端发出连接释放报文段。服务端向客户端发送连接释放报文（FIN = 1, ACK = 1），主动关闭连接，同时等待A的确认，服务端进入 LAST_ACK（最后确认）状态。 序列号 seq = w，即服务端上次发送的报文的最后一个字节的序号 + 1，可能在半关闭状态服务端又发送了一些数据。 确认号 ack = u + 1，与第二次挥手相同，因为这段时间客户端没有发送数据。 第四次挥手：客户端收到服务端的连接释放报文后，立即发出确认报文（ACK = 1），序列号 seq = u + 1，确认号为 ack = w + 1。 此时，客户端就进入了 TIME_WAIT（时间等待）状态。注意此时客户端到 TCP 连接还没有释放，必须经过2 * MSL（最长报文段寿命）的时间后，才进入 CLOSED 状态。而服务端只要收到客户端发出的确认，就立即进入 CLOSED 状态。可以看到，服务端结束 TCP 连接的时间要比客户端早一些。 2.9 为什么连接的时候是三次握手，关闭的时候却是四次握手？ 服务器在收到客户端的 FIN 报文段后，可能还有一些数据要传输，所以不能马上关闭连接，但是会做出应答，返回 ACK 报文段. 接下来可能会继续发送数据，在数据发送完后，服务器会向客户单发送 FIN 报文，表示数据已经发送完毕，请求关团连接。服务器的 ACK 和 FIN 一般都会分开发送，从而导致多了一次，因此一共需要四次挥手。 2.10 为什么客户端的TIME_WAIT状态必须等待2MSL？ 主要有两个原因: 确保最后一个 ACK 报文段能够到达服务端，从而使服务端正常关闭连接。 第四次挥手时，客户端第四次挥手的 ACK 报文段不一定会到达服务端。服务端会超时重传 FIN/ACK 报文段，此时如果客户端已经断开了连接，那么就无法响应服务端的二次请求，这样服务端迟迟收不到 FIN/ACK 报文段的确认，就无法正常断开连接。 MSL 是报文段在网络上存活的最长时间。客户端等待 2MSL 时间，即：客户端 ACK 报文段 1MSL 超时 + 服务端 FIN 报文段 1MSL 传输，就能够收到服务端重传的 FIN/ACK 报文段，然后客户端重传一次 ACK 报文段，并重新启动 2MSL 计时器。如此保证服务端能够正常关闭。 如果服务端重发的 FIN 报文段没有成功地在 2MSL 时间里传给客户端，服务端则会继续超时重试直到断开连接。 防止已失效的连接请求报文段出现在之后的连接中。 TCP 要求在 2MSL 内不使用相同的序列号。客户端在发送完最后一个 ACK 报文段后，再经过时间 2MSL，就可以保证本连接持续的时间内产生的所有报文段都从网络中消失。这样就可以使下一个连接中不会出现这种旧的连接请求报文段。或者即使收到这些过时的报文，也可以不处理它。 2.11 如果已经建立了连接，但是客户端出现故障了怎么办？ 通过定时器 + 超时重试机制，尝试获取确认，直到最后会自动断开连接。 具体而言，TCP 设有一个保活计时器。服务器每收到一次客户端的数据，都会重新设置这个计时器，时间通常是设置为2小时。若2小时还没有收到客户端的任何数据，服务器就发送一个探测报文段，之后则每隔75秒钟发送一次，若一连发送10个探测报文段后客户端依然没有响应，那么服务器就认为客户端出现故障，接着就关闭这个连接。 2.12 TIME_WAIT是服务器端还是客户端的状态? TIME_WAIT 是主动断开连接的一方会进入的状态，一般情况下，都是客户端所处的状态，服务器端一般设置不主动关闭连接。 TIME_WAIT 需要等待 2MSL，在大量短连接的情况下，TIME_WAIT 会太多，这也会消耗很多系统资源。对于服务器来说，在 HTTP 协议里指定 KeepAlive（浏览器重用一个 TCP 连接来处理多个 HTTP 请求），由浏览器来主动断开连接，可以一定程度上减少服务器的这个问题。 2.13 TCP协议如何保证可靠性，即如何实现可靠传输？ TCP 主要提供了检验和、序列号/确认应答、超时重传、滑动窗口、拥塞控制和流量控制等方法实现了可靠性传输。 检验和：通过检验和的方式，接收端可以检测出来数据是否有差错和异常，假如有差错就会直接丢弃 TCP 报文段，重新发送。 序列号/确认应答：序列号的作用不仅仅是应答的作用，有了序列号能够将接收到的数据根据序列号排序，并且去掉重复序列号的数据。 TCP 传输的过程中，每次接收方收到数据后，都会对传输方进行确认应答。也就是发送 ACK 报文段，这个 ACK 报文段当中带有对应的确认序列号，告诉发送方，接收到了哪些数据，下一次的数据从哪里发。 滑动窗口：滑动窗口既提高了报文传输的效率，也避免了发送方发送过多的数据而导致接收方无法正常处理的异常。 超时重传：超时重传的时间是指发送出去的数据包到接收到确认包之间的时间，如果超过了这个时间会被认为是丢包了，需要重传。最大超时时间是动态计算的。 拥塞控制：在数据传输过程中，可能由于网络状态的问题，造成网络拥堵，此时引入拥塞控制机制，在保证 TCP 可靠性的同时，提高性能。 流量控制：如果主机A一直向主机B发送数据，不考虑主机B的接收能力，则可能导致主机B的接收缓冲区满了而无法再接收数据，从而会导致大量的数据丢包，引发重传机制。而在重传的过程中，若主机B的接收缓冲区情况仍未好转，则会将大量的时间浪费在重传数据上，降低传送数据的效率。所以引入流量控制机制，主机B通过告诉主机A自己接收缓冲区的大小，来使主机A控制发送的数据量。流量控制与 TCP 协议报头中的窗口大小有关。 2.14 详细讲一下TCP的滑动窗口？ 在进行数据传输时，如果传输的数据比较大，就需要拆分为多个数据包进行发送。TCP 协议需要对数据进行确认后，才可以发送下一个数据包。这样一来，就会在等待确认应答包环节浪费时间。 为了避免这种情况，TCP 引入了窗口概念。窗口大小指的是不需要等待确认应答包而可以继续发送数据包的最大值。 滑动窗口里面也分为有三种类型的数据，第一种是已经发送且收到确认但是未按序到达，即没有在窗口尾部形成一段连续的序列；第二种是已经发送但是未被确认的数据；第三种是等待发送的数据。随着已发送的数据不断被确认，窗口内等待发送的数据也会不断被发送。整个窗口就会不断往前移动，让还没轮到的数据进入窗口内。 可以看到滑动窗口起到了一个限流的作用，也就是说当前滑动窗口的大小决定了当前 TCP 发送包的速率，而滑动窗口的大小取决于拥塞控制窗口和流量控制窗口的两者间的最小值。 2.15 详细讲一下拥塞控制？ TCP 一共使用了四种算法来实现拥塞控制： 慢开始（slow-start） 拥塞避免（congestion avoidance） 快重传（fast retransmit） 快恢复（fast recovery） 发送方维持一个叫做拥塞窗口 cwnd（congestion window）的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。 慢开始：不要一开始就发送大量的数据，由小到大逐渐增加拥塞窗口的大小。 例如一开始发送方先设置 cwnd = 1，发送第一个报文段，等发送方接收到对方的确认后把 cwnd 从1增大到2。此后每经过一个传输轮次，拥塞窗口 cwnd 就加倍。 为了防止拥塞窗口 cwnd 增长过大引起网络拥塞，还需要设置一个慢开始门限 ssthresh 状态变量。 当 cwnd &lt; ssthresh 时，使用慢开始算法。 当 cwnd &gt; ssthresh 时，停止使用慢开始算法改用拥塞避免算法。 当 cwnd = ssthresh 时，即可使用慢开始算法，也可使用拥塞避免算法。 拥塞避免：拥塞避免算法让拥塞窗口缓慢增长，即每经过一个往返时间 RTT 就把发送方的拥塞窗口 cwnd 加一而不是加倍。这样拥塞窗口按线性规律缓慢增长。 快重传：我们可以剔除一些不必要的拥塞报文，提高网络吞吐量。比如接收方在收到一个失序的报文段后就立即发出重复确认，而不要等到自己发送数据时捎带确认。快重传规定：发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传计时器时间到期。 快恢复：主要是配合快重传，当发送方连续收到三个重复确认时，就执行乘法减小算法，把 ssthresh 门限减半（为了预防网络发生拥塞），但接下来并不执行慢开始算法，因为如果网络出现拥塞的话就不会收到好几个重复的确认，收到三个重复确认说明网络状况还可以。 3. HTTP/HTTPS 3.1 HTTP常见的状态码有哪些？ 常见状态码： 200：服务器已成功处理了请求。通常，这表示服务器提供了请求的网页。 301：请求的网页已永久移动到新位置。服务器返回此响应（对 GET 或 HEAD 请求的响应）时，会自动将请求者转到新位置（永久移动）。 302：服务器目前从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求（临时移动）。 400：客户端请求有语法错误，不能被服务器所理解。 403：服务器收到请求，但是拒绝提供服务。 404：服务器找不到请求的网页。 500：服务器遇到错误，无法完成请求。 状态码开头代表类型： 类别 原因短语 1XX Informational（信息性状态码） 接收的请求正在处理 2XX Success（成功状态码） 请求正常处理完毕 3XX Redirection（重定向状态码） 需要进行附加操作以完成请求 4XX Client Error（客户端错误状态码） 服务器无法处理请求 5XX Server Error（服务器错误状态码） 服务器处理请求出错 3.2 状态码301和302的区别是什么？ 共同点：301和302状态码都表示重定向，就是说浏览器在拿到服务器返回的这个状态码后会自动跳转到一个新的 URL 地址，这个地址可以从响应的 Location 首部中获取（用户看到的效果就是他输入的地址A瞬间变成了另一个地址B）。 不同点：301表示旧地址A的资源已经被永久地移除了（这个资源不可访问了），搜索引擎在抓取新内容的同时也将旧的网址交换为重定向之后的网址。302表示旧地址A的资源还在（仍然可以访问），这个重定向只是临时地从旧地址A跳转到地址B，搜索引擎会抓取新的内容而保存旧的网址。SEO 中302好于301。 重定向原因： 网站调整（如改变网页目录结构）。 网页被移到一个新地址。 网页扩展名改变（如应用需要把 .php 改成 .html 或 .shtml）。 3.3 HTTP常用的请求方式有哪些？ 方法 作用 GET 获取资源 POST 传输实体主体 PUT 上传文件 DELETE 删除文件 HEAD 和 GET 方法类似，但是只返回报文首部，不返回报文实体主体部分 PATCH 对资源进行部分修改 OPTIONS 查询指定的 URL 支持的方法 CONNECT 要求用隧道协议连接代理 TRACE 服务器会将通信路径返回给客户端 为了方便记忆，可以将 PUT、DELETE、POST、GET 理解为客户端对服务端的增删改查： PUT：上传文件，向服务器添加数据。 DELETE：删除文件。 POST：传输数据，向服务器提交数据，对服务器数据进行更新。 GET：获取资源，查询服务器资源。 3.4 GET请求和POST请求的区别？ 使用上的区别： 作用：GET 用于获取资源，而 POST 用于传输实体。 参数：GET 使用 URL 或 Cookie 传参，而 POST 将数据放在 Request Body 中，这个是因为 HTTP 协议用法的约定。 缓存：GET 请求会被浏览器主动缓存，而 POST 不会，除非手动设置。 请求长度：GET 方式提交的数据有长度限制，基本为2kb，而 POST 的数据则可以非常大，这个是因为它们使用的操作系统和浏览器设置的不同引起的区别。 安全性：POST 比 GET 安全，因为数据在地址栏上不可见，而 GET 的参数直接暴露在 URL 上。这个说法没毛病，但依然不是 GET 和 POST 本身的区别。 本质区别：GET 和 POST 最大的区别主要是 GET 请求是幂等性的，POST 请求不是。这个是它们本质区别。（幂等性是指一次和多次请求某一个资源应该具有同样的副作用。简单来说意味着对同一 URL 的多个请求应该返回同样的结果） 3.5 解释一下HTTP长连接和短连接？ 在 HTTP/1.0 中，默认使用的是短连接。也就是说，浏览器和服务器每进行一次 HTTP 操作，就建立一次连接，但任务结束就中断连接。如果客户端浏览器访问的某个 HTML 或其他类型的 Web 页中包含有其他的 Web 资源，如 JavaScript 文件、图像文件、CSS 文件等，当浏览器每遇到这样一个 Web 资源，就会建立一个 HTTP 会话。 但从 HTTP/1.1 起，默认使用长连接，用以保持连接特性。使用长连接的 HTTP 协议，会在响应头加入这行代码：Connection:keep-alive。 在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输 HTTP 数据的 TCP 连接不会关闭，如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的连接。Keep-Alive 不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（例如 Apache）中设定这个时间。实现长连接要客户端和服务端都支持长连接。 HTTP 协议的长连接和短连接，实质上是 TCP 协议的长连接和短连接。 3.6 HTTP1.0和HTTP1.1的区别？ 长连接：HTTP1.1 支持长连接（Persistent Connection）和请求的流水线（Pipelining）处理，在一个 TCP 连接上可以传送多个 HTTP 请求和响应，减少了建立和关闭连接的消耗和延迟，在 HTTP1.1 中默认开启 Connection: keep-alive，一定程度上弥补了 HTTP1.0 每次请求都要创建连接的缺点。 缓存处理：在 HTTP1.0 中主要使用 header 里的 If-Modified-Since, Expires 来做为缓存判断的标准，HTTP1.1 则引入了更多的缓存控制策略，可供选择的缓存头来控制缓存策略。 带宽优化及网络连接的使用：HTTP1.0 中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP1.1 则在请求头引入了 range 头域，它允许只请求资源的某个部分，即返回码是206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。 错误通知的管理：在 HTTP1.1 中新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突，410（Gone）表示服务器上的某个资源被永久性的删除。 Host 头处理：在 HTTP1.0 中认为每台服务器都绑定一个唯一的 IP 地址，因此，请求消息中的 URL 并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个 IP 地址。HTTP1.1 的请求消息和响应消息都应支持 Host 头域，且请求消息中如果没有 Host 头域会报告一个错误（400 Bad Request）。 3.7 HTTP1.1和HTTP2.0的区别？ HTTP2.0 相比 HTTP1.1 支持的特性： 新的二进制格式：HTTP1.1 的解析是基于文本的。基于文本协议的格式解析存在天然缺陷，文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，二进制则不同，只认0和1的组合。基于这种考虑 HTTP2.0 的协议解析决定采用二进制格式，实现方便且健壮。 多路复用：即连接共享，每一个 request 都是用作连接共享机制的。一个 request 对应一个 id，这样一个连接上可以有多个 request，每个连接的 request 可以随机的混杂在一起，接收方可以根据 request 的 id 将 request 再归属到各自不同的服务端请求里面。 头部压缩：HTTP1.1 的头部（header）带有大量信息，而且每次都要重复发送。HTTP2.0 使用 encoder 来减少需要传输的 header 大小，通讯双方各自 cache 一份 header fields 表，既避免了重复 header 的传输，又减小了需要传输的大小。 服务端推送：服务器除了对最初请求的响应外，服务器还可以额外地向客户端推送资源，而无需客户端明确的请求。 3.8 HTTP和HTTPS的区别？ HTTP HTTPS 端口 80 443 安全性 无加密，安全性较差 有加密机制，安全性较高 资源消耗 较少 由于加密处理，资源消耗更多 是否需要证书 不需要 需要 协议 运行在 TCP 协议之上 运行在 SSL 协议之上，SSL 运行在 TCP 协议之上 3.9 HTTPS的优缺点？ 优点： 安全性： 使用 HTTPS 协议可认证用户和服务器，确保数据发送到正确的客户机和服务器。 HTTPS 协议是由 SSL + HTTP 协议构建的可进行加密传输、身份认证的网络协议，要比 HTTP 协议安全，可防止数据在传输过程中不被窃取、改变，确保数据的完整性。 HTTPS 是现行架构下最安全的解决方案，虽然不是绝对安全，但它大幅增加了中间人攻击的成本。 SEO 方面：谷歌曾在2014年8月份调整搜索引擎算法，并称比起同等 HTTP 网站，采用 HTTPS 加密的网站在搜索结果中的排名将会更高。 缺点： 在相同网络环境中，HTTPS 相比 HTTP 无论是响应时间还是耗电量都有大幅度上升。 HTTPS 的安全是有范围的，在黑客攻击、服务器劫持等情况下几乎起不到作用。 在现有的证书机制下，中间人攻击依然有可能发生。 HTTPS 需要更多的服务器资源，也会导致成本的升高。 3.10 HTTPS的原理？ 客户端请求 HTTPS 网址，例如：，然后连接到 Server 的443端口（HTTPS 默认端口，类似于 HTTP 的80端口）。 采用 HTTPS 协议的服务器必须要有一套数字 CA（Certification Authority）证书。颁发证书的同时会产生一个私钥和公钥。私钥由服务端自己保存，不可泄漏。公钥则是附带在证书的信息中，可以公开的。证书本身也附带一个证书电子签名，这个签名用来验证证书的完整性和真实性，可以防止证书被篡改。 服务器响应客户端请求，将证书传递给客户端，证书包含公钥和大量其他信息，比如证书颁发机构信息，公司信息和证书有效期等。 客户端解析证书并对其进行验证。如果证书不是可信机构颁布，或者证书中的域名与实际域名不一致，或者证书已经过期，就会向访问者显示一个警告，由其选择是否还要继续通信。 如果证书没有问题，客户端就会从服务器证书中取出服务器的公钥A。然后客户端还会生成一个随机码 KEY，并使用公钥A将其加密。 客户端把加密后的随机码 KEY 发送给服务器，作为后面对称加密的密钥。 服务器在收到随机码 KEY 之后会使用私钥B将其解密。经过以上这些步骤，客户端和服务器终于建立了安全连接，完美解决了对称加密的密钥泄露问题，接下来就可以用对称加密愉快地进行通信了。 服务器使用密钥（随机码 KEY）对数据进行对称加密并发送给客户端，客户端使用相同的密钥（随机码 KEY）解密数据。 双方使用对称加密愉快地传输所有数据。 3.11 什么是Cookie和Session？ HTTP Cookie（也叫 Web Cookie 或浏览器 Cookie）是服务器发送到用户浏览器并保存在本地的一小块数据，它会在浏览器下次向同一服务器再发起请求时被携带并发送到服务器上。通常，它用于告知服务端两个请求是否来自同一浏览器，如保持用户的登录状态。Cookie 使基于无状态的 HTTP 协议记录稳定的状态信息成为了可能。 Cookie 主要用于以下三个方面： 会话状态管理（如用户登录状态、购物车、游戏分数或其它需要记录的信息）。 个性化设置（如用户自定义设置、主题等）。 浏览器行为跟踪（如跟踪分析用户行为等）。 Session 代表着服务器和客户端一次会话的过程。Session 对象存储特定用户会话所需的属性及配置信息。这样，当用户在应用程序的 Web 页之间跳转时，存储在 Session 对象中的变量将不会丢失，而是在整个用户会话中一直存在下去。当客户端关闭会话，或者 Session 超时失效时会话结束。 3.12 Cookie和Session是如何配合的呢？ 用户第一次请求服务器的时候，服务器根据用户提交的相关信息，创建对应的 Session，请求返回时将此 Session 的唯一标识信息 SessionID 返回给浏览器，浏览器接收到服务器返回的 SessionID 信息后，会将此信息存入到 Cookie 中，同时 Cookie 记录此 SessionID 属于哪个域名。 当用户第二次访问服务器的时候，请求会自动判断此域名下是否存在 Cookie 信息，如果存在则自动将 Cookie 信息也发送给服务端，服务端会从 Cookie 中获取 SessionID，再根据 SessionID 查找对应的 Session 信息，如果没有找到说明用户没有登录或者登录失效，如果找到 Session 证明用户已经登录可执行后面操作。 根据以上流程可知，SessionID 是连接 Cookie 和 Session 的一道桥梁，大部分系统也是根据此原理来验证用户登录状态。 3.13 Cookie和Session的区别？ 作用范围不同：Cookie 保存在客户端（浏览器），Session 保存在服务器端。 存取方式的不同：Cookie 只能保存 ASCII，Session 可以存任意数据类型，一般情况下我们可以在 Session 中保持一些常用变量信息，比如说UserID等。 有效期不同：Cookie 可设置为长时间保持，比如我们经常使用的默认登录功能，Session 一般失效时间较短，客户端关闭或者 Session 超时都会失效。 隐私策略不同：Cookie 存储在客户端，比较容易遭到不法获取，早期有人将用户的登录名和密码存储在 Cookie 中导致信息被窃取；Session 存储在服务端，安全性相对 Cookie 要好一些。 存储大小不同：单个 Cookie 保存的数据不能超过4K，Session 可存储数据远高于 Cookie。 "},{"title":"Hexo使用Abbrlink生成文章固定链接","date":"2022-11-23T03:11:00.000Z","url":"/posts/18884.html","tags":[["Hexo","/tags/Hexo/"]],"categories":[["Hexo","/categories/Hexo/"]],"content":" 本文介绍如何使用 Abbrlink 插件生成形如  的 Hexo 文章固定编号链接。 Hexo 默认的静态 url 格式是：:year/:month/:day/:title，也就是按照年、月、日、文章标题来生成固定链接的。如：。 使用 Abbrlink 插件可以使每篇文章都有一个唯一的编号，并将文章的链接用这个编号唯一区别，这样链接中不会出现中文，也不会因为修改文章的日期而导致链接的改变。 首先我们先安装插件，在博客根目录中打开命令行，输入以下命令： 修改根目录下的 _config.yml 文件，修改文件中的 permalink: 配置项，且添加一个配置项 abbrlink:，修改后的结果如下： 其中，alg 属性表示算法，目前支持 crc16 和 crc32 算法，默认值是 crc16。rep 表示形式，即生成的链接可以是十六进制格式也可以是十进制格式，默认值是十进制格式，示例如下： 注意：在生成之前就要改好算法和形式，不然后面再改的话会导致链接不统一。"},{"title":"本科阶段学习生活回顾","date":"2022-11-20T03:33:00.000Z","url":"/posts/62882.html","tags":[["Essay","/tags/Essay/"]],"categories":[["Essay","/categories/Essay/"]],"content":" 大学本科生涯即将结束，浅浅地在此总结回忆一下这段生活吧~ 2019-2020 2019.09 录取的专业是兰州理工大学土木工程学院测绘工程专业，高中同班同学ycy去了同校的电子信息科学与技术专业（我的一志愿，他比我高了一分被他抢了hhh） 第一次出远门挺不适应的，和爸妈分别也是很难受QAQ。 刚去学校没有经验，和ycy坐动车先到武汉再去兰州，从早上一直坐到晚上十一点多，还花了一千多块，简直俩冤大头。到站后去旅馆住了一晚，第二天早上去学校。 我拖了一个29寸的大行李箱和一个20寸的登机箱，带了被子还有几件厚外套 刚到学校时认不清路，学长学姐来问我时我也没搭理hhh，自己摸索着办完了注册手续然后找到了宿舍楼。 到学校的第一顿午餐就和ycy干了一碗牛肉面（兰州拉面），感觉不算很好吃，但是确实便宜，5r一碗。 我的舍友都挺不错的，有一个是福建的老乡lcq，一个辽宁的lty，然后剩下三个是甘肃本地的yhy、bm、zx。 学校的图书馆确实很大很新，夜景也很漂亮，让人有种想进去学习的欲望~ 进入了为期两周的军训生活 军训期间有点累，突然更想家了，加上刚来学校，那段时间就挺难受的。 军训结束后和舍友出去吃了一顿，顺便逛了一圈（怀念爆发 COVID-19 前的生活） 然后到了大一正式的学习生活 上了大学感觉和高中最明显的差别就是班集体的概念淡化了很多，基本上平时只会和舍友走在一起。 大一每天早上有跑操，晚上有晚自习，有时候感觉自己像个高中生。 刚上高数课时感觉大家都很积极，每天前排都老早就被抢了，我和我舍友基本也是抢前两排。 入学英语考试考进了A班，可以在12月就报名英语四级考试，因此开始每天碎片化背单词。 课余时间基本都会跑去图书馆，第一次体验在图书馆学习的感觉，和家里氛围确实差太多了，在图书馆基本不会有摸鱼的想法hhh。 2019.10 国庆节和舍友一起去了成都，在火车上对面做了几个本校的学姐，聊了几句 在成都去了杜甫草堂，感觉不是很有意思，有个舍友特别喜欢文学，在那里面待了一整天。然后去了动物园，第一次看到大熊猫ovo。春熙路、太古里也是必去的地方，还有九眼桥下的整条 Bar 街，属实是体会到了什么叫做灯红酒绿。 顺便去逛了一圈四川大学，985名校确实大多了，建筑也豪华不少，还在纪念品店买了一些川大纪念品。 国庆结束后回学校的晚上感觉天气就变凉了不少，感觉已经快和福建的冬天差不多了。 2019.11-2020.01 国庆假期结束后就该好好学习了，什么时候努力都不晚~ 大一上册基本就是学那几门必修课，时间比较充裕，因此课余时间基本都在学英语。 而且当时想着是跨专业考研计算机，考研的话英语也是非常重要的，可以提前积累。 考四级的时候我的英语词汇量从高中的2k提升到了8k左右，语法还是零基础，买了一套四级练习基本全刷完了，第一次四级也挺顺利的，554分通过！ 四级考完就该准备期末考了。因为对计算机感兴趣，就提前学完了C语言，很多宿舍的人都跑来这问我题目hhh。 最后大一上的期末考也很顺利，高数和C语言这类高学分的课都是接近满分，总成绩自然也被拉上去了。 2020.02-2020.08 年前去逛了两场漫展，然后差不多就爆发了 COVID-19 过年的这段时间基本天天在家里，然后看新闻，不过我家这边的小县城倒是很安全。 大一下的开学也延期了，正式开始了网课生活。网课期间我在家也不像在学校那样有动力了，基本天天挂着网课打游戏。 那时候在肝 Destiny 2 和 GTA V，感觉又找回了高中打游戏的状态hhh。 到了六月左右学校又通知返校，那时候我的其他高中同学就没听谁说还要返校的。 回学校抓紧速成了一下，顺利通过期末考，再次回家。 2020-2021 2020.09 这时候通知可以报名转专业，当时感觉转专业可能会很难，但还是报了一下 转专业期间认识了班上的一位女生cl，发现她也报的和我一个专业，然后慢慢地就熟了。 最后我竟然总分第一成功转来了计算机科学与技术！当时自己也想不到。cl调剂去了同院的大数据。 那么这时候目标就改成了考研本专业！这样就不用跨专业了。 大概在同一时间段，大一学年的评奖评优也出来了，评上了校一等奖学金和校三好学生。 转专业后分配了一个转专业宿舍，一共5个人，其中有一个是通信工程的zaj 我转专业后没多久也搬宿舍了，这时候新宿舍只有我和zaj，剩下几位还待原宿舍不搬。 zaj简直是带动整个宿舍的优良作息规律，必须早睡早起。 九月份考了因为 COVID-19 影响而推迟的英语六级，最后六级也是一次过，虽然是428分低分飘过hhh。 2020.10-2021.01 继续进入学习状态，学计算机后才知道自学远比上课听讲更有效 转专业后得补修没修过的大一课程，最难搞的就是大物了。 我也是提前自学完了 C++，然后十月份报名了团体程序设计天梯赛的校赛，排在八十多名，打了个酱油hhh。 此后开始正式进入算法学习阶段，每天一整天在图书馆都在学算法和写题，后来颈椎都快出问题了。 在做 C++ 课设的时候认识了lj老师，是个很不错的老师，她知道课设都是我自己写的，答辩的时候就没问我问题了，然后开始和我闲聊，问我是不是想打比赛，然后说之后培训我可以去听。 到了十二月，先是考了第二次六级，这次511分通过！ 这时候苦练了大概两个月的算法，继续参加了天梯赛校赛，拿了第十名！lj和zh老师说我有潜力。 同月的 ACM-ICPC 校队选拔赛也是第一名！ 到了学期末的时候另一个舍友hxc也搬过来了，感觉宿舍热闹了一点，还建了一个三个人的小群。 比赛结束后又是进入紧张的期末备考阶段，然后顺利考完，回家过年。 2021.02-2021.08 大二下学期就进入了比赛的主场了，基本所有比赛的省赛国赛都在下学期 返校后也是复健了一下算法，寒假在家基本又颓废了。 寒假组队 ICPC 的时候认识了dyy，大学真正的导师来了hhh。 三四月开始继续训练算法，四月有 ICPC 昆明站、蓝桥杯省赛和天梯赛国赛。 第一次打 ICPC 的时候属实被打蒙了，感觉自己是个 Newbie，成功打铁。 蓝桥杯比较水，拿下省一，天梯赛状态也不错，wjt第一名220+，我211分，拿下个人国三。 打完算法比赛后dyy拉我开发项目参加计算机设计大赛。当时我对开发技术一无所知。 dyy教我学了 Qt，然后用 C++ 开发了一个线上考试系统，但是当时时间紧张加上我太 Newbie 所以没实现什么功能，但是最后拿下西北赛区一等奖，震惊。 五月份还和dyy参加了数学建模校赛，教我 Word 的排版方法，感觉要被嫌弃死hhh。之后去了宁夏理工学院参加 ICPC 银川站，成功打铁，但是和dyy还有sy玩的还挺开心的（旅游组实锤）。 六月打完了蓝桥杯国赛，拿下国二。去了西北工业大学参加 ICPC 全国邀请赛，成功打铁，这次没怎么旅游，还是考完数据库连夜赶过去的，都没休息好。 大二学年结束后发现成绩排到了专业第一，这时候又改变目标了，要不冲保研吧ovo。 2021-2022 2021.09-2022.01 大二结束后暑假没有回家，而是留学校培训数学建模，然后继续训练算法 暑假期间第四个舍友zyj也搬过来了，因为原学院要搬去本部了，被迫驱逐。 我、hxc、twl、zyj都留校培训数学建模，兰州夏天也是挺难熬的，早上热晚上蚊子多，天天晚上打蚊子打到深夜。 暑假结束后直接建模国赛，感觉做的很好但是最后没获奖，不会被一眼假了吧，震惊。 到了大三评奖评优的时候了 大二一整年感觉应该算是状态最巅峰的时期，因此打算直接冲一波国家奖学金，这国奖答辩阵仗属实挺大的（吓）。 答完辩之后本来好像是没戏了，但是突然发现隔壁班的一个同学加分项有点浑水摸鱼，就直接去找导员重新核对，最后算下来险胜拿下了国奖（8K真是美滋滋~）。 然后就是继续训练算法，十月份的天梯赛校赛拿下第一名。这次校赛鲲鹏有赞助，还拿了个华为耳机ovo。 到了期末还是冲刺考试，想保研成绩肯定得稳住，学期结束还是稳住了专业第一。 2022.02-2022.08 保研前最后一个学期的冲刺了 这个学期的天梯赛不是cy姥姥出题了，换了一个，难度比之前大了不少，直接翻车车，国赛169分没拿到个人国三，最后是团队国三。 蓝桥杯国赛 LCA 公式写错，DJ 模板题建成单向边，继续翻车车，只有国三。 被dyy拉着搞计算机设计大赛，开发一个线上编辑器，我和舍友hxc基本都是划水的。最后拿下西北赛区一等奖，合理，没拿到国奖，震惊。 到了七月份，突然爆发 COVID-19，7月9日中午还在学校做课设，突然就通知能回家的赶紧回家，提前放假，当晚就和hxc还有dyy到了机场，第二天直接润回家。 我先飞到了南昌，然后路上还认识了一个江西的也是我们学校的一个能动院学弟，最后没有留联系方式，短暂的缘分hhh。 2022-2023 2022.09-2023.01 九月突然爆发 COVID-19，没办法返校，保研的事情线上办完了，最后也是以第一名的总分保研了。 9月28日，填了推免系统信息，最后录取了西安电子科技大学的人工智能学院，也算是给过去三年的努力画上了句号。 十月除学校组织返校，我去福州度假了hhh，就申请了不返校，结果这波是我预判了兰州的预判。 这波返校后兰州 COVID-19 大爆发，一直到现在十一月都快结束了，那边还很严重，突然感觉当时没返校待家里是多幸福的一件事。 2023.02-2023.06 一转眼就到了本科的最后一个学期了呀，不得不感慨时间过的真的太快了 这是本科生涯的最后一次返校了，有大半年没见到舍友了。 通信的舍友zaj考研南理工353分，本来我们寝室几个舍友还都挺开心的，结果出来复试线是350分后大家都惊了，去年复试线是325，今年由于 COVID-19 原因很多考研er都很不容易，我们宿舍本以为复试线大概会在320左右顶天了，350也是真的从来没想过。 擦线进复试，他情绪瞬间低落到谷底，开始纠结要不要去线下复试，我们其他几个安慰了好久最后还是决定冲一把，由于hxc之前大创挂过他的名字，因此就给他讲了很多项目内容帮助他充实简历和自我介绍，实验部分找了对门的zsy来辅导，就这么持续了一两周后他就去南京复试了，但是很遗憾最后还是被刷了。 复试完他先回家休息了几天才来学校，在这期间我和hxc商量着等他回来咱宿舍一起去吃顿饭，然后我俩一起请他，他回来后我们去吃了一顿海底捞，还叫上了隔壁班的好兄弟nzx，吃完后去兰州老街逛了一圈，在回学校的路上下起了雨夹雪，105路车上的显示屏放的是去年祝福兰理工毕业生的海报，感觉在车上心情复杂，即将要毕业了。 zaj过了一段时间恢复心态后决定二战，我们也很支持他，hxc是今年一战，因为去年秋招就业不太好找工作，所以他们俩就互相鼓励，最后一定会上岸的！！！ 2023.04.17这也是在学校过的最后一个生日了 生日的时候cl给我买了个蛋糕，当天把蛋糕带到了东三和学弟们一起吃，wfy和cxq分别还带了小红包和零食，学弟们真的都很好。 四五月份的主要工作就是毕业设计了，我的题目是《基于机器学习的北半球积雪覆盖数据分析系统》，指导老师是lj，对于这个毕设我还是比较划水的，因为主要心思还是放在学习未来研究方向的基础，然后师兄偶尔也会给我派点小活，这个学期基本都在创客和学弟们吹水水，偶尔还会出去玩玩。 这段时间真的过的很快很快，从开题报告到中期答辩很快就到了终期答辩，老师让我和dyy去6.6的系级答辩，然后我俩都进了院级答辩，时间就在第二天6.7，最后我俩在院级答辩时被刷了，这下是真正毕业啦！ 答辩完后由于其他同学都是在6.12才小组答辩，我就先和对门的zsy出去拍了几张照片（偷穿他们电信院的学士服hhh）。 其实那段时间还挺欢乐的，并没有多想什么，很快就到了6.16，这是我们院集体拍毕业照的时候，早上我们班拍完后下午我们宿舍就约上zsy一起出去拍，还叫上了学弟们一起来拍，拍的过程还是很欢乐的，傍晚和nzx回红柳创客梦工厂把照片导出来，并且挑了一部分比较好的稍微修了一下（其实很多照片都很nice！），这时候看到这些照片中大家的笑容后心情就变得有些复杂了起来，可能这就是定格我们本科时候的最后一组照片了。 过了两天cl约上我和hxc出去用手机拍了几张，还叫上了dyy，到了6.20就是我们院拿双证的时候了，拿证时遇到了曾经一起搞数学建模的女同学qmh，她也拉着我还有身边的zaj一起在求是楼5楼很简单地拍了一张。 6.21一大早七点zaj就要走了，刚好我醒了，伸出头看了一眼，他说了句再见了兄弟们，然后就出门了，简简单单的告别，瞬间让我感觉到了太多的不舍，随后我和nzx去本部给成绩单盖章，我拉上了原专业的舍友也是简单地拍了几张合影，可惜福建老乡老早就润回家了，他考上了福大，剩下几个考上的学校都是在西安，还有一个找工作也是找在了西安那边，大家未来还有很多机会团建hhh。 江西舍友zyj在中午的时候也走了，我那时候还在本部，下午回来后，简单在宿舍和hxc随便聊了会，到了差不多六点，他也准备走了，我和他还有nzx一起去东门吃了顿“叫了只炸鸡”，吃完后我和nzx就送他上车了，还是很简单的告别，走了兄弟们，以后再见。 这晚回宿舍后就剩我一个人了，作为最后一个走的确实感觉到有很多的不舍，一地凌乱的垃圾，被搬空的床位，还有空荡荡的桌子… 6.21晚上几个学弟们把我叫出去通宵，先是去玩了剧本杀，然后去了一家酒吧喝到了早上六点，这一晚过的也很快乐，这段时间nlh动不动就说“易哥你延毕吧我舍不得你”，虽然说的时候感觉都是在说说笑笑，但是内心确实多少会有波澜，只是真的没办法阻挡时间的流逝。 早上回学校后头有点晕就去小睡了一会，起来后也差不多要到我要走的时候了，这时候对门zsy也还没走，突然过来和我说了一句“昨天几句很简单的话语不经意间就送走了好多兄弟”，我说没事以后还有机会再见的，他说“话都是这么说，实际上很难再完整地聚在一块了”。 这时候内心的心情真的很难受，看着宿舍，一幕幕往事瞬间浮现出来，确实，未来真的有机会再见吗？要几年后，五年还是十年？还是等到有人结婚？ 临走之前拍了一张宿舍的照片，还有对门zsy的宿舍，zsy刚好不在宿舍，就给他发了条简单的消息，真是造化弄人唉。 nzx送我去南门坐105路，上车后也是简单的告别，祝我路上注意安全，一路顺风。 在车上越想越难受，流了眼泪但是没滴下来，果然人多想就会陷入其中，不过最后还是得离开这边，再见了兰州。 写在最后 2019~2023，四年大学时光到此告一段落，并不后悔不远千里来到了兰州，当然并不是因为学校也不是因为兰州，是因为在这里遇到的朋友，勤奋上进还有趣的所有舍友们（hxc、zaj、zyj、twl）、严厉但是热心而且实力超强的dyy、外向热心但是说话攻击性极强的cl、说话亲切幽默还有实力的nzx、帅气的苏州富豪学霸zsy、班级唯一一个主动找我聊天的女生lr、幽默风趣动不动闯到我们宿舍来的wh、帅气的全栈大佬lsl、在各个协会风生水起的铁兄弟nlh、经常送礼十分热情的cxq、物联网算法卷王也很热情的wfy、气场强大的syh、整天修仙看妹妹的ycl、创客第一大帅哥ylq、… 虽然不善于交际，认识的人不算多，但是完全足够了，每一个朋友都值得作为一份永久的回忆，天下没有不散的筵席，大家最后都会各奔东西，为了各自的生活继续努力着，每个人都只是去到祖国最需要他们的地方了，但是希望未来真的还能有机会再见面，不管在多少年后。 如舍友hxc所说的，不怀念兰州，怀念兄弟们，怀念朋友们。"},{"title":"Hexo搭建Github博客教程","date":"2022-11-19T16:11:00.000Z","url":"/posts/47192.html","tags":[["Hexo","/tags/Hexo/"]],"categories":[["Hexo","/categories/Hexo/"]],"content":" 使用 Hexo 搭建我的 Github 个人网站：My Github Blog。 1. 环境配置 （1）安装 Git Bash：Windows 安装配置 Git 教程。 （2）安装 NodeJS：NodeJS 的安装及配置。 （3）修改 npm 镜像源： （4）安装 Hexo： （5）安装部署插件： 此处如果出现 npm ERR! Error: EPERM: operation not permitted, mkdir 'E:\\NodeJS\\node_modules\\.corepack-xTCBGLKh 之类的错误需要以管理员身份打开 cmd，然后在 cmd 中安装。 2. 本地博客搭建 首先创建文件夹 Hexo，然后进入该文件夹，创建文件夹 blog，使用管理员身份打开 cmd，进入 blog 文件夹，初始化 Hexo 博客： 然后在本地启动一下看看效果： 然后打开链接： 查看一下页面内容，之后我们进行页面调试都是在这个本地链接进行的。 使用 VS Code 打开 blog 文件夹，其中，source/_posts 文件夹下存放我们写的文章，themes 文件夹存放博客的主题，_config.yml 是博客的全局配置文件，_config.landscape.yml 是博客的主题配置文件。 3. 部署至Github 在 Github 创建一个名为 用户名.github.io 的仓库，例如：AsanoSaki.github.io 在 VS Code 中打开 blog 文件夹，找到 _config.yml 文件，找到 deploy，按照以下格式进行修改： 最后执行以下命令： 在部署的时候如果出现警告：LF will be replaced by CRLF the next time Git touches it，可以用以下指令禁用将 LF 自动转换为 CRLF： 然后访问域名：https://用户名.github.io/ 即可进入自己的博客啦。 4. 博客主题设置 Hexo 主题官网：Hexo Themes。 将下载好的主题放到 blog/themes 文件夹中，然后将根目录下的 _config.yml 中的主题修改为下载的主题即可，例如： 然后进入主题的文件夹，该文件夹下也有一个 _config.yml 文件，修改这个文件的内容即可修改当前博客主页的样式。 5. 博客备份 在 Github 新建一个名为 blog 的私有仓库，然后在 Hexo/blog 目录下打开 Git Bash，执行以下命令： 如果在执行 git add . 时出现警告：You've added another git repository inside your current repository.，可以将 blog 目录中的 .deploy_git 文件夹删除。"},{"title":"Web学习笔记-React（配置环境、ES6语法补充、Components）","date":"2022-11-18T10:00:00.000Z","url":"/posts/60453.html","tags":[["Web","/tags/Web/"]],"categories":[["Web","/categories/Web/"]],"content":" 本文记录 React 的学习过程，内容为配置环境、ES6 语法补充、Components。 React 是一个用于构建用户界面的库。React 不是一个框架，它的应用甚至不局限于 Web 开发，它可以与其他库一起使用以渲染到特定环境。 React 官网：React。 1. React配置环境 React 是一个声明式，高效且灵活的用于构建用户界面的 JavaScript 库。使用 React 可以将一些简短、独立的代码片段组合成复杂的 UI 界面，这些代码片段被称作 components。React 能够构建那些数据会随时间改变的大型应用。 React 特性： React 为了能够方便地去维护我们的页面，它在内存里面创建了一个虚拟的 DOM 树：Virtual DOM，这是一个轻量级的虚拟的 DOM，就是 React 抽象出来的一个对象，描述 DOM 应该什么样子的，应该如何呈现。通过这个 Virtual DOM 去更新真实的 DOM，由这个 Virtual DOM 管理真实 DOM 的更新。 数据驱动：当某一个元素里的数据发生变化后，React 会重新将有可能修改的元素都修改一遍，然后与真实的 DOM 树对比是否有区别，React 分析完后最终只会修改真实改变的结点。由于在内存里修改对象的速度很快，因此 React 效率很高。 React 一般不直接手写 JS，而是通过编写 JSX 文件，JSX 比 JS 更好写一点，React 会先将 JSX 编译成 JS。 （1）安装 Git Bash：Windows 安装配置 Git 教程。 （2）安装 NodeJS：NodeJS 的安装及配置。 （3）安装 create-react-app： 打开 Git bash，执行以下命令： 如果速度很慢，可以先修改镜像源再尝试安装： 如果安装完成后出现警告：npm WARN deprecated tar@2.2.2: This version of tar is no longer supported, and will not receive security updates. Please upgrade asap.，可以先更新 tar 试试： 如果还是有警告，且创建项目时（例如执行 create-react-app react-app）报错：bash: create-react-app: command not found，使用 npx 创建项目： 或者用 npm 创建项目： 创建好后进入项目文件夹启动项目： 启动后访问 localhost:3000 即可访问页面，ctrl+c 可停止服务。 （4）配置 VS Code 插件：Simple React Snippets、Prettier - Code formatter Simple React Snippets 为 React 智能化自动补全插件。 例如输入 imrc 即可补全出以下内容： 输入 cc 即可补全出以下内容： Prettier - Code formatter 为代码格式化插件，安装好后在 JSX 代码中通过 VS Code 一键格式化快捷键：ctrl + k + f 即可配置格式化插件。 （5）创建 React App： 在目标目录下右键打开 Git Bash，在终端中执行： 启动成功后会在本地开一个3000端口，页面效果已在上文展示。此时使用 VS Code 打开 react-app 文件夹可以看到项目的目录结构。 其中，node_modules 用来维护各种 JS 库，未来安装的所有依赖项都会放在该文件夹下；public 中的 index.html 就是我们未来渲染出的页面，该文件中只有一个 &lt;div id=&quot;root&quot;&gt;&lt;/div&gt;；src 中的 index.js 代码如下： 其中 App 的定义在 App.js 中： 该 App 组件就定义了页面的具体内容，且我们能够发现该 JS 文件中有 HTML 代码，因此该文件即为 JSX 文件，能够在 JavaScript 的基础上支持 XML（可扩展标记语言），HTML 也是一种特殊的 XML。 JSX 是 React 中的一种语言，会被 Babel 编译成标准的 JavaScript。 2. ES6语法补充 ES6，全称 ECMAScript 6.0，是 JavaScript 的版本标准。此处添加一些 React 中常用的语法糖。 （1）使用 bind() 函数绑定 this 取值 在 JavaScript 中，函数里的 this 指向的是执行时的调用者，而非定义时所在的对象。例如： 运行结果为： 使用 bind() 函数绑定 this 的取值为 person。例如： 运行结果为： （2）箭头函数的简写方式 当函数参数只有一个时可以将括号去掉，当函数体只有一个 return 语句时可以把 return 和 &#123;&#125; 一起去掉，例如： 等价于： （3）箭头函数不重新绑定 this 的取值 （4）对象的解构 （5）数组和对象的展开 （6）Named exports 与 Default exports Named Export：可以 export 多个，import 的时候需要加大括号，名称需要匹配，即之前使用的方式。 Default Export：最多 export 一个，import 的时候不需要加大括号，可以直接定义别名。 3. Components React 应用程序是由组件（Component）组成的。组件是一段可重用代码，一个组件是 UI（用户界面）的一部分，它拥有自己的逻辑和外观，用于渲染、管理和更新应用中的 UI 元素。组件可以小到一个按钮，也可以大到整个页面。 （1）创建项目 首先创建一个新项目 box-app： 安装 bootstrap 库： bootstrap 的引入方式： （2）创建 Component 在 src 文件夹中创建一个文件夹 components 存放组件，然后在 components 文件夹中创建一个 JSX 文件 box.jsx（使用 .js 后缀也一样，只是用 .jsx 后区分起来跟明显一点）其框架如下： 然后我们需要在 index.js 中将组件渲染出来： （3）创建按钮 由于 Component 中的 render() 函数只能 return 一个元素，因此当子节点数量大于1时，可以用 &lt;div&gt; 或 &lt;React.Fragment&gt; 将其括起来。 （4）内嵌表达式 JSX 中使用 &#123;&#125; 在 HTML 标签中嵌入表达式。 （5）设置属性 由于 class 是 JS 中的关键字，因此 HTML 标签中的 class 需要改为 className。CSS 属性也需要修改，例如：background-color 修改为 backgroundColor，其它属性也是类似的。 以上的综合示例： （6）数据驱动改变 Style 例如： （7）渲染列表 可以使用 map 函数渲染一个列表，每个元素需要具有唯一的 key 属性，用来帮助 React 快速找到被修改的 DOM 元素，例如： （8）Conditional Rendering 利用逻辑表达式的短路原则： 与表达式中 expr1 &amp;&amp; expr2，当 expr1 为假时返回 expr1 的值，否则返回 expr2 的值。 或表达式中 expr1 || expr2，当 expr1 为真时返回 expr1 的值，否则返回 expr2 的值。 （9）绑定事件 例如可以使用 onClick 绑定按钮的点击事件，注意需要妥善处理好绑定事件函数的 this，示例如下： （10）修改 state 需要使用 this.setState() 函数，每次调用 this.setState() 函数后，会自动重新调用 this.render() 函数，用来修改虚拟 DOM 树。React 只会修改不同步的实际 DOM 树节点。例如： （11）给事件函数添加参数 可以定义一个临时函数绑定事件，然后在该函数中调用原函数并传入参数，或者直接在绑定事件的时候用一个临时的箭头函数返回传入参数的原函数。例如： 上一章：Web学习笔记-JavaScript。 下一章：Web学习笔记-React（组合Components）。"},{"title":"Windows安装配置Git教程","date":"2022-11-18T03:45:00.000Z","url":"/posts/31252.html","tags":[["Others","/tags/Others/"]],"categories":[["Others","/categories/Others/"]],"content":" Windows 下安装 Git 以及配置与 Github 的远程连接。 （1）首先前往 Git 官网，下载安装文件： （2）打开安装程序，设置好安装路径，然后点击下一步后在此处可以选用默认设置，也可以勾上 Additional icons（桌面图标） 和 Add a Git Bash Profile to Windows Terminal： （3）直接点击下一步： （4）此处选择编辑器，一般直接使用 Vim 即可： （5）此处是设置 Git 初始化分支的名称，默认为 master，也可以选择自定义： （6）此处选择使用 Git 的方式，通常选第二个： （7）选择 SSH，第一个即可： （8）选择 HTTPS 传输后端，第一个选项使用 OpenSSL 库，第二个选项使用本机 Windows 安全通道库，选第一个即可： （9）配置结束行转换方式，也就是 Git 处理文本结束行的方式，Windows 选择第一个即可： （10）配置终端使用 Git Bash，第一个选项是使用 MinTTY 作为终端模拟器，第二个选项是使用 Windows 的默认控制台，一般选择第一个： （11）选择 git pull 的默认行为，第一个选项是 git pull 的标准行为：尽可能快进当前分支到一个被捕获的分支，否则创建合并提交；第二个选项是将当前分支改为获取的分支。如果没有要重基的本地提交，这相当于快进；第三个选项是仅仅快进，快进到获取的分支，如果不可能，就失败。此处选择第一个选项即可： （12）选择 Git 凭证助手，选第一个即可： （13）配置额外特性，第一个选项是启用文件系统缓存，第二个选项是支持符号链接，勾上第一个即可： （14）实验特性，一般不用选，直接安装即可： （15）安装好后在任意目录下点击鼠标右键应该能看到 Open Git Bash here 选项，我们可以通过该选项在某一目录中打开 Git Bash，然后我们打开后输入 git -v，即可看到 Git 的版本号： 在窗口顶部右键即可打开设置页面（Options），可以调整字体大小与窗口的默认大小： （16）输入以下命令配置 Git： 例如： 生成公钥： 将 ~/.ssh/id_rsa.pub 中的内容复制到 GitHub 的 SSH Keys 中： 测试是否连接上 GitHub： 结果如下说明配置成功： 如果出现报错提示：ssh: connect to host github.com port 22: Connection timed out，说明22端口可能被防火墙屏蔽了，可以尝试连接 GitHub 的443端口，我们将 ~/.ssh/config 文件修改成以下内容，这样 SSH 连接 GitHub 的时候就会使用443端口： "},{"title":"Web学习笔记-JavaScript","date":"2022-11-14T03:14:00.000Z","url":"/posts/40580.html","tags":[["Web","/tags/Web/"]],"categories":[["Web","/categories/Web/"]],"content":" 本文记录 JavaScript 的学习过程。 JavaScript 是一种具有函数优先的轻量级，解释型或即时编译型的编程语言。目前 JavaScript 的标准是 ECMAScript6，简称 ES6。 1. JS的调用方式与执行顺序 JS 常见使用方式有以下几种： 直接在 HTML 的 &lt;script type=&quot;module&quot;&gt;&lt;/script&gt; 标签内写 JS 代码。 直接引入 .js 文件：&lt;script type=&quot;module&quot; src=&quot;/static/js/index.js&quot;&gt;&lt;/script&gt;。 将所需的代码通过 import 关键字引入到当前作用域。 例如 /static/js/index.js 文件中的内容为： &lt;script type=&quot;module&quot;&gt;&lt;/script&gt; 中的内容为： 执行顺序： 类似于 HTML 与 CSS，按从上到下的顺序执行。 事件驱动执行。 HTML、CSS、JavaScript 三者之间的关系： CSS 控制 HTML； JavaScript 控制 HTML 与 CSS； 为了方便开发与维护，尽量按照上述顺序写代码。例如：不要在 HTML 中调用 JavaScript 中的函数。 2. 变量与运算符 （1）let 与 const：用来声明变量，作用范围为当前作用域。 let 用来定义变量。 const 用来定义常量。 例如： （2）变量类型： number：数值变量，例如：1, 2.5。 string：字符串，例如：&quot;acwing&quot;, 'AsanoSaki'，单引号与双引号均可。字符串中的每个字符为只读类型。 boolean：布尔值，例如：true, false。 object：对象，类似于 C++ 中的指针，例如：[1, 2, 3]，&#123; name: &quot;AsanoSaki&quot;, age: 18 &#125;，null。 undefined：未定义的变量。 类似于 Python，JavaScript 中的变量类型可以动态变化。 （3）运算符： 与 C++、Python、Java 类似，不同点： ** 表示乘方。 等于与不等于用 === 和 !==。 3. 输入与输出 （1）输入 从 HTML 与用户的交互中输入信息，例如通过 input、textarea 等标签获取用户的键盘输入，通过 click、hover 等事件获取用户的鼠标输入。 通过 Ajax 与 WebSocket 从服务器端获取输入。 标准输入。 （2）输出 调试用 console.log()，会将信息输出到浏览器控制台。 改变当前页面的 HTML 与 CSS。 通过 Ajax 与 WebSocket 将结果返回到服务器。 通过 HTML 输入输出示例： 通过标准输入输出示例： （3）格式化字符串 字符串中填入数值： 定义多行字符串： 保留两位小数： 4. 判断语句 JavaScript 中的 if-else 语句与 C++、Python、Java 中类似。例如： JavaScript 中的逻辑运算符也与 C++、Java 中类似：&amp;&amp; 表示与、|| 表示或、! 表示非。 5. 循环语句 JavaScript 中的循环语句与 C++ 中类似，也包含 for、while、do while 循环。 （1）for 循环 枚举对象或数组时可以使用： for-in 循环：可以枚举数组中的下标，以及对象中的 key。 for-of 循环：可以枚举数组中的值，以及对象中的 value。 例如： （2）while 循环 （3）do while 循环 do while 语句与 while 语句非常相似。唯一的区别是，do while 语句限制先循环体后检查条件。不管条件的值如何，我们都要至少执行一次循环体。 6. 对象 英文名称：Object。 类似于 C++ 中的 map，由 key:value 对构成。 value 可以是变量、数组、对象、函数等。 函数定义中的 this 用来引用该函数的“拥有者”。 例如： 对象属性与函数的调用方式： （1）用 . 调用：person.name、person.add_money()。 （2）用 [] 调用：person[&quot;name&quot;]、person[&quot;add_money&quot;]()。 7. 数组 数组是一种特殊的对象，类似于 C++ 中的数组，但是 JavaScript 数组中的元素类型可以不同（数组中的元素可以是变量、数组、对象、函数）。例如： 可以通过下标访问数组元素，例如： 数组的常用属性和函数： 属性 length：返回数组长度。注意 length 是属性，不是函数，因此调用的时候不要加 ()。 函数 push()：向数组末尾添加元素。 函数 pop()：删除数组末尾的元素。 函数 splice(a, b)：删除从下标 a 开始的 b 个元素。 函数 sort()：将整个数组从小到大排序。 自定义比较函数：array.sort(cmp)，函数 cmp 输入两个需要比较的元素，返回一个实数，负数表示第一个参数排在第二个参数前面，零表示相等，正数表示第一个参数排在第二个参数后面。因此如果要实现从大到小排序只需要令函数为：function(a, b) &#123; return b - a; &#125;。 8. 函数 JavaScript 中的函数是用对象来实现的，定义完函数后是允许再对这个对象进行修改的。函数的定义方式如下： 函数返回值：如果未定义返回值，则返回 undefined。 9. 类 与 C++ 中的 Class 类似，但是不存在私有成员，this 指向类的实例。 （1）定义 （2）继承 注意： super 这个关键字，既可以当作函数使用，也可以当作对象使用。 作为函数调用时，代表父类的构造函数，且只能用在子类的构造函数之中。 作为对象时，指向父类的原型对象。 在子类的构造函数中，只有调用 super 之后，才可以使用 this 关键字。 成员重名时，子类的成员会覆盖父类的成员，类似于 C++ 中的多态。 （3）静态方法 在成员函数前添加 static 关键字即可。静态方法可以被子类继承，但是不会被类的实例继承，只能通过类名来调用。例如： （4）静态变量 在 ES6 中，只能通过 class.propname 定义和访问，子类可以继承父类的静态变量，即可以通过子类名访问静态变量。例如： 10. 事件 JavaScript 的代码一般通过事件触发。可以通过 addEventListener 函数为元素绑定事件的触发函数。 常见的触发函数如下： （1）鼠标 click：鼠标左键点击。 dblclick：鼠标左键双击。 contextmenu：鼠标右键点击。 mousedown：鼠标按下，包括左键、滚轮、右键。 event.button：0表示左键，1表示中键，2表示右键。 mouseup：鼠标弹起，包括左键、滚轮、右键。 event.button：0表示左键，1表示中键，2表示右键。 例如： （2）键盘 keydown：某个键是否被按住，事件会连续触发。 event.code：返回按的是哪个键。 event.altKey、event.ctrlKey、event.shiftKey 分别表示是否同时按下了 alt、ctrl、shift 键。 keyup：某个按键是否被释放。 event 常用属性同上。 keypress：紧跟在 keydown 事件后触发，只有按下字符键时触发，适用于判定用户输入的字符。 event 常用属性同上。 keydown、keyup、keypress 的关系类似于鼠标的 mousedown、mouseup、click。 （3）表单 focus：聚焦某个元素。 blur：取消聚焦某个元素。 change：某个元素的内容发生了改变。 （4）窗口 需要作用到 window 元素上。 resize：当窗口大小放生变化。 scroll：滚动指定的元素。 load：当元素全部被加载完成。 11. 常用库 11.1 jQuery jQuery 能够让我们更加方便地去获取前端的某一个标签、绑定某个事件、改变前端的某个标签的 CSS 属性。 （1）下载地址：jQuery 官网。 （2）使用方式：在 &lt;head&gt; 元素中添加：&lt;script src=&quot;/Web Application Lesson/static/js/jquery-3.6.1.min.js&quot;&gt;&lt;/script&gt;。 （3）选择器 $(selector)，selector 类似于 CSS 的选择器。例如： （4）事件 $(selector).on(event, func) 绑定事件，例如： $(selector).off(event, func) 删除事件，例如： 当存在多个相同类型的事件触发函数时，可以通过 click.name 来区分，例如： 在事件触发的函数中的 return false 等价于同时执行： e.stopPropagation()：阻止事件向上传递。例如 a 是 div 的子标签，当点击 a 时同样会触发 div 的 click 事件，当在 a 的事件触发函数中加上该语句时点击 a 就不会触发 div 的 click 事件。 e.preventDefault()：阻止事件的默认行为。例如点击 a 时不打开链接，并向上传递触发 div 的 click 事件。 （5）元素的隐藏、展现 $A.hide()：隐藏，可以添加参数，表示消失时间（毫秒）。 $A.show()：展现，可以添加参数，表示出现时间。 $A.fadeOut()：颜色淡退至消失，可以添加参数，表示消失时间。 $A.fadeIn()：颜色淡增至出现，可以添加参数，表示出现时间。 （6）元素的添加、删除 $('&lt;div class=&quot;mydiv&quot;&gt;&lt;span&gt;Hello World&lt;/span&gt;&lt;/div&gt;')：构造一个 jQuery 对象。 $A.append($B)：将 $B 添加到 $A 的末尾。 $A.prepend($B)：将 $B 添加到 $A 的开头。 $A.remove()：删除元素 $A。 $A.empty()：清空元素 $A 的所有儿子。 （7）对类的操作（此处 class_name 无需加 .） $A.addClass(class_name)：添加某个类。 $A.removeClass(class_name)：删除某个类。 $A.hasClass(class_name)：判断某个类是否存在。 （8）对 CSS 的操作 $(&quot;div&quot;).css(&quot;background-color&quot;)：获取某个 CSS 的属性。 $(&quot;div&quot;).css(&quot;background-color&quot;, &quot;yellow&quot;)：设置某个 CSS 的属性。 同时设置多个 CSS 的属性（注意 JS 中带 - 的标签必须加引号，如果不加会被当做减号）： （9）对标签属性的操作（除 class、id 外可以随意创造新的属性，例如：&lt;div abc=&quot;abc&quot;&gt;&lt;/div&gt;） $('div').attr('id')：获取属性。 $('div').attr('id', 'ID')：设置属性。 （10）对 HTML 内容、文本的操作 不需要背每个标签该用哪种，用到的时候 Google 或者 Bing 即可。 $A.html()：获取、修改 HTML 内容（加参数即可修改），例如：&lt;div&gt;&lt;span&gt;span content&lt;/span&gt;&lt;/div&gt; 输出为 &lt;span&gt;span content&lt;/span&gt;。 $A.text()：获取、修改文本信息，例如：&lt;div&gt;&lt;span&gt;span content&lt;/span&gt;&lt;/div&gt; 输出为 span content。 $A.val()：获取、修改文本的值，一般用在 input、textarea 中。 （11）查找 $(selector).parent(filter)：查找父元素。 $(selector).parents(filter)：查找所有祖先元素。 $(selector).children(filter)：在所有子元素中查找。 $(selector).find(filter)：在所有后代元素中查找。 （12）ajax ajax 可以让我们在不刷新页面的情况下只从服务器端获取某些数据，一般是获取一个 json 数据。 GET 方法（从服务器端获取内容）： POST 方法（把表单内容提交给服务器）： 11.2 setTimeout与setInterval （1）setTimeout(func, delay) 经过 delay 毫秒后，执行函数 func()，可以使用 clearTimeout() 关闭定时器。例如： （2）setInterval(func, delay) 每隔 delay 毫秒，执行一次函数 func()，第一次在第 delay 毫秒后执行，可以使用 clearInterval() 关闭周期执行的函数。例如： 11.3 requestAnimationFrame requestAnimationFrame(func) 函数会在下次浏览器刷新页面之前执行一次，一般浏览器每秒刷新60次，因此通常会用递归写法使其每秒执行60次 func() 函数。调用时会向 func() 传入一个参数，表示函数执行的时间戳，单位为毫秒。例如： 使用 setTimeout 和 setInterval 实现以上效果的代码如下： 与 setTimeout 和 setInterval 的区别： requestAnimationFrame 渲染动画的效果更好，性能更佳。该函数可以保证每两次调用之间的时间间隔相同，但 setTimeout 与 setInterval 不能保证这点。setTmeout 两次调用之间的间隔包含回调函数的执行时间；setInterval 只能保证按固定时间间隔将回调函数压入栈中，但具体的执行时间间隔仍然受回调函数的执行时间影响。 当页面在后台时，因为页面不再渲染，因此 requestAnimationFrame 不再执行。但 setTimeout 与 setInterval 函数会继续执行。 11.4 Map与Set （1）Map Map 对象保存键值对。 用 for...of 或者 forEach 可以按插入顺序遍历。 键值可以为任意值，包括函数、对象或任意基本类型。 常用 API： set(key, value)：插入键值对，如果 key 已存在，则会覆盖原有的 value。 get(key)：查找关键字，如果不存在，返回 undefined。 size：返回键值对数量。 has(key)：返回是否包含关键字 key。 delete(key)：删除关键字 key。 clear()：删除所有元素。 例如： （2）Set Set 对象允许你存储任何类型的唯一值，无论是原始值或者是对象引用。 用 for...of 或者 forEach 可以按插入顺序遍历。 常用 API： add()：添加元素。 has()：返回是否包含某个元素。 size：返回元素数量。 delete()：删除某个元素。 clear()：删除所有元素 例如： 11.5 localStorage localStorage 可以在用户的浏览器上存储键值对。常用 API： setItem(key, value)：插入。 getItem(key)：查找。 removeItem(key)：删除。 clear()：清空。 例如： 11.6 JSON JSON 对象用于序列化对象、数组、数值、字符串、布尔值和 null。常用 API： JSON.parse()：将字符串解析成对象。 JSON.stringify()：将对象转化为字符串。 例如： 11.7 日期 （1）返回值为整数的 API，数值为 1970-1-1 00:00:00 UTC（世界标准时间）到某个时刻所经过的毫秒数： Date.now()：返回现在时刻。 Date.parse(&quot;2022-04-15T15:30:00.000+08:00&quot;)：返回北京时间2022年4月15日15:30:00的时刻。 （2）与 Date 对象的实例相关的 API： new Date()：返回现在时刻。 new Date(&quot;2022-04-15T15:30:00.000+08:00&quot;)：返回北京时间2022年4月15日15:30:00的时刻。 两个 Date 对象实例的差值为毫秒数。 getDay()：返回星期，0表示星期日，1-6表示星期一至星期六。 getDate()：返回日，数值为1-31。 getMonth()：返回月，数值为0-11。 getFullYear()：返回年份。 getHours()：返回小时。 getMinutes()：返回分钟。 getSeconds()：返回秒。 getMilliseconds()：返回毫秒。 例如： 11.8 WebSocket 与服务器建立全双工连接。常用 API： new WebSocket('ws://localhost:8080');：建立 WS 连接。 send()：向服务器端发送一个字符串。一般用 JSON 将传入的对象序列化为字符串。 onopen：类似于 onclick，当连接建立时触发。 onmessage：当从服务器端接收到消息时触发。 close()：关闭连接。 onclose：当连接关闭后触发。 11.9 window window.open(&quot;;)：在新标签栏中打开页面。 location.reload()：刷新页面。 location.href = &quot;;：在当前标签栏中打开页面。 11.10 Canvas Canvas 教程参考：Canvas Tutorial (English Version)、Canvas 教程（中文）。 上一章：Web学习笔记-CSS。 下一章：Web学习笔记-React（配置环境、ES6语法补充、Components）。"},{"title":"Web学习笔记-CSS","date":"2022-11-09T07:49:00.000Z","url":"/posts/11050.html","tags":[["Web","/tags/Web/"]],"categories":[["Web","/categories/Web/"]],"content":" 本文记录 CSS 的学习过程。 CSS（层叠样式表）是一种用来为结构化文档（如 HTML 文档或 XML 应用）添加样式（字体、间距和颜色等）的计算机语言，CSS 文件扩展名为：.css。 1. 样式定义方式 （1）行内样式表（inline style sheet） 直接定义在标签的 style 属性中，仅对当前标签产生影响。 （2）内部样式表（internal style sheet） 定义在 style 标签中，通过选择器影响对应的标签，可以对同一个页面中的多个标签产生影响。 （3）外部样式表（external style sheet） 定义在 .css 样式文件中，通过选择器影响对应的标签。可以用 link 标签引入某些页面，可以对多个页面产生影响。 首先在 /static/css 文件夹下创建 style.css 文件，将之前定义的样式代码移到该文件下： 然后在 .html 文件中用 link 链接该样式表即可： 2. 选择器 （1）标签选择器 例如选择所有 div 标签： （2）ID 选择器 例如选择 ID 为 rect_1 的标签： （3）类选择器 例如选择所有 rectangle 类的标签（注意：习惯上一个页面的 id 是唯一的，而 class 不是唯一的；且一个标签可以同时有多个 class，用空格隔开即可，多个 class 的效果根据 .css 文件中的定义顺序进行覆盖，后定义的覆盖先定义的样式）： （4）伪类选择器 伪类用于定义元素的特殊状态。 链接伪类选择器： :link：链接访问前的样式 :visited：链接访问后的样式 :hover：鼠标悬停时的样式 :active：鼠标点击后长按时的样式 :focus：聚焦后的样式 位置伪类选择器：:nth-child(n)：选择是其父标签第 n 个子元素的所有元素。 目标伪类选择器：:target：当 url 指向该元素时生效。 以上就是较为常用的选择器，现在来看一个综合示例，首先是 index.html 代码： style.css代码： （5）复合选择器 由两个及以上基础选择器组合而成的选择器。 element1, element2：同时选择元素 element1 和元素 element2。 element.class：选则包含某类的 element 元素。 element1 + element2：选择紧跟 element1 的 element2 元素。 element1 element2：选择 element1 内的所有 element2 元素。 element1 &gt; element2：选择父标签是 element1 的所有 element2 元素。 （6）通配符选择器 *：选择所有标签。 [attribute]：选择具有某个属性的所有标签。 [attribute=value]：选择 attribute 值为 value 的所有标签。 （7）伪元素选择器 将特定内容当做一个元素，选择这些元素的选择器被称为伪元素选择器。 ::first-letter：选择第一个字母。 ::first-line：选择第一行。 ::selection：选择已被选中的内容。 ::after：可以在元素后插入内容。 ::before：可以在元素前插入内容。 （8）样式渲染优先级 权重大小，越具体的选择器权重越大：!important &gt; 行内样式 &gt; ID 选择器 &gt; 类与伪类选择器 &gt; 标签选择器 &gt; 通用选择器。 权重相同时，后面的样式会覆盖前面的样式。 继承自父元素的权重最低。 3. 颜色 （1）预定义的颜色值 black、white、red、green、blue、lightblue 等。 （2）16进制表示法 使用6位16进制数表示颜色，例如：#ADD8E6。 其中第1-2位表示红色，第3-4位表示绿色，第5-6位表示蓝色。 简写方式：#ABC，等价于 #AABBCC。 （3）RGB 表示法 rgb(173, 216, 230)，其中第一个数表示红色，第二个数表示绿色，第三个数表示蓝色。 （4）RGBA 表示法 rgba(173, 216, 230, 0.5)，前三个数同上，第四个数表示透明度。 （5）取色方式 网页里的颜色，可以在 Chrome 浏览器的调试模式下获取 其他颜色可以使用 QQ 的截图软件：直接按 c 键，可以复制 RGB 颜色值；按住 shift 再按 c 键，可以复制16进制颜色值。 4. 文本 长度单位： px：设备上的像素点 %：相对于父元素的百分比 em：相对于当前元素的字体大小（倍） rem：相对于根元素的字体大小（倍） vw：相对于视窗宽度的百分比 vh：相对于视窗高度的百分比 （1）text-align text-align 属性定义行内内容（例如文字）如何相对它的块父元素对齐。text-align 并不控制块元素自己的对齐，只控制它的行内内容的对齐。 （2）line-height line-height 属性用于设置多行元素的空间量，如多行文本的间距。对于块级元素，它指定元素行盒（line boxes）的最小高度。对于非替代的 inline 元素，它用于计算行盒（line box）的高度。当 line-height 与 height 相等时可以让字体竖直居中。 （3）letter-spacing letter-spacing 属性用于设置文本字符的间距。 （4）text-indent text-indent 属性能定义一个块元素首行文本内容之前的缩进量。 （5）text-decoration text-decoration 属性是用于设置文本的修饰线外观的（下划线、上划线、贯穿线/删除线或闪烁）它是 text-decoration-line，text-decoration-color，text-decoration-style，和新出现的text-decoration-thickness 属性的缩写。 （6）text-shadow text-shadow 为文字添加阴影。可以为文字与 text-decorations 添加多个阴影，阴影值之间用逗号隔开。每个阴影值由(X方向的偏移量 Y方向的偏移量 模糊半径 颜色值)组成。 综合示例： 5. 字体 （1）font-size font-size 属性指定字体的大小。因为该属性的值会被用于计算 em 和 ex 长度单位，定义该值可能改变其他元素的大小。 （2）font-style font-style 属性允许你选择 font-family 字体下的 italic 或 oblique 样式。 （3）font-weight font-weight 属性指定了字体的粗细程度。一些字体只提供 normal 和 bold 两种值。 （4）font-family font-family 属性允许您通过给定一个有先后顺序的，由字体名或者字体族名组成的列表来为选定的元素设置字体。属性值用逗号隔开。浏览器会选择列表中第一个该计算机上有安装的字体，或者是通过 @font-face 指定的可以直接下载的字体。 综合示例： 6. 背景 （1）background-color background-color 属性会设置元素的背景色，属性的值为颜色值或关键字 transparent（透明）二者选其一。 （2）background-image background-image 属性用于为一个元素设置一个或者多个背景图像。渐变色：linear-gradient(rgba(0, 0, 255, 0.5), rgba(255, 255, 0, 0.5))。 （3）background-size background-size 属性设置背景图片大小。图片可以保有其原有的尺寸，或者拉伸到新的尺寸，或者在保持其原有比例的同时缩放到元素的可用空间的尺寸。 （4）background-repeat background-repeat 属性定义背景图像的重复方式。背景图像可以沿着水平轴，垂直轴，两个轴重复，或者根本不重复。 （5）background-position background-position 属性为背景图片设置初始位置。 （6）background-attachment background-attachment 属性决定背景图像的位置是在视口内固定，或者随着包含它的区块滚动。 综合示例： 7. 边框 （1）border-style border-style 属性用来设定元素所有边框的样式。其内容为：(border-top-style border-right-style border-bottom-style border-left-style)，之后的所有属性设置内容格式也如此。 （2）border-width border-width 属性用于设置元素的边框宽度。 （3）border-color border-color 属性用于设置元素四个边框的颜色。 （4）border-radius border-radius 属性允许你设置元素外边框的圆角。当使用一个半径时确定一个圆形，当使用两个半径时确定一个椭圆。这个（椭）圆与边框的交集形成圆角效果。 （5）border-collapse border-collapse 属性是用来决定表格的边框是分开的还是合并的。在分隔模式下，相邻的单元格都拥有独立的边框。在合并模式下，相邻单元格共享边框。 综合示例： 8. 元素展示格式 （1）display block：独占一行，width、height、margin、padding 均可控制，width 默认100%。例如 &lt;div&gt;。 inline：可以共占一行，width 与 height 无效，水平方向的 margin 与 padding 有效，竖直方向的 margin 与 padding 无效，width 默认为本身内容宽度。例如 &lt;span&gt;。 inline-block：可以共占一行，width、height、margin、padding 均可控制，width 默认为本身内容宽度。例如 &lt;img&gt;。 （2）white-space white-space 属性是用来设置如何处理元素中的空白。 （3）text-overflow text-overflow 属性确定如何向用户发出未显示的溢出内容信号。它可以被剪切，显示一个省略号或显示一个自定义字符串。 （4）overflow overflow 属性定义当一个元素的内容太大而无法适应块级格式化上下文的时候该做什么。它是 overflow-x 和 overflow-y 的简写属性。 9. 内边距与外边距 （1）margin margin 属性为给定元素设置所有四个（上下左右）方向的外边距属性。 可以接受1~4个值（上、右、下、左的顺序）。 可以分别指明四个方向：margin-top、margin-right、margin-bottom、margin-left。 可取值： length：固定值，例如：20px。 percentage：相对于包含块的宽度，以百分比值为外边距，例如：20%。 auto：让浏览器自己选择一个合适的外边距。有时，在一些特殊情况下，该值可以使元素居中。 外边距重叠： 块的上外边距 margin-top 和下外边距 margin-bottom 有时合并（折叠）为单个边距，其大小为单个边距的最大值（或如果它们相等，则仅为其中一个），这种行为称为边距折叠。 父元素与后代元素：父元素没有上边框和 padding 时，后代元素的 margin-top 会溢出，溢出后父元素的 margin-top 会与后代元素取最大值。 （2）padding padding 属性控制元素所有四条边的内边距区域。 可以接受1~4个值（上、右、下、左的顺序）。 可以分别指明四个方向：padding-top、padding-right、padding-bottom、padding-left。 可取值： length：固定值。 percentage：相对于包含块的宽度，以百分比值为内边距。 10. 盒子模型 box-sizing：定义了 user agent 应该如何计算一个元素的总宽度和总高度。 content-box：是默认值，设置 border 和 padding 均会增加元素的宽高。 border-box：设置 border 和 padding 不会改变元素的宽高，而是挤占内容区域。 11. 位置 position：用于指定一个元素在文档中的定位方式。 定位类型： 定位元素（positioned element）是 position 值为 relative、absolute、fixed 或 sticky 的元素。（换句话说，它是除 static 以外的任何东西）。 相对定位元素（relatively positioned element）是 position 值为 relative 的元素。top 和 bottom 属性指定相对其正常位置的垂直偏移量，left 和 right 属性指定水平偏移量。 绝对定位元素（absolutely positioned element）是 position 值为 absolute 或 fixed 的元素。 粘性定位元素（stickily positioned element）是 position 值为 sticky 的元素。 取值： static：该关键字指定元素使用正常的布局行为，即元素在文档常规流中当前的布局位置。此时 top、right、bottom、left 和 z-index 属性无效，其中 z-index 属性指定元素在Z轴上在第几层，也就是垂直于屏幕朝外的方向。 relative：该关键字下，元素先放置在未添加定位时的位置，然后在不改变页面布局的前提下调整元素位置（因此会在此元素未添加定位时所在位置即初始位置留下空白）。top、right、bottom、left 等调整元素相对于初始位置的偏移量。 absolute：元素会被移出正常文档流，并不为元素预留空间，通过指定元素相对于最近的非 static 定位祖先元素的偏移，来确定元素位置。绝对定位的元素可以设置外边距（margins），且不会与其他边距合并。 fixed：元素会被移出正常文档流，并不为元素预留空间，而是通过指定元素相对于屏幕视口（viewport）的位置来指定元素位置。元素的位置在屏幕滚动时不会改变。 sticky：元素根据正常文档流进行定位，然后相对它的最近滚动祖先（nearest scrolling ancestor）和 containing block（最近块级祖先，nearest block-level ancestor），包括 table-related 元素，基于 top、right、bottom 和 left 的值进行偏移。偏移值不会影响任何其他元素的位置。 综合示例： 12. 浮动 （1）float float 属性指定一个元素应沿其容器的左侧或右侧放置，允许文本和内联元素环绕它。该元素从网页的正常流动（文档流）中移除，尽管仍然保持部分的流动性（与绝对定位相反）。 由于 float 意味着使用块布局，它在某些情况下修改 display 值的计算值：display 为 inline 或 inline-block 时，使用 float 后会统一变成 block。 取值： left：表明元素必须浮动在其所在的块容器左侧的关键字。 right：表明元素必须浮动在其所在的块容器右侧的关键字。 （2）clear 有时，你可能想要强制元素移至任何浮动元素下方。比如说，你可能希望某个段落与浮动元素保持相邻的位置，但又希望这个段落从头开始强制独占一行。此时可以使用 clear。 取值： left：清除左侧浮动。 right：清除右侧浮动。 both：清除左右两侧浮动。 13. 中期实战 现在来实现一个效果如下图所示的用户个人信息卡以及B站个人资料卡，限于篇幅就不放上代码了： 14. flex布局 flex 属性设置了 flex 项目如何增大或缩小以适应其 flex 容器中可用的空间。 （1）flex-direction flex-direction 属性指定了内部元素是如何在 flex 容器中布局的，定义了主轴的方向（正方向或反方向）。 取值： row：flex 容器的主轴被定义为与文本方向相同。主轴起点和主轴终点与内容方向相同。 row-reverse：表现和 row 相同，但是置换了主轴起点和主轴终点。 column：flex 容器的主轴和交叉轴相同。主轴起点与主轴终点和书写模式的前后点相同。 column-reverse：表现和 column 相同，但是置换了主轴起点和主轴终点。 （2）flex-wrap flex-wrap 属性指定 flex 元素单行显示还是多行显示。如果允许换行，这个属性允许你控制行的堆叠方向。 取值： nowrap：默认值，不换行。 wrap：换行，第一行在上方。 wrap-reverse：换行，第一行在下方。 （3）flex-flow flex-flow 属性是 flex-direction 和 flex-wrap 的简写。默认值为：row nowrap。 （4）justify-content justify-content 属性定义了浏览器如何沿 flex 容器的主轴和 grid 容器的内联轴分配内容项之间和周围的空间。 取值： flex-start：默认值，沿主轴起点方向对齐。 flex-end：沿主轴终点方向对齐。 start：如果主轴是 row 或 row-reverse，则左对齐，如果主轴是 column 或 column-reverse，则上对齐。 end：如果主轴是 row 或 row-reverse，则右对齐，如果主轴是 column 或 column-reverse，则下对齐。 space-between：沿主轴的两端对齐。 space-around：在主轴上均匀分配弹性元素。相邻元素间距离相同。第一个元素到主轴起始位置的距离和最后一个元素到主轴结束位置的距离将会是相邻元素之间距离的一半。 space-evenly：沿着主轴均匀分布在指定的对齐容器中。相邻元素之间的间距，主轴起始位置到第一个元素的间距，主轴结束位置到最后一个元素的间距，都完全一样。 center：容器中的元素居中，且元素之间紧贴没有空隙。 （5）align-items align-items 属性将所有直接子节点上的 align-self 值设置为一个组。align-self 属性设置项目在其包含块中在交叉轴方向上的对齐方式。 取值： flex-start：元素向交叉轴起点对齐。 flex-end：元素向交叉轴终点对齐。 center：元素在交叉轴居中。 stretch：元素在交叉轴方向被拉伸到与 flex 容器相同的高度或宽度（元素未被设定高度或宽度的前提下）。 （6）align-content align-content 属性设置了浏览器如何沿着 flex 布局的交叉轴和 grid 布局的主轴在内容项之间和周围分配空间。 取值： flex-start：所有行从交叉轴起点开始填充，第一行的交叉轴起点边和容器的交叉轴起点边对齐，接下来的每一行紧跟前一行中间没有空隙。 flex-end：所有行从交叉轴末尾开始填充，最后一行的交叉轴终点边和容器的交叉轴终点边对齐，同时所有后续行与前一行紧贴。 center：所有行朝向容器的交叉轴中心填充，每行互相紧挨，相对于容器居中对齐，容器的交叉轴起点边和第一行的距离相等于容器的交叉轴终点边和最后一行的距离。 stretch：拉伸所有行来填满剩余空间。剩余空间平均地分配给每一行。 （7）order order 属性定义 flex 项目的顺序，值越小越靠前。 （8）flex-grow flex-grow 属性设置 flex 容器主尺寸的 flex 增长系数，也就是 flex 容器中的元素随着容器尺寸的增大而增大（nowrap 前提下）。负值无效，默认为0。 （9）flex-shrink flex-shrink 属性指定了 flex 元素的收缩规则。flex 元素仅在默认宽度之和大于 flex 容器的时候才会发生收缩，其收缩的大小是依据 flex-shrink 的值。负值无效，默认为1。 （10）flex-basis flex-basis 属性指定了 flex 元素在主轴方向上的初始大小。取值可以是长度例如：100px，也可以是一个相对于其父 flex 容器主轴尺寸的百分比。不允许为负值。默认为 auto。 （11）flex flex-grow、flex-shrink、flex-basis 的缩写。 常用取值： auto：flex: 1 1 auto none：flex: 0 0 auto 综合样例： 15. 响应式布局 （1）media 查询：可以查询屏幕的各种信息，比如查询宽度，当屏幕宽度满足特定条件时应用某种 CSS。例如： （2）栅格系统：预先将屏幕宽度分为12份，然后设定各元素在不同的屏幕宽度下应该占几份，例如： （3）Bootstrap 根据上述例子可以发现如果自己手动实现 CSS 代码会很长，因此可以直接使用 Bootstrap，首先前往官网下载：Bootstrap 官网。然后在 static 中创建一个新文件夹 third_party（第三方），将下载好的 Bootstrap 放进来。在代码中引入 bootstrap.min.css 以及 bootstrap.min.js 后即可直接使用，例如： Bootstrap 中一共定义了以下几种类型： 此外，Bootstrap 还提供了很多设计好的组件，直接在官网中根据示例代码即可学习与使用，例如实现一个简单好看的表单如下图所示： 代码如下： 上一章：Web学习笔记-HTML。 下一章：Web学习笔记-JavaScript。"},{"title":"Web学习笔记-HTML","date":"2022-11-08T04:05:00.000Z","url":"/posts/28433.html","tags":[["Web","/tags/Web/"]],"categories":[["Web","/categories/Web/"]],"content":" 本文记录 HTML 的学习过程。 MDN 官方文档：MDN Web Docs。 1. VS Code环境配置 （1）Live Server 由于一般写网站时都是部署在 Linux 上，该插件可以模拟一个终端，相当于模拟了一个真正的开发环境（后端）。 （2）Auto Rename Tag 当修改 HTML 标签时，自动修改对应的标签对。 （3）自动格式化 在 Setting-Text Editor-Formatting 中勾选 Format On Save，这样保存代码的时候会自动格式化代码。 2. HTML基础标签 2.1 HTML文件结构 HTML 的所有标签为树形结构，一般都有一个开始标签和一个结束标签，开始标签和结束标签之间的标签就是子节点，同级的标签就是兄弟节点，例如： （1）&lt;html&gt; 标签 表示一个HTML文档的根（顶级元素），所以它也被称为根元素。所有其他元素必须是此元素的后代。 （2）&lt;head&gt; 标签： 规定文档相关的配置信息（元数据），包括文档的标题，引用的文档样式和脚本等。 （3）&lt;body&gt; 标签： 表示文档的内容。document.body 属性提供了可以轻松访问文档的 body 元素的脚本。 （4）&lt;title&gt; 标签： 定义文档的标题，显示在浏览器的标题栏或标签页上。它只应该包含文本，若是包含有标签，则它包含的任何标签都将被忽略。 （5）&lt;meta&gt; 标签： 表示那些不能由其它 HTML 元相关（meta-related）元素（&lt;base&gt;、&lt;link&gt;、&lt;script&gt;、&lt;style&gt; 或 &lt;title&gt;）之一表示的任何元数据信息。 常见属性： charset：这个属性声明了文档的字符编码。如果使用了这个属性，其值必须是与 ASCII 大小写无关（ASCII case-insensitive）的 utf-8。 name：name 和 content 属性可以一起使用，以 名 - 值 对的方式给文档提供元数据，其中 name 作为元数据的名称，content 作为元数据的值。 （6）link 标签： 规定了当前文档与外部资源的关系。该元素最常用于链接样式表，此外也可以被用来创建站点图标 &lt;icon&gt;，例如： （7）&lt;!-- 多行注释 --&gt;： HTML 中只有多行注释，没有单行注释。 综合示例如下： 2.2 文本标签 文本标签虽然很多，但大部分可看成是预定好样式的 &lt;div&gt; 和 &lt;span&gt;。 （1）&lt;div&gt; 标签： &lt;div&gt; 元素（或 HTML 文档分区元素）是一个通用型的流内容容器，在不使用 CSS 的情况下，其对内容或布局没有任何影响。其他块级标签例如：&lt;h1&gt;, &lt;p&gt;, &lt;pre&gt;, &lt;ul&gt;, &lt;ol&gt;, &lt;table&gt;。方便后续为某一块内容设置样式，且在 JS 中可以对各个 div 进行操作。即在逻辑上将某一块代码归为一类。 （2）&lt;span&gt; 标签： &lt;span&gt; 元素是短语内容的通用行内容器，并没有任何特殊语义。可以使用它来编组元素以达到某种样式意图（通过使用类 class 或者 id 属性），或者这些元素有着共同的属性，比如 lang。应该在没有其他合适的语义元素时才使用它。&lt;span&gt; 与 &lt;div&gt; 元素很相似，但 &lt;div&gt; 是一个块元素而 &lt;span&gt; 则是行内元素。其他内联标签例如：&lt;i&gt;, &lt;b&gt;, &lt;del&gt;, &lt;ins&gt;, &lt;td&gt;, &lt;a&gt;。 （3）&lt;h1&gt; - &lt;h6&gt; 标签： HTML 标题（Heading）元素呈现了六个不同的级别的标题，&lt;h1&gt; 级别最高，而 &lt;h6&gt; 级别最低。 （4）&lt;p&gt; 标签： &lt;p&gt; 元素（或者说 HTML 段落元素）表示文本的一个段落。该元素通常表现为一整块与相邻文本分离的文本，或以垂直的空白隔离或以首行缩进。另外，&lt;p&gt; 是块级元素。 （5）&lt;pre&gt; 标签： &lt;pre&gt; 元素表示预定义格式文本。在该元素中的文本通常按照原文件中的编排，以等宽字体的形式展现出来，文本中的空白符（比如空格和换行符）都会显示出来。（紧跟在 &lt;pre&gt; 开始标签后的换行符也会被省略） （6）&lt;br&gt; 标签： &lt;br&gt; 元素在文本中生成一个换行（回车）符号。此元素在写诗和地址时很有用，这些地方的换行都非常重要。 （7）&lt;hr&gt; 标签： &lt;hr&gt; 元素表示段落级元素之间的主题转换（例如，一个故事中的场景的改变，或一个章节的主题的改变）。 在 HTML 的早期版本中，它是一个水平线。现在它仍能在可视化浏览器中表现为水平线，但目前被定义为语义上的，而不是表现层面上。所以如果想画一条横线，请使用适当的 CSS 样式来修饰。 （8）&lt;i&gt; 标签： &lt;i&gt; 元素用于表现因某些原因需要区分普通文本的一系列文本。例如技术术语、外文短语或是小说中人物的思想活动等，它的内容通常以斜体显示。 （9）&lt;b&gt; 标签： HTML 提醒注意（Bring Attention To）元素 &lt;b&gt; 用于吸引读者的注意到该元素的内容上（如果没有另加特别强调）。这个元素过去被认为是粗体（Boldface）元素，并且大多数浏览器仍然将文字显示为粗体。尽管如此，你不应将 &lt;b&gt; 元素用于显示粗体文字；替代方案是使用 CSS 中的 font-weight 属性来创建粗体文字。 （10）&lt;del&gt; 标签： &lt;del&gt; 元素表示一些被从文档中删除的文字内容，显示效果为在文字内容中间划一道横线。比如可以在需要显示修改记录或者源代码差异的情况使用这个标签。&lt;ins&gt; 标签的作用恰恰于此相反：表示文档中添加的内容。 （11）&lt;ins&gt; 标签： &lt;ins&gt; 元素定义已经被插入文档中的文本，显示效果为在文字内容底部划一道横线。 2.3 图片 &lt;img&gt; 元素将一份图像嵌入文档。默认为行内元素，即 display: inline。 （1）src 属性： 该属性是必须的，它包含了你想嵌入的图片的文件路径。 （2）alt 属性： 该属性包含一条对图像的文本描述，这不是强制性的，但对可访问性而言，它十分有用。屏幕阅读器会将这些描述读给需要使用阅读器的使用者听，让他们知道图像的含义。或者如果由于某种原因无法加载图像，普通浏览器也会在页面上显示 alt 属性中的备用文本：例如，网络错误、内容被屏蔽或链接过期时。 （3）height 属性： 图像的高度，在 HTML5 中的单位是 CSS 像素，在 HTML4 中既可以是像素也可以是百分比。可以只指定 width 和 height 中的一个值，浏览器会根据原始图像比例进行缩放。 （4）width 属性： 图像的宽度，在 HTML5 中的单位是 CSS 像素，在 HTML4 中既可以是像素也可以是百分比。 2.4 音频与视频 （1）&lt;audio&gt; 标签： &lt;audio&gt; 元素用于在文档中嵌入音频内容。&lt;audio&gt; 元素可以包含一个或多个音频资源，这些音频资源可以使用 src 属性或者 &lt;source&gt; 元素来进行描述：浏览器将会选择最合适的一个来使用。也可以使用 MediaStream 将这个元素用于流式媒体。 使用 src 属性播放： &lt;audio&gt; 与多个 &lt;source&gt; 元素： 这个例子包含了多个 &lt;source&gt; 元素。如果能够播放的话，浏览器就会试图去加载第一个 source 元素；如果不行，那就退而求其次去加载第二个。 （2）&lt;video&gt; 标签： &lt;video&gt; 元素用于在 HTML 或者 XHTML 文档中嵌入媒体播放器，用于支持文档内的视频播放。你也可以将 &lt;video&gt; 标签用于音频内容，但是 &lt;audio&gt; 元素可能在用户体验上更合适。该标签的使用方法与 &lt;audio&gt; 相同。 2.5 超链接 &lt;a&gt; 元素（或称锚元素）可以通过它的 href 属性创建通向其他网页、文件、同一页面内的位置、电子邮件地址或任何其他 URL 的超链接。&lt;a&gt; 中的内容应该指明链接的意图。如果存在 href 属性，当 &lt;a&gt; 元素聚焦时按下回车键就会激活它。如果点击链接打开新标签页面需要加入属性：target=&quot;_blank&quot;。 2.6 表单 （1）&lt;form&gt; 标签： &lt;form&gt; 元素表示文档中的一个区域，此区域包含交互控件，用于向 Web 服务器提交信息。其常用的属性介绍如下： action：处理表单提交的 URL，就是想让表单提交到的地址。 method：浏览器提交表单使用的 HTTP 请求方式，若为 &quot;get&quot; 表单数据会附加在 action 属性的 URL 中，并以 '?' 作为分隔符；若为 &quot;post&quot; 表单数据会包含在表单体内然后发送给服务器。 （2）&lt;input&gt; 标签： &lt;input&gt; 元素表示一个用来填写内容的输入框，常见类型有： &lt;input type=&quot;text&quot;&gt;：创建基础的单行文本框。 &lt;input type=&quot;number&quot;&gt;：用于让用户输入一个数字。其包括内置验证以拒绝非数字输入。浏览器可能会选择提供步进箭头，让用户可以使用鼠标增加和减少输入的值，或者只需用指尖敲击即可。 &lt;input type=&quot;email&quot;&gt;：带有 email（电子邮箱）类型标记的输入框元素（&lt;input&gt;）能够让用户输入或编辑一个电子邮箱地址，此外，如果指定了 multiple 属性，用户还可以输入多个电子邮箱地址。在表单提交前，输入框会自动验证输入值是否是一个或多个合法的电子邮箱地址（非空值且符合电子邮箱地址格式）CSS 伪标签 :valid 和 :invalid 能够在校验后自动应用。 &lt;input type=&quot;password&quot;&gt;：&lt;input&gt; 元素里有一种叫做 password 的值，给我们一个方法让用户更加安全的输入密码。这个元素是作为一行纯文本编辑器控件呈现的，其中文本被遮蔽以致于无法读取，通常通过用诸如星号（*）或点（•）等符号替换每个字符来实现。这个符号会根据用户的浏览器和操作系统来具体显示哪个。 &lt;input type=&quot;radio&quot;&gt;：&lt;input&gt; 的 radio 类型元素默认渲染为小型圆圈图表，填充即为激活，类似于复选框（checkbox）类型。单选按钮允许你选择单一的值（value 属性）来提交表单。name 属性相同的为一组 radio，提交表单时的数据为 name=value 形式。 &lt;input type=&quot;checkbox&quot;&gt;：复选框，如果添加 checked 属性表示该复选框是否被默认选中（当页面加载时），当表单被提交时，只有当前被选中的复选框的 value 属性的值会被提交给服务器。 常用属性有： name：名称，提交表单时以 name=value 的形式提交 id：唯一ID maxlength：最大长度 minlength：最小长度 required：是否必填 placeholder：当表单控件为空时，控件中显示的内容 综合示例如下： （3）&lt;textarea&gt; 标签： &lt;textarea&gt; 元素表示一个多行纯文本编辑控件，当你希望用户输入一段相当长的、不限格式的文本，例如评论或反馈表单中的一段意见时，这很有用。参数 rows 指定初始的行数，cols 指定初始的列数。 （4）&lt;select&gt; 与 &lt;option&gt; 标签： &lt;select&gt; 元素表示一个提供选项菜单的控件。 （5）&lt;button&gt; 标签： &lt;button&gt; 元素表示一个可点击的按钮，可以用在表单或文档其它需要使用简单标准按钮的地方。默认情况下，HTML 按钮的显示样式接近于 user agent 所在的宿主系统平台（用户操作系统）的按钮，但你可以使用 CSS 来改变按钮的样貌。 2.7 列表 （1）&lt;ul&gt; 与 &lt;li&gt; 标签： &lt;ul&gt; 元素（或称 HTML 无序列表元素）表示一个内可含多个元素的无序列表或项目符号列表，通常渲染为一个用小点或者小圆圈表示的列表。 （2）&lt;ol&gt; 与 &lt;li&gt; 标签： &lt;ol&gt; 元素表示有序列表，通常渲染为一个带编号的列表。 （3）&lt;dl&gt;、&lt;dt&gt; 与 &lt;dd&gt; 标签： &lt;dl&gt; 元素（或 HTML 描述列表元素）是一个包含术语定义以及描述的列表，通常用于展示词汇表或者元数据（键-值对列表）。 2.8 表格 （1）&lt;table&gt; 标签： table 元素表示表格数据，即通过二维数据表表示的信息。 （2）&lt;thead&gt; 标签： &lt;thead&gt; 元素定义了一组定义表格的列头的行。 （3）&lt;tbody&gt; 标签： &lt;tbody&gt; 元素定义一组数据行。 （4）&lt;tr&gt; 标签： &lt;tr&gt; 元素定义表格中的行。同一行可同时出现 &lt;td&gt; 和 &lt;th&gt; 元素。 （5）&lt;th&gt; 标签： &lt;th&gt; 元素定义表格内的表头单元格。 （6）&lt;td&gt; 标签： &lt;td&gt; 元素定义了一个包含数据的表格单元格。 （7）&lt;caption&gt; 标签： &lt;caption&gt; 元素（or HTML 表格标题元素）展示一个表格的标题，它常常作为 &lt;table&gt; 的第一个子元素出现，同时显示在表格内容的最前面，但是，它同样可以被 CSS 样式化，所以，它同样可以出现在相对于表格的任意位置。 综合示例： 2.9 语义标签 （1）&lt;header&gt;： &lt;header&gt; 元素用于展示介绍性内容，通常包含一组介绍性的或是辅助导航的实用元素。它可能包含一些标题元素，但也可能包含其他元素，比如 Logo、搜索框、作者名称，等等。 （2）&lt;nav&gt;： &lt;nav&gt; 元素表示页面的一部分，其目的是在当前文档或其他文档中提供导航链接。导航部分的常见示例是菜单，目录和索引。 （3）&lt;section&gt;： &lt;section&gt; 元素表示一个包含在 HTML 文档中的独立部分，它没有更具体的语义元素来表示，一般来说会有包含一个标题。 （4）&lt;figure&gt;： &lt;figure&gt; 元素代表一段独立的内容，经常与说明（caption）&lt;figcaption&gt; 配合使用，并且作为一个独立的引用单元。当它属于主内容流（main flow）时，它的位置独立于主体。这个标签经常是在主文中引用的图片，插图，表格，代码段等等，当这部分转移到附录中或者其他页面时不会影响到主体。 （5）&lt;figcaption&gt;： &lt;figcaption&gt; 元素是与其相关联的图片的说明/标题，用于描述其父节点 &lt;figure&gt; 元素里的其他数据。这意味着 &lt;figcaption&gt; 在 &lt;figure&gt; 块里是第一个或最后一个。同时 HTML Figcaption 元素是可选的；如果没有该元素，这个父节点的图片只是会没有说明/标题。 （6）&lt;article&gt;： &lt;article&gt; 元素表示文档、页面、应用或网站中的独立结构，其意在成为可独立分配的或可复用的结构，例如它可能是论坛帖子、杂志或新闻文章、博客、用户提交的评论、交互式组件，或者其他独立的内容项目。 （7）&lt;aside&gt;： &lt;aside&gt; 元素表示一个和其余页面内容几乎无关的部分，被认为是独立于该内容的一部分并且可以被单独的拆分出来而不会使整体受影响。其通常表现为侧边栏或者标注框（call-out boxes）。 （8）&lt;footer&gt;： &lt;footer&gt; 元素表示最近一个章节内容或者根节点（sectioning root）元素的页脚。一个页脚通常包含该章节作者、版权数据或者与文档相关的链接等信息。 综合示例： 2.10 特殊符号 &amp;lt;：&lt;，小于号或显示标记； &amp;gt;：&gt;，大于号或显示标记； &amp;amp;：&amp;，可用于显示其它特殊字符； &amp;quot;：&quot;，引号； &amp;reg;：®，已注册； &amp;copy;：©，版权； &amp;trade;：™，商标； &amp;nbsp;：空格。 上一章：无。 下一章：Web学习笔记-CSS。"},{"title":"Linux学习笔记-管道、环境变量与Docker","date":"2022-09-28T07:14:00.000Z","url":"/posts/63179.html","tags":[["Linux","/tags/Linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 本文记录 Linux 的学习过程，内容为管道、环境变量与 Docker。 Docker 官网：Docker Hub。 1. 管道 （1）概念 管道类似于文件重定向，可以将前一个命令的 stdout 重定向到下一个命令的 stdin。 （2）要点 管道命令仅处理 stdout，会忽略 stderr。 管道右边的命令必须能接受 stdin。 多个管道命令可以串联。 （3）与文件重定向的区别 文件重定向左边为命令，右边为文件。 管道左右两边均为命令，左边有 stdout，右边有 stdin。 （4）举例 统计当前目录下所有 Python 文件的总行数，其中 find、xargs、wc 等命令可以参考：Linux学习笔记-命令、Tmux与Vim。 2. 环境变量 （1）概念 Linux 系统中会用很多环境变量来记录配置信息。 环境变量类似于全局变量，可以被各个进程访问到。我们可以通过修改环境变量来方便地修改系统配置。 （2）查看 列出当前环境下的所有环境变量： 输出某个环境变量的值： （3）修改 环境变量的定义、修改、删除操作可以参考Linux学习笔记-Shell这一节的内容。 为了将对环境变量的修改应用到未来所有环境下，可以将修改命令放到 ~/.bashrc 文件中。修改完 ~/.bashrc 文件后，需要执行 source ~/.bashrc，来将修改应用到当前的 bash 环境下。 为何将修改命令放到 ~/.bashrc，就可以确保修改会影响未来所有的环境呢？ 每次启动 bash，都会先执行 ~/.bashrc。 每次 ssh 登陆远程服务器，都会启动一个 bash 命令行给我们。 每次 tmux 新开一个 pane，都会启动一个 bash 命令行给我们。 所以未来所有新开的环境都会加载我们修改的内容。 （4）常见环境变量 HOME：用户的家目录。 PATH：可执行文件（命令）的存储路径。路径与路径之间用 : 分隔。当某个可执行文件同时出现在多个路径中时，会选择从左到右数第一个路径中的执行。下列所有存储路径的环境变量，均采用从左到右的优先顺序。 LD_LIBRARY_PATH：用于指定动态链接库（.so 文件）的路径，其内容是以冒号分隔的路径列表。 C_INCLUDE_PATH：C 语言的头文件路径，内容是以冒号分隔的路径列表。 CPLUS_INCLUDE_PATH：CPP 的头文件路径，内容是以冒号分隔的路径列表。 PYTHONPATH：Python 导入包的路径，内容是以冒号分隔的路径列表。 JAVA_HOME：JDK 的安装目录。 CLASSPATH：存放 Java 导入类的路径，内容是以冒号分隔的路径列表。 3. Docker 3.1 Docker安装 Ubuntu 系统 Docker 官网安装教程：Docker Install Docs。 本文安装 Docker 所使用的 OS 版本为：Ubuntu 22.04 (LTS)。依次执行以下命令安装 Docker： （1）更新 apt： （2）允许 apt 通过 HTTPS 使用存储库： （3）添加 Docker 的官方 GPG 密钥： （4）设置 repository： （5）安装 Docker Engine，首先更新 apt： （6）安装 Docker Engine、containerd、Docker Compose： （7）检查版本： 3.2 Docker教程 （1）将当前用户添加到 docker 用户组 为了避免每次使用 docker 命令都需要加上 sudo 权限，可以将当前用户加入安装中自动创建的 docker 用户组（可以参考官方文档）： 执行完此操作后，需要退出服务器（即关闭系统），再重新登录回来，才可以省去 sudo 权限。 （2）镜像（images） 一个 Docker 中可以有很多镜像，镜像就相当于模板，每个镜像中又可以有很多容器。 用相同镜像生成的容器环境都一样，如果 Docker 安装在云服务器上，那么每个容器也就相当于是一个独立的云服务器。 迁移项目的时候即将容器先生成一个镜像，然后把镜像传到远程服务器上。 docker pull ubuntu:20.04 或 docker pull ubuntu:latest：拉取一个镜像。 docker images：列出本地所有镜像。 docker save -o ubuntu_latest.tar ubuntu:latest：将镜像 ubuntu:latest 导出到本地文件 ubuntu_latest.tar 中，导出后记得给文件加上可读权限：chmod +r ubuntu_latest.tar。 docker image rm ubuntu:latest 或 docker rmi ubuntu:latest：删除镜像 ubuntu:latest。 docker [container] commit CONTAINER IMAGE_NAME:TAG：创建某个 container 的镜像，[] 表示 container 为可选字段。 docker load -i ubuntu_latest.tar：将镜像 ubuntu:latest 从本地文件 ubuntu_latest.tar 中加载出来。 （3）容器（container） docker [container] create -it ubuntu:latest：利用镜像 ubuntu:latest 创建一个容器。 docker ps -a：查看本地的所有容器，docker ps 为查看运行中的容器。 docker [container] start CONTAINER：启动容器，CONTAINER 可以是 ID 或 NAMES。 docker [container] stop CONTAINER：停止容器。 docker [container] restart CONTAINER：重启容器。 docker [contaienr] run -itd ubuntu:latest：创建并启动一个容器，可以加上参数 -p 20000:22 表示将容器的22端口映射到本地的20000端口，因为本地的22端口已经被占用了，且如果是在云服务器安装 Docker 还需要修改云服务器安全组配置，把20000端口放行。 docker [container] attach CONTAINER：进入容器。 先按 Ctrl+p，再按 Ctrl+q 可以挂起容器，即退出但不关闭容器。 按 Ctrl+d 可以退出并关闭容器。 docker [container] exec CONTAINER COMMAND：在容器中执行 COMMAND 命令。 docker [container] rm CONTAINER：删除容器。 docker container prune：删除所有已停止的容器。 docker export -o xxx.tar CONTAINER：将容器导出到本地文件 xxx.tar 中。 docker import xxx.tar image_name:tag：将本地文件 xxx.tar 导入成镜像，并将镜像命名为 image_name:tag。 docker export/import 与 docker save/load 的区别： export/import 会丢弃历史记录和元数据信息，仅保存容器当时的快照状态。 save/load 会保存完整记录，体积更大。 docker top CONTAINER：查看某个容器内的所有进程。 docker stats：查看所有容器的统计信息，包括 CPU、内存、存储、网络等信息。 docker cp xxx CONTAINER:xxx 或 docker cp CONTAINER:xxx xxx：在本地和容器间复制文件。 docker rename NAMES1 NAMES2：将 NAMES1 容器重命名为 NAMES2。 docker update CONTAINER --memory 500MB：修改容器限制，更多修改内容可以在官网查找。 进入容器后输入 passwd 可以设置 root 密码。 （4）云服务器配置示例 首先在 AC Terminal 中操作： 然后去云平台控制台中修改安全组配置，放行端口：20000。 返回 AC Terminal，即可通过 SSH 登录自己的 Docker 容器： 创建 acs 用户： 最后可以配置 Docker 容器的别名和免密登录。 Tips：如果 apt-get 下载软件速度较慢，可以参考清华大学开源软件镜像站中的内容，修改软件源。 （5）数据迁移 如果想保留 Docker 数据库里的数据，需要用 save 和 laod 迁移。 首先将容器打包成镜像： 导出镜像： 将导出的镜像文件传到目标主机上，然后导入镜像： 查看镜像： 生成一个新的容器： 上一章：Linux学习笔记-Thrift。 下一章：无。"},{"title":"Linux学习笔记-Thrift","date":"2022-09-26T06:24:00.000Z","url":"/posts/20905.html","tags":[["Linux","/tags/Linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 本文记录 Linux 的学习过程，内容为 RPC 软件框架：Thrift。 Thrift 官网：Apache Thrift。 1. Thrift概述 1.1 基本概念 Thrift 是一个 RPC（远程过程调用协议 Remote Procedure Call Protocol）软件框架，用来进行可扩展且跨语言的服务的开发。它结合了功能强大的软件堆栈和代码生成引擎，以构建在 C++、Java、Go、Python、PHP、Ruby、Erlang、Perl、Haskell、C#、Cocoa、JavaScript、Node.js、Smalltalk、OCaml 这些编程语言间无缝结合的、高效的服务。Thrift 允许定义一个简单的定义文件中的数据类型和服务接口，以作为输入文件，编译器生成代码用来方便地生成 RPC 客户端和服务器通信的无缝跨编程语言。 1.2 Thrift IDL Thrift 采用接口定义语言 IDL（Interface Definition Language）来定义通用的服务接口，然后通过 Thrift 提供的编译器，可以将服务接口编译成不同语言编写的代码，通过这个方式来实现跨语言的功能。 通过命令调用 Thrift 提供的编译器将服务接口编译成不同语言编写的代码。 这些代码又分为服务端和客户端，将所在不同进程（或服务器）的功能连接起来。 1.3 如何创建Thrift服务？ 定义服务接口（存放接口的文件夹就是 Thrift 文件）。 作为服务端的服务，需要生成 server。 作为请求端的服务，需要生成 client。 1.4 实例讲解 假设我们要实现一个游戏的匹配系统，这个游戏的功能可能运行在一个或多个服务器（进程）上，而 Thrift 就是将不同服务器不同语言的功能连接起来。 游戏本体（假设用 Python 实现）、匹配系统（假设用 C++ 实现）、数据存储服务器这三个节点（功能）是完全独立的，既可以在同一个服务器上，也可以在不同服务器上。每一个节点就是一个进程，每个进程可以使用不同的语言来实现。 游戏节点到匹配节点需要实现一条有向边（可以包含多个函数），表示向匹配系统添加和移除玩家 add_user、remove_user，因此游戏节点需要实现 match_client 端，表示可以调用匹配服务器的函数；匹配系统需要实现 match_server 端，表示可以让游戏节点的 client 端调用自身的函数。同时匹配系统还需实现 save_client 端，因为需要将数据传给服务器存储 save_data（假设数据存储服务器已实现 save_server 端）。 2. Thrift教程 首先创建一个游戏系统文件夹 game、匹配系统文件夹 match_system、保存各种接口的文件夹 thrift。 2.1 match_server框架 在 thrift 文件夹中创建一个文件：match.thrift，内容如下： 前往 Thrift 官网，点击 Tutorial，再点击 C++，即可看到如何通过这个接口生成一个 C++ 版本的服务器。命令如下： 在 match_system 文件夹中创建一个文件夹 src，表示源文件。在 src 文件夹中输入以下命令： 执行后会发现该目录下生成了一个 gen-cpp 的文件夹，为了后续方便操作，将文件夹改个名： 将自动实现好的文件移出来： 由于该文件里的函数还没有进行逻辑实现，因此先在每个函数中加上 return 0; 后编译一遍，文件内容如下（可以使用 gg=G 进行格式化）： 接下来进行编译链接，链接的时候需要用到 Thrift 的动态链接库，需要加上 -lthrift： 这时输入 ./main 即可运行程序，但是此时什么内容都没有。Thrift 只是将接口实现好了，具体的业务逻辑没有实现。我们可以先将文件上传至 Git，上传的时候注意一般不将 .o 文件和可执行文件上传： 2.2 match_client框架与实现 首先同样在 game 文件夹中创建 src 文件夹，进入 src 文件夹后我们需要生成 Python 代码： 生成后该目录下有个文件夹 gen-py，也就是生成了 Python 的服务器端，同样将其改个名： 创建文件 client.py，将官网中 Python 客户端的代码（前四行是为了将当前路径加入到 Python 的环境变量中，可以删掉）复制过来，并进行简单的修改： 然后我们将 match_system/src 中的 main 执行后，再执行 game/src 中的 client.py： 可以看到 main 程序那边输出：add_user。说明我们的 match_client 端和 match_server 端已经初步实现了，此时更新一下 Git，注意 .pyc 文件也最好不要上传： 接着我们进行优化，从控制台输入用户信息，并指定是添加还是删除用户，修改后的 client.py 代码如下： 这样我们的 match_client 端就算是完成了。 2.3 match_server_v2.0实现 由于 server 端一方面需要读入或者移出用户，另一方面还要不断地去匹配，因此需要有一个线程去不断添加用户进来，一个线程去进行匹配，匹配完后再将信息传给一个服务器，且这两个操作是完全独立的，有可能长时间没有用户添加进来，但是匹配系统能够匹配两个已经匹配了很久的人。因此在这里需要用到并行技术，C++ 多线程需要使用到 &lt;thread&gt; 头文件。 多线程相关知识点： IP 和端口：如果把 IP 地址比作一间房子，端口就是出入这间房子的门。真正的房子只有几个门，但是一个 IP 地址的端口可以有65536个之多！端口是通过端口号来标记的，端口号只有整数，范围是从0到65535。同一个端口只能由一个进程来监听。所以我们一旦启动了一个服务，那么这个服务就不能在被另一个进程启动了。服务器的端口号要与客户端的端口号相同。 &lt;thread&gt; 库：C++ 中有一个 thread 的库，可以用来开线程。通过定义一个变量将函数名作为参数，就能开一个线程了，具体使用可以看后文代码。 首先定义线程的操作：并行中经典的生产者和消费者模型。生产者、消费者是两个线程。本样例中的生产者：add_user()、remove_user()；消费者：匹配用户的功能。 生产者和消费者之间需要一个媒介。这个媒介可以有很多种方法。比如：消费队列。很多语言都有自己实现的消费队列，也可以自己实现消费队列。实现消费队列，就需要用到一些锁（Mutex）。锁是并行编程的基本概念。 互斥锁：在编程中，引入了对象互斥锁的概念，来保证共享数据操作的完整性。每个对象都对应于一个可称为“互斥锁”的标记，这个标记用来保证在任一时刻，只能有一个线程访问该对象。 锁有两个操作：一个 P 操作（上锁），一个 V 操作（解锁）。 定义互斥锁：mutex m。锁一般使用信号量来实现的，mutex 其实就是一个信号量（它特殊也叫互斥量）。互斥量就是同一时间能够分给一个人，即 S = 1。信号量 S = 10 表示可以将信号量分给10个人来用。 P 操作的主要动作是： （1）S - 1； （2）若 S - 1 后仍大于或等于0，则进程继续执行； （3）若 S - 1 后小于0，则该进程被阻塞后放入等待该信号量的等待队列中，然后转进程调度。 V 操作的主要动作是： （1）S + 1； （2）若 S + 1 后结果大于0，则进程继续执行； （3）若 S + 1 后结果小于或等于0，则从该信号的等待队列中释放一个等待进程，然后再返回原进程继续执行或转进程调度。 对于 P 和 V 都是原子操作，就是在执行 P 和 V 操作时，不会被插队，从而实现对共享变量操作的原子性。 特殊：S = 1 表示互斥量，表示同一时间，信号量只能分配给一个线程。 多线程为啥要用锁？因为多线程可能共享一个内存空间，导致出现重复读取并修改的现象。 我们将程序功能修改为傻瓜式匹配，只要匹配池中的玩家数大于等于2，那么就将前两名玩家进行匹配，修改后的 match_server 端 main.cpp 代码如下： 由于使用了线程库，因此编译的时候需要加上参数 -pthread： 此时可以打开 match_server 端和 match_client 端，然后在 client 端添加玩家看看 server 端的匹配结果。 2.4 save_client实现 假设 save_server 端已经实现，在 thrift 文件夹中创建文件 save.thrift，内容如下： 可以看到直接调用 save_server 端的接口函数 save_data 即可（该函数的实现我们不关心）。 在 match_system/src 文件夹中输入以下指令生成接口的 C++ 实现，并重命名，然后需要将自动生成的服务端代码删去： 接下来我们将 Thrift 官网 C++ 教程中的 Client 端代码抄下来并相应进行修改，修改后的 main.cpp 内容如下： 2.5 match_server_v3.0实现 通过修改匹配函数 match() 实现将分差小于等于50的玩家进行匹配，修改后的代码如下： 2.6 match_server_v4.0实现 通过 Thrift 官网 C++ 教程下的 Server 端代码可以将 match_server 改为多线程，修改后的 main.cpp 代码如下： 2.7 match_server_v5.0实现 通过对匹配机制的修改，实现玩家每等待一秒钟，匹配的分数区间扩大50分，修改后的 main.cpp 代码如下： 上一章：Linux学习笔记-SSH与Git。 下一章：Linux学习笔记-管道、环境变量与Docker。"},{"title":"Linux学习笔记-SSH与Git","date":"2022-04-30T05:56:00.000Z","url":"/posts/20491.html","tags":[["Linux","/tags/Linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 本文记录 Linux 的学习过程，内容为 SSH 与 Git。 1. SSH 1.1 SSH登录 （1）基本用法： 远程登录服务器： 第一次登录时会提示： 输入 yes，然后回车即可。这样会将该服务器的信息记录在 ~/.ssh/known_hosts 文件中。然后输入密码即可登录到远程服务器中。 默认登录端口号为 22。如果想登录某一特定端口可以加参数 -p： （2）配置文件： 创建文件 ~/.ssh/config。 然后在文件中输入： 例如： 之后再使用服务器时，可以直接使用别名 myserver1、myserver2。 （3）配置公钥免密登录： 创建密钥： 然后一直回车即可，执行结束后，在 ~/.ssh/ 目录下会多两个文件： 之后想免密码登录哪个服务器，就将公钥传给哪个服务器即可。 例如，想免密登录 myserver 服务器。则将公钥中的内容，复制到 myserver 中的 ~/.ssh/authorized_keys 文件里即可。 也可以使用如下命令一键添加公钥： （4）执行命令： 命令格式： 例如： 或者： 或者： 1.2 SCP远程拷贝文件 命令格式： 功能：将 source 路径下的文件复制到 destination 中。 一次复制多个文件： 复制文件夹（将本地家目录中的 tmp 文件夹复制到 myserver 服务器中的 /home/acs/ 目录下）： 将本地家目录中的 tmp 文件夹复制到 myserver 服务器中的 ~/homework/ 目录下： 将 myserver 服务器中的 ~/homework/ 文件夹复制到本地的当前路径下： 指定服务器的端口号： 注意：scp 的 -r -P 等参数尽量加在 source 和 destination 之前。 使用 scp 配置其他服务器的 vim 和 tmux： 2. Git 2.1 Git基本概念 工作区：仓库的目录。工作区是独立于各个分支的。 暂存区：数据暂时存放的区域，类似于工作区写入版本库前的缓存区。暂存区是独立于各个分支的。 版本库：存放所有已经提交到本地仓库的代码版本 版本结构：树结构，树中每个节点代表一个代码版本。 2.2 Git常用命令 git config --global user.name xxx：设置全局用户名，信息记录在 ~/.gitconfig 文件中。 git config --global user.email xxx@xxx.com：设置全局邮箱地址，信息记录在 ~/.gitconfig 文件中。 git init：将当前目录配置成 Git 仓库，信息记录在隐藏的 .git 文件夹中。 git add XX：将 XX 文件添加到暂存区。 git add .：将所有待加入暂存区的文件加入暂存区。 git rm --cached XX：将文件从仓库索引目录中删掉。 git commit -m &quot;给自己看的备注信息&quot;：将暂存区的内容提交到当前分支。 git status：查看仓库状态。 git diff XX：查看 XX 文件相对于暂存区修改了哪些内容。 git log：查看当前分支的所有版本。 git log --pretty=oneline：每个版本用一行显示。 git reflog：查看HEAD指针的移动历史（包括被回滚的版本）。 git reset --hard HEAD^ 或 git reset --hard HEAD~：将代码库回滚到上一个版本。 git reset --hard HEAD^^：往上回滚两次，以此类推。 git reset --hard HEAD~100：往上回滚100个版本。 git reset --hard 版本号：回滚到某一特定版本。 git checkout -- XX 或 git restore XX：将 XX 文件尚未加入暂存区的修改全部撤销。 git restore --staged XX：将 XX 文件从暂存区撤出，不会更改文件的内容。 git remote add origin git@git.acwing.com:xxx/XXX.git：将本地仓库关联到远程仓库。 git remote -v：查看当前 Git 仓库有没有关联远程仓库，如果已经有关联则会显示具体远程仓库路径，如果没有返回，说明没有关联任何远程仓库。 git remote rm XXX：解除与远程仓库的关联，例如：git remote rm origin。 git push -u (第一次需要-u以后不需要)：将当前分支推送到远程仓库。 git push origin branch_name：将本地的某个分支推送到远程仓库。 git clone git@git.acwing.com:xxx/XXX.git：将远程仓库 XXX 下载到当前目录下。 git checkout -b branch_name：创建并切换到 branch_name 这个分支。 git branch：查看所有分支和当前所处分支。 git checkout branch_name：切换到 branch_name 这个分支。 git merge branch_name：将分支 branch_name 合并到当前分支上。 git branch -d branch_name：删除本地仓库的 branch_name 分支。 git branch branch_name：创建新分支 branch_name。 git push --set-upstream origin branch_name：设置本地的 branch_name 分支对应远程仓库的 branch_name 分支。 git push -d origin branch_name：删除远程仓库的 branch_name 分支。 git pull：将远程仓库的当前分支与本地仓库的当前分支合并。 git pull origin branch_name：将远程仓库的 branch_name 分支与本地仓库的当前分支合并。 git branch --set-upstream-to=origin/branch_name1 branch_name2：将远程的 branch_name1 分支与本地的 branch_name2 分支对应。 git checkout -t origin/branch_name：将远程的 branch_name 分支拉取到本地。 git stash：将工作区和暂存区中尚未提交的修改存入栈中。 git stash apply：将栈顶存储的修改恢复到当前分支，但不删除栈顶元素。 git stash drop：删除栈顶存储的修改。 git stash pop：将栈顶存储的修改恢复到当前分支，同时删除栈顶元素。 git stash list：查看栈中所有元素。 上一章：Linux学习笔记-Shell。 下一章：Linux学习笔记-Thrift。"},{"title":"Linux学习笔记-Shell","date":"2022-03-17T01:39:00.000Z","url":"/posts/31281.html","tags":[["Linux","/tags/Linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 本文记录 Linux 的学习过程，内容为 Shell 命令语言。 1. Shell 概论 Shell 是我们通过命令行与操作系统沟通的语言。 Shell 脚本可以直接在命令行中执行，也可以将一套逻辑组织成一个文件，方便复用。 Linux 中常见的 Shell 脚本有很多种，常见的有： Bourne Shell（/usr/bin/sh 或 /bin/sh） Bourne Again Shell（/bin/bash） C Shell（/usr/bin/csh） K Shell（/usr/bin/ksh） zsh … Linux 系统中一般默认使用 bash，所以接下来讲解 bash 中的语法。 文件开头需要写 #! /bin/bash，指明 bash 为脚本解释器。 新建一个 test.sh 文件，内容如下： 运行方式： （1）作为可执行文件 （2）用解释器执行 2. 注释 （1）单行注释 每行中 # 之后的内容均是注释： （2）多行注释 其中 EOF 可以换成其它任意字符串，例如： 3. 变量 3.1 定义变量 定义变量不需要加 $ 符号，例如： 3.2 使用变量 使用变量，需要加上 $ 符号，或者 $&#123;&#125; 符号。花括号是可选的，主要为了帮助解释器识别变量边界。 3.3 只读变量 使用 readonly 或者 declare 可以将变量变为只读。 3.4 删除变量 unset 可以删除变量。 3.5 变量类型 自定义变量（局部变量）：子进程不能访问的变量。 环境变量（全局变量）：子进程可以访问的变量。 自定义变量改成环境变量： 环境变量改为自定义变量： 3.6 字符串 字符串可以用单引号，也可以用双引号，也可以不用引号，不用引号与双引号是一样的。 单引号与双引号的区别： 单引号中的内容会原样输出，不会执行、不会取变量。 双引号中的内容可以执行、可以取变量。 获取字符串长度： 提取子串： 4. 默认变量 4.1 文件参数变量 在执行 Shell 脚本时，可以向脚本传递参数。$1 是第一个参数，$2 是第二个参数，以此类推。特殊的，$0 是文件名（包含路径）。例如： 创建文件 test.sh： 然后执行该脚本： 4.2 其它参数相关变量 $#：代表文件传入的参数个数，如上例中值为4。 $*：由所有参数构成的用空格隔开的字符串，如上例中值为 &quot;$1 $2 $3 $4&quot;。 $@：每个参数分别用双引号括起来的字符串，如上例中值为 &quot;$1&quot; &quot;$2&quot; &quot;$3&quot; &quot;$4&quot;。 $$：脚本当前运行的进程 ID。 $?：上一条命令的退出状态（注意不是 stdout，而是 exit code）。0表示正常退出，其他值表示错误。 $(command)：返回 command 这条命令的 stdout（可嵌套） `command`：返回 command 这条命令的 stdout（不可嵌套）。注意是 ~ 下面的那个点号。 5. 数组 数组中可以存放多个不同类型的值，只支持一维数组，初始化时不需要指明数组大小，数组下标从0开始。 5.1 数组定义 数组用小括号表示，元素之间用空格隔开。例如： 也可以直接定义数组中某个元素的值： 5.2 读取数组中元素的值 格式： 例如： 5.3 读取整个数组 格式： 例如： 5.4 数组长度 类似于字符串： 例如： 6. expr命令 expr 命令用于求表达式的值，格式为： 表达式说明： 用空格隔开每一项。 用反斜杠放在 Shell 特定的字符前面（发现表达式运行错误时，可以试试转义）。 对包含空格和其他特殊字符的字符串要用引号括起来。 expr 会在 stdout 中输出结果。如果为逻辑关系表达式，则若结果为真，stdout 为1，否则为0。 expr 的 exit code：如果为逻辑关系表达式，则若结果为真，exit code 为0，否则为1。 6.1 字符串表达式 length STRING：返回 STRING 的长度。 index STRING CHARSET：CHARSET 中任意单个字符在 STRING 中最前面的字符位置，下标从1开始。如果在 STRING 中完全不存在 CHARSET 中的字符，则返回0。 substr STRING POSITION LENGTH：返回 STRING 字符串中从 POSITION 开始，长度最大为 LENGTH 的子串。如果 POSITION 或 LENGTH 为负数、0或非数值，则返回空字符串。 示例： 6.2 整数表达式 expr 支持普通的算术操作，算术表达式优先级低于字符串表达式，高于逻辑关系表达式。 + -：加减运算。两端参数会转换为整数，如果转换失败则报错。 * / %：乘、除与取模运算。两端参数会转换为整数，如果转换失败则报错。注意 * 需要转义。 ()：可以改变优先级，但需要用反斜杠转义。 示例： 6.3 逻辑关系表达式 |：如果第一个参数非空且非0，则返回第一个参数的值，否则返回第二个参数的值，但要求第二个参数的值也是非空或非0，否则返回0。如果第一个参数是非空或非0时，不会计算第二个参数。 &amp;：如果两个参数都非空且非0，则返回第一个参数，否则返回0。如果第一个参为0或为空，则不会计算第二个参数。 &lt; &lt;= = == != &gt;= &gt;：比较两端的参数，如果为 true，则返回1，否则返回0。== 是 = 的同义词。expr 首先尝试将两端参数转换为整数，并做算术比较，如果转换失败，则按字符集排序规则做字符比较。 ()：可以改变优先级，但需要用反斜杠转义。 示例： 7. read命令 read 命令用于从标准输入中读取单行数据。当读到文件结束符时，exit code 为1，否则为0。 参数说明： -p：后面可以接提示信息。 -t：后面跟秒数，定义输入字符的等待时间，超过等待时间后会自动忽略此命令。 实例： 8. echo命令 echo 用于输出字符串。命令格式： （1）显示普通字符串 （2）显示转义字符 （3）显示变量 （4）显示换行 输出结果： （5）显示不换行 输出结果： （6）显示结果定向至文件 （7）原样输出字符串，不进行转义或取变量（用单引号） 输出结果： （8）显示命令的执行结果 输出结果： 9. printf命令 printf 命令用于格式化输出，类似于 C/C++ 中的 printf 函数。默认不会在字符串末尾添加换行符。 命令格式： 脚本内容： 输出结果： 10. test命令与判断符号[] 10.1 逻辑运算符 &amp;&amp; 表示与，|| 表示或。 二者具有短路原则： expr1 &amp;&amp; expr2：当 expr1 为假时，直接忽略 expr2。 expr1 || expr2：当 expr1 为真时，直接忽略 expr2。 表达式的 exit code 为0，表示真；为非零，表示假（与 C/C++ 中的定义相反）。 10.2 test命令 在命令行中输入 man test，可以查看 test 命令的用法。 test 命令用于判断文件类型，以及对变量做比较。 test 命令用 exit code 返回结果，而不是使用 stdout。0表示真，非0表示假。 例如： （1）文件类型判断 其它参数如下： -e：文件是否存在。 -f：是否为文件。 -d：是否为目录。 （2）文件权限判断 其它参数如下： -r：文件是否可读。 -w：文件是否可写。 -x：文件是否可执行。 -s：是否为非空文件。 （3）整数间比较 其它参数如下： -eq：a 是否等于 b。 -ne：a 是否不等于 b。 -gt：a 是否大于 b。 -lt：a 是否小于 b。 -ge：a 是否大于等于 b。 -le：a 是否小于等于 b。 （4）字符串比较 test -z STRING：判断 STRING 是否为空，如果为空，则返回 true。 test -n STRING：判断 STRING 是否非空，如果非空，则返回 true（-n 可以省略）。 test str1 == str2：判断 str1 是否等于 str2。 test str1 != str2：判断 str1 是否不等于 str2。 （5）多重条件判定 其它参数如下： -a：两条件是否同时成立。 -o：两条件是否至少一个成立。 !：取反，如 test ! -x file，当 file 不可执行时，返回 true。 10.3 判断符号[] [] 与 test 用法几乎一模一样，更常用于 if 语句中。另外 [[]] 是 [] 的加强版，支持的特性更多。 例如： 注意： [] 内的每一项都要用空格隔开。 [] 内的变量，最好用双引号括起来。 [] 内的常数，最好用单或双引号括起来。 例如： 11. 判断语句 11.1 if…then形式 类似于 C/C++ 中的 if-else 语句。 （1）单层 if： 示例： 输出结果： （2）单层 if-else： 示例： 输出结果： （3）多层 if-elif-elif-else： 示例： 输出结果： 11.2 case…esac形式 类似于 C/C++ 中的 switch 语句。 命令格式： 示例： 输出结果： 12. 循环语句 12.1 for…in…do…done 命令格式： 示例一，输出 a 2 cc，每个元素一行： 示例二，输出当前路径下的所有文件名，每个文件名一行： 示例三，输出1到10： 示例四，使用 &#123;1..10&#125; 或者 &#123;a..z&#125;： 12.2 for ((…;…;…)) do…done 命令格式： 示例，输出1到10，每个数占一行： 12.3 while…do…done while...do...done：当条件为假时结束。 命令格式： 示例，文件结束符为 Ctrl+d，输入文件结束符后 read 指令返回 false： 12.4 until…do…done until...do...done：当条件为真时结束。 命令格式： 示例，当用户输入 yes 或者 YES 时结束，否则一直等待读入： 12.5 break break：跳出当前一层循环，注意与 C/C++ 不同的是：break 不能跳出 case 语句。 示例，每读入非 EOF 的字符串，会输出一遍1到7。该程序可以输入 Ctrl+d 文件结束符来结束，也可以直接用 Ctrl+c 杀掉该进程： 12.6 continue continue：跳出当前循环。 示例，输出1到10中的所有奇数： 直接关闭进程的方式： 使用 top 命令找到进程的 PID。 输入 kill -9 PID 即可关掉此进程。 13. 函数 bash 中的函数类似于 C/C++ 中的函数，但 return 的返回值与 C/C++ 不同，返回的是 exit code，取值为 [0, 255]，0表示正常结束。 如果想获取函数的输出结果，可以通过 echo 输出到 stdout 中，然后通过 $(function_name) 来获取 stdout 中的结果。 函数的 return 值可以通过 $? 来获取。 命令格式： （1）不获取 return 值和 stdout 值 输出结果： （2）获取 return 值和 stdout 值（不写 return 时，默认 return 0） 输出结果： （3）函数的输入参数 在函数内，$1 表示第一个输入参数，$2 表示第二个输入参数，依此类推。 注意：函数内的 $0 仍然是文件名，而不是函数名。 输出结果： （4）函数内的局部变量 可以在函数内定义局部变量，作用范围仅在当前函数内。可以在递归函数中定义局部变量。 命令格式： 例如： 输出结果： 第一行为函数内的 name 变量，第二行为函数外调用 name 变量，会发现此时该变量不存在。 14. exit命令 exit 命令用来退出当前 Shell 进程，并返回一个退出状态；使用 $? 可以接收这个退出状态。 exit 命令可以接受一个整数值作为参数，代表退出状态。如果不指定，默认状态值是0。 exit 退出状态只能是一个介于0到255之间的整数，其中只有0表示成功，其它值都表示失败。 示例，创建脚本 test.sh，内容如下： 执行该脚本： 15. 文件重定向 每个进程默认打开3个文件描述符： stdin：标准输入，从命令行读取数据，文件描述符为0。 stdout：标准输出，向命令行输出数据，文件描述符为1。 stderr：标准错误输出，向命令行输出数据，文件描述符为2。 可以用文件重定向将这三个文件重定向到其他文件中。 重定向命令列表： command &gt; file：将 stdout 重定向到 file 中。 command &lt; file：将 stdin 重定向到 file 中。 command &gt;&gt; file：将 stdout 以追加方式重定向到 file 中。 command n&gt; file：将文件描述符 n 重定向到 file 中。 command n&gt;&gt; file：将文件描述符 n 以追加方式重定向到 file 中。 输入和输出重定向： 同时重定向 stdin 和 stdout： 创建 bash 脚本： 创建 input.txt，里面的内容为： 执行命令： 16. 引入外部脚本 类似于 C/C++ 中的 include 操作，bash 也可以引入其他文件中的代码。 语法格式： 示例，创建 test1.sh，内容为： 然后创建 test2.sh，内容为： 执行命令： 上一章：Linux学习笔记-命令、Tmux与Vim。 下一章：Linux学习笔记-SSH与Git。"},{"title":"Linux学习笔记-命令、Tmux与Vim","date":"2022-03-15T10:06:00.000Z","url":"/posts/53725.html","tags":[["Linux","/tags/Linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 本文记录 Linux 的学习过程，内容为 Linux 常用文件管理命令、Tmux、Vim。 1. 常用命令 Linux 中描述路径有两种方式（假设当前用户的目录为 AsanoSaki）： 绝对路径：从根目录（即 /）开始描述，例如：/home/AsanoSaki/main.cpp。 相对路径：从当前的路径开始描述，例如：AsanoSaki/main.cpp（当前在 home 中）。 绝对路径的开头一定是 /，相对路径开头一定不是 /。 . 表示当前目录，.. 表示上一目录，假如当前在 AsanoSaki 目录下，则路径 ../AsanoSaki/./../AsanoSaki 表示同一路径。 ~/ 表示家目录，等价于 /home/AsanoSaki。 1.1 常用文件管理命令 ctrl+c：取消命令，并且换行。如当前有一个程序正在运行且一直无法停止，则可以使用该操作将当前正在运行的程序中止。另一个作用是中断当前正在输入的这一行，直接跳到下一行重新输入。 ctrl+u：清空本行命令。 tab：可以补全命令和文件名，如果补全不了快速按两下 tab 键，可以显示备选选项。 ls：列出当前目录下所有文件，蓝色的是文件夹，白色的是普通文件，绿色的是可执行文件。 pwd：显示当前路径。 cd XXX：进入 XXX 目录下，cd .. 表示返回上层目录，cd - 表示返回上一个待过的目录。cd 后面既可以用相对路径也可以用绝对路径，不加目录则默认返回家目录。 cp XXX YYY：将 XXX 文件复制成 YYY，XXX 和 YYY 可以是一个路径，比如将目录 a 中的文件 tmp.txt 复制到目录 b 中：cp a/tmp.txt b。如果想顺带将复制后的文件重命名则可以写成：cp a/tmp.txt b/tmp2.txt。如果想把目录 a 整个复制到目录 b 下则可以写：cp a b -r。 mkdir XXX：创建目录 XXX。如在当前目录下创建文件夹 a：mkdir a。使用绝对路径在目录 a 下创建文件夹 b：mkdir /home/AsanoSaki/a/b。直接创建 a 里有 b，b 里有 c 的目录：mkdir a/b/c -p。 rm XXX：删除普通文件。rm XXX -r：删除文件夹。删除多个文件：rm tmp1.txt tmp2.txt。删除当前目录下的所有 txt 文件：rm *.txt。删除 a 中的所有文件：rm a/*。 mv XXX YYY：将 XXX 文件移动（剪切）到 YYY，和 cp 命令一样，XXX 和 YYY 可以是一个路径，重命名也是用这个命令。 touch XXX：创建一个文件。 cat XXX：展示文件 XXX 中的内容。 1.2 其它常用命令 （1）系统状况 top：查看所有进程的信息（Linux 的任务管理器）。 打开后，输入 M：按使用内存排序。 打开后，输入 P：按使用 CPU 排序。 打开后，输入 q：退出。 df -h：查看硬盘使用情况。 free -h：查看内存使用情况。 du -sh：查看当前目录占用的硬盘空间。 ps aux：查看所有进程。 kill -9 pid：杀死编号为 pid 的进程。 传递某个具体的信号：kill -s SIGTERM pid。 netstat -nt：查看所有网络连接。 w：列出当前登陆的用户。 ping www.baidu.com：检查是否连网。 （2）文件权限 chmod：修改文件权限 chmod +x xxx：给 xxx 添加可执行权限。 chmod -x xxx：去掉 xxx 的可执行权限。 chmod 777 xxx：将 xxx 的权限改成 777（三个数字按顺序分别表示 Owner、Group、Other Users，每个数字的二进制例如7的二进制为111，表示具有 rwx 权限，某一位为0表示没有该权限）。 chmod 777 xxx -R：递归修改整个文件夹的权限。 （3）文件检索 find /path/to/directory/ -name '*.py'：搜索某个文件路径下的所有 *.py 文件。 grep xxx：从 stdin 中读入若干行数据，如果某行中包含 xxx，则输出该行；否则忽略该行。 wc：统计行数、单词数、字节数。 既可以从 stdin 中直接读入内容；也可以在命令行参数中传入文件名列表。 wc -l：统计行数。 wc -w：统计单词数。 wc -c：统计字节数。 tree：展示当前目录的文件结构。 tree /path/to/directory/：展示某个目录的文件结构。 tree -a：展示隐藏文件。 ag xxx：搜索当前目录下的所有文件，检索 xxx 字符串。 cut：分割一行内容。 从 stdin 中读入多行数据。 echo $PATH | cut -d ':' -f 3,5：输出 PATH 用 : 分割后第3、5列数据。 echo $PATH | cut -d ':' -f 3-5：输出 PATH 用 : 分割后第3-5列数据。 echo $PATH | cut -c 3,5：输出 PATH 的第3、5个字符。 echo $PATH | cut -c 3-5：输出 PATH 的第3-5个字符。 sort：将每行内容按字典序排序。 可以从 stdin 中读取多行数据。 可以从命令行参数中读取文件名列表。 xargs：将 stdin 中的数据用空格或回车分割成命令行参数。 find . -name '*.py' | xargs cat | wc -l：统计当前目录下所有 Python 文件的总行数 （4）查看文件内容 more：浏览文件内容。 回车：下一行。 空格：下一页。 b：上一页。 q：退出。 less：与 more 类似，功能更全。 回车：下一行。 y：上一行。 Page Down：下一页。 Page Up：上一页。 q：退出。 head -3 xxx：展示 xxx 的前3行内容。 同时支持从 stdin 读入内容。 tail -3 xxx：展示 xxx 末尾3行内容。 同时支持从 stdin 读入内容。 （5）用户相关 history：展示当前用户的历史操作。内容存放在 ~/.bash_history 中。 （6）工具 md5sum：计算 md5 哈希值。 可以从 stdin 读入内容。 也可以在命令行参数中传入文件名列表。 time command：统计 command 命令的执行时间。 ipython3：交互式 Python3 环境。可以当做计算器，或者批量管理文件。 ! echo &quot;Hello World&quot;：! 表示执行 shell 脚本。 watch -n 0.1 command：每0.1秒执行一次 command 命令。 tar：压缩文件。 tar -zcvf xxx.tar.gz /path/to/file/*：压缩。 tar -zxvf xxx.tar.gz：解压缩。 diff xxx yyy：查找文件 xxx 与 yyy 的不同点。 （7）安装软件 sudo command：以 root 身份执行 command 命令。 apt-get install xxx：安装软件。 pip install xxx --user --upgrade：安装 Python 包。 2. Tmux与Vim 2.1 Tmux （1）功能 分屏。 允许断开 Terminal 连接后，继续运行进程。 （2）结构 一个 Tmux 可以包含多个 session，一个 session 可以包含多个 window，一个 window 可以包含多个 pane。 （3）常用操作 tmux [-u]：新建一个 session，其中包含一个 window，window 中包含一个 pane，pane 里打开了一个 shell 对话框，-u 参数可以在 Tmux 中显示中文内容。 按下 Ctrl + a 后手指松开，然后按 %：将当前 pane 左右平分成两个 pane。 按下 Ctrl + a 后手指松开，然后按 &quot;（注意是双引号）：将当前 pane 上下平分成两个 pane。 Ctrl + d：关闭当前 pane；如果当前 window 的所有 pane 均已关闭，则自动关闭 window；如果当前 session 的所有 window 均已关闭，则自动关闭 session。 鼠标点击可以选则 pane。 按下 Ctrl + a 后手指松开，然后按方向键：选择相邻的 pane。 鼠标拖动 pane 之间的分割线，可以调整分割线的位置。 按住 Ctrl + a 的同时按方向键，可以调整 pane 之间分割线的位置。 按下 Ctrl + a 后手指松开，然后按 z：将当前 pane 全屏/取消全屏。 按下 Ctrl + a 后手指松开，然后按 d：挂起当前 session。 tmux a：打开之前挂起的 session。 按下 Ctrl + a 后手指松开，然后按 s：选择其它 session： 方向键上：选择上一项； 方向键下：选择下一项； 方向键右：展开当前项； 方向键左：闭合当前项。 按下 Ctrl + a 后手指松开，然后按 c：在当前 session 中创建一个新的 window。 按下 Ctrl + a 后手指松开，然后按 w：选择其他 window，操作方法与选择 session 完全相同。 按下 Ctrl + a 后手指松开，然后按 PageUp/PageDown：翻阅当前 pane 内的内容。注意第一次唤醒该操作时只能按 PageUp。 鼠标滚轮：翻阅当前 pane 内的内容。 在 tmux 中选中文本时，需要按住 shift 键。（仅支持 Windows 和 Linux，不支持 Mac，不过该操作并不是必须的，因此影响不大） tmux 中复制/粘贴文本的通用方式： 按下 Ctrl + a 后松开手指，然后按 [。 用鼠标选中文本，被选中的文本会被自动复制到 tmux 的剪贴板。 按下 Ctrl + a 后松开手指，然后按 ]，会将剪贴板中的内容粘贴到光标处。 注意：Tmux 的配置文件为 ~/.tmux.conf，默认 Tmux 前缀快捷键是 Ctrl + b！本文是已经修改过了配置文件后的操作说明，不过基本上操作逻辑都是一样的。 2.2 Vim （1）功能 命令行模式下的文本编辑器。 根据文件扩展名自动判别编程语言。支持代码缩进、代码高亮等功能。 使用方式：vim &lt;filename&gt;。 如果已有该文件，则打开它。 如果没有该文件，则打开个一个新的文件，并命名为 filename。 （2）模式 一般命令模式 默认模式。命令输入方式：类似于打游戏放技能，按不同字符，即可进行不同操作。可以复制、粘贴、删除文本等。 编辑模式 在一般命令模式里按下 i，会进入编辑模式。 按下 ESC 会退出编辑模式，返回到一般命令模式。 命令行模式 在一般命令模式里按下 :/? 三个字符中的任意一个，会进入命令行模式。命令行在最下面。 可以查找、替换、保存、退出、配置编辑器等。 （3）操作 i：进入编辑模式。 ESC：进入一般命令模式。 h 或左方向键：光标向左移动一个字符。 j 或下方向键：光标向下移动一个字符。 k 或上方向键：光标向上移动一个字符。 l 或右方向键：光标向右移动一个字符。 n&lt;Space&gt;：n 表示数字，按下数字后再按空格，光标会向右移动 n 个字符。 0 或 Home：光标移动到本行开头。 $ 或 End：光标移动到本行末尾。 G：光标移动到最后一行。 :n 或 nG：n 为数字，光标移动到第 n 行。 gg：光标移动到第一行，相当于 1G。 n&lt;Enter&gt;：n 为数字，光标向下移动 n 行。 /word：向光标之下寻找第一个值为 word 的字符串。 ?word：向光标之上寻找第一个值为 word 的字符串。 n：重复前一个查找操作。 N：反向重复前一个查找操作。 :n1,n2s/word1/word2/g：n1 与 n2 为数字，在第 n1 行与 n2 行之间寻找 word1 这个字符串，并将该字符串替换为 word2。 :1,$s/word1/word2/g：将全文的 word1 替换为 word2。 :1,$s/word1/word2/gc：将全文的 word1 替换为 word2，且在替换前要求用户确认。 v：选中文本，连续按两次 ESC 取消选中。 d：删除选中的文本。 dd：删除当前行（其实是剪切）。 ggdG：删除全部内容。 y：复制选中的文本。 yy：复制当前行。 ggyG：复制全部内容。 p：将复制的数据在光标的下一行/下一个位置粘贴。 u：撤销。 Ctrl + r：取消撤销。 &gt;：将选中的文本整体向右缩进一次。 &lt;：将选中的文本整体向左缩进一次。 :w：保存。 :w!：强制保存。 :q：退出。 :q!：强制退出。 :wq：保存并退出。 :set paste：设置成粘贴模式，取消代码自动缩进。 :set nopaste：取消粘贴模式，开启代码自动缩进。 :set nu：显示行号。 :set nonu：隐藏行号。 gg=G：将全文代码格式化。 :noh：关闭查找关键词高亮。 Ctrl + q：当 Vim 卡死时，可以取消当前正在执行的命令。 异常处理： 每次用 Vim 编辑文件时，会自动创建一个 &lt;filename&gt;.swp 的临时文件。 如果打开某个文件时，该文件的 swp 文件已存在，则会报错。此时解决办法有两种： 找到正在打开该文件的程序，并退出。 直接删掉该 swp 文件即可。 Vim 的配置文件在 ~/.vimrc 中。 上一章：无。 下一章：Linux学习笔记-Shell。"},{"title":"VS Code实用插件推荐与使用教程","date":"2022-01-25T09:57:00.000Z","url":"/posts/45632.html","tags":[["Others","/tags/Others/"]],"categories":[["Others","/categories/Others/"]],"content":" 推荐一些 VS Code 基础插件，例如：简体中文、C++、背景、主题等。 1. Chinese (Simplified) 简体中文插件，不用多说了，上来第一个先装这个。 2. C/C++ 需要编写调试运行 C/C++ 文件所需的插件，安装完成后在工作空间的顶层文件夹中新建一个 .vscode 文件夹，新建两个文件名字分别为 tasks.json 和 launch.json。 其中，tasks.json 文件内容固定如下： launch.json 文件内容如下，注意 &quot;miDebuggerPath&quot; 的路径需要选择自己电脑上安装的 MinGW 的 gdb.exe 路径： 如果使用 TDM-GCC 那么 launch.json 文件的内容如下： 配置完成后编写 C++ 代码即可成功编译运行啦！（注意源文件的路径中不能有任何中文的文件夹） 3. Markdown All in One 安装了该插件后即可编写 Markdown 文件（文件后缀名为 .md），效果如下： 4. MASM/TASM 编写汇编语言代码必备插件，效果如下： 5. background 更换背景的插件，效果如前文中的图片所示，安装完成后点击&quot;文件&quot;-“首选项”-“设置”-“扩展”，打开 Plugin background config. background 插件配置，点击&quot;在 settings.json 中编辑&quot;，添加一段配置代码（注意得添加在大括号内）： 6. Atom One Dark Theme 最经典的一款黑色皮肤，强烈推荐！效果参考前文所示。 7. Python Python 这个插件必装！别问为啥！因为它是微软 VS Code 开发团队自己开发的，亲儿子的级别。虽然 VS Code 不安装任何插件也能高亮 Python 代码，但该件提供的功能远不止如此，还有很多强大的功能。 注意：如果装有 Code Runner 插件，运行 Python 代码时可能会出现中文乱码问题，其实 Python 运行不需要 Code Runner 插件。 8. Python Extension Pack 这是一个 Python 扩展包，它依赖于以下扩展包： Python：高亮、调试（多线程、远程）、智能提示、代码格式化、重构、单元测试、代码片段、数据科学（使用 Jupyter）、PySpark 等等。 Jinja：对 Visual Studio 代码的 Jinja 模板语言支持。 Django：为有期限的完美主义者提供了漂亮的语法和限定范围的片段。 Visual Studio IntelliCode：在 Visual Studio 代码中为 Python 开发人员提供人工智能辅助的生产力功能，并基于对代码的理解和机器学习提供见解。 Python Environment Manager：提供从一个地方查看和管理所有 Python 环境和包的能力。 Python Docstring Generator：基于多个可选模板模式，为类和方法快速插入带有上下文推断参数的 Python 注释块。 Python Indent：在 Visual Studio 代码中更正 Python 缩进。 Jupyter：为 Python 语言提供 Jupyter Notebook 支持，用于数据科学、科学计算和机器学习。 "},{"title":"Linux与Windows下Vim配置方案推荐","date":"2021-11-21T02:15:00.000Z","url":"/posts/11764.html","tags":[["Others","/tags/Others/"]],"categories":[["Others","/categories/Others/"]],"content":" Vim 真香！！！不会吧不会还有人在用 IDE 吧（bushi） 1. 前言 可能很多萌新程序员会问为什么很多大佬写代码时用的都是 Vim 而不是自己熟悉的 Visual Studio、VS Code、IDEA、PyCharm、CLion 等这些 IDE 呢？用 Vim 编辑有什么优点呢？ Vim 对硬件需求小：如果只是编写一个相对来说不是那么庞大的程序的话，用 IDE 有种“大材小用”的感觉，启动、编译运行相比 Vim 来说都满了很多，占用内存资源也很大，因此平常自己写写代码不是搞大型开发的话用 Vim 绝对会感觉又快又方便； Vim 自由度高：无论什么 IDE，那终究还是别人搭建好的编辑平台，你永远驯服不了，而对于 Vim，从头到尾的元素都是可以由自己 DIY 的，就像是自己专属的开发环境，里面的元素都是自己最喜欢且最熟悉的； Vim 光标移动效率高：对于新手而言，肯定会觉得 Vim 很难用，效率很低，那是因为还不熟悉 Vim 的整套光标移动快捷键，一旦熟练了快捷键后，你会发现用鼠标移动光标是一件特别慢还特别麻烦的事情，用 Vim 后完全可以脱离鼠标，让光标移动的比鼠标操作灵活很多（毫不夸张）； Vim 编辑效率高：同样的，Vim 有着一整套编辑操作的快捷键组合，且可以扩展各种插件，编辑代码的功能方面绝不逊于各大 IDE。 那么如何配置一个属于自己的 Vim 环境呢？本文就来介绍一下 .vimrc 文件的配置（Windows 操作系统环境下）。 2. Windows下.vimrc文件的配置 在 Windows 操作系统环境下，用户可以进入自己的用户根目录（C:\\Users\\你的用户id）下，新建一个文本文件，在其中输入以下内容： 然后保存，将文件名改成 .vimrc ，这时候再打开 Vim 看一下效果，如下图所示： 配置文件中的每一行代码的功能在边上都有相应的注释（&quot; 之后的内容即为注释），要打开/关闭某一项功能只需在代码的最前面加上/删去 &quot; 即可。 对于主题，个人喜好的是 elflord 和 desert，字体的话 Consolas yyds！剩下的元素大家可以根据自己的喜好进行调整。 对于 F5 自动编译功能，如果使用的是 Linux 环境，那么需要将以下红框部分的代码修改为 exec &quot;! ./%&lt;&quot; 3. Linux下.vimrc文件的配置 3.1 Onedark主题配置 本人较为喜欢的主题为 Onedark，因此在本文中介绍一下 Linux 环境下怎么配置 Onedark 主题。 首先在 ~/.vim 文件夹中创建两个文件夹：colors 和 autoload，如果已经有了那么跳过此步。 然后将Onedark主题中的东西下载至 .vim 文件夹中，可以直接使用命令行： 下载完成后在 .vim 文件夹中有个 onedark.vim 文件夹，将 onedark.vim/colors 文件夹中的 onedark.vim 文件复制到 ~/.vim/colors 中，将 onedark.vim/autoload 文件夹中的 onedark.vim 文件复制到 ~/.vim/autoload 中： 然后修改 .vimrc 文件，将原本的主题设置注释掉，添加一行 colorscheme onedark，至此配置就完成了。 3.2 Linux环境下.vimrc文件推荐 以下为本人目前正在使用的一套 Vim 配置，配置代码如下： "},{"title":"NodeJS的安装及配置","date":"2021-10-13T03:38:00.000Z","url":"/posts/11062.html","tags":[["Others","/tags/Others/"]],"categories":[["Others","/categories/Others/"]],"content":" Node.js 是一个基于 Chrome V8 引擎的 JavaScript 运行环境。本文介绍如何安装与配置Node.js。 使用教程见官方文档：Node.js Docs。 一、官网下载及安装NodeJS 官网下载地址：NodeJS Download。 本文的安装路径为：D:\\NodeJS。 安装完成后打开命令行窗口校验版本： 测试 npm 是否安装成功，由于新版的 NodeJS 已经集成了 npm，所以之前 npm 也一并安装好了，同样可以使用命令行窗口校验： 二、环境变量配置 在 D:\\NodeJS 目录下新建两个文件夹：node_global 和 node_cache： 长按 Shift + 鼠标右键，选择打开 PowerShell 窗口或者打开 Git Bash： 输入以下命令： 接着配置环境变量，右键&quot;我的电脑&quot;-“属性”-“高级系统设置”，点击&quot;高级&quot;选项卡，选择&quot;环境变量&quot;进行配置。在&quot;系统变量&quot;下点击&quot;新建&quot;，变量名和路径如下图所示（node_modules 目录直接手打即可）： 在&quot;用户变量&quot;的 Path 变量上点击&quot;编辑&quot;，更改 npm 安装的默认路径，如下图所示： 修改为： 配置完成后，尝试在命令行窗口中安装一些环境（若安装出错可用管理员身份打开命令行窗口）： "},{"title":"算法竞赛C++ STL详解","date":"2021-10-05T09:40:00.000Z","url":"/posts/46520.html","tags":[["C++","/tags/C/"]],"categories":[["C++","/categories/C/"]],"content":" 本文介绍了什么是 STL 以及如何使用 STL 更高效偷懒地解题。本篇文章将会长期更新，欢迎大家一起监督学习。 一、STL概念 STL（Standard Template Library，标准模板库），是惠普实验室开发的一系列软件的统称。现主要出现在 C++ 中，STL 从广义上分为：容器（Container）、算法（Algorithm）和迭代器（Iterator）。STL 几乎所有的代码都采用了模板类或者模板函数，这相比传统的由函数和类组成的库来说提供了更好的代码重用机会。 二、STL六大组件 STL 提供了六大组件，彼此之间可以组合套用，这六大组件分别是容器、算法、迭代器、仿函数、适配器、空间配置器。其中，在算法竞赛中用到最多的为容器、算法与迭代器。 容器（Container）：STL 容器为各种数据结构，如 vector、stack、queue、map、set 等，用来存放数据，从实现角度来看，STL 容器是一种 class template。 算法（Algorithm）：STL 的算法多数定义在 &lt;algorithm&gt; 头文件中，其中包括了各种常用的算法，如 sort、find、copy、reverse 等，从实现角度来看，STL 算法是一种 function template。 迭代器（Iterator）：STL 迭代器扮演了容器与算法之间的胶合剂，共有五种类型，从实现角度来看，迭代器是一种将 opetator*、opetator-&gt;、operator++ 等指针相关操作予以重载的 class template。所有 STL 容器都附带有自己专属的迭代器，只有容器的设计者才知道如何遍历自己的元素。 仿函数（Functor）：行为类似函数，可作为算法的某种策略，从实现角度来看，仿函数是一种重载了 operator() 的 class 或者 class template。 适配器（Adaptor）：一种用来修饰容器或仿函数或迭代器接口的东西。 空间配置器（Allocator）：负责空间的配置与管理。从实现角度来看，配置器是一个实现了动态空间配置、空间管理、空间释放的 class template。 三、STL容器 相信很多人学习 STL 就是为了在比赛中能够更好地装B运用各种数据结构和算法，提高解题速度。确实，使用 STL 中的容器能够不需要自己手写定义各种数据结构，使用 STL 中的算法能够不需要自己手写实现各种基本算法，因此本部分对于算法巨巨们是最为重要的一部分，那么 STL 容器究竟有哪些呢？在做题中该如何使用呢？ 3.1 vector vector 又称变长数组，定义在 &lt;vector&gt; 头文件中，vector 容器是动态空间，随着元素的加入，它的内部机制会自动扩充空间以容纳新的元素。因此 vector 的运用对于内存的合理利用与运用的灵活性有很大的帮助。 vector 的定义方式： vector 的常用内置函数： 3.2 stack stack 又称栈，是一种后进先出（Last In First Out，LIFO）的数据结构，定义在 &lt;stack&gt; 头文件中，stack 容器允许新增元素、移除元素、取得栈顶元素，但是除了最顶端以外，没有任何方法可以存取 stack 的其它元素，换言之，stack 不允许有遍历行为。 stack 的定义方式： stack 的常用内置函数： 3.3 string string 又称字符串，定义在 &lt;string&gt; 头文件中。C 风格的字符串（以空字符结尾的字符数组）太过复杂难于掌握，因此 C++ 标准库定义了一种 string 类。string 和 vector&lt;char&gt; 在数据结构、内存管理等方面都是相同的。但是，vector&lt;char&gt; 只是单纯的一个“char 元素的容器”，而 string 不仅是一个“char 元素的容器”，它还扩展了一些针对字符串的操作，例如 string 可以使用 c_str() 函数转换为 C 风格的字符串，vector 中并未对输入输出流操作符进行重载，因此无法直接对 vector&lt;char&gt; 进行 cin 或者 cout 这样的操作，但是 string 可以，且 vector&lt;char&gt; 并不能直接实现字符串的拼接，但是 string 可以，string 中重载了 +, += 运算符。 string 的定义方式： string 的常用内置函数： string 的 erase() 与 remove() 函数的用法： 3.4 queue/priority_queue queue 又称队列，是一种先进先出（First In First Out，FIFO）的数据结构，定义在 &lt;queue&gt; 头文件中，queue 容器允许从一端（称为队尾）新增元素（入队），从另一端（称为队头）移除元素（出队）。 priority_queue 又称优先队列，同样定义在 &lt;queue&gt; 头文件中，与 queue 不同的地方在于我们可以自定义其中数据的优先级，优先级高的排在队列前面，优先出队。priority_queue 具有 queue 的所有特性，包括基本操作，只是在这基础上添加了内部的一个排序，它的本质是用堆实现的，因此可分为小根堆与大根堆，小根堆中较小的元素排在前面，大根堆中较大的元素排在前面。（创建 priority_queue 时默认是大根堆！） queue/priority_queue 的定义方式： queue/priority_queue 的常用内置函数： 3.5 deque deque 又称双端队列，定义在 &lt;deque&gt; 头文件中，vector 容器是单向开口的连续内存空间，deque 则是一种双向开口的连续线性空间。所谓的双向开口，意思是可以在头尾两端分别做元素的插入和删除操作，当然，vector也可以在头尾两端插入元素，但是在其头部进行插入操作效率很低。deque 和 vector 最大的差异一是在于 deque 允许使用常数项时间在头部进行元素的插入和删除操作，二是在于 deque 没有容量的概念，因为它是动态的以分段连续空间组合而成，随时可以增加一段新的空间并链接起来。 deque 的定义方式： deque 的常用内置函数： 3.6 map/multimap map/multimap 又称映射，定义在 &lt;map&gt; 头文件中，map 和 multimap 的底层实现机制都是红黑树。map 的功能是能够将任意类型的元素映射到另一个任意类型的元素上，并且所有的元素都会根据元素的键值自动排序。map 所有的元素都是 pair，同时拥有键值和实值（即 (key, value) 对），key 被视为键值，value 被视为实值，map 不允许两个元素有相同的键值。multimap 和 map 的操作类似，唯一区别是 multimap 的键值允许重复。 map/multimap 的定义方式： map/multimap 的常用内置函数： 3.7 set/multiset set/multiset 又称集合，定义在 &lt;set&gt; 头文件中。set 的特性是所有元素都会根据元素的键值自动被排序，set 的元素不像 map 那样可以同时拥有键值和实值，set 的元素既是键值又是实值，set 不允许两个元素有相同的键值，因此总结来说就是 set 中的元素是有序且不重复的。multiset 的特性和用法和 set 完全相同，唯一的区别在于 multiset 允许有重复元素，set 和 multiset 的底层实现都是红黑树。 set/multiset 的定义方式： set/multiset 的常用内置函数： 3.8 unordered_map/unordered_set unordered_map/unordered_set 分别定义在 &lt;unordered_map&gt; 与 &lt;unordered_set&gt; 头文件中，内部采用的是 hash 表结构，拥有快速检索的功能。与 map/set 相比最大的区别在于 unordered_map/unordered_set 中的元素是无序的，增删改查的时间复杂度为 O(1)（map/set 增删改查的时间复杂度为 O(logn)），但是不支持 lower_bound()/upper_bound() 函数。 unordered_map/unordered_set 的定义方式： unordered_map/unordered_set 的常用内置函数： 四、STL算法 C++ 标准库定义了一组泛型算法，之所以称为泛型指的是它们可以操作在多种容器上，不但可以作用于标准库类型，还可以用在内置数组类型甚至其它类型的序列上。泛型算法定义在 &lt;algorithm&gt; 头文件中，标准库还定义了一组泛化的算术算法（Generalized Numeric Algorithm），定义在 &lt;numeric&gt; 头文件中。使用方法如下： "}]