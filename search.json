[{"title":"Determined AI部署与使用教程","date":"2024-03-22T13:14:00.000Z","url":"/posts/32409.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" GPU 算力池管理工具 Determined AI 使用教程，适用于实验室、企业的服务器 GPU 管理。 官方文档：Determined AI。 1. 概念 1.1 什么是Determined？ Determined AI 是一个全功能的深度学习平台，兼容 PyTorch 和 TensorFlow。它主要负责以下几个方面： 分布式训练：Determined AI 可以将训练工作负载分布在多个 GPU（可能在多台计算机上）上，而无需更改代码。无论是在一台计算机上利用2个 GPU 还是在多台计算机上利用16个 GPU，都只需更改配置即可。 超参数调优：Determined AI 提供了自动超参数搜索功能，可以帮助你找到最优的模型参数。 资源管理：Determined AI 可以有效地管理和调度计算资源，以降低云 GPU 的成本。 实验跟踪：Determined AI 可以跟踪和记录实验的过程，方便你分析结果和复现实验。 Determined AI 的主要组件之一是命令行接口（CLI），它提供了一种高效的方式来管理和控制系统的各个方面。例如，你可以使用 CLI 来创建、列出和管理实验，以及访问重要的实验指标和日志。CLI 还可以帮助你管理作业队列，监控正在进行的任务的进度，甚至根据需要优先处理或取消作业。 此外，Determined AI 还支持自由形式的任务，如命令和 Shell。命令和 Shell 使开发人员可以在不必编写符合试验 API 的代码的情况下，使用 Determined 集群和其 GPU。 1.2 Determined集群中的Master和Agent分别表示什么意思？ 在 Determined 集群中，Master 和 Agent 有特定的角色和功能。 Master：Master 是 Determined 集群的核心，它负责管理和调度所有的任务。Master 接收来自用户（通过网页、CLI 等方式）的请求，并将这些请求发送到 Agent 进行处理。Master 的行为可以通过设置配置变量来控制，这可以通过使用配置文件、环境变量或命令行选项来完成。 Agent：Agent 是执行任务的节点，它们在 Master 的调度下运行任务。每个 Agent 都有一个唯一的 ID（默认为当前机器的主机名），并且在一个集群中必须是唯一的。Agent 节点通常是配备 GPU 的服务器，用于运行计算密集型的深度学习任务。 总的来说，Master 负责管理和调度任务，而 Agent 负责执行这些任务。 2. 部署与配置 2.1 在本地部署 在部署 Master 与 Agent 节点的服务器上需要先安装好 Docker，安装教程（Ubuntu 系统下）：Install Docker Engine on Ubuntu。 安装 nvidia-container-toolkit（Master 和存储节点不需要 GPU，也无需安装 Nvidia 相关的内容），否则部署使用 GPU 的集群会出现报错 Internal Server Error (&quot;could not select device driver &quot;nvidia&quot; with capabilities: [[gpu utility]]&quot;)： 安装 Determined 库并在本地启动集群，对于本地开发或小型集群（例如 GPU 工作站），您可能希望同时安装 Master 和 Agent 位于同一节点上，因此可以使用 cluster-up： 如果 det 命令识别不到可能是没有配置环境变量： 要停止 Determined 集群，请在当前运行 Determined 集群的计算机上，运行： 在许多情况下，Determined 集群将由多个节点组成。在这种情况下，您将需要分别启动 Master 和 Agents。启动和停止独立 Master： 若要在计算机上部署独立的 Agent，请运行以下命令之一，&lt;master_hostname&gt; 为 Master 的主机名： 与用户相关的指令： 访问 ，用户名为 determined，密码留空，即可登录。 2.2 配置参考 查看当前 Master 配置信息： 我们可以自定义 Master 的配置，在需要部署 Master 的服务器上创建 Master 配置文件 master-config.yaml，具体配置教程见：Master Configuration Reference，参考内容如下： 然后即可使用配置文件启动集群： 在需要部署 Agent 的服务器上创建 Agent 配置文件 agent-config.yaml，具体配置教程见：Agent Configuration Reference，参考内容如下： 这里主要配置3个内容： Master 节点主机名和端口号，用于识别 Master 节点。 agent_id 和 resource_pool，分别对应本机 ID 和资源池，根据显卡型号命名即可，注意资源池需要已在 Master 配置文件中定义过。 科学上网代理。 完成后，所有 Agent 节点使用如下命令启动（最后的 0.29.0 为 Determined AI 的版本号，根据自己安装的版本修改即可）： 2.3 命令行、Notebook、Shell的使用方法 Determined 主要有两种使用方式：Web 和 CLI。其中 Web 可以直接通过 &lt;Master节点IP&gt;:8080 进行访问，但是不推荐使用 Web 方式，因为网页端创建任务无法自定义存储路径和加载的 Docker 镜像，灵活度十分受限。因此接下来详细介绍 CLI 的用法。 首先在终端机安装好 Determined，然后在环境变量中配置 Master 节点 IP： 接下来可以使用 Determined CLI 创建任务，首先在终端机需要登录，在 Admin 用户（用户名为 admin，密码留空）中可以管理其他用户，假设已经给终端机创建好账号即可登录： 然后写一个开启任务的 Yaml 配置文件，这里给出一个参考： Determined 官方给出了很多个版本的 CUDA 和框架的组合，可以自行选择所需要的镜像，Determined AI Docker 镜像列表：Docker Hub DeterminedAI。 完成后，可以选择开启 Jupyter Notebook 或者终端，这取决于使用者的习惯： 对应的停止任务指令如下： 任务的 &lt;ID&gt; 可通过 det task list 查看所有任务信息获得，写前8位即可。 2.4 创建实验 先下载官方的项目示例代码：mnist_pytorch.tgz。 将其解压到当前目录： 可以看到该目录下有单卡运行实验以及多卡并行运行实验的实验配置文件 const.yaml 和 distributed.yaml，此处给出 distributed.yaml 配置参考内容如下： 接下来即可通过配置文件创建任务（最后一个 . 表示上传当前目录中的所有文件，作为模型的上下文目录。Determined 将模型上下文目录内容复制到试验容器工作目录）： 2.5 通过VSCode SSH连接 首先确保 VSCode 已经安装 Remote - SSH 扩展，当用户开启了 Shell 后，可以在终端机上执行以下命令获取 Shell 的 SSH 登录命令： 复制 SSH 命令，在 VS Code 的 Remote Explorer 页面下即可添加连接。"},{"title":"SpringBoot学习笔记-实现微服务：匹配系统（下）","date":"2023-12-05T13:00:00.000Z","url":"/posts/24133.html","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 本节内容为实现完整的匹配系统微服务。 1. 重构项目 1.1 初始化Spring Cloud项目 现在需要把匹配系统设计成一个微服务，也就是一个独立的系统，可以认为是一个新的 SpringBoot 后端，当之前的服务器获取到两名玩家的匹配请求后会向后台的匹配系统服务器发送 HTTP 请求，匹配系统类似于之前的 Game，在接收到请求之后也会单独开一个新的线程来匹配，可以设计成每隔一秒扫一遍匹配池中已有的玩家，然后判断能否匹配出来，如果可以就将匹配结果通过 HTTP 请求返回。 匹配系统和网站后端是两个并列的后端项目，因此可以修改一下项目结构，将这两个后端改为子项目，然后新建一个新的父级项目。 我们新建一个 Spring 项目，项目名为 backendcloud，还是选用 Maven 管理项目，组名为 com.kob。注意 2023.11.24 之后 SpringBoot 2.x 版本正式弃用，SpringBoot 3.x 版本需要 Java 17 及以上。我们现在选择 SpringBoot 3.2.0 版本，依赖选上 Spring Web 即可。 父级项目是没有逻辑的，因此可以把 src 目录删掉，然后修改一下 pom.xml，首先在 &lt;description&gt;backendcloud&lt;/description&gt; 后添加一行：&lt;packaging&gt;pom&lt;/packaging&gt;，然后添加 Spring Cloud 的依赖，前往 Maven 仓库，搜索并安装以下依赖： spring-cloud-dependencies 接着在 backendcloud 目录下创建匹配系统子项目，选择新建一个模块（Module），选择空项目，匹配系统的名称为 matchingsystem，在高级设置中将组 ID 设置为 com.kob.matchingsystem。 这个新建的子项目本质上也是一个 SpringBoot，我们将父级目录的 pom.xml 中的 Spring Web 依赖剪切到 matchingsystem 中的 pom.xml，也就是以下这段依赖： 1.2 创建匹配系统框架 由于有两个 SpringBoot 服务，因此需要修改一下匹配系统的端口，在 resources 目录下创建 application.properties 文件： 在 com.kob.matchingsystem 包下创建 controller 和 service 包，在 service 包下创建 impl 包。先在 service 包下创建 MatchingService 接口： 然后简单实现一下 MatchingServiceImpl： 最后在 controller 包下创建 MatchingController，在 Spring 的 WebClient 中，如果你想要发送一个 Map 类型的数据，你需要将它转换为 MultiValueMap。这是因为 HTTP 的表单数据通常可以包含多个值，而 Map 只能包含一个值。因此，WebClient 使用 MultiValueMap 来表示表单数据： 注意，匹配系统应该只能接收来自 Web 后端服务器发来的请求，而不能接收其他外网的请求，因此需要使用 Spring Security 实现用户授权，在 matchingsystem 子项目中添加上 spring-boot-starter-security 依赖后创建一个 config 包，然后创建和之前类似的 SecurityConfig： 我们将自定义的 IpAddressMatcher 类定义为 record，这是 Java 14 中引入的一个新特性。它是一种类似于类的结构，但用于表示不可变数据。相比于传统的 Java 类，record 在定义数据类时更为简洁、易读和易用，record 申明的类，具备以下特点： 它是一个 final 类。 自动实现 equals、hashCode、toString 方法。 成员变量均为 final，且有对应的 public 访问器方法。 相当于以下的类定义（省略了自动实现的方法）： 现在需要将这个匹配系统子项目变为 Spring 项目，将 Main 改名为 MatchingSystemApplication，然后将其修改为 SpringBoot 的入口： 1.3 迁移Web后端 在 backendcloud 目录下创建一个新的模块名为 backend，然后在高级设置中将组 ID 改为 com.kob.backend，创建好后先将 src 目录删掉，然后将之前的 Web 后端项目中的 src 复制过来，还需要将之前的 pom.xml 配置信息中的依赖配置 &lt;dependencies&gt; 也复制过来，可以将 spring-boot-starter-thymeleaf 依赖删掉。 由于我们新建的 backendcloud 项目升级到了 SpringBoot 3.2.0，因此迁移过来的 backend 需要重构一部分代码，首先是 javax.servlet 和 javax.websocket 已经被弃用，需要用 Jakarta 提供的依赖包，先在 backend 的 pom.xml 中添加以下依赖： jakarta.websocket-api jakarta.websocket-client-api 然后修改 JwtAuthenticationTokenFilter 的包导入代码： 修改 WebSocketServer 的包导入代码： 然后修改 Spring Security 的配置文件 SecurityConfig： 可以注意到我们在放行链接时 IP 地址为 0:0:0:0:0:0:0:1，代码中的 request.getRemoteAddr() 方法用于获取发起 HTTP 请求的客户端的 IP 地址，Vue 前端使用 Ajax 向后端发送 HTTP 请求时所用的 localhost 通过该方法会返回 IPv6 的回环地址 0:0:0:0:0:0:0:1，而如果是 SpringBoot 后端之间发送 HTTP 请求则 localhost 会返回 IPv4 的回环地址 127.0.0.1。 这时候在 IDEA 中连接上数据库后启动一下 backend 项目会看到报错：Invalid value type for attribute 'factoryBeanObjectType': java.lang.String，这是由于 MyBatis-Plus 中的 MyBatis-Spring 版本较低，以后更新成新版本应该能解决问题，目前我们先在 mybatis-plus-boot-starter 依赖中排除掉 mybatis-spring，然后自己添加一个新的 mybatis-spring 依赖： 2. 实现匹配系统微服务 2.1 数据库更新 我们将 rating 放到用户身上而不是 BOT 上，每个用户对应一个自己的天梯分。在 user 表中创建 rating，并将 bot 表中的 rating 删去，然后需要修改对应的 pojo，还有 service.impl.user.account 包下的 RegisterServiceImpl 类以及 service.impl.user.bot 包下的 AddServiceImpl 和 UpdateServiceImpl 类。 2.2 Web后端与匹配系统后端通信 先在 backend 项目的 config 包下创建 RestTemplateConfig 类，便于之后在其他地方注入 RestTemplate。RestTemplate 能够在应用中调用 REST 服务。它简化了与 HTTP 服务的通信方式，统一了 RESTful 的标准，封装了 HTTP 链接，我们只需要传入 URL 及返回值类型即可： 我们将 WebSocketServer 的简易匹配代码删去，然后使用 HTTP 请求向 matchingsystem 后端发送匹配请求，注意我们将 startGame() 方法改为 public，因为之后需要在处理匹配成功的 Service 中调用该方法来启动游戏： 需要注意的是，从 Spring 5 开始，Spring 引入了一个新的 HTTP 客户端叫做 WebClient。WebClient 是 RestTemplate 的一个现代化的替代品，它不仅提供了传统的同步 API，还支持高效的非阻塞和异步方法。 WebClient 在 WebFlux 模块中，WebFlux 是 Spring 5.0 之后引入的一种基于响应式编程的 Web 框架。它是完全非阻塞式的，与传统的 Spring MVC 相比，WebFlux 是基于 NIO（新 IO，也叫非阻塞 IO），所以在 IO 性能上会比传统的 MVC 性能要好一些。 如果需要替换成 WebClient，需要先安装依赖：Spring Boot Starter WebFlux，然后在 config 包下创建 WebClientConfig： WebClient 接口提供了三个不同的静态方法来创建 WebClient 实例： 利用 create() 创建。 利用 create(String baseUrl) 创建。 利用 builder() 创建（推荐）。 使用 builder() 返回一个 WebClient.Builder，然后再调用 build() 就可以返回 WebClient 对象，我们可以用 WebClient.Builder 实例配置 WebClient： baseUrl(String baseUrl)：这个方法用于设置 WebClient 的基础 URL，所有的请求都会基于这个 URL。例如，如果你设置了 baseUrl(&quot;;)，那么后续的 .uri(&quot;/user/1&quot;) 实际上是在请求  这个 URL。 defaultHeader(String headerName, String... headerValues)：这个方法用于设置默认的 HTTP 头，这些头会被添加到所有的请求中。例如，你可以使用 defaultHeader(HttpHeaders.USER_AGENT, &quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko)&quot;) 来设置 User-Agent 头。 defaultCookie(String cookieName, String... cookieValues)：这个方法用于设置默认的 HTTP Cookie。这些 Cookie 会被添加到所有的请求中。例如，你可以使用 defaultCookie(&quot;ACCESS_TOKEN&quot;, &quot;test_token&quot;) 来设置一个名为 ACCESS_TOKEN 的 Cookie。 例如： 在 WebSocketServer 中按如下方式发送请求： 现在将两个后端项目都启动起来，可以在 IDEA 下方的服务（Services）选项卡的 Add Service 中点击 Run Configuration Type，然后选中 Spring Boot，这样就能在下方窗口中看到两个 SpringBoot 后端的情况。 尝试在前端中开始匹配，可以看到 matchingsystem 后端控制台输出：Add Player: 1, Rating: 1500。 2.3 实现匹配逻辑 匹配系统需要将当前正在匹配的用户放到一个匹配池中，然后开一个新线程每隔一段时间去扫描一遍匹配池，将能够匹配的玩家匹配在一起，我们的匹配逻辑是匹配两名分值接近的玩家，且随着时间的推移，两名玩家的分差可以越来越大。 首先需要添加 Project Lombok 依赖，我们使用与之前 Web 后端相同的依赖版本： 在 matchingsystem 项目的 service.impl 包下创建 utils 包，然后在其中创建 Player 类： 由于我们需要在 matchingsystem 项目中向 backend 发请求，因此也需要用 WebClient，将 backend 的 config 包中的 WebClientConfig 复制到 matchingsystem 的 config 包中（别忘了也需要在 matchingsystem 项目的 pom.xml 中添加 Spring Boot Starter WebFlux 依赖）。 接着创建 MatchingPool 类用来维护我们的这个新线程： 从匹配池中删除玩家需要遍历找出 userId 所对应的玩家，players.removeIf(player -&gt; player.getUserId().equals(userId)); 就等价于以下代码： 现在即可将这个线程在 MatchingServiceImpl 中定义出来： 可以在启动 matchingsystem 项目的时候就将该线程启动，即在 MatchingSystemApplication 这个主入口处启动： 2.4 Web后端接收匹配结果 我们的 Web 后端还需要从 matchingsystem 接收请求，即接收匹配系统匹配成功的信息。在 backend 项目的 service 以及 service.impl 包下创建 pk 包，然后在 service.pk 包下创建 StartGameService 接口： 然后在 service.impl.pk 包下创建接口的实现 StartGameServiceImpl 接着在 controller.pk 包下创建 StartGameController： 实现完最后需要放行这个 URL，修改 SecurityConfig： "},{"title":"SpringBoot学习笔记-实现微服务：匹配系统（中）","date":"2023-11-30T15:13:00.000Z","url":"/posts/5710.html","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 本节内容为同步玩家的移动与碰撞检测，并实现游戏结束的前端界面以及持久化游戏对局信息。 1. 同步玩家位置 1.1 游戏信息的记录 两名玩家初始位置需要由服务器确定，且之后的每次移动都需要在服务器上判断。我们需要在 Game 类中添加 Player 类用来记录玩家的信息，在 consumer.utils 包下创建 Player 类： 然后就可以在 Game 中创建玩家： 在 WebSocketServer 中创建 Game 时传入两名玩家的 ID，并且我们将与游戏内容相关的信息全部包装到一个 JSONObject 类中： 前端也需要进行相应的修改，在 store/pk.js 中创建两名玩家的信息： 在 GameMap.vue 中需要先将 GameMap 对象存下来，之后会在 PKIndexView 中用到： PKIndexView 中要传入从后端获取到的 game 数据： 1.2 实现多线程同步移动 我们需要实现两名玩家的客户端以及服务器端的移动同步，假如 Client1 发出了移动指令，那么就会将这个消息发送给服务器，同理另一个客户端 Client2 发出移动指令时也会将消息发送给服务器，服务器在接收到两名玩家的消息后再将消息同步给两名玩家。 我们在 WebSocketServer 中会维护一个游戏 Game，这个 Game 也有自己的执行流程，它会先创建地图 creatMap，接着会一步一步执行，即 nextStep，每一步会等待两名玩家的操作，这个操作可以是键盘输入，也可以是由 Bot 代码执行的微服务返回回来的结果。获取输入后会将结果发送给一个评判系统 judge，来判断两名玩家下一步是不是合法的，如果有一方不合法就游戏结束。 在等待用户输入时会有一个时间限制，比如5秒，如果有一方还没有输入则表示输了，同样也是游戏结束。否则如果两方输入的下一步都是合法的则继续循环 nextStep。这个 nextStep 流程是比较独立的，而且每个游戏对局都有这个独立的过程，如果 Game 是单线程的，那么在等待用户输入时这个线程就会卡死，如果有多个游戏对局的话那么只能先卡死在某个对局中，其他对局的玩家体验就会很差，因此 Game 不能作为一个单线程来处理，每次在等待用户输入时都需要另起一个新的线程，这就涉及到了多线程的通信以及加锁的问题。 ReentrantLock 是 Java 中常用的锁，属于乐观锁类型，多线程并发情况下能保证共享数据安全性，线程间有序性。它的作用和 synchronize 是一样的，都是实现锁的功能，但是 ReentrantLock 需要手写代码对锁进行获取和释放（一定要在 finally 块里面释放），要不然就永远死锁了。ReentrantLock 通过原子操作和阻塞实现锁原理，一般使用 lock() 获取锁，unlock() 释放锁。 我们将 Game 类继承自 Thread 类即可转为多线程，然后需要实现 Thread 的入口函数，使用快捷键 Alt + Insert，选择重写方法，需要重写的是 run() 方法，这是新线程的入口函数： 然后前端 GameMap.js 中在移动时需要向后端通信，现在两名玩家的键盘输入操作就只需要 W/S/A/D 了： WebSocketServer 对于每局游戏对局都会创建一个 Game 类，通过 start() 方法可以新开一个线程运行 Game 中的 run() 方法，由于我们需要在 Game 中使用 WebSocketServer 的 users，还需要将 users 修改为 public，然后需要接收前端的移动请求： 最后在 PKIndexView 中处理接收到后端发来的移动消息以及游戏结束消息： 2. 同步碰撞检测 现在还需要将碰撞检测放到后端进行判断，先将 Snake.js 中的碰撞检测判断代码删掉，并将死后变白的逻辑放到 render() 函数中： 接下来需要实现后端中的 judge() 方法，在判断的时候需要知道当前蛇的身体有哪些，先在 comsumer.utils 包下创建 Cell 类表示身体的每一格： 然后在 Player 类中创建一个方法能够根据玩家历史走过的路径找出当前这条蛇身体的每一格： 最后即可在 Game 类中实现 judge() 方法： 3. 实现游戏结束界面 首先我们需要将输的玩家记录到前端的全局变量中，在 store/pk.js 中添加 loser 变量： 然后在 PKIndexView 组件的游戏结束处理语句块中添加更新 loser 的语句： 游戏结束后需要给用户展示谁赢谁输的界面，并提供一个重开按钮，在 components 目录下创建 ResultBoard.vue： 注意 state.pk 中的 a_id 和 b_id 是整数，而之前 state.user 中的 id 是字符串，因此需要做类型转换再判断是否相等。 4. 持久化游戏状态 4.1 创建数据库表 最后我们还需要将游戏过程存到数据库中，方便用户之后回看游戏录像，在数据库中创建 record 表用来记录每局对战的信息： id: int（主键、自增、非空） a_id: int a_sx: int a_sy: int b_id: int b_sx: int b_sy: int a_steps: varchar(1000) b_steps: varchar(1000) map: varchar(1000) loser: varchar(10) createtime: datetime 创建该数据库表的 SQL 语句如下： 在 pojo 包下创建 Record 类如下： 在 mapper 包下创建 RecordMapper 类如下： 4.2 保存游戏对局信息 可以在向前端发送游戏结果消息之前将对局信息存下来，首先需要在 WebSocketServer 中将 RecordMapper 创建出来： 然后在 Player 中创建辅助函数用来返回 steps 的字符串形式： 最后就可以在 Game 中将游戏记录保存至数据库中： "},{"title":"Spring面试题总结","date":"2023-11-24T09:09:00.000Z","url":"/posts/2329.html","tags":[["Interview","/tags/Interview/"]],"categories":[["Interview","/categories/Interview/"]],"content":" Spring 面试题总结，涉及 Spring、Spring MVC、Spring Boot、Spring Cloud、MyBatis 等内容，文章将不断更新。 1. Spring 1.1 Spring 框架是什么？有什么优势？核心特性是什么？ Spring 框架是一个轻量级的 Java 开发框架，目的是为了解决企业级应用开发的业务逻辑层和其他各层的耦合问题。它是一个分层的 JavaSE/JavaEE full-stack（一站式）轻量级开源框架，为开发 Java 应用程序提供全面的基础架构支持。Spring 的最根本的使命是解决企业级应用开发的复杂性，即简化 Java 开发。 Spring 框架的优势主要有： 方便解耦，简化开发：Spring 就是一个大工厂，可以将所有对象的创建和依赖关系的维护，交给 Spring 管理。 AOP 编程的支持：Spring 提供面向切面编程，可以方便的实现对程序进行权限拦截、运行监控等功能。 声明式事务的支持：只需要通过配置就可以完成对事务的管理，而无需手动编程。 方便程序的测试：Spring 对 Junit4 支持，可以通过注解方便地测试 Spring 程序。 方便集成各种优秀框架：Spring 不排斥各种优秀的开源框架，其内部提供了对各种优秀框架（如：Struts、Hibernate、MyBatis 等）的直接支持。 降低 JavaEE API 的使用难度：Spring 对 JavaEE 开发中非常难用的一些 API（如 JDBC、JavaMail、远程调用等），都提供了封装，使这些 API 应用难度大大降低。 Spring 框架的核心特性为： IoC 容器：Spring 通过控制反转实现了对象的创建和对象间的依赖关系管理，开发者只需要定义好 Bean 及其依赖关系，Spring 容器负责创建和组装这些对象。 AOP：面向切面编程，允许开发者定义横切关注点，例如事务管理、安全控制等独立于业务逻辑的代码。通过 AOP，可以将这些关注点模块化，提高代码的可维护性和可重用性。 事务管理：Spring 提供了一致的事务管理接口，支持声明式和编程式事务，开发者可以轻松地进行事务管理，而无需关心具体的事务 API。 MVC 框架：Spring MVC 是一个基于 Servlet API 构建的 Web 框架，采用了模型-视图-控制器（MVC）架构。它支持灵活的 URL 到页面控制器的映射，以及多种视图技术。 1.2 什么是 Spring IoC 容器？ Spring IoC（Spring Inversion of Control）容器，是 Spring 框架的核心部分。IoC 即控制反转，是一种设计思想，在 Java 开发中，IoC 意味着将你设计好的对象交给容器控制，而不是传统的在你的对象内部直接控制。 Spring IoC 容器负责实例化、定位、配置应用程序中的对象及建立这些对象间的依赖。具体来说，Spring IoC 容器负责创建对象、管理对象（通过依赖注入）、装配对象、配置对象，并且管理这些对象的整个生命周期。 使用 IoC 的目的，主要是为了降低类之间的耦合。通过控制反转，对象的创建和对象之间的依赖关系处理，交给 Spring 容器来管理，不用程序员自己创建和维护。这样，应用程序无需直接在代码中创建相关的对象，应用程序由 IoC 容器进行组装。这种方式不仅降低了类之间的耦合，也使得代码更加简洁，更易于测试和维护。 1.3 什么是依赖注入？ 依赖注入（Dependency Injection，DI）是一种设计模式，也是 Spring 框架的核心概念之一。它的主要作用是去除 Java 类之间的依赖关系，实现松耦合，以便于开发测试。 在传统的程序设计过程中，当某个角色（可能是一个 Java 实例，调用者）需要另一个角色（另一个 Java 实例，被调用者）的协助时，通常由调用者来创建被调用者的实例。这种方式会导致调用者与被调用者之间产生紧密的耦合关系，使得代码难以修改和测试。 依赖注入的思想是，不在类内部直接创建依赖类的对象，而是将依赖的类对象在外部创建好之后，通过构造函数、函数参数等方式传递（或注入）给需要的类来使用。例如，A 类要依赖 B 类，A 类不再直接创建 B 类，而是把这种依赖关系配置在外部 XML（或 Java Config）文件中，然后由 Spring 容器根据配置信息创建、管理 bean 类。 这样，调用者不需要关心被调用者的创建和销毁，只需要关心如何使用被调用者，从而实现了调用者和被调用者之间的解耦。这种方式不仅降低了类之间的耦合，也使得代码更加简洁，更易于测试和维护。 举一个例子，加入我们有一个接口和两个实现类： 然后，我们有一个使用 MessageService 的 MessagePrinter 类： 在 Spring 的配置文件中，我们可以定义 MessageService 和 MessagePrinter 的 bean，并通过构造器注入依赖： 在这个例子中，MessagePrinter 依赖于 MessageService。通过 Spring IoC 容器和依赖注入，我们可以在外部配置文件中定义这种依赖关系，而不需要在 MessagePrinter 类中硬编码依赖的实现类。这样，我们可以轻松地更改 MessageService 的实现，而无需修改 MessagePrinter 类的代码，这就是依赖注入的优势。 如果我们运行 MessagePrinter 的 printMessage() 方法，它将打印出 Text Message。这是因为我们在 Spring 的配置文件中将 TextMessageService 定义为 MessageService 的实现类，然后通过构造器注入的方式将其注入到了 MessagePrinter 中。所以，当我们调用 printMessage() 方法时，它会调用 TextMessageService 的 getMessage() 方法。 总的来说，依赖注入是一种消除类之间依赖关系的设计模式，它使得对象之间的依赖关系更加清晰，代码更加灵活，更易于测试和维护。 1.4 依赖注入有几种方式？ 在 Spring 中，有四种常见的依赖注入方式： （1）属性注入（Field Injection）：直接在需要注入的字段上使用 @Autowired 或 @Resource 等注解： （2）Setter 注入（Setter Injection）：在 setter 方法上使用 @Autowired 或 @Resource 等注解。在 SpringBoot 中，由于 WebSocket 的特殊性，它是由容器管理的，而不是由 Spring 管理的 Bean，每次 WebSocket 连接都会创建一个新的 WebSocket 实例（非单例模式），因此不能直接使用属性注入的方式来注入 WebSocket。假如我们要在 WebSocket 中使用 Mapper，那么我们就可以用 Setter 注入的方式： （3）构造器注入（Constructor Injection）：在构造器上使用 @Autowired 或 @Resource 等注解： （4）静态工厂的方法注入：通过静态工厂方法创建 Bean，并在 Spring 配置文件中声明： 在 Spring 配置文件中声明： 1.5 如何理解 IoC 和 DI？ IoC（Inversion of Control，控制反转）和 DI（Dependency Injection，依赖注入）是 Spring 框架的核心概念，它们是面向对象编程的重要设计原则。IoC 和 DI 是同一个概念的不同角度描述，IoC 是一种设计思想，DI 是这种思想的一种实现方式。 IoC 是一种设计思想，不是一种技术。在 Java 开发中，IoC 意味着将你设计好的对象交给容器控制，而不是传统的在你的对象内部直接控制。这意味着，所有的类都会在 Spring 容器中登记，告诉 Spring 你是什么，你需要什么，然后 Spring 会在系统运行到适当的时候，把你要的东西主动给你，同时也把你交给其他需要你的东西。所有的类的创建、销毁都由 Spring 来控制，也就是说控制对象生存周期的不再是引用它的对象，而是 Spring。 DI 是 IoC 的一种实现方式。DI 是一种将调用者与被调用者分离的思想，组件之间的依赖关系由容器在运行时决定，形象的说，是由容器动态地将某个依赖关系注入到组件之中，这样你就可以使用 @Autowired、@Resource 等注解来实现自动注入。 1.6 什么是 AOP？ AOP（Aspect Oriented Programming），即面向切面编程，是一种通过预编译方式和运行期动态代理实现程序功能的统一维护的技术。它是 OOP（面向对象编程）的延续，也是 Spring 框架中的一个重要内容，是函数式编程的一种衍生范型。 AOP 的主要目标是将业务处理逻辑与系统服务分离开来，然后通过声明性的方式将系统服务应用到业务处理逻辑中。简单来说，AOP 就是把我们程序重复的代码抽取出来，在需要执行的时候使用动态代理技术在不修改源码的基础上，对我们的已有方法进行增强。 AOP 的主要术语包括： 切面（Aspect）：切面是一个横切关注点的模块化，一个切面能够包含同一个类型的不同增强方法，比如说事务处理和日志处理可以理解为两个切面。 连接点（JoinPoint）：程序执行过程中明确的点，如方法的调用或特定的异常被抛出。 切入点（PointCut）：切入点是对连接点进行拦截的条件定义。 通知（Advice）：通知是指拦截到连接点之后要执行的代码，包括了 around、before 和 after 等不同类型的通知。 目标对象（Target）：目标对象指将要被增强的对象，即包含主业务逻辑的类对象。 织入（Weaving）：织入是将切面和业务逻辑对象连接起来，并创建通知代理的过程。 1.7 AOP 的实现方式有哪些？ 通过 Spring API 实现：这种方式的核心是通过编写增强类来继承 Spring API 提供的接口。例如，你可以编写业务接口和实现类，然后编写增强类，并实现 Spring API 相关接口的方法。然后在 resource 目录下新建 applicationContext 文件，实现 Java 类的创建和 AOP 的织入，最后编写测试类。 通过自定义类来实现：这种方式比较推荐。你可以自定义切入类，然后在 Spring 中配置，最后编写测试类。 使用注解实现：你可以自定义增强类（注解实现），然后在 Spring 配置文件中，注册 Bean，并增加支持注解的配置，最后编写测试类。 使用 JDK 提供的代理方式：这种方式不依赖于 Spring。你可以使用 JDK 提供的代理方式来实现 AOP，包括静态和动态两种方式。 使用 Spring 纯配置实现：你可以通过 Spring 的配置文件来实现 AOP。 使用 Spring 注解：你可以通过 Spring 的注解来实现 AOP。 动态代理和字节码增强：Spring AOP 的实现主要基于动态代理和字节码增强两种技术。动态代理是一种在运行时生成代理对象的技术，在代理对象中可以添加额外的逻辑，比如切面逻辑。Spring AOP 通过 JDK 动态代理和 CGLIB 动态代理两种方式实现代理对象的生成。 1.8 Spring 框架中用到了哪些设计模式？ （1）单例模式（Singleton）：Spring 中的 Bean 默认都是单例的，这就是单例模式的应用。例如： 在这个例子中，myService Bean 在整个应用中只有一个实例。在单例模式中，Spring容器会确保每个由 @Bean 注解定义的 Bean 在整个应用中只有一个实例。 （2）工厂模式（Factory）：Spring 使用工厂模式通过 BeanFactory、ApplicationContext 来创建 Bean。同单例模式的示例代码所示，在这个例子中，AppConfig 类就像一个工厂，myService() 方法就是工厂方法，用来创建 MyService 的实例。Spring 的 @Bean 注解同时实现了这两种模式。 （3）模板方法模式（Template Method）：Spring 的 JdbcTemplate 和 HibernateTemplate 等都是模板方法模式的应用。例如： （4）代理模式（Proxy）：Spring AOP 就是通过代理模式实现的。例如： 在这个例子中，LoggingAspect 创建了一个代理，它在 MyService 的所有方法执行前打印日志。 （5）观察者模式（Observer）：Spring 事件处理就是观察者模式的一个例子。当一个事件被发布时，所有注册的监听器都会收到通知。例如： （6）包装器设计模式：我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。 （7）适配器模式：Spring AOP 的增强或通知（Advice）使用到了适配器模式、spring MVC 中也是用到了适配器模式适配 Controller。 1.9 Spring 提供了哪些配置方式？ Spring 提供了以下三种主要的配置方式： 基于 XML 的配置：在 Spring1.x 时代，都是基于 XML 来进行配置，用 XML 文件来管理 Bean 之间的关系。例如，你可以在 XML 文件中定义一个 Bean，然后在需要的地方引用这个 Bean。 基于注解的配置：Spring2.5 以后开始支持用注解的方式来配置依赖注入。可以用注解的方式来替代 XML 方式的 Bean 描述，可以将 Bean 描述转移到组件类的内部，只需要在相关类上、方法上或者字段声明上使用注解即可。例如，你可以使用 @Component 或其子类（@Repository、@Service、@Controller）来定义 Bean。 基于 Java API 的配置：Spring3.0 以后，提供了 Java 配置的能力，Spring4.x和SpringBoot都推荐使用Java配置2。例如，你可以使用@Configuration和@Bean注解来定义和配置bean12。 注意 Spring 框架默认并不启用注解配置方式，你需要在配置文件中添加相应的配置才能启用注解配置方式： &lt;context:annotation-config/&gt; 是 Spring 框架中的一个 XML 配置元素，用于启用注解驱动的 Spring 容器。它会自动扫描 Spring 容器中的所有组件，包括 @Service、@Repository、@Controller、@Component 等注解标注的类，并将它们注册到 Spring 容器中。 也可以使用 &lt;context:component-scan/&gt;，如果使用了 &lt;context:component-scan/&gt;，那么 &lt;context:annotation-config/&gt; 就不再需要，因为 &lt;context:component-scan/&gt; 除了具有 &lt;context:annotation-config/&gt; 的功能之外，还可以在指定的 Package 下扫描以及注册 Java Bean。 1.10 Spring 中的 Bean 是什么？ 在 Spring 中，构成应用程序主干并由 Spring IoC 容器管理的对象称为 Bean。Bean 是一个由 Spring IoC 容器实例化、组装和管理的对象，即 Spring 容器中存储的主要就是 Bean 对象。简而言之，Spring Bean 是 Spring 框架在运行时管理的对象，是任何 Spring 应用程序的基本构建块，我们编写的大多数应用程序逻辑代码都将放在 Spring Bean 中。Spring Bean 的管理包括：创建一个对象，提供依赖项（例如其他 Bean，配置属性），拦截对象方法调用以提供额外的框架功能，销毁一个对象。 1.11 Bean 的作用域有哪些？ singleton：单例模式，在整个 Spring IoC 容器中，使用 singleton 定义的 Bean 将只有一个实例。这是 Spring 的默认作用域。 prototype：原型模式，每次通过容器的 getBean() 方法获取 prototype 定义的 Bean 时，都将产生一个新的 Bean 实例。 request：对于每次 HTTP 请求，使用 request 定义的 Bean 都将产生一个新实例。只有在 Web 应用中使用 Spring 时，该作用域才有效。 session：对于每次 HTTP Session，使用 session 定义的 Bean 都将产生一个新实例。同样只有在 Web 应用中使用 Spring 时，该作用域才有效。 application：限定一个 Bean 的作用域为 ServletContext 的生命周期。该作用域仅适用于 Web 的 Spring WebApplicationContext 环境。 globalSession：全局 Session 作用域，仅在基于 portlet 的 Web 应用中才有意义，Spring5 已经没有了。 在 Spring 配置文件中，可以通过标签的 scope 属性来指定 Bean 的作用域。例如： 在 Spring Boot 或基于 Java 的配置中，可以通过 @Scope 注解来指定 Bean 的作用域。例如： 1.12 Bean 的生命周期有几个阶段？ 实例化（Instantiation）：Spring 根据配置文件或注解等方式创建 Bean 的实例。 属性赋值（Populate）：Spring 将实例化后的 Bean 的属性值设置到对应的属性中。 初始化（Initialization）：如果 Bean 实现了 InitializingBean 接口或在配置文件中通过 init-method 指定了初始化方法，则在 Bean 初始化完成后调用该方法。 销毁（Destruction）：如果 Bean 实现了 DisposableBean 接口或在配置文件中通过 destroy-method 指定了销毁方法，则在容器关闭时会调用该方法。 1.13 详细讲一下 Bean 的生命周期是什么？ Spring Bean 的生命周期是指一个 Bean 从被创建、初始化、使用到最终被销毁的整个过程。Spring 容器（ApplicationContext 或 BeanFactory）负责管理这个生命周期。 Spring Bean 生命周期的详细阶段如下： Bean 定义加载与解析：Spring 容器启动时（如 ApplicationContext.refresh()），读取配置文件，将配置信息解析为内部的 BeanDefinition 对象，存储在 BeanFactory 的注册表中，BeanDefinition 包含了创建 Bean 所需的所有元数据（类名、作用域、初始化/销毁方法名、属性值、构造函数参数等）。 实例化（Instantiation）：容器根据 BeanDefinition 的信息，创建 Bean 的实例。通常通过反射调用构造函数（Class.newInstance() 或 Constructor.newInstance()）来完成。对于工厂方法创建的 Bean，则调用指定的静态工厂方法或实例工厂方法。此时 Bean 只是一个“空壳”，属性未被设置，依赖未被注入。 属性赋值/依赖注入（Population of Properties）：容器根据 BeanDefinition 中的配置，为 Bean 实例设置属性值，包括注入其他 Bean 的引用（@Autowired、@Resource、@Inject），注入基本类型或 String 等值，注入集合（List、Set、Map、Properties），解析和注入 @Autowired 标注的字段、Setter 方法或构造函数参数。 Bean 后置处理器（BeanPostProcessor）- 初始化前（postProcessBeforeInitialization）：如果容器中注册了实现了 BeanPostProcessor 接口的 Bean，容器会在每个 Bean 初始化之前调用其 postProcessBeforeInitialization(Object bean, String beanName) 方法，作用是在 Bean 初始化之前对 Bean 实例进行修改或包装（例如生成代理对象，如 AOP 代理通常在此阶段生成）。 初始化（Initialization）：经过前置处理后，Bean 进入初始化阶段，容器会调用 Bean 上定义的初始化回调方法，作用是执行 Bean 创建后、使用前的自定义初始化逻辑，例如：建立数据库连接池、加载配置文件、验证依赖注入是否完整、启动后台线程、执行复杂的数据结构初始化。这些方法按固定顺序执行： @PostConstruct 注解方法：由 JSR-250 规范定义。这是最常用、最推荐的方式。方法上标注 @PostConstruct。 InitializingBean.afterPropertiesSet()：Spring 特定接口。Bean 实现 InitializingBean 接口并覆写 afterPropertiesSet() 方法。不推荐使用，因为它将代码与 Spring 接口耦合。 自定义 init-method：在配置中指定的方法（XML 的 init-method 属性，Java Config 的 @Bean(initMethod = &quot;myInit&quot;)）。方法签名通常是 void xxx() 无参方法。 Bean 后置处理器（BeanPostProcessor）- 初始化后（postProcessAfterInitialization）：在 Bean 执行完自身的初始化方法后，容器会调用所有 BeanPostProcessor 的 postProcessAfterInitialization(Object bean, String beanName) 方法，作用是在 Bean 初始化之后对 Bean 实例进行最终的修改或增强，很多 Spring 的高级功能（如 AOP 的最终代理包装）在此阶段完成。 Bean 就绪（Ready for Use）：此时，Bean 已经完成了创建、依赖注入、初始化和所有后处理，完全初始化完毕，驻留在 Spring 容器（通常是单例池 singletonObjects）中，应用程序可以通过 getBean() 方法或依赖注入的方式获取并使用这个 Bean。 销毁（Destruction）：当容器关闭时（例如调用 ApplicationContext.close() 或在 Web 应用中容器关闭），对于作用域为 singleton 且实现了销毁回调的 Bean，容器会触发销毁过程，作用是执行 Bean 销毁前的自定义清理逻辑，例如：关闭数据库连接池，释放连接、停止后台线程、保存状态到文件、释放文件句柄以及网络连接等资源。销毁回调的执行顺序与初始化相反： @PreDestroy 注解方法：JSR-250 规范定义。最常用、最推荐。方法上标注 @PreDestroy。 DisposableBean.destroy()：Spring 特定接口。Bean 实现 DisposableBean 并覆写 destroy() 方法。不推荐使用（耦合）。 自定义 destroy-method：配置中指定的方法（XML 的 destroy-method 属性，Java Config 的 @Bean(destroyMethod = &quot;myDestroy&quot;)）。方法签名通常是 void xxx() 无参方法。 1.14 Bean 加载/销毁前后，如果想实现某些逻辑，可以怎么做？ 在 Spring 框架中，如果希望在 Bean 加载（即实例化、属性赋值、初始化等过程完成后）或销毁前后执行某些逻辑，可以使用 Spring 的生命周期回调接口或注解。这些接口和注解允许你定义在 Bean 生命周期的关键点执行的代码。 （1）使用 init-method 和 destroy-method： 在 XML 配置中，你可以通过 init-method 和 destroy-method 属性来指定 Bean 初始化后和销毁前需要调用的方法： 然后，在你的 Bean 类中实现这些方法： （2）实现 InitializingBean 和 DisposableBean 接口： Bean 类可以实现 org.springframework.beans.factory.InitializingBean 和 org.springframework.beans.factory.DisposableBean 接口，并分别实现 afterPropertiesSet() 和 destroy() 方法： （3）使用 @PostConstruct 和 @PreDestroy 注解： （4）使用 @Bean 注解的 initMethod 和 destroyMethod 属性： 1.15 BeanFactory 和 ApplicationContext 有什么区别？ BeanFactory 和 ApplicationContext 都是 Spring 框架中用于创建和管理 Bean 的容器。 BeanFactory 是 Spring 框架的最基本的容器，它是 Spring 的核心部分。BeanFactory 通过一个配置文件来管理和创建 Bean。BeanFactory 中的 Bean 是懒加载的，也就是说只有在调用 getBean() 方法去请求某个 Bean 时才会创建实例，这样可以提高程序的性能和启动速度，帮助我们节省资源。 ApplicationContext 是 BeanFactory 的子接口，它是一个更加强大的容器。ApplicationContext 可以像 BeanFactory 一样创建和管理 Bean，但是它还可以提供其他的功能，比如支持国际化、事件传播、资源加载等。ApplicationContext 是在程序启动时就将所有的 Bean 全部实例化，因此在程序运行时可以直接获取已经创建好的 Bean，从而提高了程序的响应速度。 1.16 什么是 Spring 事务管理模型？ Spring 事务是 Spring 框架提供的一个核心功能，旨在简化在 Java 应用中管理数据库事务（以及扩展到其他支持事务的资源）的复杂性。它提供了一套声明式和编程式的事务管理模型，将事务管理逻辑从业务代码中分离出来，使开发者能更专注于核心业务逻辑。 事务（Transaction）是指一组数据库操作（如多个 SQL 语句）被视为一个单一的工作单元。这些操作要么全部成功提交（Commit），要么全部失败回滚（Rollback），这是保证数据一致性和完整性的关键机制。其具有 ACID 特性： A（原子性，Atomicity）：事务是不可分割的最小单元，要么全部执行，要么全部不执行。 C（一致性，Consistency）：事务将数据库从一个一致状态转换到另一个一致状态。 I（隔离性，Isolation）：并发执行的事务之间应该相互隔离，防止互相干扰。 D（持久性，Durability）：一旦事务提交，它对数据库的改变就是永久性的，即使系统故障也不会丢失。 Spring 提供两种主要的事务管理方式： 编程式事务管理（Programmatic Transaction Management）：开发者直接在代码中通过 Spring 提供的 API 显式控制事务的边界（开始、提交、回滚）。优点为精细控制，事务边界清晰可见，缺点为事务管理代码侵入业务代码，代码重复度高，维护性稍差，适用于需要非常精细控制事务边界，或者声明式事务无法满足特定需求的情况。实现方式如下： 使用 TransactionTemplate：这是最常用的编程方式。它封装了事务管理的样板代码（try-catch-finally），开发者只需在 execute() 方法内部实现需要事务的业务逻辑（通过 TransactionCallback 或 Lambda 表达式）。 使用 PlatformTransactionManager：直接调用 getTransaction()、commit()、rollback() 方法，更底层，更灵活，但也更繁琐。 声明式事务管理（Declarative Transaction Management）：这是 Spring 推荐且最常用的方式，开发者通过配置（注解或 XML）来声明哪些方法（或类）需要事务支持以及事务的属性（传播行为、隔离级别等）。事务管理的具体操作（开启、提交、回滚）由 Spring 框架在运行时（通常基于 AOP 代理）自动完成。其具有非侵入性、事务管理与业务逻辑分离、配置灵活、易于维护、减少样板代码等优点，适用于绝大多数需要事务管理的情况，实现方式如下： 基于 @Transactional 注解：将注解标注在方法或类上，标注在类上表示该类的所有 public 方法都应用该事务属性，方法上的注解会覆盖类上的注解，可以在注解中指定 propagation、isolation、timeout、readOnly、rollbackFor、noRollbackFor 等属性。需要注意的是 @Transactional 生效依赖于 AOP 代理。这意味着调用必须是通过代理对象进行的（从 Spring 容器中获取的 Bean 已经是代理对象）。 基于 XML 配置（&lt;tx:advice&gt; 和 &lt;aop:config&gt;）：在 XML 文件中定义事务通知（&lt;tx:advice&gt;）并配置其属性（传播行为等），使用 AOP 切面配置（&lt;aop:config&gt;）将事务通知织入到指定的方法（通常通过切入点表达式 pointcut）。 1.17 Spring 的事务什么情况下会失效？ Spring Boot 通过 Spring 框架的事务管理模块来支持事务操作。事务管理在 Spring Boot 中通常是通过 @Transactional 注解来实现的。事务可能会失效的一些常见情况包括： 未捕获异常：如果一个事务方法中发生了未捕获的异常，并且异常未被处理或传播到事务边界之外，那么事务会失效，所有的数据库操作会回滚。 非受检异常：Spring 默认只在抛出 RuntimeException 及其子类或 Error 时回滚事务。如果方法抛出的是检查型异常（Checked Exception，如 IOException、SQLException）或其自定义非运行时异常，事务默认会提交！ 事务传播属性设置不当：如果在多个事务之间存在事务嵌套，且事务传播属性配置不正确，可能导致事务失效。特别是在方法内部调用有 @Transactional 注解的方法时要特别注意。 多数据源的事务管理：如果在使用多数据源时，事务管理没有正确配置或者存在多个 @Transactional 注解时，可能会导致事务失效。 跨方法调用事务问题：如果一个事务方法内部调用另一个方法，而这个被调用的方法没有 @Transactional 注解，这种情况下外层事务可能会失效。 事务在非公开方法中失效：Spring 的事务管理（基于 AOP 代理或 AspectJ）默认只对 public 方法生效，如果 @Transactional 注解标注在私有方法上或者非 public 方法上，事务也会失效。 1.18 Spring 的事务，使用 this 调用是否生效？ 在 Spring 中使用 this 关键字调用同一个类中的 @Transactional 方法，事务是不会生效的，这是 Spring 事务失效最常见的原因之一，被称为自调用问题。 Spring 的声明式事务管理 @Transactional 是通过 AOP（面向切面编程）实现的。当你在一个 Bean 上标注 @Transactional 时，Spring 容器会为该 Bean 创建一个代理对象（JDK 动态代理或 CGLIB 代理）。当外部代码（其他 Bean 的代码）调用这个 Bean 的事务方法时，实际上是调用了代理对象的方法。代理对象在调用目标方法（你写的原始方法）之前和之后，会插入事务管理的逻辑（开启事务、提交/回滚事务）。 当你在同一个 Bean 的一个非事务方法 A() 中，使用 this.事务方法B() 来调用本 Bean 的事务方法 B() 时，A() 在被调用时，确实是代理对象在拦截执行（如果 A() 本身有事务，事务会生效）。但在 A() 方法体内部，this 关键字指向的是目标对象本身（即你写的原始 Bean 实例），而不是 Spring 创建的代理对象。所以 this.B() 这个调用是直接发生在目标对象内部，完全绕过了代理对象。代理对象的事务拦截逻辑（TransactionInterceptor）根本没有机会介入到 B() 的调用过程中。 如何解决自调用导致的事务失效呢？首选且推荐的最佳实践是将事务方法抽取到另一个独立的 Spring Bean 中，然后通过依赖注入进行调用。将需要事务管理的方法 B() 移动到另一个 Service 类中（例如 InventoryService），在原来的 Service（OrderService）中注入这个新的 InventoryService，然后在 A() 方法中调用 inventoryService.deductInventory(...)： 因为 inventoryService 是另一个 Spring Bean，对它方法的调用必然通过其代理对象，事务拦截器就能正常工作。 2. Spring MVC 2.1 什么是 Spring MVC？工作原理是什么？ Spring MVC 是 Spring 框架的一部分，它是一个基于 Java 的全功能 MVC Web 应用程序框架。MVC（Model-View-Controller）代表模型-视图-控制器，这是一种设计模式，用于将应用程序的数据访问、用户界面和业务逻辑分离开来。MVC 具体介绍如下： 视图（View）：为用户提供使用界面，与用户直接进行交互。 模型（Model）：代表一个存取数据的对象或 Java POJO（Plain Old Java Object，简单 Java 对象）。它也可以带有逻辑，主要用于承载数据，并对用户提交请求进行计算的模块。模型分为两类，一类称为数据承载 Bean，一类称为业务处理 Bean。所谓数据承载 Bean 是指实体类（如 User 类），专门为用户承载业务数据的；而业务处理 Bean 则是指 Service 或 Dao 对象，专门用于处理用户提交请求的。 控制器（controller）：用于将用户请求转发给相应的 Model 进行处理，并根据 Model 的计算结果向用户提供相应响应，它使视图与模型分离。 Spring MVC 的工作原理如下： 用户发送请求：用户通过浏览器发送一个 HTTP 请求，直接请求到 DispatcherServlet。 DispatcherServlet（前端控制器）：请求被 Spring MVC 的 DispatcherServlet 捕获。DispatcherServlet 的作用类似于一个中央处理器，它会负责调用其他组件来处理请求。 HandlerMapping（控制器映射器）：DispatcherServlet 会调用 HandlerMapping 解析请求对应的 Handler，即找出处理这个请求的 Controller。 Controller（控制器）：找到合适的 Controller 后，DispatcherServlet 会将请求交给它。Controller 是真正处理请求的地方，它会处理用户的请求，并返回一个 ModelAndView 对象。ModelAndView 包含了模型（Model）数据和视图（View）名称。 ViewResolver（视图解析器）：DispatcherServlet 会把 ModelAndView 对象传给 ViewResolver。ViewResolver 会根据视图名称解析出真正的视图。 View（视图）：最后，DispatcherServlet 会渲染视图，并把模型数据填充进去。这个视图就是最终呈现给用户的页面。 2.2 介绍一下 Spring MVC 的核心组件 Spring MVC 的核心组件主要包括以下几个： DispatcherServlet（前端控制器）：这是 Spring MVC 框架的核心，负责将请求路由到其他组件。它处理所有的 HTTP 请求和响应。 HandlerMapping（控制器映射器）：它的任务是根据请求的 URL 找到正确的 Controller。 Controller（控制器）：这是应用程序的实际控制器，负责处理用户请求并返回一个模型和视图。 HandlerAdapter（控制器适配器）：它负责调用 Controller 中的方法。 ViewResolver（视图解析器）：它负责解析视图名并返回一个具体的视图对象。 View（视图）：这是最终呈现给用户的页面。 除此之外，Spring MVC 还有一些其他的组件： HandlerExceptionResolver（处理器异常解析器）：它负责处理在 Controller 执行过程中抛出的异常。 LocaleResolver（区域解析器）：它用于确定用户的区域，这对于国际化和本地化非常重要。 MultipartResolver（多部分解析器）：它用于处理 multipart 请求，例如文件上传。 ThemeResolver（主题解析器）：它用于确定应用程序的主题，这对于个性化布局非常有用。 RequestToViewNameTranslator（请求到视图名转换器）：它用于在 Controller 没有明确返回视图名时，提供一个默认的视图名。 FlashMapManager（Flash 映射管理器）：它用于存储和检索 FlashMap 模型，FlashMap 模型用于在重定向场景中存储属性。 2.3 HandlerMapping 和 HandlerAdapter 有了解吗？ （1）HandlerMapping 作用：HandlerMapping 负责将请求映射到控制器（Controller）。 功能：根据请求的 URL、请求参数等信息，找到处理请求的控制器。 类型：Spring 提供了多种 HandlerMapping 实现，如 BeanNameUrlHandlerMapping、 RequestMappingHandlerMapping 等。 工作流程：根据请求信息确定要请求的控制器。HandlerMapping 可以根据 URL、请求参数等规则确定对应的控制器。 （2）HandlerAdapter 作用：HandlerAdapter 负责调用控制器来处理请求。 功能：控制器可能有不同的接口类型（Controller 接口、HttpRequestHandler 接口等），HandlerAdapter 根据接口类型来选择合适的方法调用控制器。 类型：Spring 提供了多个 HandlerAdapter 实现，用于适配不同类型的控制器。 工作流程：根据控制器的接口类型，选择相应的 HandlerAdapter 来调用控制器。 （3）工作流程 当客户端发送请求时，HandlerMapping 根据请求信息找到对应的控制器。 HandlerAdapter 根据控制器接口的类型选择合适的方法来调用控制器。 控制器执行相应的业务逻辑，生成 ModelAndView。 HandlerAdapter 将控制器的执行结果包装成 ModelAndView。 视图解析器根据 ModelAndView 找到对应的视图（View）进行渲染。 将渲染后的视图返回给客户端。 HandlerMapping 和 HandlerAdapter 协同工作，通过将请求映射到控制器，并调用控制器来处理请求，实现了请求处理的流程。它们的灵活性使得在 Spring MVC 中可以支持多种处理器和处理方式，提高了框架的扩展性和适应性。 3. Spring 注解 3.1 Spring 中常用的注解有哪些？ @Component：标注一个普通的 Spring Bean 类，当一个类被 @Component 注解标记时，Spring 会将其实例化为一个 Bean，并将其添加到 Spring 容器中。 @Configuration：用于标记一个类作为 Spring 的配置类。配置类可以包含 @Bean 注解的方法，用于定义和配置 Bean，作为全局配置： @Controller：标注一个控制器组件类，它是 @Component 注解的特例，用于标记控制层的 Bean。这是 MVC 结构的另一个部分，加在控制层： @Bean：用于标记一个方法作为 Spring 的 Bean 工厂方法。当一个方法被 @Bean 注解标记时，Spring 会将该方法的返回值作为一个 Bean，并将其添加到 Spring 容器中，如果自定义配置，经常用到这个注解。 @Service：标注一个业务逻辑层组件类，它也是 @Component 注解的特例，用于标记服务层的 Bean，一般标记在业务 Service 的实现类： @Repository：标注一个数据访问层组件类，它也是 @Component 注解的特例，用于标记数据访问层的 Bean。这个注解很容易被忽略，导致数据库无法访问。 @Autowired：由 Spring 提供的注解，用于自动装配 Bean，当 Spring 容器中存在与要注入的属性类型匹配的 Bean 时，它会自动将 Bean 注入到属性中。就跟我们 new 对象一样： @RequestMapping：用于映射 Web 请求，包括访问路径和参数。 @ResponseBody：支持将返回值放在 response 内，而不是一个页面，通常用户返回 JSON 数据。 @RestController：该注解为一个组合注解，相当于 @Controller 和 @ResponseBody 的组合，注解在类上，意味着，该 Controller 的所有方法都默认加上了 @ResponseBody。 @ExceptionHandler：用于全局处理控制器里的异常。 @PathVariable：用于接收路径参数，比如 @RequestMapping(&quot;/hello/&#123;name&#125;&quot;) 申明的路径，将注解放在参数中前，即可获取该路径的 name 值，通常作为 RESTful 的接口实现方法。 @EnableAsync：在配置类中，通过此注解开启对异步任务的支持。 @Async：在实际执行的 Bean 方法使用该注解来申明其是一个异步任务。 @EnableScheduling：在配置类上使用，开启计划任务的支持。 @Scheduled：来申明这是一个任务，包括 cron、fixDelay、fixRate 等类型。 3.2 @Controller 和 @RestController 有什么区别？ （1）@Controller：@Controller 注解表示该类是一个 Web 控制器，通常与 @RequestMapping 注解一起使用，用于处理 HTTP 请求。在 @Controller 中，我们可以返回一个视图（View），这在 Spring Web MVC 中非常常见。例如： 在这个例子中，BookController 类被标记为一个控制器，/books 是它的请求映射路径。getBook() 方法用于处理对 /books/&#123;id&#125; 路径的 GET 请求，其中 &#123;id&#125; 是路径变量。 （2）@RestController：@RestController 是 @Controller 的特化，它包含了 @Controller 和 @ResponseBody 两个注解。这意味着，当一个类被 @RestController 注解标记后，该类的所有方法都会默认添加 @ResponseBody 注解。因此通常用于创建 RESTful Web 服务。例如： 在这个例子中，BookRestController 类被 @RestController 注解标记，因此不需要再每个请求处理方法上都添加 @ResponseBody 注解。 总的来说二者的主要区别在于，@Controller 通常用于处理返回视图的请求，而 @RestController 通常用于处理返回 JSON 或 XML 响应的请求。 3.3 @GetMapping、@PostMapping 和 @RequestMapping 有什么区别？ （1）@RequestMapping：这是一个通用的注解，可以处理所有类型的 HTTP 请求。你可以通过 method 属性来指定处理的 HTTP 方法类型（如 GET、POST 等）。例如： （2）@GetMapping：这是 @RequestMapping 的一个特化版本，用于处理 GET 请求，等价于 @RequestMapping(method = RequestMethod.GET)。例如： （3）@PostMapping：同样是 @RequestMapping 的一个特化版本，用于处理 POST 请求，等价于 @RequestMapping(method = RequestMethod.POST)。例如： 3.4 @RequestParam 和 @PathVariable 有什么区别？ （1）@RequestParam：用于从请求参数中提取值。例如，对于 URL：，你可以使用 @RequestParam 来获取 id 参数的值： （2）@PathVariable：用于从 URI 路径中提取值。例如，对于 URL：，你可以使用 @PathVariable 来获取 id： 3.5 详细讲一下 @Autowired 有什么用？ 在 Spring 框架中，@Autowired 注解用于实现自动依赖注入。这意味着你不需要在代码中明确指定依赖关系，Spring 会自动为你完成这个工作，从而简化了代码并提高了可维护性。 @Autowired 注解可以应用于字段、构造器和方法： （1）字段上的 @Autowired：当 @Autowired 注解应用于字段时，Spring 会在创建 Bean 时自动注入相应的依赖。在下面这个例子中，myService 字段会被自动注入一个 MyService 类型的 Bean： （2）构造器上的 @Autowired：当 @Autowired 注解应用于构造器时，Spring 会在创建 Bean 时自动注入构造器的参数。在下面这个例子中，MyClass 的构造器会被自动注入一个 MyService 类型的 Bean： （3）方法上的 @Autowired：当 @Autowired 注解应用于方法时，Spring 会在创建 Bean 时自动注入方法的参数。这通常用于 Setter 方法，但也可以用于其他任何方法。例如： 4. Spring Boot 4.1 为什么要用 Spring Boot？ Spring Boot 是 Spring 框架的一个扩展，它的目标是简化 Spring 应用程序的配置和部署，Spring Boot 具有以下优势： 快速开发：Spring Boot 通过提供一系列的开箱即用的组件和自动配置，简化了项目的配置和开发过程，开发人员可以更专注于业务逻辑的实现，而不需要花费过多时间在繁琐的配置上。例如，使用 Spring MVC 需要大量的 XML Bean 定义和自定义 servlet 类，但使用 Spring Boot 只需要添加一个 starter 依赖即可，无需任何代码生成或 XML 配置。 快速启动：Spring Boot 提供了快速的应用程序启动方式，可通过内嵌的 Tomcat、Jetty 或 Undertow 等容器快速启动应用程序，无需额外的部署步骤，方便快捷。 自动化配置：Spring Boot 通过自动配置功能，根据项目中的依赖关系和约定俗成的规则来配置应用程序，减少了配置的复杂性，使开发者更容易实现应用的最佳实践。 有用的 Starters：Spring Boot Starters 是包含库和一些自动配置的 Maven 描述符。这些 Starters 可以为 Spring Boot 应用程序提供功能。例如，你想设置数据库连接，或者你想与消息队列进行通信或发送电子邮件，Spring Boot 都可以覆盖。 嵌入式 Web 服务器：Spring Boot 提供了对嵌入式 Tomcat、Jetty 和 Undertow 服务器的开箱即用支持。这样，开发人员不必担心在传统的应用服务器中部署 Web 应用程序。 丰富的 IDE 支持：所有主要的 IDE 都为 Spring Boot 提供了代码辅助支持。 生产就绪的功能：Spring Boot 提供了开箱即用的监控、度量和日志记录功能。这些功能使开发人员可以避免额外的配置。 4.2 Spring Boot 比 Spring 好在哪里？ Spring Boot 提供了自动化配置，大大简化了项目的配置过程。通过约定优于配置的原则，很多常用的配置可以自动完成，开发者可以专注于业务逻辑的实现。 Spring Boot 提供了快速的项目启动器，通过引入不同的 Starter，可以快速集成常用的框架和库（如数据库、消息队列、Web 开发等），极大地提高了开发效率。 Spring Boot 默认集成了多种内嵌服务器（如 Tomcat、Jetty、Undertow），无需额外配置，即可将应用打包成可执行的 JAR 文件，方便部署和运行。 4.3 Spring Boot 用到哪些设计模式？ 代理模式：Spring 的 AOP 通过动态代理实现方法级别的切面增强，有静态和动态两种代理方式，采用动态代理方式。 策略模式：Spring AOP 支持 JDK 和 CGLIB 两种动态代理实现方式，通过策略接口和不同策略类，运行时动态选择，其创建一般通过工厂方法实现。 装饰器模式：Spring 用 TransactionAwareCacheDecorator 解决缓存与数据库事务问题增加对事务的支持。 单例模式：Spring Bean 默认是单例模式，通过单例注册表（如 HashMap）实现。 简单工厂模式：Spring 中的 BeanFactory 是简单工厂模式的体现，通过工厂类方法获取 Bean 实例。 工厂方法模式：Spring 中的 FactoryBean 体现工厂方法模式，为不同产品提供不同工厂。 观察者模式：Spring 观察者模式包含 Event 事件、Listener 监听者、Publisher 发送者，通过定义事件、监听器和发送者实现，观察者注册在 ApplicationContext 中，消息发送由 ApplicationEventMulticaster 完成。 模板模式：Spring Bean 的创建过程涉及模板模式，体现扩展性，类似 Callback 回调实现方式。 适配器模式：Spring MVC 中针对不同方式定义的 Controller，利用适配器模式统一函数定义，定义了统一接口 HandlerAdapter 及对应适配器类。 4.4 怎么理解 Spring Boot 中的约定优于配置？ 约定优于配置是 Spring Boot 的核心设计理念，它通过预设合理的默认行为和项目规范，大幅减少开发者需要手动配置的步骤，从而提升开发效率和项目标准化程度。 理解 Spring Boot 中的“约定优于配置”原则，可以从以下几个方面来解释： 自动化配置：Spring Boot 提供了大量的自动化配置，通过分析项目的依赖和环境，自动配置应用程序的行为。开发者无需显式地配置每个细节，大部分常用的配置都已经预设好了。例如，引入 spring-boot-starter-web 后，Spring Boot 会自动配置内嵌 Tomcat 和 Spring MVC，无需手动编写 XML。 默认配置：Spring Boot 为诸多方面提供大量默认配置，如连接数据库、设置 Web 服务器、处理日志等。开发人员无需手动配置这些常见内容，框架已做好决策。例如，默认的日志配置可让应用程序快速输出日志信息，无需开发者额外繁琐配置日志级别、输出格式与位置等。 约定的项目结构：Spring Boot 提倡特定项目结构，通常主应用程序类（含 Main 方法）置于根包，控制器类、服务类、数据访问类等分别放在相应子包，如 com.example.demo.controller 放控制器类，com.example.demo.service 放服务类等。此约定使团队成员更易理解项目结构与组织，新成员加入项目时能快速定位各功能代码位置，提升协作效率。 4.5 Spring Boot 的项目结构是怎么样的？ 开放接口层：定义系统对外暴露的协议（HTTP/RPC）和 API 规范，Controller 是其具体实现载体。 终端显示层（半对应 Controller）：处理前端交互（渲染 HTML/JSON），在前后端分离架构中弱化此层。当前主要是 velocity 渲染、JS 渲染、JSP 渲染、移动端展示等。 Web 层（直接对应 Controller）：处理 HTTP 请求路由、参数校验、返回响应，主要是对访问控制进行转发，各类基本参数校验，或者不复用的业务简单处理等。 Service 层（直接对应 Service）：业务逻辑层，实现核心业务逻辑，一般还会分为 Service 接口层和 Service 实现层，用面向接口的编程思想，为后续功能的解耦和扩展留下余地。 Manager 层：通用业务处理层，它有如下特征： 对第三方平台封装的层，预处理返回结果及转化异常信息，适配上层接口。 对 Service 层通用能力的下沉，如缓存方案、中间件通用处理。 与 DAO 层交互，对多个 DAO 的组合复用。 DAO 层（直接对应 Mapper）：数据访问（持久）层，与底层 MySQL、Oracle、Hbase、OceanBase 等进行数据交互。 Pojo 层（直接对应 Model）：数据载体层，Entity 对应纯数据库映射对象（与 DB 强耦合），DTO/VO 用于传输，DTO 对应业务数据传输对象（包含业务字段），VO 对应前端展示对象（包含 UI 特定字段）。 第三方服务：包括其它部门 RPC 服务接口，基础平台，其它公司的 HTTP 接口，如淘宝开放平台、支付宝付款服务、高德地图服务等，通常作为 Service 的依赖组件。 外部接口：外部（应用）数据存储服务提供的接口，多见于数据迁移场景中。 4.6 Spring Boot 自动装配原理是什么？ Spring Boot 的自动装配原理是基于 Spring Framework 的条件化配置和 @EnableAutoConfiguration 注解实现的。这种机制允许开发者在项目中引入相关的依赖，Spring Boot 将根据这些依赖自动配置应用程序的上下文和功能。 Spring Boot 定义了一套接口规范，这套规范规定：Spring Boot 在启动时会扫描类路径下以及外部引用 Jar 包中的所有 META-INF/spring.factories 文件（Spring Boot 2.x）或 META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports 文件（Spring Boot 3.x+），将文件中配置的类型信息加载到 Spring 容器（此处涉及到 JVM 类加载机制与 Spring 的容器知识），并执行类中定义的各种操作。对于外部 Jar 来说，只需要按照 Spring Boot 定义的标准，就能将自己的功能装置进 Spring Boot。 通俗来讲，自动装配就是通过注解或一些简单的配置就可以在 Spring Boot 的帮助下开启和配置各种功能，比如数据库访问、Web 开发。 点进 @SpringBootApplication 注解的内部可以看到： 这些注解的作用如下： @Target(&#123;ElementType.TYPE&#125;)：该注解指定了这个注解可以用来标记在类上。在这个特定的例子中，这表示该注解用于标记配置类。 @Retention(RetentionPolicy.RUNTIME)：这个注解指定了注解的生命周期，即在运行时保留。这是因为 Spring Boot 在运行时扫描类路径上的注解来实现自动配置，所以这里使用了 RUNTIME 保留策略。 @Documented：该注解表示这个注解应该被包含在 Java 文档中。它是用于生成文档的标记，使开发者能够看到这个注解的相关信息。 @Inherited：这个注解指示一个被标注的类型是被继承的。在这个例子中，它表明这个注解可以被继承，如果一个类继承了带有这个注解的类，它也会继承这个注解。 @SpringBootConfiguration：这个注解表明这是一个 Spring Boot 配置类。如果点进这个注解内部会发现与标准的 @Configuration 没啥区别，只是为了表明这是一个专门用于 Spring Boot 的配置。 @EnableAutoConfiguration：这个注解是 Spring Boot 自动装配的核心。它告诉 Spring Boot 启用自动配置机制，根据项目的依赖和配置自动配置应用程序的上下文。通过这个注解，Spring Boot 将尝试根据类路径上的依赖自动配置应用程序。 @ComponentScan：这个注解用于配置组件扫描的规则。在这里，它告诉 Spring Boot 在指定的包及其子包中查找组件，这些组件包括被注解的类、@Component 注解的类等。其中的 excludeFilters 参数用于指定排除哪些组件，这里使用了两个自定义的过滤器，分别是 TypeExcludeFilter 和 AutoConfigurationExcludeFilter。 4.7 Spring Boot 中如何实现对不同环境的属性配置文件的支持？ 在 Spring Boot 中，你可以使用 Spring 的 Profile 功能来支持不同环境的属性配置文件。你可以为每个环境创建一个单独的配置文件，然后在运行应用程序时指定要使用的配置文件。 例如，假设你有开发环境（dev）、质量保证环境（qa）和生产环境（prod）。你可以在与 application.properties 文件相同的位置创建三个文件： application-dev.properties：用于开发环境。 application-qa.properties：用于质量保证环境。 application-prod.properties：用于生产环境。 然后，你只需要在 application.properties 文件中设置 spring.profiles.active 属性，来指定当前的环境。例如，如果你想使用质量保证环境，你可以设置 spring.profiles.active=qa。 此外，你还可以通过 JVM 参数来指定活动的配置文件。例如，你可以在启动应用程序时设置 -Dspring.profiles.active=dev。 4.8 如何理解 Spring Boot 中的 Starters？ 在 Spring Boot 中，Starters 可以理解为启动器，它包含了一系列可以集成到应用里面的依赖包。你可以一站式集成 Spring 及其他技术，快速地添加和管理项目的依赖，而不需要到处找示例代码和依赖包。 例如，如果你想使用 Spring JPA 访问数据库，只要加入 spring-boot-starter-data-jpa 启动器依赖就能使用了。同样，如果你想创建一个 RESTful 的 Web 应用，你可以添加 spring-boot-starter-web。 4.9 Spring Boot Starters 的工作原理是什么？ Spring Boot Starters 的工作原理主要包括以下几个步骤： 引入模块所需的相关 Jar 包：Spring Boot Starter 会将具备某种功能的 Jar 包打包到一起，可以简化依赖导入的过程。例如，我们导入 spring-boot-starter-web 这个 Starter，则和 Web 开发相关的 Jar 包都一起导入到项目中了。 自动配置各个模块所需的属性：Spring Boot 在启动时会去依赖的 Starter 包中寻找 resources/META-INF/spring.factories 文件，然后根据文件中配置的 Jar 包去扫描项目所依赖的 Jar 包。接着根据 spring.factories 配置加载 AutoConfigure 类。根据 @Conditional 注解的条件，进行自动配置并将 Bean 注入 Spring Context。 Bean 的发现和加载：Spring Boot 默认扫描启动类所在的包下的主类与子类的所有组件，但并没有包括依赖包中的类，那么依赖包中的 Bean 是如何被发现和加载的？Spring Boot 在启动类上我们一般会加入 @SpringBootApplication 注解，此注解的源码中的 @EnableAutoConfiguration 注解引入了 @Import 这个注解，该注解导入自动配置功能类 AutoConfigurationImportSelector，主要方法 getCandidateConfigurations() 使用了 SpringFactoriesLoader.loadFactoryNames() 方法加载 META-INF/spring.factories 的文件（spring.factories 声明具体自动配置）。 4.10 介绍几个 Starter？ spring-boot-starter-web：这是最常用的起步依赖之一，它包含了 Spring MVC 和 Tomcat 嵌入式服务器，用于快速构建 Web 应用程序。 spring-boot-starter-security：提供了 Spring Security 的基本配置，帮助开发者快速实现应用的安全 性，包括认证和授权功能。 mybatis-spring-boot-starter：这个 Starter 是由 MyBatis 团队提供的，用于简化在 Spring Boot 应用中集成 MyBatis 的过程。它自动配置了 MyBatis 的相关组件，包括 SqlSessionFactory、MapperScannerConfigurer 等，使得开发者能够快速地开始使用 MyBatis 进行数据库操作。 spring-boot-starter-data-jpa 或 spring-boot-starter-jdbc：如果使用的是 Java Persistence API（JPA）进行数据库操作，那么应该使用 spring-boot-starter-data-jpa。这个 Starter 包含了 Hibernate 等 JPA 实现以及数据库连接池等必要的库，可以让你轻松地与 MySQL 数据库进行交互。你需要在 application.properties 或 application.yml 中配置 MySQL 的连接信息。如果倾向于直接使用 JDBC 而不通过 JPA，那么可以使用 spring-boot-starter-jdbc，它提供了基本的 JDBC 支持。 spring-boot-starter-data-redis：用于集成 Redis 缓存和数据存储服务。这个 Starter 包含了与 Redis 交互所需的客户端（默认是 Jedis 客户端，也可以配置为 Lettuce 客户端），以及 Spring Data Redis 的支持，使得在 Spring Boot 应用中使用 Redis 变得非常便捷。同样地，需要在配置文件中设置 Redis 服务器的连接详情。 spring-boot-starter-test：包含了单元测试和集成测试所需的库，如 JUnit、Spring Test、AssertJ 等，便于进行测试驱动开发（TDD）。 4.11 Spring Boot 的核心注解是什么？主要由哪几个注解组成？ Spring Boot 的核心注解是 @SpringBootApplication。这个注解实际上是以下三个注解的组合： @SpringBootConfiguration：组合了 @Configuration 注解，实现配置文件的功能。 @EnableAutoConfiguration：打开自动配置的功能，也可以关闭某个自动配置的选项。如关闭数据源的自动配置功能：@SpringBootApplication(exclude = &#123; DataSourceAutoConfiguration.class &#125;)。 @ComponentScan：Spring 组件扫描功能，让 Spring Boot 扫描到 Configuration 类并把它加入到程序上下文。 4.12 Spring Boot 中如何使用 Bean？ 在 Spring Boot 中，你可以通过使用 @Bean 注解来声明一个 Bean。@Bean 注解告诉 Spring 一个方法会返回一个对象，这个对象应该被注册为 Spring 应用上下文中的 Bean。默认情况下，Bean 的名称是由方法名决定的，但你也可以在 @Bean 注解中通过 name 属性来设置 Bean 的名称。 例如，以下是一个简单的 @Bean 注解的使用示例： 在这个例子中，myBean() 方法被注解为 @Bean，所以它会返回一个新的 MyBean 实例，这个实例将被注册为 Spring 应用上下文中的 Bean。 总的来说，你不需要在 Spring Boot 中手动配置 Bean，你只需要使用 @Bean 注解，Spring Boot 就会自动为你创建和管理 Bean。 4.13 RESTful 是什么？ RESTful 是一种软件架构风格，它主要用于客户端和服务器交互类的软件。在 RESTful 风格中，用户发起请求的发送方式有 GET、POST、DELETE、PUT 等方式对请求的处理方法进行区分。这样可以在前后端分离式的开发中使得前端开发人员不会对请求的资源地址产生混淆和大量的检查方法名的麻烦，形成一个统一的接口，使得 Web 服务变得更加简洁、有层次，易于实现缓存等机制。 在 Spring Boot 中，开发 RESTful 接口非常简单，通过不同的注解来支持前端的请求，除了经常使用的 @RestController 注解外，Spring Boot 还提供了一些组合注解。这些注解来帮助简化常用的 HTTP 方法的映射，并更好地表达被注解方法的语义。 例如，Spring Boot 提供了与 REST 操作方式（GET、POST、PUT、DELETE）对应的注解： @GetMapping：处理 GET 请求。 @PostMapping：处理 POST 请求。 @PutMapping：用于更新资源。 @DeleteMapping：处理删除请求。 @PatchMapping：用于更新部分资源。 这些注解就是我们使用的 @RequestMapping 的简写版本：@GetMapping 其实就等于 @RequestMapping(value = &quot;/xxx&quot;, method = RequestMethod.GET)。 5. Spring Cloud 5.1 什么是 Spring Cloud？和 Spring Boot 的区别是什么？ Spring Boot 是用于构建单个 Spring 应用的框架，而 Spring Cloud 则是用于构建分布式系统中的微服务架构的工具。 Spring Cloud 是一系列框架的有序集合，它利用 Spring Boot 的开发便利性巧妙地简化了分布式系统基础设施的开发。Spring Cloud 为开发人员提供了快速构建分布式系统中一些常见模式的工具（例如配置管理、服务注册与发现、断路器、负载均衡、智能路由、微代理、控制总线）。分布式系统的协调导致了样板模式，使用 Spring Cloud 开发人员可以快速地支持实现这些模式的服务和应用程序。 Spring Cloud 并不重复造轮子，而是将市面上开发得比较好的模块集成进去，进行封装，从而减少了各模块的开发成本。 总的来说，Spring Cloud 是微服务系统架构的一站式解决方案，是各个微服务架构落地技术的集合体，俗称微服务全家桶。 5.2 介绍一下用过哪些微服务组件？ 注册中心：注册中心是微服务架构最核心的组件。它起到的作用是对新节点的注册与状态维护，解决了如何发现新节点以及检查各节点运行状态的问题。微服务节点在启动时会将自己的服务名称、IP、端口等信息在注册中心登记，注册中心会定时检查该节点的运行状态。注册中心通常会采用心跳机制最大程度保证已登记过的服务节点都是可用的。 负载均衡：负载均衡解决了如何发现服务及负载均衡如何实现的问题。通常微服务在互相调用时，并不是直接通过 IP、端口进行访问调用。而是先通过服务名在注册中心查询该服务拥有哪些节点，注册中心将该服务可用节点列表返回给服务调用者，这个过程叫服务发现，因服务高可用的要求，服务调用者会接收到多个节点，必须要从中进行选择。因此服务调用者一端必须内置负载均衡器，通过负载均衡策略选择合适的节点发起实质性的通信请求。 服务通信：服务通信组件解决了服务间如何进行消息通信的问题。服务间通信采用轻量级协议，通常是 HTTP RESTful 风格。但因为 RESTful 风格过于灵活，必须加以约束，通常应用时对其封装。例如在 Spring Cloud 中就提供了 Feign（[fen]）和 RestTemplate 两种技术屏蔽底层的实现细节，所有开发者都是基于封装后统一的 SDK 进行开发，有利于团队间的相互合作。 配置中心：配置中心主要解决了如何集中管理各节点配置文件的问题。在微服务架构下，所有的微服务节点都包含自己的各种配置文件，如 JDBC 配置、自定义配置、环境配置、运行参数配置等。要知道有的微服务可能可能有几十个节点，如果将这些配置文件分散存储在节点上，发生配置更改就需要逐个节点调整，将给运维人员带来巨大的压力。配置中心便由此而生，通过部署配置中心服务器，将各节点配置文件从服务中剥离，集中转存到配置中心。一般配置中心都有 UI 界面，方便实现大规模集群配置调整。 集中式日志管理：集中式日志主要是解决了如何收集各节点日志并统一管理的问题。微服务架构默认将应用日志分别保存在部署节点上，当需要对日志数据和操作数据进行数据分析和数据统计时，必须收集所有节点的日志数据。那么怎么高效收集所有节点的日志数据呢？业内常见的方案有 ELK、EFK。通过搭建独立的日志收集系统，定时抓取各节点增量日志形成有效的统计报表，为统计和分析提供数据支撑。 分布式链路追踪：分布式链路追踪解决了如何直观的了解各节点间的调用链路的问题。系统中一个复杂的业务流程，可能会出现连续调用多个微服务，我们需要了解完整的业务逻辑涉及的每个微服务的运行状态，通过可视化链路图展现，可以帮助开发人员快速分析系统瓶颈及出错的服务。 服务保护：服务保护主要是解决了如何对系统进行链路保护，避免服务雪崩的问题。在业务运行时，微服务间互相调用支撑，如果某个微服务出现高延迟导致线程池满载，或是业务处理失败。这里就需要引入服务保护组件来实现高延迟服务的快速降级，避免系统崩溃。 5.3 详细介绍一下分布式项目与微服务架构 分布式项目是指将一个大型的项目切割成多个小项目，每个小项目都是一套独立的系统。这些小项目被打成 Jar 包，然后通过互相引用（以 Jar 包的形式）来组装成原来的完整项目。每个子业务都是一套独立的系统，子业务之间相互协作，最终完成整体的大业务。这种方式可以提高系统的可扩展性和高可用性，解决高并发的问题，并且可以利用分布式存储将数据分片到多个节点上，不仅可以提高性能，同时也可以使用多个节点对同一份数据进行备份。 微服务架构是一种软件开发框架，它将一个大型的应用程序划分为许多小的、独立的服务。每个服务都有自己的技术栈，包括数据库和数据管理模型。这些服务通常通过 REST API、事件流和消息代理进行通信，并按照业务能力进行组织。 微服务架构的主要优点包括： 代码更容易更新：可以直接添加新特性或功能，而不必更新整个应用。 团队可以对不同的组件使用不同的技术栈和不同的编程语言。 组件可以相互独立地扩展，从而减少与必须扩展整个应用相关的浪费和成本。 然而，微服务架构也带来了一些挑战，例如管理复杂性的增加、日志记录数据的增加、新版本可能导致的向后兼容性问题、应用涉及更多网络连接可能导致的延迟和连接问题等。尽管如此，微服务架构仍然被广泛采用，因为它可以提高开发效率，使组织能够更快地响应业务需求。 微服务架构和分布式系统是两个不同的概念，它们的主要区别在于设计目标和实现方式： 分布式系统：分布式系统的核心就是拆分，只要是将一个项目拆分成了多个模块，并将这些模块分开部署，那就算是分布式。分布式解决的是系统性能问题，即解决系统部署上单点的问题，尽量让组成系统的子系统分散在不同的机器上进而提高系统的吞吐能力。分布式是部署层面的东西，即强调物理层面的组成，系统的各子系统部署在不同计算机上。 微服务架构：微服务架构通过更细粒度的服务切分，使得整个系统的迭代速度和并行程度更高，但是运维的复杂度和性能会随着服务的粒度更细而增加。微服务重在解耦合，使每个模块都独立。微服务是设计层面的东西，一般考虑如何将系统从逻辑上进行拆分，也就是垂直拆分。微服务可以是分布式的，即可以将不同服务部署在不同计算机上，当然如果量小也可以部署在单机上。 总的来说，分布式系统和微服务架构都是为了提高系统的可扩展性和可维护性，但它们的关注点和实现方式有所不同。 5.4 使用 Spring Cloud 有什么优势？ 约定优于配置：Spring Cloud 提供了一套默认的配置，使得开发人员可以更专注于业务逻辑的开发。 适用于各种环境：无论是开发环境、部署 PC Server，还是各种云环境（例如阿里云、AWS 等），Spring Cloud 都可以适用。 隐藏了组件的复杂性：Spring Cloud 提供了声明式、无 XML 的配置方式，隐藏了组件的复杂性。 开箱即用、快速启动：Spring Cloud 提供了一套完整的微服务解决方案，使得开发人员可以快速启动项目。 轻量级的组件：Spring Cloud 整合的组件大多比较轻量。 组件丰富、选型中立、功能齐全：Spring Cloud 为微服务架构提供了非常完整的支持，有丰富的组件选择，开发人员可以根据需求选择合适的组件。 灵活：Spring Cloud 的组成部分是解耦合的，开发人员可以按需灵活挑选技术选型。 服务拆分粒度更细：有利于资源重复利用，提高开发效率。 采用去中心化思想：服务之间采用轻量级通讯，适合互联网时代，产品迭代周期更短。 5.5 服务注册和发现是什么意思？Spring Cloud 如何实现？ 服务注册是指将服务的元数据（例如服务名、IP 地址、端口号等）注册到注册中心中，以便其他服务可以发现它。例如，一个微服务启动后，会将自己的信息（通常是这个服务的 IP 和端口）注册到一个公共的组件上去（比如 ZooKeeper、Consul）。 服务发现是指客户端从注册中心中查找和选择可用的服务实例，并通过负载均衡策略来分配请求。也就是说，新注册的这个服务模块能够及时的被其他调用者发现。不管是服务新增和服务删减都能实现自动发现。 Spring Cloud 实现服务注册和发现的方式主要有以下几种： Eureka（[jʊ'ri:kə]）：Eureka 是 Netflix 开源的一款提供服务注册和发现的产品。Spring Cloud 封装了 Netflix 公司开发的 Eureka 模块来实现服务治理。在微服务应用启动后，Eureka Client 会向 Eureka Server 发送心跳。如果 Eureka Server 在多个心跳周期内没有接收到某个节点的心跳，则会将该节点移除。 Consul：Consul 是一种服务网格解决方案，提供了包括服务发现、配置和分段功能。这些功能中的每一个都可以根据需要独立使用，也可以一起使用以构建全堆栈服务网格。Consul 是适用于底层服务发现和配置的工具。 ZooKeeper：ZooKeeper 是一个开源的分布式应用程序协调服务，是 Google 的 Chubby 一个开源的实现，是 Hadoop 和 Hbase 的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。 5.6 负载均衡有什么作用？ 负载均衡是一种重要的网络技术，它可以有效地提高网络服务的性能、可用性和安全性。负载均衡的主要作用包括： 解决并发压力：通过将客户端的请求分发到多个服务器，负载均衡可以有效地解决并发压力，提高应用处理性能，增加吞吐量，加强网络处理能力。 提供故障转移：负载均衡可以检测后端服务的运行状况，自动检测异常实例，并快速实施故障转移；当实例恢复正常时，它将自动恢复负载。这样，即使某个服务器出现故障，负载均衡也可以保证服务的高可用性。 提供网站伸缩性（扩展性）：当业务压力增加时，可以通过将主机添加到后端服务器池来提高性能。当压力降低时，可以减少宿主。这样，负载均衡可以根据业务需求动态地添加或减少服务器数量，提供网站的伸缩性。 安全防护：负载均衡设备上可以做一些过滤，黑白名单等处理，提供安全防护。 5.7 负载均衡有哪些算法？ 简单轮询：将请求按顺序分发给后端服务器上，不关心服务器当前的状态，比如后端服务器的性能、当前的负载。 加权轮询：根据服务器自身的性能给服务器设置不同的权重，将请求按顺序和权重分发给后端服务器，可以让性能高的机器处理更多的请求。 简单随机：将请求随机分发给后端服务器上，请求越多，各个服务器接收到的请求越平均。 加权随机：根据服务器自身的性能给服务器设置不同的权重，将请求按各个服务器的权重随机分发给后端服务器。 一致性哈希：根据请求的客户端 IP、或请求参数通过哈希算法得到一个数值，利用该数值取模映射出对应的后端服务器，这样能保证同一个客户端或相同参数的请求每次都使用同一台服务器。 最小活跃数：统计每台服务器上当前正在处理的请求数，也就是请求活跃数，将请求分发给活跃数最少的后台服务器。 5.8 介绍一下服务熔断？ 服务熔断是应对微服务雪崩效应的一种链路保护机制，类似股市、保险丝。 比如说，微服务之间的数据交互是通过远程调用来完成的。服务 A 调用服务 B，服务 B 调用服务 C，某一时间链路上对服务 C 的调用响应时间过长或者服务 C 不可用，随着时间的增长，对服务 C 的调用也越来越多，然后服务 C 崩溃了，但是链路调用还在，对服务 B 的调用也在持续增多，然后服务 B 崩溃，随之 A 也崩溃，导致雪崩效应。 服务熔断是应对雪崩效应的一种微服务链路保护机制。例如在高压电路中，如果某个地方的电压过高，熔断器就会熔断，对电路进行保护。同样，在微服务架构中，熔断机制也是起着类似的作用。当调用链路的某个微服务不可用或者响应时间太长时，会进行服务熔断，不再有该节点微服务的调用，快速返回错误的响应信息。当检测到该节点微服务调用响应正常后，恢复调用链路。 所以，服务熔断的作用类似于我们家用的保险丝，当某服务出现不可用或响应超时的情况时，为了防止整个系统出现雪崩，暂时停止对该服务的调用。 在 Spring Cloud 框架里，熔断机制通过 Hystrix（[hɪst'rɪks]）实现。Hystrix 会监控微服务间调用的状况，当失败的调用到一定阈值，缺省是5秒内20次调用失败，就会启动熔断机制。 5.9 介绍一下服务降级？ 服务降级一般是指在服务器压力剧增的时候，根据实际业务使用情况以及流量，对一些服务和页面有策略地不处理或者用一种简单的方式进行处理，从而释放服务器资源的资源以保证核心业务的正常高效运行。 服务器的资源是有限的，而请求是无限的。在用户使用即并发高峰期，会影响整体服务的性能，严重的话会导致宕机，以至于某些重要服务不可用。故高峰期为了保证核心功能服务的可用性，就需要对某些服务降级处理，可以理解为舍小保大。 服务降级是从整个系统的负荷情况出发和考虑的，对某些负荷会比较高的情况，为了预防某些功能（业务场景）出现负荷过载或者响应慢的情况，在其内部暂时舍弃对一些非核心的接口和数据的请求，而直接返回一个提前准备好的退路（fallback）错误处理信息。这样，虽然提供的是一个有损的服务，但却保证了整个系统的稳定性和可用性。 5.10 Spring Cloud 和 Dubbo 有什么区别？ Spring Cloud 和 Dubbo 都是现在主流的微服务框架，但它们之间存在一些主要的区别： 初始定位不同：Spring Cloud 定位为微服务架构下的一站式解决方案，而 Dubbo 是 SOA（Service-Oriented Architecture，面向服务的架构）时代的产物，它的关注点主要在于服务的调用和治理。 生态环境不同：Spring Cloud 依托于 Spring 平台，具备更加完善的生态体系；而 Dubbo 一开始只是做 RPC（Remote Procedure Call，远程过程调用）通信协议的远程调用，生态相对匮乏，现在才逐渐丰富起来。 调用方式不同：Spring Cloud 是采用 HTTP 协议做远程调用，接口一般是 REST 风格，比较灵活；Dubbo 是采用 Dubbo 协议，接口一般是 Java 的 Service 接口，格式固定。 服务网关：Dubbo 没有服务网关，而 Spring Cloud 使用的是 Spring Cloud Netflix Zuul。 分布式配置：Dubbo 没有分布式配置，而 Spring Cloud 使用的是 Spring Cloud Config。 服务跟踪：Dubbo 没有服务跟踪，而 Spring Cloud 使用的是 Spring Cloud Sleuth。 消息总线：Dubbo 没有消息总线，而 Spring Cloud 使用的是 Spring Cloud Bus。 数据流：Dubbo 没有数据流，而 Spring Cloud 使用的是 Spring Cloud Stream。 5.11 Sping Cloud 微服务之间如何通讯？ 在 Spring Cloud 中，微服务之间的通信主要有两种方式： 同步通信：Dubbo 通过 RPC 远程过程调用，而 Spring Cloud 通过 REST 接口 JSON 调用等（HTTP 通信）。 异步通信：消息队列，如：RabbitMq、ActiveMq、Kafka 等。 6. MyBatis 6.1 与传统的 JDBC 相比，MyBatis 的优点是什么？ 基于 SQL 语句编程，相当灵活，不会对应用程序或者数据库的现有设计造成任何影响，SQL 写在 XML 里，解除 SQL 与程序代码的耦合，便于统一管理；提供 XML 标签，支持编写动态 SQL 语句，并可重用。 与 JDBC 相比，减少了 50% 以上的代码量，消除了 JDBC 大量冗余的代码，不需要手动开关连接。 很好的与各种数据库兼容，因为 MyBatis 使用 JDBC 来连接数据库，所以只要 JDBC 支持的数据库 MyBatis 都支持。 能够与 Spring 很好的集成，开发效率高。 提供映射标签，支持对象与数据库的 ORM 字段关系映射；提供对象关系映射标签，支持对象关系组件维护。 6.2 MyBatis 在哪方面做的比较好？ MyBatis 在 SQL 灵活性、动态 SQL 支持、结果集映射和与 Spring 整合方面表现卓越，尤其适合重视 SQL 可控性的项目。 SQL 与代码解耦，灵活可控：MyBatis 允许开发者直接编写和优化 SQL，相比全自动 ORM（如 Hibernate），MyBatis 让开发者明确知道每条 SQL 的执行逻辑，便于性能调优。 动态 SQL 的强大支持：比如可以动态拼接 SQL，通过 &lt;if&gt;、&lt;choose&gt;、&lt;foreach&gt; 等标签动态生成 SQL，避免 Java 代码中繁琐的字符串拼接。 上面的例子中使用 MyBatis 的 &lt;where&gt; 和 &lt;if&gt; 标签实现动态 SQL，&lt;where&gt; 会自动去除多余的 AND/OR，且当内部条件为空时，不生成 WHERE 关键字，&lt;if&gt; 标签根据参数动态添加条件，例如 test=&quot;name != null&quot; 表示当 name 参数非空时生效。根据传入参数不同可以生成不同的 SQL： 传入参数 生成的 SQL 说明 name=&quot;张%&quot; SELECT * FROM user WHERE name LIKE '张%' 模糊查询姓张的用户 status=1 SELECT * FROM user WHERE status = 1 查询状态为1的用户 name=&quot;李%&quot; status=0 SELECT * FROM user WHERE name LIKE '李%' AND status = 0 组合查询 无参数 SELECT * FROM user 查询所有用户 自动映射与自定义映射结合：自动将查询结果字段名与对象属性名匹配（如驼峰转换）。例如在下面这个例子中，resultMap 实现了用户-角色的一对多关系映射，&lt;resultMap&gt; 定义结果集映射规则，映射到 User 类；&lt;id&gt; 为主键映射（标识对象唯一性），将数据库的 user_id 字段映射到 User 对象的 id 属性；&lt;result&gt; 为普通字段映射，将数据库的 user_name 字段映射到 User 对象的 name 属性；&lt;collection&gt; 为一对多集合映射（用户对应多个角色），映射到 User 对象的 roles 属性，其中嵌套了对象的字段映射，将数据库的 role_name 字段映射到 Role 对象的 roleName 属性。 插件扩展机制：可编写插件拦截 SQL 执行过程，实现分页、性能监控、SQL 改写等通用逻辑。 与 Spring 生态无缝集成：通过 @MapperScan 快速扫描 Mapper 接口，结合 Spring 事务管理，配置简洁高效。 6.3 JDBC 连接数据库的步骤是什么？ 使用 Java JDBC 连接数据库的一般步骤如下： 加载数据库驱动程序：在使用 JDBC 连接数据库之前，需要加载相应的数据库驱动程序。可以通过 Class.forName(&quot;com.mysql.jdbc.Driver&quot;) 来加载 MySQL 数据库的驱动程序。不同数据库的驱动类名会有所不同。 建立数据库连接：使用 DriverManager 类的 getConnection(url, username, password) 方法来连接数据库，其中 url 是数据库的连接字符串（包括数据库类型、主机、端口等），username 是数据库用户名，password 是密码。 创建 Statement 对象：通过 Connection 对象的 createStatement() 方法创建一个 Statement 对象，用于执行 SQL 查询或更新操作。 执行 SQL 查询或更新操作：使用 Statement 对象的 executeQuery(sql) 方法来执行 SELECT 查询操作，或者使用 executeUpdate(sql) 方法来执行 INSERT、UPDATE 或 DELETE 操作。 处理查询结果：如果是 SELECT 查询操作，通过 ResultSet 对象来处理查询结果。可以使用 ResultSet 的 next() 方法遍历查询结果集，然后通过 getXXX() 方法获取各个字段的值。 关闭连接：在完成数据库操作后，需要逐级关闭数据库连接相关对象，即先关闭 ResultSet，再关闭 Statement，最后关闭 Connection。 以下是一个简单的示例代码，请注意在实际应用中，需要进行异常处理以确保资源的正确释放，以及使用 try-with-resources 来简化代码和确保资源的及时关闭： 6.4 如果项目中要用到原生的 MyBatis 去查询，该怎样写？ 配置 MyBatis：在配置文件中配置数据源、MyBatis 的 Mapper 文件位置等信息。 创建实体类：创建与数据库表对应的实体类，字段名和类型需与数据库表保持一致： 编写 SQL 映射文件：在 resources 目录下创建 XML 文件，定义 SQL 语句和映射关系： 编写 DAO 接口：创建 DAO 接口，定义查询方法： 编写具体的 SQL 查询语句：在 XML 文件中编写对应的 SQL 语句。 调用查询方法：在服务层或控制层中调用 DAO 接口中的方法进行查询： 6.5 MyBatis 里的 # 和 $ 区别是什么？ MyBatis 在处理 #&#123;&#125; 时，会创建预编译的 SQL 语句，将 SQL 中的 #&#123;&#125; 替换为 ? 号（参数占位符），在执行 SQL 时会为预编译 SQL 中的占位符（?）赋值，调用 PreparedStatement 的 set 方法来赋值，预编译的 SQL 语句执行效率高，并且可以防止 SQL 注入（SQL 注入是攻击者通过在输入参数中插入恶意 SQL 代码，篡改原始 SQL 语句逻辑的攻击方式），提供更高的安全性，适合传递参数值。 MyBatis 在处理 $&#123;&#125; 时，只是创建普通的 SQL 语句，然后在执行 SQL 语句时 MyBatis 将参数直接拼入到 SQL 里（简单的字符串替换），不能防止 SQL 注入，因为参数直接拼接到 SQL 语句中，如果参数未经过验证、过滤，可能会导致安全问题。 #&#123;&#125; 使用 JDBC 预编译机制，将参数值独立传输给数据库，数据库先编译 SQL 结构，再将参数值作为纯数据处理：恶意代码会被视为数据值而非可执行代码： 例如我们有以下两种写法： 不同输入的执行结果如下： 传入参数 safeSearch 生成的 SQL dangerSearch 生成的 SQL 结果 Alice SELECT * FROM user WHERE name = 'Alice' SELECT * FROM user WHERE name = 'Alice' 正常查询 Alice ' OR 1=1 -- SELECT * FROM user WHERE name = &quot;\\' OR 1=1 --&quot; SELECT * FROM user WHERE name = '' OR 1=1 --' 前者查询 \\' OR 1=1 -- 用户的信息，后者非法查询所有用户的信息 具体看一下这两条 SQL 语句的执行逻辑： 6.6 MyBatisPlus 和 MyBatis 的区别？ MyBatisPlus 是一个基于 MyBatis 的增强工具库，旨在简化开发并提高效率。以下是 MyBatisPlus 和 MyBatis 之间的一些主要区别： CRUD 操作：MyBatisPlus 通过继承 BaseMapper 接口，提供了一系列内置的快捷方法，使得 CRUD 操作更加简单，无需编写重复的 SQL 语句。 代码生成器：MyBatisPlus 提供了代码生成器功能，可以根据数据库表结构自动生成实体类、Mapper 接口以及 XML 映射文件，减少了手动编写的工作量。 通用方法封装：MyBatisPlus 封装了许多常用的方法，如条件构造器、排序、分页查询等，简化了开发过程，提高了开发效率。 分页插件：MyBatisPlus 内置了分页插件，支持各种数据库的分页查询，开发者可以轻松实现分页功能，而在传统的 MyBatis 中，需要开发者自己手动实现分页逻辑。 多租户支持：MyBatisPlus 提供了多租户的支持，可以轻松实现多租户数据隔离的功能。 注解支持：MyBatisPlus 引入了更多的注解支持，使得开发者可以通过注解来配置实体与数据库表之间的映射关系，减少了 XML 配置文件的编写。 6.7 MyBatis 运用了哪些常见的设计模式？ 代理模式：MyBatis 实现的核心，比如 MapperProxy、ConnectionLogger，用的 JDK 的动态代理；还有 executor.loader 包使用了 CGLIB 或者 Javassist 达到延迟加载的效果。 单例模式：例如 ErrorContext 和 LogFactory。 工厂模式：例如 SqlSessionFactory、ObjectFactory、MapperProxyFactory。 建造者模式（Builder）：例如 SqlSessionFactoryBuilder、XMLConfigBuilder、XMLMapperBuilder、XMLStatementBuilder、CacheBuilder 等。 组合模式：例如 SqlNode 和各个子类 ChooseSqlNode 等。 模板方法模式：例如 BaseExecutor 和 SimpleExecutor，还有 BaseTypeHandler 和所有的子类例如 IntegerTypeHandler。 适配器模式：例如 Log 的 MybBatis 接口和它对 JDBC、Log4j 等各种日志框架的适配实现。 装饰者模式：例如 Cache 包中的 cache.decorators 子包中等各个装饰者的实现。 迭代器模式：例如迭代器模式 PropertyTokenizer。 "},{"title":"Java进阶面试题总结","date":"2023-11-24T02:32:00.000Z","url":"/posts/53737.html","tags":[["Interview","/tags/Interview/"]],"categories":[["Interview","/categories/Interview/"]],"content":" Java 进阶面试题总结，涉及集合、JVM、并发等内容，文章将不断更新。 1. Java集合 1.1 Java中常见的集合有哪些？ Java 中的集合主要可以分为四个部分：List、Set、Map 和工具类（如 Iterator 迭代器、Enumeration 枚举类、Arrays 和 Collections）。这些集合类主要由两个接口派生而来，即 Collection（包含 List、Set、Queue）和 Map，它们是集合框架的根接口。 List：一种有序列表的集合，例如，按索引排列的元素的 List。常见的实现类有 ArrayList、LinkedList 和 Vector。 Set：一种保证没有重复元素的集合。常见的实现类有 HashSet、LinkedHashSet 和 TreeSet。 Map：一种通过键值（Key-Value）查找的映射表集合。常见的实现类有 HashMap 和 TreeMap。 1.2 线程安全的集合有哪些？ Java 中的线程安全集合主要包括以下几种： Vector：这是一个线程安全的动态数组，它提供了与 ArrayList 类似的功能，但每个方法都是同步的，这意味着在多线程环境下，它的性能会比 ArrayList 差。 Hashtable：这是一个线程安全的哈希表实现，类似于 HashMap，但是 Hashtable 的方法都是同步的。 Stack：这是一个线程安全的栈实现，它继承自 Vector。 ConcurrentLinkedQueue：这是一个线程安全的队列实现，使用了非阻塞算法。 BlockingQueue 接口的实现类，如 ArrayBlockingQueue，LinkedBlockingQueue 等。 CopyOnWriteArrayList 和 CopyOnWriteArraySet：这是两个线程安全的集合，它们会在修改操作时复制一份数据，避免了修改时的并发问题。 ConcurrentHashMap：这是一个线程安全的 HashMap，它通过将数据分段，从而达到并发控制，性能要优于 Hashtable。 ConcurrentSkipListMap：线程安全且排序的哈希表。 值得注意的是，为了保证集合是线程安全的，相应的效率也比较低；线程不安全的集合效率相对会高一些。如果你的代码只在一个线程中运行，或者多个线程只是读取集合而不修改集合，那么你可以选择线程不安全的集合，因为它们的性能通常会更好。 1.3 ArrayList和LinkedList的异同点？ ArrayList 和 LinkedList 都是 Java 中常用的 List 实现类，它们有一些共同点，也有一些不同点。 共同点： ArrayList 和 LinkedList 都是单列集合中 List 接口的实现类，它们都是存取允许重复，且有序的元素。 不同点： 内部实现：ArrayList 是基于动态数组实现的，底层使用数组来存储元素。而 LinkedList 是基于链表实现的，底层使用双向链表来存储元素。 随机访问：对于随机访问 get 和 set 方法，ArrayList 的速度通常优于 LinkedList，因为 ArrayList 可以根据下标以 O(1) 的时间复杂度对元素进行随机访问，而 LinkedList 的每一个元素都依靠地址指针和它后一个元素连接在一起，查找某个元素的时间复杂度是 O(n)。 插入和删除操作：对于插入和删除操作，LinkedList 的速度通常优于 ArrayList，因为当元素被添加到 LinkedList 任意位置的时候，不需要像 ArrayList 那样重新计算大小或者是更新索引。 内存占用：LinkedList 比 ArrayList 更占内存，因为 LinkedList 的节点除了存储数据，还存储了两个引用，一个指向前一个元素，一个指向后一个元素。而 ArrayList 使用数组来存储元素，因此插入和删除元素时需要移动其他元素占用内存，所以在频繁进行插入和删除操作时，ArrayList 的性能会比较低，且可能会造成内存浪费。 1.4 ArrayList的扩容机制是什么？ ArrayList 的扩容机制是其核心特性之一。在 ArrayList 中添加元素时，如果当前的数组已经满了，那么 ArrayList 会创建一个新的、更大的数组，并将原有数组的元素复制到新的数组中，这个过程就叫做扩容。 具体来说，ArrayList 的扩容机制如下： 当向 ArrayList 中添加元素时，首先会检查 ArrayList 的当前大小（也就是它内部的数组大小）是否能够容纳新的元素。如果可以，那么新元素就直接被添加到 ArrayList 中。 如果 ArrayList 的当前大小不足以容纳新的元素，那么 ArrayList 就需要进行扩容操作。在扩容操作中，ArrayList 会创建一个新的数组，新数组的大小是原数组大小的1.5倍，这个值是在 JDK 的源码中定义的。 接着，ArrayList 会使用 System.arraycopy 方法，将原有数组中的所有元素复制到新的数组中。 最后，新的数组会替代原有的数组，成为 ArrayList 的内部数组。 值得注意的是，ArrayList 的扩容操作需要重新分配内存空间，并将原来的元素复制到新的数组中，这可能会导致性能问题。因此，在实例化 ArrayList 时设置足够的初始容量，并且尽可能减少数组扩容的次数，可以帮助提高性能： 1.5 HashMap的底层数据结构是什么？ HashMap 的底层数据结构主要包括哈希表（数组）、链表和红黑树。 哈希表（数组）：HashMap 主要依赖于哈希表来存储数据。哈希表中的每个元素被称为 bucket。数组的每个位置都可以存放一个元素（键值对），数组的索引是通过键的哈希码经过哈希函数计算得来的。这样我们就可以通过键快速定位到数组的某个位置，取出相应的值，这就是 HashMap 快速获取数据的原理。 链表：在理想的情况下，哈希函数将每个键均匀地散列到哈希表的各个位置。但在实际中，我们可能会遇到两个不同的键计算出相同的哈希值，这就是所谓的哈希冲突。HashMap 通过使用链表来解决这个问题。当哈希冲突发生时，HashMap 会在冲突的 bucket 位置增加一个链表，新的元素会被添加到链表的末尾。每个链表中的元素都包含了相同哈希值的键值对。 红黑树：从 Java 8 开始，如果链表的长度超过一定的阈值（默认为8），那么链表会被转换为红黑树。红黑树是一种自平衡的二叉查找树，通过保持树的平衡，可以提高查找效率。 1.6 为什么在解决Hash冲突的时候，不直接用红黑树，而是先用链表，再转红黑树？ 在解决 Hash 冲突的时候，HashMap 在链表长度大于8的时候才会将链表转换为红黑树，而不是直接使用红黑树，这主要有以下几个原因： 查询效率：红黑树的平均查找长度是 log(n)，当链表长度为8时，查找长度为 log(8) = 3，而链表的平均查找长度为 n/2，当长度为8时，平均查找长度为 8 / 2 = 4。因此，当链表长度小于等于8时，使用链表的查询效率其实并不比红黑树差。 插入效率和空间效率：链表的插入操作比红黑树快，且链表的空间占用也比红黑树小。因此，在元素数量较少时，使用链表比红黑树更高效1。 防止频繁转换：如果链表长度在8左右徘徊，且频繁地进行插入和删除操作，可能会导致链表和红黑树之间频繁地转换，这会降低效率。因此，HashMap 设计了两个阈值，链表长度超过8时转为红黑树，少于6时转回链表，这样可以减少转换的频率。 总的来说，这种设计是为了在保证查询效率的同时，尽可能地提高插入效率和空间效率，以及减少因频繁转换而带来的开销。 1.7 什么是负载因子？为什么HashMap的默认负载因子为0.75？ 负载因子是用于表示哈希表中元素填满的程度的一个参数。在哈希表（如 Java 的 HashMap）中，负载因子是和扩容机制有关的，当哈希表中的元素个数超过了容量乘以负载因子时，就会进行扩容。例如，如果当前的容器容量是16，负载因子是0.75，16 * 0.75 = 12，也就是说，当容量达到了12的时候就会进行扩容操作。 负载因子的大小对哈希表的性能有重要影响。如果负载因子过大，那么哈希表中的冲突会更频繁，导致查找效率降低。反之，如果负载因子过小，那么哈希表的空间利用率就会降低，导致内存浪费。因此，选择一个合适的负载因子，可以在时间效率和空间效率之间达到一个平衡。在 Java 的 HashMap 中，负载因子的默认值是0.75，这是一个在时间和空间效率之间的折衷选择。"},{"title":"Java基础面试题总结","date":"2023-11-23T08:13:00.000Z","url":"/posts/36915.html","tags":[["Interview","/tags/Interview/"]],"categories":[["Interview","/categories/Interview/"]],"content":" Java 基础常见面试题总结，涉及 Java 基本概念、OOP、反射等内容，文章将不断更新。 1. Java基本概念 1.1 Java语言有哪些特点？ 面向对象（封装、继承、多态）：Java 是一种面向对象编程（OOP）的语言，它对类、对象、继承、封装、多态、接口、包等内容均有很好的支持。为了简单起见，Java 只支持类之间的单继承，但是可以使用接口来实现多继承。 平台无关性：Java是“一次编写，到处运行”（Write Once, Run Anywhere）的语言，因此采用 Java 语言编写的程序具有很好的可移植性，而保证这一点的正是 Java 的虚拟机机制，在引入虚拟机之后，Java 语言在不同的平台上运行不需要重新编译。 可靠性、安全性：Java 是被设计成编写高可靠和稳健软件的。Java 消除了某些编程错误，使得用它写可靠软件相当容易。 支持多线程：C++ 没有内置的多线程机制，因此必须调用操作系统的多线程功能来进行多线程程序设计，而 Java 具备内置的多线程功能，可以将一个程序的不同程序段设置为不同的线程，使各线程并发、独立执行，提高系统的运行效率。 支持网络编程：Java 诞生本身就是为简化网络编程设计的，因此 Java 语言不仅支持网络编程而且很方便。 编译与解释并存：Java 是一种先编译后解释的语言，Java 程序在 Java 平台运行时会被编译成字节码文件，然后可以在有 Java 环境的操作系统上运行。 动态性：Java 语言设计成适应于变化的环境，它是一个动态的语言。例如，Java 中的类是根据需要载入的，甚至有些是通过网络获取的。 1.2 Java和C++有什么联系和区别？ 联系： 面向对象：Java 和 C++ 都支持面向对象编程，包括类、对象、继承、封装和多态。 语法：Java 的语法在很大程度上受到了 C++ 的影响，因此这两种语言在语法上有很多相似之处。 区别： 内存管理：Java 有垃圾回收机制，可以自动回收不再使用的内存，而 C++ 需要程序员手动管理内存。 指针和引用：C++ 支持指针，但 Java 没有指针的概念。相反，Java 使用引用来实现某些相似的功能。 继承：C++ 支持多重继承，而 Java 不支持多重继承，但允许一个类实现多个接口。 运行环境：Java 程序在 Java 平台上运行，可以在任何安装了 Java 虚拟机的系统上运行，而 C++ 程序是直接编译成特定操作系统的机器码。 异常处理：Java 有一套完整的异常处理机制，而 C++ 的异常处理机制相对较弱。 1.3 Java和Python有什么联系和区别？ 联系： 面向对象：两者均支持面向对象编程（封装、继承、多态），但 Python 还支持函数式编程、过程式编程等范式。 跨平台性：Java 通过 JVM（Java 虚拟机）实现“一次编写，到处运行”；Python 通过解释器（如 CPython、PyPy）实现跨平台，但需安装对应环境。 丰富的生态系统：两者都有庞大的第三方库和框架支持。Java：Spring（企业级开发）、Hibernate（ORM）、Android SDK（移动开发）；Python：Django/Flask（Web 开发）、NumPy/Pandas（数据分析）、TensorFlow/PyTorch（机器学习）。 自动内存管理：均通过垃圾回收机制（GC）自动管理内存，开发者无需手动分配/释放内存。 区别： 语言设计哲学：Java 为静态类型（编译时检查类型），而 Python 为动态类型（运行时推断类型）；Java 语法严格，需分号和大括号，而 Python 语法简洁，依赖缩进（强制代码可读性）；Java “一次编写，到处运行”（强调稳定性和性能），而 Python “优雅、明确、简单”（强调开发效率和可读性）。 执行方式：Java 先编译为字节码（.class），再由 JVM 执行，而 Python 解释执行（逐行翻译），但会生成字节码（.pyc）缓存；Java 接近 C++，适合高性能场景（如服务器、大型系统），而 Python 解释执行较慢，但可通过 C 扩展（如 NumPy）或 JIT（PyPy）优化。 应用场景：Java 代码量大，适合长期维护的大型项目，如企业级应用、Android 开发、金融系统、大数据（Hadoop/Spark），而 Python 代码简洁，适合快速原型开发，如数据分析、机器学习、中小型 Web 开发、脚本自动化、科学计算。 并发模型：Java 原生支持多线程，JVM 利用操作系统线程，通过 java.util.concurrent 包实现高效并发，而 Python 受 GIL（全局解释器锁）限制，多线程在 CPU 密集型任务中效率低，可使用多进程（multiprocessing 模块）或异步编程（asyncio）绕过 GIL 限制。 1.4 JVM、JRE和JDK的关系是什么？ JVM、JRE 和 JDK 是 Java 开发和运行的三个核心组件，它们之间的关系可以概括为：JDK 包含 JRE，而 JRE 包含 JVM。下面是对这三者的详细介绍： JVM（Java Virtual Machine）：Java 虚拟机，是 Java 能够实现跨平台的核心机制。JVM 只认识 .class 后缀的文件，它能将 class 文件中的字节码指令进行识别并调用操作系统向上的 API 完成动作。 JRE（Java Runtime Environment）：Java 运行环境，包括 Java 虚拟机（JVM）和 Java 程序所需的核心类库等。如果想要运行一个开发好的 Java 程序，计算机中只需要安装 JRE 即可。 JDK（Java Development Kit）：Java 的开发工具包，是提供给 Java 开发人员使用的，其中包含了 Java 的开发工具和 JRE。其中的开发工具包括：运行工具（java.exe）、编译工具（javac.exe）、打包工具（jar.exe）等。 所以，简单来说，JDK 是用于开发 Java 应用的，JRE 是用于运行 Java 应用的，而 JVM 则是使 Java 能够跨平台的核心。 1.5 什么是字节码？采用字节码的好处是什么？ 字节码是一种中间状态的二进制文件，是由源码编译过来的，可读性没有源码的高。CPU 并不能直接读取字节码，在 Java 中，字节码需要经过 JVM 转译成机器码之后，CPU 才能读取并运行。采用字节码的好处主要有以下几点： 跨平台性：字节码可以在不同的平台上运行，只需要有一个能够识别并解释字节码的解释器即可。 高效率：字节码可以在运行时动态编译为机器代码，这样就可以在保证程序执行效率的同时避免了额外的编译步骤。Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点，所以 Java 程序运行时比较高效。 可调试性：字节码是可读的，因此可以方便地进行调试和分析。 可扩展性：字节码可以被扩展以支持新的特性，而不需要更改现有的机器代码。 1.6 Java有哪些数据类型？ Java 的数据类型可以分为两大类：基本数据类型和引用数据类型。 基本数据类型包括以下八种： byte：8位有符号二进制整数，取值范围为 -128~127。 short：16位有符号二进制整数，取值范围为 -32768~32767。 int：32位有符号二进制整数，取值范围为 -2147483648~2147483647。 long：64位有符号二进制整数，取值范围为 -9223372036854775808~9223372036854775807。 float：32位单精度浮点数。 double：64位双精度浮点数。 boolean：布尔值，只有两个取值：true 和 false。 char：单个16位 Unicode 字符，取值范围为 \\u0000~\\uffff。 引用数据类型包括： 类（Class）：由程序员定义的一种数据类型，它将数据和对数据的操作封装在一起。 接口（Interface）：一种引用类型，类似于类，由完全抽象的方法和常量组成。 数组（Array）：可以保存多个同类型变量的容器。 2. 面向对象 2.1 面向对象的三大特性是什么？ 封装：封装是指利用抽象数据类型将数据和基于数据的操作封装在一起，使其构成一个不可分割的独立实体。数据被保护在抽象数据类型的内部，尽可能地隐藏内部的细节，只保留一些对外接口使之与外部发生联系。 继承：继承是使用已存在的类的定义作为基础建立新类的技术，新类的定义可以增加新的数据或新的功能，也可以用父类的功能，但不能选择性地继承父类。通过使用继承我们能够非常方便地复用以前的代码。 多态：多态是指允许不同类的对象对同一消息做出响应。也就是说，同一个接口可以具有多种实现方式。 2.2 访问修饰符的区别？ Java 有四种访问修饰符，它们分别是 public、protected、default（默认，无修饰符）和 private。以下是它们的详细介绍： public：公共访问修饰符，被声明为 public 的类、方法、构造方法和接口能够被任何其他类访问。如果几个相互访问的 public 类分布在不同的包中，则需要导入相应 public 类所在的包。由于类的继承性，类所有的公有方法和变量都能被其子类继承。 protected：受保护的访问修饰符，被声明为 protected 的变量、方法和构造器能被同一个包中的任何其他类访问；同时，被 protected 所修饰的成员也能被该类的所有子类继承下来，无论子类和基类是否在同一包中。 default（默认，无修饰符）：包私有访问修饰符，表示只能在当前包中的类访问该成员。被 default 所修饰的成员只能被该类所在同一个包中的子类所继承下来。 private：私有访问修饰符，表示只能在当前类中访问该成员，除了当前类都不能访问。私有访问修饰符是最严格的访问级别，所有被声明为 private 的方法、变量和构造方法只能被所属类访问，并且类和接口不能声明为 private。 2.3 Java语言是如何实现多态的？ 我们通常所说的多态指的都是运行时多态，也就是编译时不确定究竟调用哪个具体方法，一直延迟到运行时才能确定，这也是为什么有时候多态方法又被称为延迟方法的原因。Java 语言实现多态主要依赖于以下三个条件： 继承：必须存在子类继承父类的继承关系，只有在存在继承关系的前提下，子类才能继承父类的属性和方法，从而实现多态。 重写：子类需要对父类中的一些方法进行重写，当调用这些方法时，会调用子类重写的方法，而不是原本父类的方法。 向上转型：在多态中需要将子类的引用赋给父类对象，只有这样，该引用才能够具备调用父类的方法和子类的方法的能力。 2.4 重载和重写的区别？ Java 中的重载和重写都是实现多态的方式，但它们的实现方式和使用场景有所不同： 重载（Overload）： 重载发生在一个类中，同名的方法如果有不同的参数列表（参数类型不同、参数个数不同或者二者都不同）则视为重载。 重载的方法可以改变返回类型；可以相同也可以不同，但不能通过返回类型是否相同来判断重载。 重载是编译时的多态性。 重写（Override）： 重写发生在子类与父类之间，重写要求子类被重写方法与父类被重写方法有相同的参数列表，有兼容的返回类型。 重写的方法不能抛出新的检查异常或者比被重写方法申明更加宽泛的检查型异常。 重写是运行时的多态性。 注意：构造器不能被继承，因此不能被重写，但可以被重载。每一个类必须有自己的构造函数，负责构造自己这部分的构造内容。子类不会覆盖父类的构造函数，相反必须在构造函数的一开始就调用父类的构造函数。 2.5 抽象类和接口的区别？ 语法层面上的区别： 接口只能定义抽象方法不能实现方法，抽象类既可以定义抽象方法，也可以实现方法。 抽象类中的成员变量可以是各种类型的，而接口中的成员变量只能是 public static final 类型的。 接口中不能含有静态代码块以及静态方法，而抽象类可以有静态代码块和静态方法。 抽象类是单继承，而接口是多继承。一个类只能继承一个抽象类，但可以实现多个接口。 抽象类的方法访问控制符无限制，只是抽象类中的 abstract 方法不能被 private 修饰；而接口有限制，接口默认为 public 控制符。 抽象类可以有构造方法，接口中不能有构造方法。 设计层面上的区别： 抽象类是对一种事物的抽象，即对类抽象，而接口是对行为的抽象。抽象类是对整个类整体进行抽象，包括属性、行为，但是接口却是对类局部（只有行为）进行抽象。 二者的设计层面不同，抽象类作为很多子类的父类，它是一种模板式设计。而接口是一种行为规范，它是一种辐射式设计。 2.6 什么是不可变对象？有什么好处？ 在 Java 中，不可变对象（Immutable Object）是指一旦被创建后，对象所有的状态及属性在其生命周期内不会发生任何变化。也就是说，一个对象在创建后，不能对该对象进行任何更改。如 String、Integer 以及其它包装类。不可变对象有很多优点： 构造、测试和使用都很简单：由于对象状态不会改变，所以可以避免了很多复杂的状态检查和同步。 线程安全且没有同步问题：线程安全是最大的好处，在并发环境下，不可变对象无需进行额外的同步操作，因此可以极大地简化并发编程。 不需要担心数据会被其它线程修改：这是因为对象的状态不会改变，所以不会出现一个线程正在读取对象状态，而另一个线程同时修改该状态的情况。 当用作类的属性时不需要保护性拷贝：如果类的属性是可变的，那么在返回属性值或者接收新的属性值时，需要进行保护性拷贝以防止属性被外部代码修改。而对于不可变对象，由于其状态不能被修改，所以无需进行保护性拷贝。 可以很好的用作 Map 键值和 Set 元素：由于不可变对象一旦创建就不能改变，所以它们是值得信赖的键值，可以确保在对象被用作键值的过程中始终保持一致性。 然而，不可变对象也有一些缺点，最大的缺点就是创建对象的开销，因为每一步修改操作都会产生一个新的对象。 2.7 equals方法和==的区别？ 在 Java 中，== 和 equals() 方法都可以用来比较两个对象，但它们的比较方式和使用场景有所不同： 首先 == 是一个运算符，而 equals() 是一个方法，二者比较的内容有以下不同： ==：如果比较的是基本数据类型，则比较的是数值是否相等；如果比较的是引用数据类型，则比较的是两个对象的内存地址是否相等。 equals()：默认情况下，比较的是两个对象的内存地址。但是，许多类（如 String、Integer 等）已经重写了 equals() 方法，使其能够比较两个对象的内容是否相等。 二者的一般使用场景如下： ==：通常用于比较基本数据类型，或者比较两个对象是否指向同一内存地址。 equals()：通常用于比较两个对象的内容是否相等。 2.8 String、StringBuffer、StringBuilder的区别是什么？ 可变性：String 是不可变的，也就是说，一旦 String 对象被创建，其值就不能被改变。如果需要修改 String，Java 会创建一个新的 String 对象。StringBuffer 和 StringBuilder 是可变的，也就是说，它们可以在原地修改字符串，而不需要创建新的对象。 线程安全性：StringBuffer 是线程安全的，因为它的所有公共方法都是同步的。这意味着在多线程环境下，StringBuffer 可以安全地使用。StringBuilder 不是线程安全的。因此，如果你的代码只在单线程环境下运行，使用 StringBuilder 通常会比 StringBuffer 更快。 性能：对于需要频繁修改字符串的情况，使用 StringBuffer 或 StringBuilder 通常比使用 String 更高效。这是因为每次修改 String 时，都会创建一个新的对象，这会对性能产生影响。在大部分情况下，StringBuilder 的性能优于 StringBuffer，这主要是因为 StringBuilder 不需要考虑线程安全。 2.9 为什么Java中的String要设计成不可变的？ 安全性：不可变对象本身是线程安全的，可以在多线程环境下安全使用，无需额外的同步。此外，String 经常被用作许多 Java 类的参数，例如网络连接和文件路径，如果是可变的，那么它的值可能在你不知情的情况下被改变，这可能会导致安全问题。 哈希码缓存：由于 String 是不可变的，所以它的哈希码是固定的，可以被缓存，这对于哈希映射（如 HashMap）来说非常有用，可以提高查找效率。 字符串池：在 Java 中，相同的字符串字面量只会在内存中存在一份，这被称为字符串池（String Pool）。这种设计可以节省内存，提高效率。如果 String 是可变的那么字符串池就无法实现了。 类加载器安全：String 是 Java 类加载器使用的关键类，如果是可变的那么可能会影响到类加载器的安全性。 2.10 基本类型和包装类型有什么区别？什么是自动装箱/拆箱？ 基本类型和包装类型的主要区别如下： 初始值：基本类型有初始值，而包装类型的默认值是 null。 null 值：包装类型可以为 null，而基本类型不可以。 存储位置：如果一个基本类型是成员变量就存储在堆内存里，如果是局部变量就存储在栈内存里；而包装类型则存储的是堆中的引用。 泛型：包装类型可用于泛型，而基本类型不可以。 比较：在使用 == 进行判断的时候，基本类型使用 == 直接判断其值是否相等，而包装类型判断的是其指向的地址是否相等。 自动装箱和自动拆箱是 Java 语言的特性，使得基本类型和包装类型之间的转换更加方便： 自动装箱：就是将基本数据类型自动转换为对应的包装类。例如，Integer i = 10;，这里的10是一个 int 类型，但 Java 会自动将其转换（装箱）为 Integer 类型。 自动拆箱：就是将包装类自动转换为基本数据类型。例如，int a = i;，这里的 i 是 Integer 类型，但 Java 会自动将其转换（拆箱）为 int 类型。 这些特性使得我们在编写 Java 代码时可以更自然地混合使用基本类型和包装类型，而不需要关心它们之间的转换细节。 2.11 B/S和C/S架构分别是什么？ B/S 架构和 C/S 架构是两种常见的软件系统体系结构。 B/S 架构，全称为 Browser/Server，即浏览器/服务器结构。它是 Web 兴起后的一种网络结构模式，Web 浏览器是客户端最主要的应用软件。这种模式统一了客户端，将系统功能实现的核心部分集中到服务器上，简化了系统的开发、维护和使用。客户端只需要安装一个浏览器，通过 Web 服务器与数据库服务器进行数据交互。B/S 架构利用了 Web 浏览器技术和 Internet 协议，实现了异构系统的连接和信息的共享。 C/S 架构，全称是 Client/Server，即客户端/服务器体系结构，主要应用于局域网内。它是一种网络体系结构，通常采取两层结构，服务器负责数据的管理，客户端负责完成与用户的交互任务。即客户端是用户运行应用程序的 PC 端或者工作站，客户端要依靠服务器来获取资源。 3. 反射 3.1 什么是反射？ 在 Java 中，反射是一种强大的工具，它允许程序在运行时访问类或对象的信息，并动态地操作它们。以下是反射的一些主要特性和用途： 动态创建对象：反射可以在运行时动态地创建任意一个类的对象。 获取类的信息：反射可以获取任意一个类的所有属性和方法，包括私有的。 动态调用方法和属性：反射可以在运行时动态地调用任意一个对象的任意方法和属性。 动态修改属性：反射可以改变对象的属性，甚至可以打破封装性，导致 Java 对象的属性不安全。 反射在许多 Java 框架中都有应用，例如 Spring 和 Hibernate，它们使用反射来实现依赖注入和对象关系映射。然而，反射也有其缺点，例如可能会消耗更多的系统资源，如果不需要动态地创建一个对象，那么就不需要用反射。此外，反射调用方法时可以忽略权限检查，因此可能会破坏封装性而导致安全问题。 3.2 举一下反射使用的例子？ （1）获取 Class 对象：我们可以通过三种方式获取 Class 对象： （2）创建对象：我们可以通过 Class 对象的 newInstance() 方法来创建对应类的对象： （3）获取方法并调用：我们可以通过 Class 对象的 getMethod() 方法来获取一个类的方法，然后通过 Method 对象的 invoke() 方法来调用这个方法： （4）获取和设置字段：我们可以通过 Class 对象的 getField() 方法来获取一个类的公有字段，然后通过 Field 对象的 get() 和 set() 方法来获取和设置这个字段的值： 3.3 介绍一下反射在JDBC和Spring中的应用 （1）在 JDBC 中的应用：我们在使用 JDBC 连接数据库时，会使用 Class.forName() 通过反射加载数据库的驱动程序。例如，假设我们有 com.mysql.cj.jdbc.Driver 这个类，如果我们使用 MySQL 数据库，那么就传入 MySQL 的驱动类： （2）在 Spring 中的应用：Spring 通过配置文件配置各种各样的 bean，你需要用到哪些 bean 就配哪些，Spring 容器就会根据你的需求去动态加载。Spring 的 IoC 容器可以动态地加载和管理 bean，创建对象。这是通过反射实现的，Spring 会读取配置文件中的类全名，然后通过反射来创建对象： 3.4 反射机制的原理是什么？ Java 反射机制的核心是在程序运行时动态加载类并获取类的详细信息，从而操作类或对象的属性和方法。本质上，当 JVM 得到 Class 对象之后，再通过 Class 对象进行反编译，从而获取对象的各种信息。Java 属于先编译再运行的语言，程序中对象的类型在编译期就确定下来了，而当程序在运行时可能需要动态加载某些类，这些类因为之前用不到，所以没有被加载到 JVM。通过反射，可以在运行时动态地创建对象并调用其属性，不需要提前在编译期知道运行的对象是谁。 反射的原理可以通过以下步骤来理解： 加载：首先，将 .class 文件读入内存，并为之创建一个 Class 对象。 反编译：然后，通过 Class 对象进行反编译，从而获取对象的各种信息。 反射机制的优点是在运行时获得类的各种内容，进行反编译，对于 Java 这种先编译再运行的语言，能够让我们很方便的创建灵活的代码，这些代码可以在运行时装配，无需在组件之间进行源代码的链接，更加容易实现面向对象。"},{"title":"SpringBoot学习笔记-实现微服务：匹配系统（上）","date":"2023-11-22T12:56:00.000Z","url":"/posts/41540.html","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 本节内容为实现前后端的 WebSocket 通信，并同步游戏地图的生成。 我们的游戏之后是两名玩家对战，因此需要实现联机功能，在这之前还需要实现一个匹配系统，能够匹配分数相近的玩家进行对战。 想要进行匹配就至少要有两个客户端，当两个客户端都向服务器发送匹配请求后并不会马上得到返回结果，一般会等待一段时间，这个时间是未知的，因此这个匹配是一个异步的过程，对于这种异步的过程或者是计算量比较大的过程我们都会用一个额外的服务来操作。 那么这个额外的用于匹配的服务可以称为 Matching System，这是另外一个程序（进程），当后端服务器接收到前端的请求后就会将请求发送给 Matching System，这个匹配系统维护了一堆用户的集合，它会不断地去匹配分数最接近的用户，当匹配成功一组用户后就会将结果返回给后端服务器，再由后端将匹配结果立即返回给对应的前端。这种服务就被称为微服务，可以用 Spring Cloud 实现。 用以前的 HTTP 请求很难达到这种效果，之前我们是在客户端向后端发送请求，且后端在短时间内就会返回结果，HTTP 请求只能满足这种一问一答式的服务。而我们现在需要实现的效果是客户端发送请求后不知道经过多长时间后端才会返回结果，对于这种情况需要使用 WebSocket 协议（WS），该协议不仅支持客户端向服务器发送请求，也支持服务器向客户端发送请求。 在前端向服务器发送请求后，服务器会维护好一个 WS 链接，这个链接其实就是一个 WebSocketServer 类的实例，所有和这个链接相关的信息都会存到这个类中。 1. 配置WebSocket 我们之前每次刷新网页就会随机生成游戏地图，该过程是在浏览器本地执行的，当我们要实现匹配功能时，地图就不能由两名玩家各自的客户端生成，否则就基本不可能完全一样了。 当匹配成功后应该由服务器端创建一个 Game 任务，将游戏放到该任务下执行，统一生成地图，且判断移动或者输赢等逻辑之后也应该移到后端来执行。 生成好地图后服务器就将地图传给两名玩家的前端，然后等待玩家的键盘输入或者是 Bot 代码的输入，Bot 代码的输入也属于一个微服务。 首先我们先在 pom.xml 文件中添加以下依赖： spring-boot-starter-websocket fastjson2 接着在 config 包下创建 WebSocketConfig 配置类： 然后我们创建一个 consumer 包，在其中创建 WebSocketServer 类： 之前我们配置的 Spring Security 设置了屏蔽除了授权之外的其他所有链接，因此我们需要在 SecurityConfig 类中放行一下 WebSocket 的链接： 如果是使用新版的配置而不是使用 WebSecurityConfigurerAdapter 可以按以下方式配置： 2. 前后端WebSocket通信 2.1 WS通信的建立 WebSocket 不属于单例模式（同一个时间每个类只能有一个实例，我们每建一个 WS 链接都会新创建一个实例），不是标准的 Spring 中的组件，因此在注入 Mapper 时不能用 @Autowired 直接注入，一般是将 @Autowired 写在一个 set() 方法上，Spring 会根据方法的参数类型从 IoC 容器中找到该类型的 Bean 对象注入到方法的行参中，并且自动反射调用该方法。 我们先假设前端传过来的是用户 ID 而不是 JWT 令牌： 然后我们先在前端的 PKIndexView 组件中调试，当组件被挂载完成后发出请求建立 WS 链接，当被卸载后关闭 WS 链接： 现在我们在对战页面每次刷新后都可以在浏览器控制台或后端控制台中看到 WS 的输出信息。 接下来我们要将 WebSocket 存到前端的 store 中，在 store 目录下创建 pk.js 用来存储和对战页面相关的全局变量： 同时要在 store/index.js 中引入进来： 2.2 加入JWT验证 现在我们直接使用用户的 ID 建立 WS 链接，这是不安全的，因为前端可以自行修改这个 ID，因此就需要加入 JWT 验证。 WebSocket 中没有 Session 的概念，因此我们在验证的时候前端就不用将信息放到表头里了，直接放到链接中就行： 验证的逻辑可以参考之前的 JwtAuthenticationTokenFilter，我们可以把这个验证的模块单独写到一个文件中，在 consumer 包下创建 utils 包，然后创建一个 JwtAuthentication 类： 然后就可以在 WebSocketServer 中解析 JWT 令牌： 3. 前后端匹配业务 3.1 实现前端页面 我们需要实现一个前端的匹配页面，并能够切换匹配和对战页面，可以根据之前在 store 中存储的 status 状态来动态展示页面。首先在 components 目录下创建 MatchGround.vue 组件，其中需要展示玩家自己的头像和用户名以及对手的头像和用户名，当点击开始匹配按钮时向 WS 链接发送开始匹配的消息，点击取消按钮时发送取消匹配的消息： 3.2 实现前后端交互逻辑 当用户点击开始匹配按钮后，前端要向服务器发出一个请求，后端接收到请求后应该将该用户放入匹配池中，由于目前还没有实现微服务，因此我们先在 WebSocketServer 后端用一个 Set 维护正在匹配的玩家，当匹配池中满两名玩家就将其匹配在一起，然后将匹配结果返回给两名玩家的前端： 接着修改一下 PKIndexView，当接收到 WS 链接从后端发送过来的匹配成功消息后需要更新对手的头像和用户名： 测试的时候需要用两个浏览器，如果没有两个浏览器可以在 Edge 浏览器的右上角设置菜单中新建 InPrivate 窗口，这样就可以自己登录两个不同的账号进行匹配测试。 3.3 同步游戏地图 现在匹配成功后两名玩家进入游戏时看到的地图是不一样的，因为目前地图还都是在每名玩家本地的浏览器生成的，那么我们就需要将生成地图的逻辑放到服务器端。 先在后端的 consumer.utils 包下创建 Game 类，用来管理整个游戏流程，然后我们在其中先实现地图的随机生成： 然后在 WebSocketServer 类中当匹配成功时创建游戏地图，暂时先将其存到局部变量中，之后再进行优化： 接下来需要在前端的 store/pk.js 文件中将地图存为全局变量： 在 PKIndexView.vue 中当接收到匹配成功的消息后需要更新地图： 然后需要在 GameMap.vue 中将全局变量传给游戏地图 GameMap.js： 最后就可以在 GameMap.js 中将地图渲染出来： "},{"title":"SpringBoot学习笔记-创建个人中心页面（下）","date":"2023-11-19T00:48:00.000Z","url":"/posts/39458.html","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 本节内容为实现个人中心的前端页面，用户能够查看自己的 Bot 信息，并能创建、修改或删除 Bot，此外还集成了 Vue Ace Editor 代码编辑器，方便用户输入 Bot 的代码。 1. 实现个人中心页面 我们要实现用户的个人中心页面，能够展示用户的头像以及自己所有 Bot 的信息，例如 Bot 名字、创建时间等，还需要具有创建/修改/删除 Bot 等功能，在很早之前创建的 MyBotsIndexView 组件中实现： 注意，我们实现了当点击创建 Bot 按钮时弹出一个模态框（Bootstrap 中的 Modal）的效果，然后用户可以在里面编辑 Bot 的信息。但对于每个 Bot 的修改和删除应该都会分别对应一个模态框，而不像创建那样只有一个模态框，因此每个模态框还需要加一个 ID 来对应。 2. POJO时区修改 现在我们会发现前端页面中显示的 Bot 创建时间和后端数据库中的不一致，需要在 pojo 中的 Bot 类中修改： 3. 集成代码编辑器 由于 Bot 的内容有代码部分，之前我们用的是 &lt;textarea&gt; 让用户编写代码，这样编写体验不好，因此我们需要集成一个代码编辑器，我们使用的是 Vue Ace Editor（Ace GitHub 官网：Ace (Ajax.org Cloud9 Editor)，Vue 3 版 Ace GitHub 官网：vue3-ace-editor）。 先在 Vue 的控制台中安装以下依赖： vue3-ace-editor ace-builds 然后就可以用 &lt;VAceEditor&gt; 替代之前的 &lt;textarea&gt;： "},{"title":"SpringBoot学习笔记-创建个人中心页面（上）","date":"2023-11-18T08:10:00.000Z","url":"/posts/26147.html","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 本节内容为实现存储用户 Bot 信息的数据表以及操作 Bot 数据的增删改查 API。 1. 更新数据库表 我们需要创建一个表来保存 Bot 的信息，新建一个 bot 表，包含以下几个列： id: int：非空、自动增加、唯一、主键。 user_id: int：非空。注意：在 pojo 中需要定义成 userId，在 queryWrapper 中的名称仍然为 user_id。 title: varchar(100)。 description: varchar(300)。 content：varchar(10000)。 rating: int：默认值为1500。 createtime: datetime，注意：在 pojo 中定义日期格式的注解：@JsonFormat(pattern = &quot;yyyy-MM-dd HH:mm:ss&quot;)。 modifytime: datetime。 可以使用如下 SQL 语句一键创建好该表： 创建好数据库表后我们需要创建一个 pojo，在 pojo 目录下创建 Bot 类： 然后就可以实现 mapper，在 mapper 目录下创建 BotMapper 接口： 2. 实现后端API 首先在 service.user 包下创建 bot 包存放与 Bot 相关的 Service 代码，然后在 service.impl.user 包下创建 bot 包存放相应的 Service 实现代码，最后在 controller.user 包下创建 bot 包存放 Controller。 我们需要实现以下四个 API： /user/bot/add/：创建一个 Bot。 /user/bot/remove/：删除一个 Bot。 /user/bot/update/：修改一个 Bot。 /user/bot/getlist/：查询 Bot 列表。 在 service.user.bot 包下创建这四个 API 的 Service 接口： （1）AddService： （2）RemoveService： （3）UpdateService： （4）GetListService： 接下来在 service.impl.user.bot 包下创建这四个 Service 接口的实现： （1）AddServiceImpl： （2）RemoveServiceImpl： （3）UpdateServiceImpl： （4）GetListServiceImpl： 最后在 controller.user.bot 包下创建对应的 Controller： （1）AddController： （2）RemoveController： （3）UpdateController： （4）GetListController： "},{"title":"SpringBoot学习笔记-配置MySQL与实现注册登录模块（下）","date":"2023-11-18T01:26:00.000Z","url":"/posts/42841.html","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 本节内容为实现登录与注册前端页面，并将 JWT 令牌存储在浏览器的 LocalStorage 中以实现登录状态的持久化。 1. 实现登录页面 打开我们的前端项目代码，在 src/views/user 目录下创建 account 目录，然后创建 UserAccountLoginView 和 UserAccountRegisterView 组件。 我们需要在全局存一些信息，例如每个页面都需要知道当前登录用户的信息，这就需要用到 Vue 的一个特性叫做 vuex。在 src/store 目录下创建 user.js： 如果使用 Axios 发送 POST 请求可以将参数放在请求体中，也就是第二个参数： 注意如果使用这种方式发送请求需要在后端 Controller 中修改接收参数的方式，从 @RequestParam 修改为 @RequestBody： @RequestParam 和 @RequestBody 都是 Spring MVC 中常用的参数绑定注解，它们在处理 HTTP 请求时有以下区别： @RequestParam 用于将 HTTP 请求中的参数绑定到方法的参数上，主要用于处理 GET 请求的参数或 POST 请求中的表单参数。 @RequestBody 用于接收整个请求体，并将其转换为方法参数所需的对象或数据类型。它主要用于处理 POST 请求，并且请求体通常是 JSON 格式。 如果后端继续使用 @RequestParam 来接收参数，那么你可以在 Axios 的 POST 请求中使用 params 对象来传递参数（注意将第二个参数设置为 null）： 使用 Axios 发送带请求头的 GET 请求方式如下： 然后需要将其引入到 store 目录下的 index.js 中： 现在就可以实现我们的登录前端页面 UserAccountLoginView： 我们的导航栏也要根据登录状态显示不同的内容，可以用 v-if 和 v-else 来根据条件决定是否显示内容： 还有别忘了更新路由，即 src/router 目录下的 index.js： 2. 实现退出登录功能 在上一节中我们没有实现退出登录的后端 API，我们的 jwt token 完全是存在用户本地的，令牌中会存有过期时间，服务器端能够判断令牌是否过期，因此不用管后端的退出登录。那么如果用户想自己退出登录也很简单，直接将 jwt token 删除即可，无需向后端发送请求，没有令牌后就无法访问后端服务器了。 现在我们是将令牌存在浏览器的内存中，一刷新自动就会重置，之后我们会将其存到 LocalStorage 中，这样即使用户刷新或者关闭浏览器都不会自动退出登录状态。 我们先来实现主动退出登录功能，在 store 目录的 user.js 中添加清空 state 的操作： 然后在 NavBar 中调用函数： 3. 设置前端页面授权机制 现在我们的前端页面还没有任何的访问限制，例如在未登录状态下也可以访问任意的页面。当未登录时访问任何页面都应该重定向到登录页面。 页面的授权控制可以在 router 中通过 beforeEach 函数实现，当我们每次在通过 router 进入某个页面之前都会先调用该函数，函数有三个参数：to 表示要跳转到哪个页面，from 表示从哪个页面跳转过去，next 表示页面执行的下一步跳转操作。 我们每次在跳转到某个页面之前需要先判断一下该页面是否需要登录，如果需要登录且当前处于未登录状态则跳转至登录页面。因此我们就需要在每个页面中存储是否需要授权的信息，可以定义在任意名字的变量中，一般可以把额外信息放在 meta 域中。 修改后的 router/index.js 如下： 4. 实现注册页面 注册页面 UserAccountRegisterView 的实现其实就和登录页面基本一致，多加一个确认密码输入框即可。注册时不会修改前端的 state 值，因此也无需将 register 函数实现在 store/user.js 中： 5. 登陆状态的持久化 我们可以将登录后获得的 jwt token 存放在浏览器的一小块硬盘空间 LocalStorage 中，首先在 store/user.js 中修改： 我们在 login 函数中将登录成功后收到的 jwt token 存在 LocalStorage 中，在 logout 函数中清除 LocalStorage 中的 jwt token。需要特别注意的是 getInfo 函数中添加了 async: false，这是表示将该 Ajax 请求变为同步的，具体作用在之后讲解。 现在当我们要跳转到某个链接前可以先取出 LocalStorage 中的 jwt token，判断是否存在并且未过期，如果有效则在跳转之前直接调用 store/user.js 中的 updateJwtToken 更新浏览器内存中的 jwt token，并通过 getInfo 函数更新用户信息。还是在 router/index.js 中的 router.beforeEach 函数中实现： 注意，在第一个 if 语句中调用了 store 的 getInfo 函数，由于 Ajax 的回调函数默认是异步的（Axios 也是异步），因此第二个 if 语句会在 success 回调函数执行前就被执行了，这会导致 jwt_token_valid 还没被更新，从而被判断成未登录状态，直接跳转至登录页面，所以我们在前面将 getInfo 函数中的 Ajax 设置为同步，保证了以上代码的正确执行逻辑。 如果使用 Axios，需要结合 async 与 await 关键字实现同步，getInfo 函数如下： router.beforeEach 函数如下： "},{"title":"SpringBoot学习笔记-配置MySQL与实现注册登录模块（中）","date":"2023-11-17T02:08:00.000Z","url":"/posts/61266.html","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 本节内容为实现用适合前后端分离的 JWT 验证替代传统的 Session 验证方式，并实现登录、获取信息以及注册三个后端 API。 1. 配置JWT验证 传统模式是使用 Session 进行验证，但是由于前后端分离后可能会存在跨域问题，因此我们用 JWT（JSON Web Tokens）验证会更加方便。JWT 验证不仅可以很容易地实现跨域，也无需在服务器端存储数据，这样就算我们有很多个服务器，那么只需要获得一次令牌后就可以登录多个服务器。 我们所有的页面大致可以分为两大类，第一类是无需登录验证就能访问（公开页面），另一类就是登录后才能访问（授权页面）。 JWT 验证的原理是用户登录后服务器会给用户返回一个 jwt token，且会将一些 ID 之类的用户信息加到 jwt token 里，之后客户端每次向服务器端发送请求的时候都会带上这个令牌，服务器端在访问授权页面时会先验证这个令牌是否合法，如果合法就会根据令牌中的用户信息从数据库中查找出这个用户并将其数据提取至上下文，接着再访问授权页面。 生成 jwt token 时服务器端会先构建一个字符串，第一段存储用户 ID，第二段存储一个只有服务器能看到的密钥，然后可以通过一些哈希算法将字符串加密（加密算法是固定的），接着会将第一段用户 ID 加上加密后的信息合在一起（即 jwt token）返回给用户，之后服务器端想要验证的话就根据接收到的用户 ID 配合自己的密钥重复一遍这个固定的加密算法，看看加密后的结果是否和接收到的 jwt token 中的加密信息一致。 现在可能会有几个问题，比如 jwt token 是存在于客户端的，那么如果用户去篡改里面的数据会怎样，比如把用户 ID 换成具有权限的另一个用户的 ID？ 这种情况是不会发生的，因为假如用户修改了 ID，但是由于加密算法的特性是不可逆的，即无法通过加密信息反推回原始字符串的信息，因此用户不知道服务器加密的密钥是什么，就没办法得到修改 ID 后再经过密钥加密的信息。 首先我们先去 Maven 仓库查找并添加以下依赖： JetBrains Java Annotations jjwt-api jjwt-impl jjwt-jackson 然后我们要实现 utils.JwtUtil 类（utils 包创建在 com.kob.backend 包下，放在哪个包下其实都是看个人习惯，不一定都要按这样写），为 JWT 工具类，用来创建、解析 jwt token： 这个类提供了创建和解析 JWT 的功能，可以用于实现基于 JWT 的身份验证和授权。但是需要注意的是，这个类中的 JWT_KEY 是硬编码的，这在实际的生产环境中是不推荐的，因为这样会导致所有的令牌都使用同一个密钥，一旦这个密钥泄露，所有的令牌都可能会被篡改或伪造。在实际的生产环境中，密钥应该被安全地存储在一个安全的地方，如环境变量或密钥管理服务中。 在 JWT 中，subject（或称为 sub）是一个预定义的声明，通常用来标识这个 JWT 所关联的主体。这个主体通常是一个用户的标识符，比如用户 ID 或用户名。这样，当你从一个 JWT 中解析出 subject 时，你就知道这个 JWT 是属于哪个用户的，从而可以进行相应的授权操作。例如，你可以根据 subject 来查找用户的角色，然后根据角色来决定用户是否有权限访问某个资源。这就是基于 JWT 的身份验证和授权的基本原理。 UUID（Universally Unique Identifier）是通用唯一识别码。它是通过特定的算法生成的一种128位的字符串，用于在全局范围内唯一地标识信息。UUID 的主要目的不是防止重复，而是减少重复的概率，使其小到可以忽略不计。 代码中 getUUID() 方法生成了一个随机的 UUID，这个 UUID 被用作 JWT 的 id，用于唯一地标识每一个 JWT。这样，即使有两个完全相同的 JWT（即具有相同的 subject 和过期时间），只要它们的 id 不同，就可以被认为是两个不同的 JWT。这对于跟踪和管理 JWT 非常有用。例如，如果一个 JWT 被盗，你可以通过它的 id 找到它，并将其加入到黑名单中，使其无法再被使用。 接下来还需要实现 config.filter.JwtAuthenticationTokenFilter 类（filter 包创建在之前的 config 包下），用来验证 jwt token，如果验证成功，则将 User 信息注入上下文中： 以上是一个基于 Spring Security 的 JWT 认证过滤器。它的主要功能是在每次 HTTP 请求到达时，从请求头中获取 JWT，解析出用户 ID，然后查询数据库获取用户信息，最后将用户信息保存到 Spring Security 的上下文中。这样，后续的处理模块就可以从 Spring Security 的上下文中获取到用户信息，进行相应的授权操作。 最后我们还需要配置一下之前实现过的 config.SecurityConfig 类，放行登录、注册等接口，因为用户未登录时需要能访问这些页面才能正常登录： 这段代码的重点部分在于 configure 方法，这个方法用于配置 HttpSecurity，它是 Spring Security 的核心配置点。在这个方法中，首先禁用了 CSRF（跨站请求伪造）保护，然后设置了会话管理策略为无状态，这意味着 Spring Security 不会创建或使用 HTTP 会话。接着，它配置了 URL 的访问权限，/user/account/login/ 和 /user/account/register/ 这两个 URL 对所有用户开放，OPTIONS 请求对所有用户开放，此外其他的所有请求都需要身份验证。最后，它在 UsernamePasswordAuthenticationFilter 之前添加了 JwtAuthenticationTokenFilter，这意味着在每次 HTTP 请求到达时，JwtAuthenticationTokenFilter 都会先于 UsernamePasswordAuthenticationFilter 执行。 较新版本的 Spring Security 5.7 会看到提示说 WebSecurityConfigurerAdapter 已经废除，不过目前对本项目没什么影响，如果一定要改可以参考以下代码： 2. 实现验证登录API 配置完成后接下来我们就可以创建后端的 API 了。在这之前我们给数据库的 user 表添加一列 photo 用来存储用户的头像链接（数据库中存储图像都是存的链接），类型为 varchar(1000)，然后还得去 pojo.User 类中添加一个字段： 现在我们编写第一个 API：/user/account/login/，功能是验证用户名和密码，验证成功后返回 jwt token。 SpringBoot 中写一个 API 一共要实现三个部分：controller 用来调用 service，service 里面实现一个接口，还需要在 service.impl 中写一个具体的接口的实现。 在 service 包下创建 user.account 包，表示用户账号相关的 API，然后创建 LoginService 接口（注意不是创建类）： 接着在 service.impl 包下创建 user.account 包，然后创建 LoginServiceImpl 类用来实现我们之前定义的接口： 最后就可以实现 controller 模块了，我们可以先把 controller.user 包下的 UserController 类删了，这是之前学习数据库操作与调试用的，然后创建一个 account 包，在包中创建 LoginController 类： 实现好后我们可以自己调试一下，如果直接从浏览器访问 URL 的话是 GET 请求，尝试访问  会看到报错状态码为405，表示方法不被允许，可以用 Postman 软件调试也可以自己打开前端调试，我们采用第二种方法。 直接在前端项目的 App.vue 文件中编写临时调试代码，使用 Ajax 发出 POST 请求： 然后我们在前端页面中打开控制台，一刷新页面即可看到输出结果。我们可以双击并复制下来控制台中的 jwt_token 内容，然后去 JWT IO 中解析一下，能够得到以下结果，其中的 sub 存储的即为用户 ID： 3. 实现返回信息API 现在我们来编写 API：/user/account/info/，功能是根据客户端传来的 jwt_token 获取用户信息。 首先在 service.user.account 包下创建 InfoService 接口： 然后在 service.impl.user.account 包下创建 InfoServiceImpl 类，来实现 InfoService 接口： 最后在 controller.user.account 包下创建 InfoController： 重启一下后端，然后我们直接访问  会看到报错代码为403，表示没有权限访问，因为我们还没登录。 还是和之前一样，我们在前端中测试，将之前登录接收到的 jwt_token 用于之后访问授权链接，此次请求不用传数据，但是需要传一个 headers 表示表头，其中有一个 Authorization 属性，由 Bearer （注意有个空格）加上 jwt_token 组成，我们先直接把前面浏览器控制台输出的 jwt_token 复制过来： 在浏览器控制台可以看到以下输出： 4. 实现注册账号API 注册功能就有一些业务逻辑需要判断，代码量会稍微多一些。 首先在 service.user.account 包下创建 RegisterService 接口： 然后在 service.impl.user.account 包下创建 RegisterServiceImpl 类，来实现 InfoService 接口： 最后在 controller.user.account 包下创建 RegisterController： 同样还是在前端编写调试代码，可以自行尝试数据为空，或者两次密码不一致等不合法操作，然后在浏览器控制台查看结果： 至此我们的后端部分实现完成了，后面就可以开始实现前端部分了。"},{"title":"SpringBoot学习笔记-配置MySQL与实现注册登录模块（上）","date":"2023-11-14T13:34:00.000Z","url":"/posts/23384.html","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 本节内容为实现利用 MyBatis-Plus 配置接入并操作 MySQL 数据库，并集成 Spring Security 进行登录身份的认证。 1. SpringBoot配置MySQL 回顾一下项目的流程，客户端（Clinet）可能会有很多个，每个客户端都会和我们的后端服务器进行通信，一般像我们这种的小项目后端只有一个服务器，项目变大后可能还会做负载均衡。 接下来我们要实现的是用户的注册登录模块，那么在登录的时候就需要将用户的用户名和密码传给服务器，然后服务器再给前端传一些信息回来。显然我们需要将用户的用户名和密码存下来，一般我们会将这些数据存在数据库里，本次项目使用的是 MySQL（安装配置教程可见：MySQL 安装配置与使用教程）。 MySQL 也是一个单独的程序，也可以接收很多请求并返回数据，SpringBoot 与 MySQL 之间也有交互关系，例如根据某个用户名向 MySQL 查询对应的密码等信息。 我们先在终端里登录一下 MySQL： 然后创建一个数据库 kob： IDEA 是可以图形化操作数据库的，在最右边的选项卡中能够看到数据库（Database），点击 +，在数据源（Data Source）中选择 MySQL，在弹出来的对话框中输入用户名 root 和数据库根用户的密码，数据库输入我们刚创建的 kob，然后点击一下测试连接，第一次连接会让我们下载驱动。在架构选项卡（Schemas）中可以勾上默认架构。最后点击确定即可连接上数据库。 我们创建一个保存用户信息的表 user，然后创建几个字段：id、username、password，其类型分别为 int、varchar(100)、varchar(100)，将 id 设置为自增且非空。创建好后先插入一条数据：(1, &quot;AsanoSaki&quot;, &quot;123456&quot;)。 接下来我们需要加一些依赖，使得 SpringBoot 能够操作数据库。 首先前往 Maven 仓库，搜索并安装以下依赖，一般选择最新版即可，我们将依赖的 Maven 代码复制到 SpringBoot 项目根目录下的 pom.xml 文件的 &lt;dependencies&gt; 代码块中： Spring Boot Starter JDBC Project Lombok MySQL Connector/J mybatis-plus-boot-starter mybatis-plus-generator 复制好后会看到爆红了，在右侧选项卡中打开 Maven，然后点击上方最左侧的刷新（重新加载所有 Maven 项目）按钮 Reload 一遍就行。 然后在 application.properties 文件中添加数据库配置信息： 2. SpringBoot项目模块介绍与构建 2.1 SpringBoot常用模块介绍 SpringBoot 中的常用模块有以下几种： pojo 层：将数据库中的表对应成 Java 中的 Class。 mapper 层（也叫 Dao 层）：将 pojo 层的 Class 中的操作，映射成 SQL 语句。 service 层：写具体的业务逻辑，组合使用 mapper 中的操作。 controller 层：负责请求转发，接收前端页面传过来的参数，传给 service 处理，接收到返回值后再传给前端页面。 数据库中的表就类似于 Java 中的 Class，pojo 层就是将表转化成 Class，例如我们有一个类 User，里面有三个变量：id、username、password。Class 中可能还有一些方法，例如增删改查，那么 mapper 层就负责将这些方法转化成 SQL 语句。我们很多业务可能并不会只处理一张表，因此 service 层就可能用到多个不同的 mapper 操作来实现某个业务。controller 层就是将前端的请求及参数选择将其传给哪个 service，相当于实现调度功能，然后再将 service 的返回结果返回给前端。 2.2 构建POJO与Mapper层 然后在 com.kob.backend 目录下创建 pojo 包，并在 pojo 包下创建 User 类（注意要和数据库表的名字一样）： 继续在 com.kob.backend 目录下创建 mapper 包，并在 mapper 包下创建 UserMapper 类，我们使用的是 MyBatis-Plus，只需要继承其实现好的接口即可： 2.3 MyBatis与MyBatis-Plus介绍 MyBatis 官方文档：MyBatis Docs。 MyBatis-Plus 官网：MyBatis-Plus。 ORM（Object-Relational Mapping），即对象关系映射，是一种技术，它允许你使用面向对象的范式来查询和操作数据库中的数据。 在面向对象编程中，我们操作的是对象，这些对象有各种属性和行为；而在关系型数据库中，我们操作的是表，这些表由行和列组成。这两种技术之间的差异，就是所谓的对象-关系不匹配。 ORM 框架的作用就是在这两者之间做一个映射，可以将 ORM 看作是连接面向对象编程（OOP）和关系数据库表的层，让我们可以用面向对象的方式来操作数据库。例如，我们可以创建一个对象，并将其属性映射到数据库表的列，然后通过操作这个对象，就可以实现对数据库的增删改查。 MyBatis 是一款用于持久层的、轻量级的半自动化 ORM 框架，它支持自定义 SQL、存储过程以及高级映射。MyBatis 免除了几乎所有的 JDBC 代码以及设置查询参数和获取结果集的工作。MyBatis 封装了 JDBC 底层访问数据库的细节，使我们不需要与 JDBC API 打交道就可以访问数据库。 MyBatis-Plus（简称 MP）是一个 MyBatis 的增强工具，在 MyBatis 的基础上做了增强，却不做改变。引入 MyBatis-Plus 不会对现有的 MyBatis 架构产生任何影响。MyBatis-Plus 提供了基本的 CRUD 功能，连 SQL 语句都不需要编写。它还提供了一些很有意思的插件，比如 SQL 性能监控、乐观锁、执行分析等。 两者的区别主要体现在以下几个方面： SQL 语句：在 MyBatis 中，所有 SQL 语句都需要自己编写；而在 MyBatis-Plus 中，由于内置了通用 Mapper 和通用 Service，因此可以实现单表的大部分 CRUD 操作，甚至无需编写 SQL 语句。 实体关系映射：在 MyBatis 中，需要手动解析实体关系映射转换为 MyBatis 内部对象并注入容器；而在 MyBatis-Plus 中，可以自动解析实体关系映射转换为 MyBatis 内部对象并注入容器。 Lambda 形式调用：MyBatis 不支持 Lambda 形式调用，而 MyBatis-Plus 支持。 条件构造器：MyBatis-Plus 提供了强大的条件构造器，满足各类使用需求。 2.4 使用MyBatis-Plus操作数据库 我们调试的时候先不把 service 和 controller 分开，直接先在 controller 中实现。 我们在 controller 包下创建 user、record、ranklist 包，本次实现的注册登录模块在 user 包中实现，因此我们先在 user 包下创建 UserController 类，来看一下如何操作数据库，先实现查询 user 表中的所有数据： 这时我们访问  即可看到返回结果如下： 然后我们再尝试通过用户的 ID 查询用户信息： 访问  即可看到返回结果如下： 我们先理一下执行流程，假设用户访问了链接 ，那么就向后端的 SpringBoot 发出了请求，SpringBoot 根据请求的路径找到所调用的函数 getUser()（由 UserController 类实现），在该函数中使用 UserMapper 类向 MySQL 查询 user 表（由 User 类实现）中 id 为1的用户数据（在这一步操作中会由 MyBatis-Plus 自动转换成类似于 select * from user where id=1; 的 SQL 语句），并将查询结果返回给客户端。 对于复杂的查询语句需要封装一个条件构造器（QueryWrapper），里面有很多 API 可以使用，我们还是和上一步一样实现根据 ID 查询用户： 为了更好地测试查询操作，我们再插入几条数据： 然后我们根据 ID 的范围来查询满足条件的所有用户： 这时候我们访问  即可查询 ID 大于等于2且小于4的所有用户，返回结果如下： 插入与删除数据一般使用 POST，这样就不是明文传输，较为安全，但是为了方便调试我们使用 GET，由于我们的 ID 设置为自增类型，因此我们创建新用户时只需要设置 username 和 password 即可： 接下来我们再尝试一下删除用户： 3. 配置用户认证机制 3.1 Spring Security对接MySQL 我们未来可能有很多操作不能让用户随便访问，需要登录成功后有授权了才可以访问，因此我们要给网站加上授权机制。比如某个用户想查看一个 Bot 的代码，那么我们需要判断这个用户是不是这个 Bot 的作者，如果不是则没有权限查询。 SpringBoot 中也具有权限判断模块 Spring Security，我们将其集成过来，先去 Maven 仓库搜索并安装依赖 spring-boot-starter-security，同样还是将其 Maven 配置代码复制到 pom.xml 文件的 &lt;dependencies&gt; 代码块中，然后别忘了重新加载 Maven。 现在我们再访问之前的 URL 会发现要我们登录，Spring Security 默认的用户名为 user，密码每次启动项目会重新生成，在项目控制台的输出中可以看到 Using generated security password: xxx 的信息，我们登录后即可正常访问之前的那些 URL。 一般网站的验证方式都是通过 Session 实现，当客户端将用户名和密码传给 SpringBoot 后，SpringBoot 会在数据库中查询是否正确，如果正确那么就会将结果返回给客户端，且会在 Session 中传一串随机字符串 SessionID，在传给客户端之前会先将 SessionID 存下来，可以存到 MySQL 也可以是 Redis 中。客户端拿到 SessionID 后会将其存到浏览器的 Cookie 中，Cookie 可以认为是浏览器自带的一个存储空间，浏览器关闭后仍然存在。未来每次再向后端发送请求时客户端都会默认从 Cookie 中取出 SessionID，然后放到 Session 中传给 SpringBoot，SpringBoot 会看一下数据库中是否存在这个 SessionID，且数据库中除了存放 SessionID 之外还会存放对应的用户名、上一次获取 SessionID 的时间等信息，如果没有过期，那么就表示登录成功，如果过期了或者 SessionID 不存在，那么就会给客户端返回一个登录页面。 另一种验证方式是 JWT，可以不需要生成 SessionID 且不需要在数据库中存储数据就实现前后端分离验证，在之后会讲到。 现在来看看如何让 Spring Security 对接我们的 MySQL 数据库，即通过数据库来判断用户是否登录。我们需要配置一下 Spring Security，首先在 com.kob.backend 包下创建一个 service.impl 包（注意有两层），然后在 impl 包下创建一个 UserDetailsServiceImpl 类（类名无所谓，一般习惯这么写），继承自 UserDetailsService 接口。 在 IDEA 中按 Alt + Insert 键可以方便查找接口中的所有方法，我们要重写（Override）loadUserByUsername 方法，该方法会传入一个 username，然后我们要返回对应的用户信息，因此就涉及到了数据库操作： 其中 UserDetailsImpl 类我们定义在 utils 包中（这个包也创建在 impl 包下）： 现在我们就可以根据用户输入的用户名在数据库中查询该用户的用户名和密码，再判断密码是否匹配。但是我们尝试一下登录会发现报错说我们没有 PasswordEncoder，因为我们的密码是明文，如果想要调试的话可以先去数据库中给密码加上 &#123;noop&#125; 前缀表示这个密码没有加密，例如：&#123;noop&#125;123456，然后再尝试登录即可登录成功。 3.2 密码加密存储 目前各种各样的加密算法包括之后要讲的 JWT 都有一个特性，就是给我们一个字符串，可以很快地将其变为一个新的字符串，且无法反向求得原始字符串（不可逆），这样假如数据库泄露了也不会将用户的密码泄露出去。 我们实现一个 config.SecurityConfig 类（config 包是在之前解决跨域问题时创建的），即可实现用户密码的加密存储： 在 Spring Security 中，PasswordEncoder 用于在保存用户密码到数据库时对其进行加密，以及在验证用户提供的密码时对其进行比较。 其中 BCryptPasswordEncoder 常见的有以下几种方法： encode()：将明文转换成密文。 matches()：判断明文和密文是否匹配。 现在我们再尝试登录就无法登录了，我们先在测试文件中把数据库中的密码加密一下，然后手动修改数据库中的密码，系统自带的测试文件是 src/test/java/com.kob.backend 中的 BackendApplicationTests.java： 将数据库中的密码改成密文后又可以正常登录了。 之前在 UserController 中实现的注册用户的功能（insertUser 方法）需要修改为存储加密后的密码，直接放上目前 UserController 的完整代码： "},{"title":"MySQL安装配置与使用教程","date":"2023-11-13T09:22:00.000Z","url":"/posts/51863.html","tags":[["Others","/tags/Others/"]],"categories":[["Others","/categories/Others/"]],"content":" 本文介绍如何安装与配置 MySQL，并演示基本 SQL 语句。 1. MySQL的安装与配置 MySQL Windows Installer 下载地址：MySQL Installer。 我们下载最新版本（目前是8.0.41）的安装包，注意要选择更大的那个，名字为 mysql-installer-community-8.0.41.0.msi。下载时会让我们登录，不用管直接点 No thanks, just start my download. 即可。 打开安装包后选择 Custom 自定义安装，然后选择要安装的服务，我们选择 MySQL Servers -&gt; MySQL Server -&gt; MySQL Server 8.0 中的 MySQL Server 8.0.41 - X64（选中后点击右箭头即可）。然后点中右边的 MySQL Server 8.0.41 - X64 后可以看到下面会出现一个 Advanced Options，可以修改安装路径，我们安装在 D:\\MySQL\\MySQL Server 8.0。 之后点击 Next -&gt; Execute 即可开始安装。装好后点 Next -&gt; Next，然后会看到 Type and Networking 界面，我们直接默认就好了，除非电脑上有不止一个 MySQL，这样就需要改一下端口号。我们继续点 Next，看到 Authentication Method 界面，还是默认就行，继续 Next，然后我们需要设置根用户的用户名和密码，如果是用于学习的话可以起个简单点的密码防止遗忘。 然后我们继续 Next，会看到 Windows Service 界面，Windows Service Name 可以改成 MySQL，这样以后启动服务的时候方便些，下面的 Start the MySOL Server at System Startup 表示开机自启动，建议默认让他勾上。然后点 Next，选择 No, I will manage the permissions after the server configuration.，最后点 Next -&gt; Execute 完成最后的安装。 安装好后配置一下环境变量，在系统变量的 Path 中添加 D:\\MySQL\\MySQL Server 8.0\\bin，然后打开终端，检查一下 MySQL 版本： 如果想要手动启动或关闭 MySQL 可以用管理员身份打开终端然后通过以下指令操作： 2. MySQL常用操作教程 我们还是在命令行中操作，首先需要连接数据库： 然后输入我们设置的根用户的密码即可连接上 MySQL，连接成功后命令行左侧会变成 mysql&gt;。 查看一下当前所有的数据库信息： 查询结果如下，默认刚开始有四个数据库： 接下来我们创建一个名为 test 的数据库： 我们每次在操作某个数据库前需要先使用这个数据库： 然后查看一下当前数据库中的表： 返回结果如下，我们当前还没有创建任何表： 我们可以创建一个名称为 user 的表，表中包含 id、username 和 password 三个属性，其中 int 表示整数，varchar 表示可变长的字符串，括号中的参数表示最大长度： 向 user 表中插入一条数据： 查询 user 表的所有数据： 返回结果如下： 也可以有条件地查询，假设我们要查询用户名为 AsanoSaki 的用户的密码： 返回结果如下： 删除用户名为 AsanoSaki 的用户数据： 然后我们可以把 user 表删除： 最后我们删除 test 数据库： "},{"title":"SpringBoot学习笔记-创建菜单与游戏页面（下）","date":"2023-11-12T14:25:00.000Z","url":"/posts/10262.html","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 本节内容为实现两名玩家即两条蛇的绘制与人工操作移动功能。 1. 地图优化改进 之前我们设计的地图尺寸是13×13，两名玩家的起点横纵坐标之和均为偶数，因此可能在同一时刻走到同一个格子上，为了避免这种情况，可以将地图改为13×14的大小（即将 GameMap.js 中的 this.cols 改为14），这样两名玩家就不会在同一时刻走到同一个格子上。这样修改完之后就不能用轴对称了，需要改为中心对称： 2. 绘制玩家的起始位置 刚开始玩家占一个格子，我们可以规定一下前七步的每一步将蛇的长度加一，之后改为每三步长度加一。每条蛇其实就是一堆格子的序列，我们可以将一个格子定义成一个 Cell 对象，在 scripts 目录下创建 Cell.js 记录格子的坐标。 我们在每个格子中绘制的是一个圆，若格子的左上角坐标为 (x, y) 则圆的圆心坐标应该是 (x + 0.5, y + 0.5)，Cell.js 如下： 此外每条蛇也可以定义成一个对象 Snake.js： 然后我们在 GameMap.js 中创建两条蛇： 3. 实现玩家移动 为了实现蛇移动的连续性，我们不对每个格子进行更新，只更新头部和尾部，头部创建一个新的点往前动，尾部直接往前动。首先在 Snake 对象中设置一些移动的属性： 由于游戏是回合制的，因此移动的判定条件应该是获取到了两名玩家的指示后才能移动一次，且该指令既可以由键盘输入也可以由 AI 代码输入，判定两条蛇是否准备好执行下一步不能各自判断，需要由上层也就是 GameMap 判定，判定条件是两条蛇都处于静止状态且都已经获取到了下一步指令： 现在我们只能从前端获得用户的操作，即获取用户的键盘输入。为了能够让 Canvas 获取键盘输入，需要添加一个 tabindex 属性，在 GameMap.vue 中进行修改： 这样我们就能够在 GameMap.js 中绑定键盘的监听事件： 现在我们即可在 Snake.js 中实现蛇的移动： 接着我们还需要实现蛇尾的移动，如果蛇的长度增加了一个单位，那么尾部不用动即可，否则尾部需要向前一个节点移动，且当移动完成后需要将尾部节点对象删去： 4. 优化蛇的身体效果 现在我们蛇的身体还是分开的若干个圆球，没有连续感。我们可以在两个相邻的圆球中间绘制一个矩形覆盖一遍即可。然后我们这边再做个小优化，将蛇的半径缩小一点，不然贴在一起时就会融合在一起不好看： 5. 碰撞检测实现 我们只要在每一轮判断一下玩家下一步移动的目标格子是不是合法的即可，如果不是两条蛇的身体或障碍物说明是合法的，同理需要在 GameMap 中判断而不能由玩家自己判断，这边需要注意如果在追蛇尾的话需要判断蛇尾是否有移动，如果没有移动则不合法： 然后在 Snake 中每次进入下一步时进行合法性判断，如果不合法则将状态更新为 die，且颜色变白： 6. 绘制蛇的眼睛 绘制眼睛时需要考虑蛇头的朝向，即上一步移动的方向，对于不同方向的眼睛偏移量打个表记录即可，完善后 Snake.js 的完整代码如下： 我们可以将 next_step() 中的 this.direction = -1; 还有 update_move() 中的 this.status = &quot;idle&quot;; 注释掉，并在 update_move() 中的 if (distance &lt; this.eps) 判断中添加一行 this.next_step();，这样就可以从回合制改为连续移动： "},{"title":"SpringBoot学习笔记-创建菜单与游戏页面（上）","date":"2023-11-11T11:19:00.000Z","url":"/posts/54295.html","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 本节内容为实现游戏界面的导航栏功能以及游戏地图的随机生成功能。 1. 页面创建与导航栏实现 首先我们解除一下 Vue 的文件驼峰命名检查，在前端的根目录 web 下的 package.json 文件的 rules 中添加一行代码，然后重启前端项目： 我们先在 src/views 目录下创建每个页面的组件存放的目录：pk、record、ranklist、user/mybots、error，分别表示对战页面、对战记录页面、排行榜页面、我的 Bots 页面、报错页面。然后在这些目录中分别创建对应的索引页面组件：PKIndexView.vue、RecordIndexView.vue、RanklistIndexView.vue、MyBotsIndexView.vue、NotFoundView.vue（之后组件称呼也可能不带 .vue 后缀），每个页面参照以下代码先初始化： 其中 Card 组件我们创建在 src/components 目录下，实现的效果是将内容包裹在一个卡片中，内容如下： 接下来我们需要设置路由，即根据不同的 URL 渲染不同的组件，Vue 的路由在 src/router 目录下的 index.js 文件中： 在 src/components 目录下创建导航栏组件 NavBar，然后可以通过 Bootstrap 快速构建一个导航栏。 首先在根 JS 文件 src/main.js 中引入 Bootstrap： 这时候会报错，提示我们没有找到模块 @popperjs/core，这时我们需要去项目管理页面的依赖页面中安装 @popperjs/core 依赖。 然后我们实现导航栏 NavBar，其中我们使用 &lt;router-link&gt; 标签替代 &lt;a&gt; 标签可以将后端渲染改成前端渲染： 2. 导航栏高亮功能 我们可以判断当前在哪个页面，将对应的导航栏图标高亮，高亮的实现是在对应的标签中添加 active 类。因此我们需要取得当前页面，可以用 vue-router 中的 useRoute 实现，然后通过 computed 实时计算其 name 是什么。在 HTML 标签中如果我们想让某个属性的值是一个表达式可以加一个 :，例如将 class 改成 :class： 3. 运动目标基类实现 看过 Django 或者 Web 笔记应该对这个概念不陌生了，游戏中运动的目标实现原理是利用浏览器每秒的刷新机制绘制若干帧图片，假设浏览器每秒钟刷新60次也就是60帧，那么 JS 的 requestAnimationFrame(func) 函数会在浏览器渲染下一帧前执行一遍 func 函数，然后我们在 func 函数内递归调用 requestAnimationFrame 函数即可实现每一帧执行一遍。 我们在 src/assets 目录下创建 scripts 目录存放脚本文件，然后在该目录下创建 AcGameObject.js 文件： 4. 游戏地图随机生成 我们设置一个正方形的游戏地图，最外围一圈为障碍物，中间的区域随机生成几个障碍物，因为需要具备公平性因此障碍物需要对称设计，左下角和右上角为两名玩家的起始位置。 地图和障碍物每帧都需要渲染（render）一遍，首先在 scripts 目录下创建 GameMap.js 表示游戏地图的对象，我们在设计的时候需要用相对距离，要能够根据网页的变化动态调整画布的大小，假设我们的地图是13*13个单位的大小，可以用 L 表示一个单位的长度： 在 PKIndexView 中需要存一个东西叫做游戏区域，我们也可以将其写成一个组件，在 components 目录下创建 PlayGround 组件： 由于游戏区域可能不仅只有地图，还可能有记分板之类的组件，因此我们再创建一个单独的地图组件 GameMap，地图是绘制在 Canvas 上的，可以使用 HTML 标签 &lt;canvas&gt; 实现，然后我们要引入之前创建的 GameMap.js 中的对象，在标签中使用 ref 属性可以让标签和我们定义的对象关联在一起，组件挂载完后我们需要创建游戏地图对象，可以用 onMounted 实现，表示组件挂载完后需要执行的操作： 这样在 GameMap.js 中我们就可以动态计算画布区域的大小了，我们需要求出 PlayGround 组件的区域中面积最大的正方形区域（该组件的长宽会随浏览器大小的变化而变化）： 现在我们要绘制地图的每一个格子，相邻的格子采用深浅不一样的绿色绘制，可以判断格子的横纵坐标相加的奇偶来区分： 我们创建障碍物对象，在 scripts 目录下创建 Wall.js： 接着我们在 GameMap.js 中即可将障碍物绘制出来，我们创建 Wall 对象是在创建完 GameMap 对象之后进行的，根据 AcGameObject 的定义，Wall 每次会在 GameMap 之后刷新，因此会覆盖 GameMap。此处我们还需要判断两名玩家的出生地即左下角和右上角不能被障碍物覆盖，且两名玩家必须要连通： 最后我们将网页的图标替换一下，将 LOGO 图标命名为 favicon.ico 然后替换掉 public 目录下的 favicon.ico 即可。然后修改一下网页的标题，如果 public 目录下的 index.html 中的标题为 &lt;title&gt;&lt;%= htmlWebpackPlugin.options.title %&gt;&lt;/title&gt;，那么可以通过修改根目录的 vue.config.js 来修改标题，添加 chainWebpack 配置： 修改后重启一下 Vue 即可。"},{"title":"SpringBoot学习笔记-项目初始化","date":"2023-11-10T14:58:00.000Z","url":"/posts/34589.html","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 本文记录 SpringBoot 的学习过程。 SpringBoot 官方文档：Spring Boot Reference Documentation。 1. 概念与项目介绍 本次开发的项目名称为 King of Bots，在本地采用 IDEA 开发，项目前后端分离，可以部署在不同的服务器上，前端使用 Vue 开发，后端使用 SpringBoot 开发。 用户通过客户端（Client）向服务器端（Server）发送一个 URL 请求，服务器端接收到请求后会向客户端返回一个 Web 页面（本质上是返回一个 HTML 字符串，浏览器会将这个字符串解析成网页）。 前后端分离是指客户端第一次访问项目时就从 Web 服务器端获取到所有静态文件，然后每次给后端发送请求时后端只返回数据，然后由前端根据数据进行渲染（动态拼接字符串）页面。 MVC，全称为 Model-View-Controller（模型-视图-控制器），是一种软件架构模式，其目标是将软件的用户界面（即前台页面）和业务逻辑分离，使代码具有更高的可扩展性、可复用性、可维护性以及灵活性。以下是对 MVC 各部分的详细解释： 模型（Model）：模型是应用程序中用于处理应用程序数据逻辑的部分。通常模型对象负责在数据库中存取数据。模型就是业务流程/状态的处理以及业务规则的制定。 视图（View）：视图是应用程序中处理数据显示的部分。通常视图是依据模型数据创建的。视图代表用户交互界面，对于 Web 应用来说，可以概括为 HTML 界面，但有可能为 XHTML、XML 和 Applet。 控制器（Controller）：控制器是应用程序中处理用户交互的部分。通常控制器负责从视图读取数据，控制用户输入，并向模型发送数据。控制可以理解为从用户接收请求，将模型与视图匹配在一起，共同完成用户的请求。 2. 创建SpringBoot项目后端 首先我们创建项目的主目录 king_of_bots，然后可以初始化一下 Git。 然后使用 IDEA 在 king_of_bots 目录下创建项目的后端，在新建项目的界面中可以在左侧看到 Spring Initializr，里面提供了一个官方网址：Spring Initializr，但是这个网站有时候可能不稳定连不上，如果连不上可以换成 ，但是可能版本会稍微旧一点。 项目配置内容中的组（Group）名设置为 com.kob，工件（Artifact）名设置为 backend，即表示我们项目的后端，使用 JDK 1.8 的版本，类型（Type）处选择 Maven 管理项目。点击下一步后选择 SpringBoot 版本为 2.7.X，如果用 SpringBoot 3 需要 Java 17，依赖选上 Web 中的 Spring Web 即可，然后可以再选上 Template Engines 中的 Thymeleaf（只是用于演示前后端不分离的写法）。最后点击创建即可。 第一次创建好项目后还需要一段时间安装相关的环境，src/main/java/com.kob.backend 中即可看到我们后端项目的入口文件 BackendApplication，运行后可以看到输出显示将服务启动到了本地的8080端口，这时我们访问 ，看到 Whitelabel Error Page 页面说明启动成功了。 3. 前后端不分离开发方式 我们之前说过客户端的一个 URL 请求一般就是对应后端的一个函数调用，我们在 com.kob.backend 包下新建一个 controller 包，用来存储我们所有的后端函数。 假设我们现在要创建一个对战页面，我们就在 controller 包中继续创建一个 pk 包，然后在该包下创建 IndexController.java 文件。如果需要将函数变成 URL 对应的函数需要加上 @Controller 注解，我们这个 Controller 的所有函数应该都在  链接下，因此我们可以加上 @RequestMapping 注解设置父目录： 如果我们的函数想要返回一个页面，需要把页面创建在 /src/main/resources/templates 目录下，我们先在该目录下创建 pk 目录，然后进去创建一个 index.html： 我们每个函数都可以返回一个 HTML 文件的路径，这个路径从 templates 目录后开始写，对于每个函数都可以指定 @RequestMapping 注解，例如 @RequestMapping(&quot;index/&quot;) 就表示访问  会调用这个函数，我们实现不加任何子目录的链接返回 index.html： 现在我们重启一下项目，然后访问  即可看到我们的页面。 假设我们有一张图片 logo.png 存放在 resources 目录下的 static/image 目录中，那么我们可以在 index.html 中使用（注意路径从 static 目录后开始写）： 4. 前后端分离开发方式 如果是前后端分离的开发方式，那么后端就不再是返回一个 HTML 页面了，而是返回一些数据。 在 pk 包下创建一个 BotInfoController 类，表示返回 Bot 信息，这时我们需要用的是 @RestController 注解： 现在访问链接  即可看到网页显示的字符串信息。 在 resources 目录下可以看到一个 application.properties 文件，如果是用 Spring 的默认网址创建的项目，这个文件内容是空的，如果是用阿里云的网址创建项目那么这个文件会自带一些内容，其中有个 server.port=8080 表示项目启动的端口号，我们可以修改这个端口号，防止和 Vue 的默认端口冲突。现在我们的该文件是空的，因此直接加一行 server.port=3000 即可。 5. 创建Vue项目前端 我们的前端使用 Vue 在 VS Code 中开发，Vue 的安装配置以及基本教程可见：Web 学习笔记-Vue3（环境配置、概念、整体布局设计）。 我们通过 Vue UI 在项目根目录下创建 Vue 前端项目，我们先创建 Web 端名为 web，创建好后记得装上插件 vue-router 和 vuex 以及依赖 bootstrap 和 jquery。然后再创建 AcApp 端名为 acapp，AcApp 端只需要安装一个 vuex 插件即可。 使用 VS Code 打开 web 目录，先将 src/router/index.js 中的 createWebHashHistory 改成 createWebHistory，这样网页的链接就可以不用加上 #。然后我们删去没用的代码，将 src/views 以及 src/components 目录下的文件删去，App.vue 改成以下内容： 然后将 src/router/index.js 改成以下内容： 现在运行一下 Web 端代码（在 Vue UI 中的 Tasks 选项卡中）即可成功运行。 6. 前后端通信 我们在 BotInfoController 中返回一个 Map 对象的信息： 然后我们在前端 App.vue 中使用 Ajax 向这个链接发起请求，获得数据后显示出来： 目前在 Vue3 中，与后端进行通信的主流方式是使用 Axios。Axios 是基于 Promise 的（JS 中的一个对象，用于处理异步操作的结果），可以解决 Ajax 嵌套问题，将原生的 Ajax 的嵌套转换为链式结构，解决了传统的回调地狱（在编写异步 JS 代码时，回调函数嵌套的现象越来越深，导致代码的可读性变差、维护性变差、复杂度增加、错误率增加等问题）。因此，你可以认为 Axios 是一种更先进的 Ajax 替代方案，在前后端项目中使用 Axios 来执行异步请求是一个很好的选择。 要使用 Axios 需要先安装 axios 依赖，然后按如下方式发送请求： 这时候我们打开浏览器的控制台，应该会看到出现了跨域问题：Access to XMLHttpRequest at '' ......，这是因为我们的前端在8080端口，而后端在3000端口，因此两个域名不一样导致跨域问题。 在后端的根包 com.kob.backend 下创建一个 config 包，然后在这个包中创建一个 CorsConfig 类，内容如下： 此时重启一下后端即可在前端接收数据并在网页中显示出来。 如果添加了以上代码无法解决跨域问题可以尝试改成以下代码： 最后我们再理一下运行流程，首先用户访问  进入前端的 Web 页面，这时候浏览器就会将 Vue 的所有前端静态文件缓存下来，然后浏览器就会执行到 Ajax/Axios 请求的代码，向后端发送一个请求，后端根据请求的地址  去查找 Controller 所匹配的路径，找到 getBotInfo() 方法后就会返回一个 Map 对象至前端，这个对象包含 name 和 rating 数据，浏览器接收到数据后就会将前端页面的 bot_name 和 bot_rating 更新成传过来的值。 我们在 web/src/assets 目录下创建 images 目录，弄一个背景图片放进去，然后在 App.vue 中添加背景： "},{"title":"Web学习笔记-Vue3（用户动态页面设计）","date":"2023-11-09T06:49:00.000Z","url":"/posts/55162.html","tags":[["Web","/tags/Web/"]],"categories":[["Web","/categories/Web/"]],"content":" 本文记录 Vue3 的学习过程，内容为用户动态页面设计。 1. 实现用户信息模块 用户动态页面可以划分为三个模块：用户信息部分、发动态部分（如果是自己才有发动态功能）、历史动态展示部分，因此我们可以用三个组件来实现（在 components 目录下创建 UserNewsInfo.vue、UserNewsSend.vue 以及 UserNewsPosts.vue 三个组件）。 其中 UserNewsInfo 内容如下： UserNewsPosts 内容如下： 然后我们在 UserNewsView 中先将这两个组件添加进去： 现在我们的网页是静态的，用户头像、名字这些内容应该都是参数，每个用户是不一样的，且用户发动态后需要在历史动态组件中展示，因此这几个组件都是需要数据交互的。 一般情况下我们会将数据存到需要交互的这几个组件的最顶层组件中，即 UserNewsView。我们可以使用 setup() 函数初始化变量，当前页面的用户一般是不会变的，因此可以定义一个 reactive，注意 reactive 只能接收对象。未来我们需要在模板中用到的属性都需要从 setup() 函数中 return 出去。在 UserNewsView 中传递数据给 UserNewsInfo： Vue 中传递数据的方式类似于 React，父组件通过 props 传递属性给子组件，子组件通过调用函数（事件）给父组件传递信息。可以使用 : 或 v-bind: 给子组件绑定属性，其后是一个表达式而非普通字符串。子组件需要将接受的数据放在 props 中，接受的 user 数据类型为 Object，且是必填的 required。 如果某个数据是动态被计算出来的可以使用 computed。在 UserNewsInfo 中接收并使用数据： 2. 实现关注用户功能 当没有关注当前用户时才会显示加关注的按钮，否则应该显示取消关注按钮。我们可以使用 v-if 属性判断某个表达式是否成立，即可以判断 user.is_followed 是否为 true 来动态显示关注和取关按钮。 还有就是当我们点击按钮时需要更改 user.is_followed 的状态，需要定义一个事件处理函数并绑定（使用 v-on:click 或 @click）到按钮的 click 属性上。 由于状态是从父组件 UserNewsView 中传过来的，因此不能直接在子组件中修改状态，而是需要在父组件中定义好函数并绑定（使用 @）到子组件上： 然后由子组件触发绑定的函数来修改父组件中的状态，其 API 为 context.emit()，setup() 函数中可以接收第二个参数 context： 3. 实现历史动态模块 我们先在 UserNewsView 中创建几条动态并传给 UserNewsPosts 子组件： 在子组件 UserNewsPosts 中我们就需要循环渲染出每条动态，可以使用 v-for 实现，注意循环时类似于 React，需要绑定一个不重复的 key 属性： 4. 实现发动态模块 Vue 可以使用 v-model 获取 &lt;textarea&gt; 的内容并绑定到某个变量上，我们先在父组件 UserNewsView 中导入子组件 UserNewsSend，然后创建发动态的函数传递给子组件： 最后是实现 UserNewsSend： 我们理一下执行过程，首先点击发送按钮后会触发绑定的 UserNewsSend 组件中的 handleSend 函数，该函数会调用父组件 UserNewsView 传递过来的 send 事件，参数是 content.value，父组件触发 send 事件后会调用其定义的 send 函数，这个函数会在父组件的 post 对象中添加数据，由于 post 对象是 reactive 类型的，因此当这个对象发生变化时，引用了该对象的组件就会重新渲染，即重新渲染 UserNewsPosts。"},{"title":"Web学习笔记-Vue3（环境配置、概念、整体布局设计）","date":"2023-11-08T10:59:00.000Z","url":"/posts/54619.html","tags":[["Web","/tags/Web/"]],"categories":[["Web","/categories/Web/"]],"content":" 本文记录 Vue3 的学习过程，内容为环境配置、概念、整体布局设计。 Vue 中文官网：Vue.js。 Vue.js 是一款用于构建用户界面的 JavaScript 框架。它基于标准 HTML、CSS 和 JavaScript 构建，并提供了一套声明式的、组件化的编程模型，帮助开发者高效地开发用户界面。无论是简单还是复杂的界面，Vue 都可以胜任。Vue 的设计非常注重灵活性和“可以被逐步集成”这个特点。根据你的需求场景，你可以用不同的方式使用Vue。 Vue 与 React 的区别主要在于： React 的思路是 HTML in JavaScript，也可以说是 All in JavaScript，通过 JavaScript 来生成 HTML，所以设计了 JSX 语法，还有通过 JS 来操作 CSS。 Vue 是把 HTML、CSS、JS 组合到一起，用各自的处理方式，Vue 有单文件组件，可以把 HTML、CSS、JS 写到一个文件中，HTML 提供了模板引擎来处理。 React 整体是函数式的思想，在 React 中是单向数据流，推崇结合 immutable 来实现数据不可变。 Vue 的思想是响应式的，也就是基于数据可变的，通过对每一个属性建立 Watcher 来监听，当属性变化的时候，响应式地更新对应的虚拟 DOM。 总的来说，React 的性能优化需要手动去做，而 Vue 的性能优化是自动的，但是 Vue 的响应式机制也有问题，就是当 state 特别多的时候，Watcher 会很多，会导致卡顿。所以大型应用（状态特别多的）一般用 React，更加可控。而考虑易用性方面，Vue 是更容易上手的，对于项目来说新人更容易接手。 1. 环境配置 首先需要安装 Node.js：NodeJS 的安装及配置。 打开 PowerShell，安装 @vue/cli： Vue 相比于 React 的其中一个好处是默认提供了一个图形化的项目管理界面，在想要创建项目的文件夹下以管理员身份打开终端输入以下命令： 然后我们创建一个名为 my_space 的项目，包管理器选择 npm，然后选上无新手指引的脚手架项目（Scaffold project without beginner instructions），预设选择 Vue3，然后创建项目。 项目创建好后在左侧能够看到导航栏，进入插件页面（Plugins）安装以下插件，其中 @vue/cli-plugin-router 用于多页面路由，@vue/cli-plugin-vuex 类似于 React 中的 Redux，可以让我们在多个组件之间维护同一个数据： 然后我们在依赖页面（Dependencies）安装 Bootstrap。 最后在任务页面（Tasks）的 serve 选项卡中可以运行项目，运行后在输出（Output）中可以看到网站链接：，访问该链接即可看到 Vue 的初始化页面。 我们用 VS Code 打开项目的根目录，源代码位于 src 目录下，其中的 views 目录类似于 Django 中的 Views，每一个页面是一个 View；router 目录是路由，打开可以看到默认有两个路由分别是 / 和 /about；components 目录用于存放各种组件（views 也能存放组件，根据个人习惯决定）；App.vue 为根组件，整个项目的入口在 main.js 文件中。 仔细看一下页面的链接会发现其中有一个 #，如果想去掉可以将 router 目录下 index.js 文件中的 createWebHashHistory 修改为 createWebHistory（有两处需要修改）。 在 main.js 文件中可以看到以下代码： 其中创建了我们的根组件 App，router 就表示路由，store 即 vuex，然后将其挂载到 #app 标签上，该标签可以在 public/index.html 中看到。 2. 使用 Vue 开发项目的基本概念 每一个 .vue 文件都会由三个部分组成：HTML、CSS、JS。其中 CSS 的标签可以加一个属性 scoped，这样不同组件之间的 CSS 选择器就不会相互影响到了： 我们的每个页面中都可能由多个组件组成，每个组件是一个 .vue 文件，可以用以下的方式引入 HelloWorld 组件，并传递 msg 属性给该组件： HelloWorld 组件需要 export 出去： 其中 props 中的属性可以在创建该组件的父组件中传递进来。 现在我们来具体了解一下 &lt;script&gt; 中的 export default 的参数： name：组件的名称。 components：存储 &lt;template&gt; 中用到的所有组件。 props：存储父组件传递给子组件的数据。 watch()：当某个数据发生变化时触发。 computed：动态计算某个数据。 setup(props, context)：初始化变量、函数。 ref：定义变量，可以用 .value 属性取值或者重新赋值，效率稍微比 reactive 低一些。 reactive：定义对象，不可重新赋值。 props：存储父组件传递过来的数据。 context.emit()：触发父组件绑定的函数。 然后是 &lt;template&gt; 中的内容： &lt;slot&gt;&lt;/slot&gt;：存放父组件传过来的 children。 v-on:click 或 @click 属性：绑定事件。 v-if、v-else、v-else-if 属性：判断。 v-for 属性：循环，注意需要用 :key 给循环的每个元素绑定唯一的 key。 v-bind: 或 :：绑定属性。 &lt;style&gt; 部分需要注意的就是添加 scope 属性后，不同组件间的 CSS 不会相互影响。 最后是第三方组件： vue-router：实现路由功能。 vuex：存储全局状态，全局唯一。 state：存储所有数据，可以用 modules 属性划分成若干模块。 getters：根据 state 中的值计算新的值。 mutations：所有对 state 的修改操作都需要定义在这里，不支持异步，可以通过 $store.commit() 触发。 actions：定义对 state 的复杂修改操作，例如使用 Ajax 向后端请求数据后再做修改操作，支持异步，可以通过 $store.dispatch() 触发。注意不能直接修改 state，只能通过 mutations 修改 state，actions 中函数的第一个参数为 context，通过 context.commit() 可以调用 mutations 中的操作。 modules：定义 state 的子模块。 这些概念先不用背，之后用到时再回来理解什么意思即可。 3. 导航栏 我们将整个页面分为导航栏（NavBar）和内容部分（Content），导航栏在每个页面都是固定不变的，变化的只有内容部分。我们将要实现首页、用户列表、用户动态、登录、注册以及404页面，每个页面我们都可以实现为一个组件。 在根 JS 文件 main.js 中引入 Bootstrap： 这时候会报错，提示我们没有找到模块 @popperjs/core，这时我们需要去项目管理页面的依赖页面中安装 @popperjs/core 依赖。 现在先把 HelloWorld.vue 删掉，然后把 HomeView.vue 中关于 HelloWorld 组件的内容删去，在 components 目录下创建 NavBar.vue，我们现在先不实现路由功能，直接去 Bootstrap 官网找一个导航栏复制过来： 然后我们在 App.vue 中把导航栏组件添加进来，并删去原本的内容： 4. 页面创建 首先我们用 Bootstrap 的 Card 先实现一个卡片组件 Card.vue： 注意，如果发现报错：Component name &quot;Card&quot; should always be multi-word vue/multi-word-component-names，说明文件名没有驼峰命名，可以在 package.json 文件的 rules 中添加一行代码，然后停止项目重新启动即可： 然后我们在 HomeView 中即可使用这个组件： 可以看到 Card 中使用 &#123;&#123; msg &#125;&#125; 在 HTML 标签中获取 prop 中 title 的值，该值由父组件 HomeView 传入。&lt;slot&gt;&lt;/slot&gt; 类似 React 中的 this.props.children。 现在我们可以根据 HomeView 创建其他页面：UserListView、UserNewsView、LoginView、RegisterView、NotFoundView。 创建好页面后我们需要实现路由，即根据地址来显示对应的组件。在 src/router/index.js 文件中修改： 最后我们在导航栏 NavBar 中实现地址的跳转，如果直接在 &lt;a&gt; 标签上写地址那么为后端渲染，即每次切换页面都需要访问一遍后端请求数据，我们可以用 &lt;router-link&gt; 实现前端渲染： 其中 :to 表示绑定 to 属性，参数是一个对象，其中的 name 属性表示名称，即之前在 router 中定义的 name。"},{"title":"Java字符串与输入输出详解","date":"2023-11-04T09:14:00.000Z","url":"/posts/62501.html","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 本文介绍了 Java 的反射机制，反射使我们摆脱了只能在编译时执行面向类型操作的限制，并且让我们能够编写一些非常强大的程序。 本文将讨论 Java 是如何在运行时发现对象和类的信息的，这通常有两种形式：简单反射，它假定你在编译时就已经知道了所有可用的类型；以及更复杂的反射，它允许我们在运行时发现和使用类的信息。 1. 为什么需要反射 面向对象编程的一个基本目标就是，让编写的代码只操纵基类的引用。我们来看下面这个例子： Shape 接口中的方法 draw() 是可以动态绑定的，因此客户程序员可以通过泛化的 Shape 引用来调用具体的 draw() 方法。在所有子类中，draw() 都被重写，并且因为它是一个动态绑定的方法，即使通过泛化的 Shape 引用来调用它，也会产生正确的行为，这就是多态。 基类里包含一个 draw() 方法，它通过将 this 传递给 System.out.println()，间接地使用了 toString() 方法来显示类的标识符（toString() 方法被声明为 abstract 的，这样就可以强制子类重写该方法，并防止没什么内容的 Shape 类被实例化）。 在此示例中，将一个 Shape 的子类对象放入 Stream&lt;Shape&gt; 时，会发生隐式的向上转型，在向上转型为 Shape 时，这个对象的确切类型信息就丢失了，对于流来说，它们只是 Shape 类的对象。 从技术上讲，Stream&lt;Shape&gt; 实际上将所有内容都当作 Object 保存。当一个元素被取出时，它会自动转回 Shape，这是反射最基本的形式，在运行时检查了所有的类型转换是否正确，这就是反射的意思：在运行时，确定对象的类型。 在这里，反射类型转换并不彻底：Object 只是被转换成了 Shape，而没有转换为最终的 Circle、Square 或 Triangle。这是因为我们所能得到的信息就是，Stream&lt;Shape&gt; 里保存的都是 Shape，在编译时，这是由 Stream 和 Java 泛型系统强制保证的，而在运行时，类型转换操作会确保这一点。 接下来就该多态上场了，Shape 对象实际上执行的代码，取决于引用是属于Circle、Square 还是 Triangle。一般来说，这是合理的：你希望自己的代码尽可能少地知道对象的确切类型信息，而只和这类对象的通用表示（在本例中为Shape）打交道。这样的话我们的代码就更易于编写、阅读和维护，并且设计也更易于实现、理解和更改。所以多态是面向对象编程的一个基本目标。 2. Class对象 要想了解 Java 中的反射是如何工作的，就必须先了解类型信息在运行时是如何表示的。这项工作是通过叫作 Class 对象的特殊对象来完成的，它包含了与类相关的信息。事实上，Class 对象被用来创建类的所有“常规”对象，Java 使用 Class 对象执行反射，即使是类型转换这样的操作也一样。Class 类还有许多其他使用反射的方式。 程序中的每个类都有一个 Class 对象，也就是说，每次编写并编译一个新类时，都会生成一个 Class 对象（并被相应地存储在同名的 .class 文件中）。为了生成这个对象，Java 虚拟机（JVM）使用被称为类加载器（class loader）的子系统。 类加载器子系统实际上可以包含一条类加载器链，但里面只会有一个原始类加载器，它是 JVM 实现的一部分。原始类加载器通常从本地磁盘加载所谓的可信类，包括 Java API 类。 类在首次使用时才会被动态加载到 JVM 中。当程序第一次引用该类的静态成员时，就会触发这个类的加载（构造器是类的一个静态方法，尽管没有明确使用 static 关键字）。因此，使用 new 操作符创建类的新对象也算作对该类静态成员的引用，构造器的初次使用会导致该类的加载。 所以，Java 程序在运行前并不会被完全加载，而是在必要时加载对应的部分，这与许多传统语言不同，这种动态加载能力使得 Java 可以支持很多行为。 类加载器首先检查是否加载了该类型的 Class 对象，如果没有，默认的类加载器会定位到具有该名称的 .class 文件（例如，某个附加类加载器可能会在数据库中查找对应的字节码）。当该类的字节数据被加载时，它们会被验证，以确保没有被损坏，并且不包含恶意的 Java 代码（这是 Java 的众多安全防线里的一条）。 一旦该类型的 Class 对象加载到内存中，它就会用于创建该类型的所有对象： 我们创建了三个具有静态代码块的类，该静态代码块会在第一次加载类时执行，输出的信息会告诉我们这个类是什么时候加载的。输出结果显示了 Class 对象仅在需要时才加载，并且静态代码块的初始化是在类加载时执行的。 所有的 Class 对象都属于 Class 类，Class 对象和其他对象一样，因此你可以获取并操作它的引用（这也是加载器所做的）。静态的 forName() 方法可以获得 Class 对象的引用，该方法接收了一个包含所需类的文本名称（注意拼写和大小写，且需要是类的完全限定名称，即包括包名称）的字符串，并返回了一个 Class 引用。 不管什么时候，只要在运行时用到类型信息，就必须首先获得相应的 Class 对象的引用，这时 Class.forName() 方法用起来就很方便了，因为不需要对应类型的对象就能获取 Class 引用。但是，如果已经有了一个你想要的类型的对象，就可以通过 getClass() 方法来获取 Class 引用，这个方法属于 Object 根类，它返回的 Class 引用表示了这个对象的实际类型。 Class 类有很多方法，下面是其中的一部分： printInfo() 方法使用 getName() 来生成完全限定的类名，使用 getSimpleName() 和 getCanonicalName() 分别生成不带包的名称和完全限定的名称，isInterface() 可以告诉你这个 Class 对象是否表示一个接口，getInterfaces() 方法返回了一个 Class 对象数组，它们表示所调用的 Class 对象的所有接口。还可以使用 getSuperclass() 来查询 Class 对象的直接基类，它将返回一个 Class 引用，而你可以对它做进一步查询。 Class 的 newInstance() 方法是实现虚拟构造器的一种途径，这相当于声明：我不知道你的确切类型，但无论如何你都要正确地创建自己。sc 只是一个 Class 引用，它在编译时没有更多的类型信息，当创建一个新实例时，你会得到一个 Object 引用，但该引用指向了一个 Toy 对象，你可以给它发送 Object 能接收的消息，但如果想要发送除此之外的其他消息，就必须进一步了解它，并进行某种类型转换。此外，使用 Class.newInstance() 创建的类必须有一个无参构造器。 注意，此示例中的 newInstance() 在 Java 8 中还是正常的，但在更高版本中已被弃用，Java 推荐使用 Constructor.newInstance() 来代替。 2.1 类字面量 Java 还提供了另一种方式来生成 Class 对象的引用：类字面量。它看起来像这样： 这更简单也更安全，因为它会进行编译时检查（因此不必放在 try 块中），另外它还消除了对 forName() 方法的调用，所以效率也更高。 注意，使用 .class 的形式创建 Class 对象的引用时，该 Class 对象不会自动初始化。实际上，在使用一个类之前，需要先执行以下三个步骤： 加载：这是由类加载器执行的，该步骤会先找到字节码（通常在类路径中的磁盘上，但也不一定），然后从这些字节码中创建一个 Class 对象。 链接：链接阶段会验证类中的字节码，为静态字段分配存储空间，并在必要时解析该类对其他类的所有引用。 初始化：如果有基类的话，会先初始化基类，执行静态初始化器和静态初始化块。 其中，初始化会被延迟到首次引用静态方法（构造器是隐式静态的）或非常量静态字段时： 仅使用 .class 语法来获取对类的引用不会导致初始化，而 Class.forName() 会立即初始化类以产生 Class 引用。如果一个 static final 字段的值是编译时常量，比如 A.STATIC_FINAL，那么这个值不需要初始化 A 类就能读取。 2.2 泛型类的引用 Class 引用指向的是一个 Class 对象，该对象可以生成类的实例，并包含了这些实例所有方法的代码，它还包含该类的静态字段和静态方法，所以一个 Class 引用表示的就是它所指向的确切类型：Class 类的一个对象。 我们可以使用泛型语法来限制 Class 引用的类型： 泛化的类引用 c2 只能分配给其声明的类型，通过使用泛型语法，可以让编译器强制执行额外的类型检查。 如果想放松使用泛化的 Class 引用时的限制，需要使用通配符 ?，它是 Java 泛型的一部分，表示任何事物： 我们不能这么写： 即使 Integer 继承自 Number，但是 Integer 的 Class 对象不是 Number 的 Class 对象的子类。 如果想创建一个 Class引用，并将其限制为某个类型或任意子类型，可以将通配符与 extends 关键字组合来创建一个界限： 将泛型语法添加到 Class 引用的一个原因是提供编译时的类型检查，这样的话，如果你做错了什么，那么很快就能发现。 下面是一个使用了泛型类语法的示例，它存储了一个类引用，然后使用 newInstance() 来生成对象： DynamicSupplier 会强制要求它使用的任何类型都有一个 public 的无参构造器，如果不符合条件，就会抛出一个异常。在上面的例子中，People 类自动生成的无参构造器不是 public 的，因为 People 类不是 public 的，所以我们必须显式定义它。 对 Class 对象使用泛型语法时，newInstance() 会返回对象的确切类型，而不仅仅是简单的 Object，但它也会受到一些限制： 如果你得到了 Kitty 的基类，那么编译器只允许你声明这个基类引用是 Kitty 的某个基类，即 Class&lt;? super Kitty&gt;，而不能被声明成 Class&lt;Cat&gt;，因为 getSuperclass() 返回了基类（不是接口），而编译器在编译时就知道这个基类是什么，在这里就是 Cat.class，而不仅仅是 Kitty 的某个基类。因为存在这种模糊性，所以 kittySuper.getConstructor().newInstance() 的返回值不是一个确切的类型，而只是一个 Object。 2.3 cast()方法 cast() 方法是用于 Class 引用的类型转换： cast() 方法接收参数对象并将其转换为 Class 引用的类型，在你不能使用普通类型转换（最后一行）的情况下很有用，如果你正在编写泛型代码并且存储了一个用于转型的 Class 引用，就可能会遇到这种情况，不过这很罕见。"},{"title":"Java字符串与输入输出详解","date":"2023-11-02T08:09:00.000Z","url":"/posts/62502.html","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 本文介绍了 Java 中的字符串类以及输入输出的方式。 1. 不可变字符串String String 类的对象是不可变的，该类中每个看起来似乎会修改 String 值的方法，实际上都创建并返回了一个全新的 String 对象，该对象包含了修改的内容。 当 s 被传递给 upcase() 时，实际上传递的是 s 对象引用的一个副本，此引用所指向的对象只存在于单一的物理位置中，在传递时被复制的只是引用。 在 upcase() 里，参数 s 只存活于这个方法的方法体里，当 upcase() 运行完成后，局部引用 s 就会消失。upcase() 返回执行的结果：一个指向新字符串的引用。 2. 重载加法运算符与StringBuilder 不变性可能会带来效率问题，一个典型的例子是操作符 +，它针对 String 对象做了重载（String 的 + 和 += 是 Java 中仅有的被重载的操作符，Java 不允许程序员重载其他操作符，这与 C++ 不同）。操作符重载意味着在与特定类一起使用时，相应的操作具有额外的意义。 假设我们有以下代码： 我们可以使用 JDK 自带的 javap 工具反编译上述代码： 也可以在 IDEA 中编译代码后通过 View -&gt; Show Bytecode 选项查看，内容如下： 像 DUP 和 INVOKEVIRTUAL 这样的语句相当于 JVM 的汇编语言。这边不用看懂是什么意思，需要重点注意的是编译器对 java.lang.StringBuilder 类的引入，我们的代码中没有用到 StringBuilder，但编译器还是决定使用它，因为它的效率更高。 在这里，编译器创建了一个 StringBuilder 对象来构建字符串，并为每个字符串调用了一次 append()，总共两次。最后，它调用了 toString() 来生成结果，并将其存为 t（使用 ASTORE 2 语句实现）。 你或许会认为可以随意使用 String，反正编译器会对字符串的使用进行优化。当创建 toString() 方法时，如果操作很简单，通常可以依赖编译器，让它以合理的方式自行构建结果。但是如果涉及循环，并且对性能也有一定要求，那就需要显式使用 StringBuilder 了。来看一个例子： 在 getString1() 中，最终的结果是用 append() 语句对每一部分分别进行拼接而成的，如果你想走捷径，执行诸如 append(rand.nextInt(10) + &quot;, &quot;) 之类的操作，编译器就会介入，并开始创建更多的 StringBuilder 对象，影响性能。 getString2() 使用了 Stream，生成的代码更加赏心悦目。实际上，Collectors.joining() 内部使用 StringBuilder 实现，所以使用这种方式不会有任何性能损失。 StringBuilder 是在 Java 5 中引入的，在此之前，Java 使用 StringBuffer，它是线程安全的，因此成本也更高，使用 StringBuilder 进行字符串操作会更快。 3. 无意识的递归 和其他类一样，Java 的标准集合最终也是从 Object 继承而来的，所以它们也包含了一个 toString() 方法，这个方法在集合中被重写，这样它生成的结果字符串就能表示容器自身，以及该容器持有的所有对象。以 ArrayList.toString() 为例，它会遍历 ArrayList 的元素并为每个元素调用 toString() 方法： 如果你希望 toString() 打印对象的内存地址，不能使用 this： 这样会得到一个很长的异常栈，因为编译器看到 String 后面加上了一个非 String 的内容，会尝试调用 toString() 方法自动转换，而这样就会递归调用自身的 toString()。因此我们需要直接调用 Object 的 toString() 方法： 4. 格式化输出 4.1 printf()与format() 有 C 语言基础的肯定对 printf() 很熟悉，printf() 使用特殊的占位符来表示数据的位置。Java 5 引入的 format() 方法可用于 PrintStream 或 PrintWriter 对象，因此也可以直接用于 System.out。 format() 和 printf() 是等价的，它们都只需要一个格式化字符串，后面跟着参数，其中每个参数都对应一个格式说明符。String 类也有一个静态的 format() 方法，它会产生一个格式化字符串，之后我们会看到。 4.2 Formatter类 Java 中所有的格式化功能都由 java.util 包里的 Formatter 类处理，你可以将 Formatter 视为一个转换器，将格式化字符串和数据转换为想要的结果。当创建一个 Formatter 对象时，你可以将信息传递给构造器，来表明希望将结果输出到哪里： 4.3 格式说明符 如果想要在插入数据时控制间距和对齐方式，你需要更详细的格式说明符。一般我们会控制字段的最小长度，或者控制浮点数的小数位数，设置长度后默认是右对齐的，可以使用 - 来设置成左对齐，例如： 我们来看一下简单的示例，它使用了生成器模式，你可以创建一个起始对象，然后向其中添加内容，最后使用 build() 方法来生成最终结果： 将 StringBuilder 传递给 Formatter 构造器后，它就有了一个构建 String 的地方，还可以使用构造器参数将其发送到标准输出甚至文件里。 注意：如果格式化输出参数 %b 用于输出非 boolean 基本类型或 Boolean 对象，那么只要参数类型不是 null，格式化结果总是 true，即使是数值0输出也为 true，这和 C 语言是不一样的。 4.4 String.format() Java 5 还借鉴了 C 语言中用来创建字符串的 sprintf()，提供了 String.format() 方法。它是一个静态方法，参数与 Formatter 类的 format() 方法完全相同，但返回一个 String： 5. 扫描输入 到目前为止，从人类可读的文件或标准输入中读取数据还是比较痛苦的，一般的解决方案是读入一行文本，对其进行分词解析，然后使用 Integer、Double 等类里的各种方法来解析数据： BufferedReader 有一个 readLine() 方法，每次可以从输入对象（此处是标准输入的 InputStreamReader）里读取一行，readLine() 方法将读入的每一行转为 String 对象，我们使用 split() 方法用空格将这个字符串分割成一个 String[]，然后使用 Stream 将数组中的每个字符串转换成整型后重新构建成一个整型数组。 Java 5 中添加的 Scanner 类大大减轻了扫描输入的负担： Scanner 的构造器可以接受任何类型的输入对象，包括 File 对象、InputStream、String，或者 Readable 接口。 在 Scanner 中，输入、分词和解析这些操作都被包含在各种不同类型的 next() 方法中。一个普通的 next() 返回下一个 String，所有的基本类型（Char 除外）以及 BigDecimal 和 BigInteger 都有对应的 next() 方法。所有的 next() 方法都是阻塞的，这意味着它们只有在输入流能提供一个完整可用的数据分词时才会返回。你也可以根据相应的 hasNext() 方法是否返回 true 来判断下一个输入分词的类型是否正确。 Scanner 会假设 IOException 表示输入结束，因此 Scanner 会把 IOException 隐藏起来。 5.1 Scanner分隔符 默认情况下，Scanner 通过空格分割输入数据，但也可以用正则表达式的形式来指定： 此示例使用逗号（由任意数量的空白符包围）作为分隔符，来处理读取的给定字符串，同样的技术也可以用来读取逗号分隔的文件： 其中 numbers.txt 文件内容如下： 5.2 使用正则表达式扫描 除了扫描预定义的基本类型，你还可以用自己定义的正则表达式模式来扫描，这在扫描更复杂的数据时非常有用。下面这个示例扫描防火墙日志中的威胁数据： next() 与特定模式一起使用时，该模式会和下一个输入分词进行匹配，结果由 match() 方法提供。"},{"title":"Java文件操作详解","date":"2023-11-01T14:22:00.000Z","url":"/posts/29756.html","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 本文介绍了 Java 处理文件与目录的方式。 1. 文件和目录路径 Path 对象代表的是一个文件或目录的路径，它是在不同的操作系统和文件系统之上的抽象。它的目的是，在构建路径时，我们不必注意底层的操作系统，我们的代码不需要重写就能在不同的操作系统上工作。 java.nio.file.Paths 类包含了重载的 static get() 方法，可以接受一个 String 序列，或一个统一资源标识符（Uniform Resource Identifier，URI），然后将其转化为一个 Path 对象： 虽然 toString() 生成的是路径的完整表示，但是可以看到 getFileName() 总是会生成文件的名字。使用 Files 工具类（后面会看到更多），我们可以测试文件是否存在，是否为“普通”文件，是否为目录等等。 在这里，我们看到了文件的 URI 是什么样子的，但 URI 可以用来描述大多数事物，不仅限于文件，然后我们成功地将 URI 转回到了一个 Path 之中。 1.1 获取Path的片段 在 getNameCount() 界定的上限之内，我们可以结合索引使用 getName()，得到一个 Path 的各个部分。Path 也可以生成 Iterator，所以我们也可以使用 for-in 来遍历。 注意，尽管这里的路径确实是以 .txt 结尾的，但 endsWith() 的结果是 false。这是因为该方法比较的是整个路径组件（即用 / 分开的每个整体部分，如 test.txt），而不是名字中的一个字串。 还可以看到我们在对 Path 进行遍历时，并没有包含根目录，只有当我们用根目录来检查 startsWith() 时才会得到 true。 1.2 获取Path信息 Files 工具类中包含了一整套用于检查 Path 的各种信息的方法： 1.3 添加或删除路径片段 我们必须能够通过对自己的 Path 对象添加和删除某些路径片段来构建 Path 对象。若想去掉某个基准路径可以使用 relativize()，想在一个 Path 对象的后面添加路径片段可以使用 resolve()： 只有 Path 为绝对路径时，才能将其用作 relativize() 方法的参数。此外还增加了对 toRealPath() 的进一步测试，除了路径不存在的情况下会抛出 IOException 异常，它总是会对 Path 进行扩展和规范化。 2. 文件系统 为了完整起见，我们需要一种方式来找出文件系统的其他信息。在这里，我们可以使用静态的 FileSystems 工具来获得默认的文件系统，也可以在一个 Path 对象上调用 getFileSystem() 来获得创建这个路径对象的文件系统。我们可以通过给定的 URI 获得一个文件系统，也可以构建一个新的文件系统（如果操作系统支持的话）。 3. 查找文件 java.nio.file 中有一个用于查找文件的类 PathMatcher，可以通过在 FileSystem 对象上调用 getPathMatcher() 来获得一个 PathMatcher 对象，并传入我们感兴趣的查找模式。模式有两个选项：glob 和 regex。glob 更简单，但实际上非常强大，可以解决很多问题，如果问题更为复杂，可以使用 regex。 在 matcher1 中，glob 表达式开头的 **/ 表示所有子目录，如果你想匹配的不仅仅是以基准目录为结尾的 Path，那么它是必不可少的，因为它匹配的是完整路径，直到找到想要的结果。单个的 * 表示任何东西，后面的花括号表示的是一系列的可能性，即查找任何以 .txt 或 .tmp 结尾的东西。 注意在 matcher2 中我们只匹配 *.txt，而我们的 filePath 并不在基准目录下，因此需要先用 map() 将文件的完整路径改为只剩最后一段 Path 片段，即 xxx.txt 这种形式，这样才能匹配上。 4. 读写文件 目前为止，我们可以做的只是对路径和目录的操作，现在来看看如何操作文件本身的内容。 java.nio.file.Files 类包含了方便读写文本文件和二进制文件的工具函数。Files.readAllLines() 可以一次性读入整个文件，生成一个 List&lt;String&gt;： Files.write() 也被重载了，可以将 byte 数组或任何实现了 Iterable 接口的类的对象写入文件： 现在我们读取文件时都是将文件全部一次性读入，这可能会有以下的问题： 这个文件非常大，如果一次性读取整个文件，可能会耗尽内存。 我们只需要文件的一部分就能得到想要的结果，所以读取整个文件是在浪费时间。 Files.lines() 可以很方便地将一个文件变为一个由行组成的 Stream： 如果把文件当作一个由行组成的输入流来处理，那么 Files.lines() 非常有用，但是如果我们想在一个流中完成读取、处理和写入，那该怎么办呢？ 因为我们是在同一个块中执行的所有操作，所以两个文件可以在相同的 try-with-resources 块中打开。PrintWriter 是一个旧式的 java.io 类，允许我们“打印”到—个文件，所以它是这个应用的理想选择。"},{"title":"Java异常（Exception）详解","date":"2023-10-31T07:51:00.000Z","url":"/posts/10258.html","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 本文介绍了 Java 的异常处理机制，Java 使用异常（Exception）提供了一个一致的错误报告模型，从而使组件可以将问题可靠地传达给客户代码。 改进错误恢复机制是增加代码稳健性的最强有力的方法之一。捕捉错误的理想时机是在编译时，也就是在你试图运行程序之前。然而，并不是所有的错误都能在编译时发现。其他问题必须在运行时通过某种正规手段来处理，这种手段应该支持这个错误的源头将适当的信息传递给知道如何正确处理该难题的某个接收者。 1. 异常处理程序 如果我们正处于一个方法之中，并抛出了异常（或者在该方法中调用的另一个方法抛出了异常），该方法将在抛出异常的过程中退出，如果不希望退出，可以在其中设置一个特殊的块来捕捉这个异常。因为要在这里“尝试”各种方法调用，所以它称为 try 块。 被抛出的异常总是要在某个地方结束，这个地方就是异常处理程序，我们可以为每种异常类型编写一个。异常处理程序紧跟在 try 块之后，用关键字 catch 来表示： 每个 catch 子句（异常处理程序）就像一个小方法，接受且只接受一个特定类型的参数。标识符（e1、e2 等）可以在处理程序中使用，就像方法参数一样。有时候我们从不使用这个标识符，因为异常的类型已经为处理该异常提供了足够多的信息，但是这个标识符必须放在这里。 异常处理程序必须紧跟在 try 块的后面，如果一个异常被抛出，异常处理机制会去查找参数与异常类型相匹配的第一个处理程序，然后进入 catch 子句，我们就认为这个异常被处理了。一旦 catch 子句完成，对异常处理程序的搜索就会停止，只有匹配的 catch 子句才会执行，它不像 switch 语句那样每个 case 之后都需要一个 break，以避免执行其余的 case。 2. 创建异常与日志记录 2.1 自定义异常 要创建自己的异常类，可以继承现有的异常类，异常类型的基类是 Exception，我们可以直接使用无参构造器，也可以创建一个接受 String 参数的构造器： 2.2 日志记录 我们可以使用 java.util.logging 工具将输出记录到日志中，常见的情况是捕捉别人的异常，并将其记录到日志中，所以我们必须在异常处理程序中生成日志信息： 3. 异常说明 Java 鼓励人们将其方法中可能会抛出的异常告知调用该方法的客户程序员。这就是异常说明（exception specification）。它是方法声明的组成部分，出现在参数列表之后。 异常规范使用了一个额外的关键字 throws，后面跟着所有可能被抛出的异常的列表，所以我们的方法定义看起来就像下面这样： 如果没有 throws 参数，意味着这个方法不会抛出异常（除了从 RuntimeException 继承而来的异常，这样的异常可以从任何地方抛出而不需要异常说明，后面会介绍）。 异常说明必须和实际情况匹配，如果方法中的代码引发了异常，但是这个方法并没有处理，编译器就会检测到并提醒我们：要么处理这个异常，要么用异常说明指出这个异常可能会从该方法中抛出。 这种在编译时被检查并强制实施的异常叫作检查型异常（checked exception）。 4. 捕捉任何异常 通过捕捉异常类型的基类 Exception，可以创建一个能捕捉任何类型异常的处理程序。 这会捕捉任何异常，所以如果使用它的话，请把它放在处理程序列表的最后，以避免它抢在其他任何异常处理程序之前捕获了异常。 4.1 Exception类的方法 Exception 类是所有对程序员很重要的异常类的基类，所以通过它我们不会得到关于异常的很多具体信息，但是我们可以调用来自其基类 Throwable 的方法： 可以发现每个方法都比前一个方法提供了更多信息，实际上每个方法都是前一个方法的超集。 4.2 多重捕捉 如果我们想以同样的方式处理一组异常，并且它们有一个共同的基类，那么直接捕捉这个基类即可。但是如果它们没有共同的基类，在 Java 7 之前，必须为每一个异常写一个 catch 子句。 利用 Java 7 提供的多重捕捉（multi-catch）处理程序，我们可以在一个 catch 子句中用 | 操作符把不同类型的异常连接起来： 4.3 栈轨迹 printStackTrace() 提供的信息也可以使用 getStackTrace() 直接访问，这个方法会返回一个由栈轨迹元素组成的数组，每个元素表示一个栈帧。第一个元素是栈顶，即序列中的最后一个方法调用（这个 Throwable 被创建和抛出的位置）： 4.4 重新抛出异常 有时我们要重新抛出刚捕获的异常，特别是当使用 Exception 来捕捉任何异常的时候我们已经有指向当前异常的引用，所以可以重新抛出它： 如果重新抛出当前的异常，在 printStackTrace() 中打印的关于异常的信息，仍将是原来的异常抛出点的信息，而不是重新抛出异常的地方的信息，要加入新的栈轨迹信息可以调用 fillInStackTrace()，它会返回一个 Throwable 对象，这个对象是它通过将当前栈的信息塞到原来的异常对象中而创建的： 可以看到在 main() 中输出异常的栈轨迹时没有 f() 的信息。fillInStackTrace() 被调用的那一行，成为这个异常的新起点。 重新抛出一个与所捕获的异常不同的异常也是可以的，这样做会得到与使用 fillInStackTrace() 类似的效果，关于这个异常的原始调用点的信息会丢失，剩下的是与新的 throw 有关的信息。 5. 使用finally块执行清理 往往会出现这样的情况：不管 try 块中是否抛出异常，都有一段代码必须执行，这通常是内存恢复之外的操作，因为内存恢复操作由垃圾收集器处理。 我们可以在所有异常处理程序的末尾使用一个 finally 子句，所以异常处理的全貌就是这样的： 我们来看一下下面这个例子： 从输出可以看出，无论是否抛出异常，finally 子句都执行了。还可以看出另一事实：Java 中的异常不允许我们回退到异常被抛出的地方。 5.1 finally的作用 在没有垃圾收集并且不会自动调用析构函数的语言中，finally 非常重要，这是因为不管在 try 块中发生了什么，它都使得程序员可以确保内存的释放。但是 Java 提供了垃圾收集器，且无需调用析构函数，那么 finally 在 Java 中什么时候需要用到？ 要清理内存之外的某些东西时，finally 子句是必要的，例如打开的文件或网络连接，画在屏幕上的东西等。 5.2 不同异常处理层及方法返回时的finally执行机制 即使抛出的异常没有被当前的这组 catch 子句捕获，在异常处理机制向更高一层中继续搜索异常处理程序之前，finally 也会执行： 因为 finally 子句总会执行，所以在一个方法中，我们可以从多个点返回，并且仍然能够确保重要的清理工作得到执行： 输出表明，方法从哪里返回并不重要，finally 子句中的内容总会运行。 6. try-with-resources语句 有时有些对象会出现如下的情况： 需要清理； 需要在特定时刻清理，例如当走出某个作用域的时候（通过正常方式或通过异常）。 一个常见的例子是 java.io.FileInputStream，传统方式下我们需要编写棘手的代码： Java 7 引入了 try-with-resources 语法，可以很好地简化上述代码： try 后面可以跟一个括号定义，我们在这里创建了 FileInputstream 对象。括号中的内容叫作资源说明头（resource specification header）。现在对象 in 在这个 try 块的其余部分都是可用的。更重要的是，不管如何退出 try 块（无论是正常方式还是通过异常），都会执行与上一个示例中的 finally 子句等同的操作，无需编写复杂棘手的代码了。 它是如何工作的呢？在 try-with-resources 定义子句中（也就是括号内）创建的对象必须实现 java.lang.AutoCloseable 接口，该接口只有一个方法：close()。 资源说明头可以包含多个定义，用分号隔开，在这个头部定义的每个对象都将在 try 块的末尾调用其 close()。 6.1 底层细节 为了研究 try-with-resources 的底层机制，可以创建自己的实现了 AutoCloseable 接口的类： 在退出 try 块时会调用两个对象的 close() 方法，而且会以与创建顺序相反的顺序关闭他们。这个顺序很重要，因为在这种配置情况下，SecondReporter 对象有可能会依赖 FirstReporter 对象。 6.2 构造器抛出异常 如果在资源说明头创建某个对象时其构造器抛出异常，结果会怎样？ 我们在资源说明头定义了三个对象，中间的对象抛出了一个异常，正因为如此，编译器强制我们提供一个 catch 子句来捕捉构造器的异常，这意味着资源说明头实际上是被这个 try 块包围的。 不出所料，FirstReporter 顺利创建，而 ExcepReporter 在创建过程中抛出了一个异常。清注意 ExcepReporter 的 close() 方法没有被调用，这是因为如果构造器失败了，我们不能假定可以在这个对象上安全地执行任何操作，包括关闭它在内。因为 ExcepReporter 抛出了异常，所以 SecondReporter 对象从未被创建，也不会被清理。 6.3 try块抛出异常 如果构造器都不会抛出异常，但是在 try 块中可能抛出异常，编译器又会强制我们提供一个 catch 子句： 注意，SecondReporter 对象永远不会得到清理，这是因为它不是在资源说明头中创建的，所以它的清理得不到保证。这一点很重要，因为 Java 在这里没有以警告或错误的形式给出提示，所以像这样的错误很容易被漏掉。 6.4 close()方法抛出异常 最后，让我们看看在 close() 方法中抛出异常的情况： 请注意，因为这三个对象都被创建出来了，所以它们又都以相反的顺序被关闭了，即使 ExcepReporter.close() 抛出了异常。当我们考虑到这一点时，这就是我们希望发生的事情，但是如果必须自己编程实现所有的逻辑，代码可能会出现漏洞，从而导致出错。因此我们应该尽可能地使用 try-with-resources，这个特性还能使生成的代码更干净且更容易理解。"},{"title":"Java流（Stream、Optional）详解","date":"2023-10-28T09:23:00.000Z","url":"/posts/36775.html","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 本文介绍了 Java8 的一个十分强大的功能：流（Stream），能够优化传统实现方式中复杂冗余的代码。 1. Java8对流的支持 流是一个与任何特定的存储机制都没有关系的元素序列。不同于在集合中遍历元素，使用流的时候，我们是从一个管道中抽取元素，并对它们逬行操作。这些管道通常会被串联到一起，形成这个流上的一个操作管线。 流的一个核心优点是，它们能使我们的程序更小，也更好理解。当配合流使用时，Lambda 表达式和方法引用就发挥出其威力了。流大大提升了 Java 8 的吸引力。 假设我们想按照有序方式显示随机选择的5~20范围内的，不重复的整数，借助流我们只需要说明想做什么即可实现： 我们先为 Random 对象设置一个随机种子，ints() 方法会生成一个流，该方法有多个重载版本，其中两个参数的版本可以设置所生成值的上下界。使用中间流操作 distinct() 去掉重复的值，再使用 limit() 选择前5个值，sorted() 表示元素是有序的，最后我们想显示每一个条目，所以使用了 forEach()，它会根据我们传递的函数，在每个流对象上执行一个操作。这里我们传递了一个方法引用 System.out::println，用于将每个条目显示在控制台上。 使用流实现的代码我们看不到任何显式的迭代机制，因此称为内部迭代，这是流编程的一个核心特性。 2. 流的创建 使用 Stream.of()，可以轻松地将一组条目变成一个流： 此外，每个 Collection 都可以使用 stream() 方法来生成一个流： 在创建了一个 List&lt;A&gt; 之后，只需要调用一下 stream() 这个所有集合类都有的方法。中间的 map() 操作接受流中的毎个元素，在其上应用一个操作来创建一个新的元素，然后将这个新元素沿着流继续传递下去，这里的 mapToInt() 将一个对象流转变成了一个包含 Integer 的 IntStream。对于 Float 和 Double 也有名字类似的操作。 collect() 操作会根据其参数将所有的流元素组合起来，当我们使用 Collectors.joining() 时，得到的结果是一个 String，每个元素都会以 joining() 的参数分隔，还有其他很多 Collectors，可以生成不同的结果。 为了从 Map 集合生成一个流，我们首先调用 entrySet() 来生成一个对象流，其中每个对象都包含着一个键和与其关联的值，然后再使用 getKey() 和 getValue() 将其分开。 2.1 随机数流 Ramdom 类在 Java 8 引入了流，有一组可以生成流的方法： 为消除冗余代码，上面的示例创建了泛型方法 show(Stream&lt;T&gt; stream)，这个特性在之后会讲。类型参数 T 可以是任何东西，所以使用 Integer、Long 和 Double 都可以。然而，Random 类只会生成 int、long 和 double 等基本类型的值。幸运的是，boxed() 流操作会自动将基本类型转换为其对应的包装器类型，使得 show() 能够接受这个流。 2.2 整型的区间范围 IntStream 类提供了一个 range() 方法，可以生成一个流（由 int 值组成的序列），在编写循环时非常方便： 现在我们编写一个 repeat() 工具函数取代简单的 for 循环： 2.3 Stream.generate() Stream.generate() 可以接受任何的 Supplier&lt;T&gt;（java.util.function 中的接口），并生成一个由 T 类型的对象组成的流。如果想创建一个由完全相同的对象组成的流，只需要将一个生成这些对象的 Lambda 表达式传给 generate()： 2.4 Stream.iterate() Stream.iterate() 从一个种子开始（第一个参数），然后将其传给第二个参数所引用的方法，其结果被添加到这个流上，并且保存下来作为下一次 iterate() 调用的第一个参数，以此类推。我们可以通过迭代生成一个之前实现过的斐波那契数列： iterate() 只会记住结果，所以必须使用 x 来记住另一个元素。我们使用了 skip() 操作，这个之前没有介绍过，它会直接丢弃由其参数指定的相应数目的流元素，这里丢弃了前10个。 2.5 流生成器 在生成器（Builder）设计模式中，我们创建一个生成器对象，为它提供多段构造信息，最后执行生成（build）动作。 Stream 库提供了这样一个 Builder，假设我们实现读取文件的每一行并将其转换为单词流： 其中的文本内容如下： 构造器添加了文件中的所有单词，但是它没有调用 build()，这意味着只要不调用 build() 就可以继续向 Builder 对象中添加单词。如果在调用 build() 之后还尝试向 Stream.Builder 中添加单词，则会产生异常。 2.6 Arrays Arrays 类中包含了名为 stream() 的静态方法，可以将数组转换为流。可以是任何对象数组，也可以是 int、long 和 double 基本类型，生成 IntStream、LongStream 和 DoubleStream： 最后一次调用 stream() 时使用了两个额外的参数，第一个表示从数组的哪个位置开始选择元素，第二个表示停止位置（开区间），即在本例中选择数组中 [1, 4) 的元素。 3. 中间操作 3.1 对流元素进行排序 使用 sorted() 可以进行默认排序（从小到大），该方法也可以接受一个 Comparator 参数，该参数可以传入一个 Lambda 表达式也可以使用预定义好的 Comparator： 3.2 移除元素 distinct()：移除流中的重复元素。 filter(Predicate)：过滤只保留符合特定条件的元素，即满足过滤函数 Predicate 为 true 的流元素。 来看一个筛选素数的例子： rangeClosed() 包含了上界值，如果流中的元素没有任何一个取余操作的结果为0，则 noneMatch() 操作返回 true，如果有任何一个计算结果等于0，则返回 false。noneMatch() 会在第一次失败后退出，而不会把后面的所有计算都尝试一遍。 4. Optional类型 在研究终结操作之前，我们必须考虑一个问题：如果我们向流中请求对象，但是流中什么都没有，这时会发生什么呢？有没有某种我们可以使用的对象，既可以作为流元素来占位，也可以在我们要找的元素不存在时友好地告知我们（也就是说，不会抛出异常）。 这个想法被实现为 Optional 类型，某些标准的流操作会返回 Optional 对象，因为它们不能确保所要的结果一定存在，这些流操作列举如下： findFirst()：返回包含第一个元素的 Optional。如果这个流为空，则返回 Optional.empty。 findAny()：返回包含任何元素的 Optional。如果这个流为空，则返回 Optional.empty。 max() 和 min() 分别返回包含流中最大值或最小值的 Optional，如果这个流为空，则返回 Optional.empty。 reduce() 的一个版本，它并不以一个 identity 对象作为其第一个参数（在 reduce() 的其他版本中，identity 对象会成为默认结果，所以不会有结果为空的风险），它会将返回值包在一个 Optional 中。 对于数值化的流 IntStream、LongStream 和 DoubleStream，average() 操作将其结果包在一个 Optional 中，以防流为空的情况。 看一下以下代码样例： 这时不会因为流是空的而抛出异常，而是会得到一个 Optional.empty 对象。Optional 有一个 toString() 方法，可以显示有用的信息。 注意，空流是通过 Stream.&lt;String&gt;empty() 创建的，如果只用了 Stream.empty() 而没有任何上下文信息，那么 Java 不知道它应该是什么类型的，而这种语法解决了该问题。如果编译器有足够的上下文信息，那么它可以推断出 empty() 调用的类型，就像下面这样： 我们接收到一个 Optional 时，首先要调用 isPresent()，看看里面是不是有东西，如果有，再使用 get() 来获取。 4.1 便捷函数 有很多便捷函数，可用于获取 Optional 中的数据，它们简化了上面先检查再处理所包含对象的过程： ifPresent(Consumer)：如果对象存在，则用这个对象来调用 Consumer，否则什么都不做。 orElse(otherObject)：如果对象存在，则返回这个对象，否则返回 otherObject。 orElseGet(Supplier)：如果对象存在，则返回这个对象，否则返回使用 Supplier 函数创建的替代对象。 orElseThrow(Supplier)：如果对象存在，则返回这个对象，否则抛出一个使用 Supplier 函数创建的异常。 4.2 创建Optional 当需要自己编写生成 Optional 的代码时，有如下三种可以使用的静态方法： empty()：返回一个空的 Optional。 of(value)：如果已经知道这个 value 不为 null，可以使用该方法将其包在一个 Optional 中。 ofNullable(value)：如果不知道这个 value 是不是 null，可以使用这个方法，如果 value 为 null，它会自动返回 Optional.empty，否则会将这个 value 包在一个 Optional 中。 来看一下示例： 4.3 Optional对象上的操作 有三种方法支持对 Optional 进行事后处理，所以如果你的流管线生成了一个 Optional，你可以在最后再做一项处理： filter(Predicate)：将 Predicate 应用于 Optional 的内容，并返回其结果。如果 Optional 与 Predicate 不匹配，则将其转换为 empty。如果 Optional 本身已经是 empty，则直接传回。 map(Function)：如果 Optional 不为 empty，则将 Function 应用于 Optional 中包含的对象，并返回结果，否则传回 Optional.empty。 flatMap(Function)：和 map() 类似，但是所提供的映射函数会将结果包在 Optional 中，这样 flatMap() 最后就不会再做任何包装了。 数值化的 Optional 上没有提供这些操作。 我们来看一下 filter 的用法： 尽管输出看上去像是一个流，其实每次进入 for 循环，它都会重新获得一个流，并跳过用 for 循环的索引设置的元素数，这就使其看上去像流中的连续元素，然后它执行 findFirst()，获得剩余元素的中的第一个，它会被包在一个 Optional 中返回。 注意，我们的 for 循环是循环到 i == elements.length，因此最后一个元素会超出这个流。不过这会自动变为 Optional.empty。 4.4 由Optional组成的流 假设有一个可能会生成 null 值的生成器，如果使用这个生成器创建了一个流，我们自然想将这些元素包在 Optional 中，当使用这个流时，我们必须弄清楚如何获得 Optional 中的对象： 这里我使用了 只保留非 empty 的 Optional，然后通过 map() 调用 get() 来获得包在其中的对象，因为每种情况都需要我们来决定“没有值”的含义，所以我们通常需要针对每种应用采取不同的方法。 5. 终结操作 这些操作接受一个流，并生成一个最终结果，它们不会再把任何东西发给某个后端的流。因此，终结操作总是我们在一个管线内可以做的最后一件事。 5.1 将流转换为一个数组 toArray()：将流元素转换到适当类型的数组中。 toArray(generator)：generator 用于在特定情况下分配自己的数组存储。 直接看样例： 5.2 在每个流元素上应用某个终结操作 forEach(Consumer)：这种用法我们已经看到过很多次了，即以 System.out::println 作为 Consumer 函数。 forEachOrdered(Consumer)：这个版本确保 forEach 对元素的操作顺序是原始的流的顺序。 第一种形式被明确地设计为可以以任何顺序操作元素，这只有在引入 parallel() 操作时才有意义。parallel() 让 Java 尝试在多个处理器上执行操作。它可以做到这一点，正是因为使用了流，它可以将流分割为多个流（通常情况是每个处理器一个流），并在不同的处理器上运行每个流。 我们在以下示例中引入 parallel() 来了解 forEachOrdered() 的作用和必要性： 在第一个流中，我们没有使用 parallel()，所以结果的显示顺序就是它们从 Arrays.stream(nums) 中出现的顺序。第二个流引入了 parallel()，即便是这么小的一个流，我们也可以看到输出的顺序和之前不一样了。这是因为有多个处理器在处理这个问题，而且如果多次运行这个程序，会发现每次的输出还会有所不同，原因在于多个处理器同时处理这个问题所带来的不确定性因素。 最后一个流仍然使用了 parallel()，但是又使用 forEachOrdered() 来强制结果回到原始的顺序。因此，对于非 parallel() 的流，使用 forEachOrdered() 不会有任何影响。 5.3 收集操作 collect(Collector)：使用这个 Collector 将流元素累加到一个结果集合中。 collect(Supplier, BiConsumer, BiConsumer)：和上面类似，但是 Supplier 会创建一个新的结果集合，第一个 BiConsumer 是用来将下一个元素包含到结果中的函数，第二个 BiConsumer 用于将两个值组合起来。 我们之前仅仅看到了 Collectors 对象的几个示例，我们可以将流元素收集到任何特定种类的集合中。假设想把我们的条目最终放到一个 TreeSet 中，由此使它们总是有序的。在 Collectors 中没有特定的 toTreeSet() 方法，只有 toSet()，但是可以使用 Collectors.toCollection()，并将任何类型的 Collection 的构造器引用传给它。下面的程序提取文件中的单词放到 TreeSet 中： 其中 FileToWordsBuilder.txt 文件内容如下： Files.lines() 打开 Path 所指向的文件，并将其变为由文本行组成的 Stream。它的下一行代码以一个或多个非单词字符（\\\\W+）为边界来分割这些文本行，这里生成的数组通过 Arrays.stream() 变为 Stream，然后其结果又被展开映射回一个由单词组成的 Stream。matches(\\\\d+) 会找到并删除全是数字的 String。接下来使用 String.trim() 去除周围可能存在的任何空白，最后把这些单词放到一个 TreeSet 中。 5.4 组合所有的流元素 reduce(BinaryOperator)：使用 BinaryOperator 来组合所有的流元素，因为这个流可能为空，所以返回的是一个 Optional。 reduce(identity, BinaryOperator)：和上面一样，但是将 identity 用作这个组合的初始值，因此，即使这个流是空的，我们仍然能得到 identity 作为结果。 reduce(identity, BiFunction, BinaryOperator)：这个更复杂（所以我们不会介绍），但是之所以把它列在这里，是因为它可能更高效。可以通过组合显式的 map() 和 reduce() 操作来更简单地表达这种需求。 来看一下最简单的用法： 我们在使用 reduce() 时，没有提供作为初始值的第一个参数，这意味着它会生成一个 Optional，只有当结果不是 empty 时，Optional.ifPresent() 方法才会调用 Consumer&lt;Apple&gt;（之所以 System.out::println 能够符合，是因为它可以通过 toString() 方法将 Apple 转化为一个 String）。 Lambda 表达式中的第一个参数 a0 是上次调用这个 reduce() 时带回的结果，第二个参数 a1 是来自流中的新值。如果 a0 的 price 大于60就接受 a0，否则就接受 a1，也就是序列中的下一个元素。 作为结果，我们得到的是流中第一个 price 大于60的 Apple，一旦找到了一个这样的对象，它就会抓住不放，哪怕还会出现其他候选。"},{"title":"Java函数式编程（Lambda表达式、方法引用）详解","date":"2023-10-26T09:16:00.000Z","url":"/posts/731.html","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 本文介绍了 Java8 中的函数式编程方式，结合代码示例详细地讲解了什么是 Lambda 表达式与方法引用。 1. 新方式与旧方式的对比 通常情况下，方法会根据所传递的数据产生不同的结果。如果想让一个方法在每次调用时都有不同的表现呢？如果将代码传递给方法，就可以控制其行为。 以前的做法是，创建一个对象，让它的一个方法包含所需行为，然后将这个对象传递给我们想控制的方法。下面的示例演示了这一点，然后增加了 Java 8 的实现方式：方法引用和 Lambda 表达式： Strategy 提供了接口，功能是通过其中唯一的 approach() 方法来承载的，通过创建不同的 Strategy 对象，我们可以创建不同的行为。 传统上，我们通过定义一个实现了 Strategy 接口的类来完成这种行为，比如 DefaultStrategy。更简洁、自然的方式是创建一个匿名内部类，不过这样仍然会存在一定数量的重复代码，而且我们总是要花点功夫才能明白这里是在使用匿名内部类。 Java 8 的 Lambda 表达式突出的特点是用箭头 -&gt; 将参数和函数体分隔开来，箭头右边是从 Lambda 返回的表达式，这和类定义以及匿名内部类实现了同样的效果，但是代码要少得多。 Java 8 的方法引用是用 ::，左边是类名或对象名，右边是方法名，但是没有参数列表。 2. Lambda表达式 Lambda 表达式是使用尽可能少的语法编写的函数定义。Lambda 表达式产生的是函数，而不是类，在 Java 虚拟机（JVM）上，一切都是类，所以幕后会有各种各样的操作让 Lambda 看起来像函数。 任何 Lambda 表达式的基本语法如下： 参数； 之后跟一个 -&gt;，可以读作“产生”； -&gt; 后面跟一个方法体。 需要注意以下几个方面： 如果只有一个参数，可以只写这个参数，不写括号，也可以使用括号，尽管这种方式更不常见。 如果有多个参数，将它们放在使用括号包裹起来的参数列表内。 如果没有参数，必须使用括号来指示空的参数列表。 如果方法体只有一行，那么方法体中表达式的结果会自动成为 Lambda 表达式的返回值，不能使用 return。 如果 Lambda 表达式需要多行代码，则必须将这些代玛行放到 &#123;&#125; 中，这种情况下需要使用 return 从 Lambda 表达式生成一个值。 递归意味着一个函数调用了自身。在 Java 中也可以编写递归的 Lambda 表达式，但是要注意这个 Lambda 表达式必须被赋值给一个静态变量或一个实例变星，否则会出现编译错误： 请注意，不能在定义的时候像这样来初始化 fact： 尽管这样的期望非常合理，但是对于 Java 编译器而言处理起来太复杂了，所以会产生编译错误。 现在我们再用递归的 Lambda 表达式实现斐波那契数列，这次使用实例变量，用构造器来初始化： 3. 方法引用 Java 8 方法引用指向的是方法，没有之前 Java 版本的历史包袱，方法引用是用类名或对象名，后面跟 ::，然后跟方法名： 3.1 Runnable java.lang 包中的 Runnable 接口也遵从特殊的单方法接口格式，其 run() 方法没有参数，也没有返回值，所以我们可以将 Lambda 表达式或方法引用用作 Runnable： 3.2 未绑定方法引用 未绑定方法引用（unbound method reference）指的是尚未关联到某个对象的普通（非静态）方法，对于未绑定引用，必须先提供对象，然后才能使用： 如果我们按以下方式将 A::f 赋值给 GetStringUnbound 编译器会报错，即使 get() 的签名和 f() 相同。问题在于，这里事实上还涉及另一个（隐藏的）参数：我们的老朋友 this。如果没有一个可供附着的 A 对象，就无法调用 f()。因此，A::f 代表的是一个未绑定方法引用，因为它没有绑定到某个对象。 为解决这个冋题，我们需要一个 A 对象，所以我们的接口事实上还需要一个额外的参数，如 GetStringBoundA 中所示，如果将 A::f 赋值给一个 GetStringBoundA，Java 则会幵心地接受。在未绑定引用的情况下，函数式方法（接口中的单一方法）的签名与方法引用的签名不再完全匹配，这样做有一个很好的理由，那就是我们需要一个对象，让方法在其上调用。 在 g.get(a) 中我们接受了未绑定引用，然后以 A 为参数在其上调用了 get()，最终以某种方式调用了 a.f()。Java 知道它必须接受第一个参数，事实上就是 this，并在它的上面调用该方法。 如果方法有更多参数，只要遵循第一个参数取的是 this 这种模式即可，即对于本例来说第一个参数为 A 即可。 3.3 构造器方法引用 我们也可以捕获对某个构造器的引用，之后通过该引用来调用那个构造器： 4. 函数式接口 方法引用和 Lambda 表达式都必须赋值，而这些赋值都需要类型信息，让编译器确保类型的正确性，尤其是 Lambda 表达式，又引入了新的要求，考虑如下代码： 我们看到返回类型必须是 String，但是 x 是什么类型呢？因为 Lambda 表达式包含了某种形式的类型推断（编译器推断出类型的某些信息，而不需要程序员显式指定），所以编译器必须能够以某种方式推断出 x 的类型。 下面是第二个示例： 现在 x 和 y 可以是支持 + 操作符的任何类型，包括两种不同的数值类型，或者是一个 String 和某个能够自动转换为 String 的其他类型。 Java 8 引入了包含一组接口的 java.util.function，这些接口是 Lambda 表达式和方法引用的目标类型，每个接口都只包含一个抽象方法，叫作函数式方法。当编写接口时，这种函数式方法模式可以使用 @FunctionalInterface 注解来强制实施： @FunctionalInterface 注解是可选的，Java 会将 main() 中的 FunctionalPrint 看作函数式接口。在 NotFunctional 接口的定义中我们可以看到 @FunctionalInterface 的作用：如果接口中的方法多于一个，则会产生一条编译错误信息。 现在我们仔细看一下 fp 的定义中发生了什么，FunctionalPrint 定义了接口，然而被赋值给它们的只是方法 hello()，而不是类，它甚至不是实现了这里定义的某个接口的类中的方法。这是 Java 8 增加的一个小魔法：如果我们将一个方法引用或 Lambda 表达式赋值给某个函数式接口（而且类型可以匹配），那么 Java 会调整这个赋值，使其匹配目标接口。而在底层，Java 编译器会创建一个实现了目标接口的类的实例，并将我们的方法引用或 Lambda 表达式包裹在其中。 使用了 @FunctionalInterface 注解的接口也叫作单一抽象方法（Single Abstract Method，SAM）类型。 4.1 默认目标接口 java.util.function 旨在创建一套足够完备的目标接口，这样一般情况下我们就不需要定义自己的接口了。这一套接口的命名遵循一定的规律，一般来说通过名字就可以了解特定的接口是做什么的，部分接口示例如下： 4.2 带有更多参数的函数式接口 java.util.function 中的接口毕竟是有限的，如果我们需要有3个参数的函数接口呢？因为那些接口相当直观，所以看一下 Java 库的源代码，然后编写我们自己的接口也很容易： 5. 高阶函数 高阶函数是一个能接受函数作为参数或能把函数当返回值的函数，有了 Lambda 表达式，在方法中创建并返回一个函数简直不费吹灰之力，要接受并使用函数，方法必须在其参数列表中正确地描述函数类型： 6. 函数组合 函数组合是指将多个函数结合使用，以创建新的函数，这通常被认为是函数式编程的一部分。java.util.function 中的一些接口也包含了支持函数组合的方法，我们以 andThen() 和 compose() 方法为例： andThen()：先执行原始操作，再执行方法参数中的操作。 compose()：先执行方法参数中的操作，再执行原始操作。 "},{"title":"Java内部类、匿名内部类、嵌套类详解","date":"2023-10-25T06:27:00.000Z","url":"/posts/26102.html","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 本文介绍了 Java 中的内部类、匿名内部类以及嵌套类机制。 1. 创建内部类 创建内部类的方式就是把类定义放在一个包围它的类之中： 2. 内部类到外部类的连接 到目前为止，内部类看上去就是一种名称隐藏和代码组织机制。当创建一个内部类时，这个内部类的对象中会隐含一个链接，指向用于创建该对象的外围对象。通过该链接，无须任何特殊条件，内部类对象就可以访问外围对象的成员。此外，内部类拥有対外围对象所有元素的访问权： Sequence 是以类的形式包装起来的定长 Object 数组，可以调用 add() 向序列末尾增加一个新的 Object（如果还有空间）。要取得 Sequence 中的每一个对象，可以使用名为 Selector 的接口，这是迭代器（Iterator）设计模式的一个例子。 SequenceSelector 中的 end()、current() 和 next() 这些方法中的每一个都用到了外部类的字段 objs，这个引用并不是 SequenceSelector 的一部分，而是外围对象的一个 private 字段。然而，内部类可以访问外围对象的所有方法和字段，就好像拥有它们一样。 这是怎么做到的呢？对于负责创建内部类对象的特定外围类对象而言，内部类对象偷偷地获取了一个指向它的引用。然后，当你要访问外围类的成员时，该引用会被用于选择相应的外围类成员。幸运的是，编译器会为你处理所有的这些细节。 3. 在内部类中生成外部类对象的引用 要生成外部类对象的引用，可以使用外部类的名字，后面加上 .this。这样生成的引用会自动具有正确的类型，而且是可以在编译时确定并检查的，所以没有任何运行时开销，如下所示： 有时我们想让其他某个对象来创建它的某个内部类的对象，要实现这样的功能，可以使用 .new 语法，在 new 表达式中提供指向其他外部类对象的引用，就像下面这样： 我们要使用外部类的对象来创建内部类的对象，正如我们在示例代码中所看到的那样，这也解决了内部类的名字作用域问题。 除非已经有了一个外部类的对象，否则创建内部类对象是不可能的，这是因为内部类的对象会暗中连接到用于创建它的外部类对象。然而，如果你创建的是嵌套类（static 修饰的内部类），它就不需要指向外部类对象的引用。 4. 匿名内部类 我们先来看一下如何创建匿名内部类： 这段代码的意思是创建一个继承自 AnonymousIf 的匿名类的对象，通过 new 表达式返回的引用会被自动地向上转型为一个 AnonymousIf 引用。 在这个匿名内部类中，AnonymousIf 是用无参构造器创建的，如果基类需要带一个参数的构造器，可以这么做： 也可以在定义匿名类中的字段时执行初始化，如果你正在定义一个匿名类，而且一定要用到一个在该匿名类之外定义的对象，编译器要求参数引用需要用 final 修饰，或者是“实际上的最终变量”（也就是说，在初始化之后它永远不会改变，所以它可以被视为 final 的）。这里变量 value 被传给了匿名类的基类构造器，并没有在匿名类的内部被直接用到，因此不是必须为 final 变量。 如果必须在匿名类中执行某个类似构造器的动作，该怎么办呢？因为匿名类没有名字，所以不可能有命名的构造器。我们可以借助实例初始化，使用 &#123;&#125; 语句块在效果上为匿名内部类创建一个构造器。 5. 嵌套类 如果不需要内部类对象和外部类对象之间的连接，可以将内部类设置为 static 的，我们通常称之为嵌套类。要理解 static 应用于内部类时的含义，请记住，普通内部类对象中隐式地保留了一个引用，指向创建该对象的外部类对象，对于 static 的内部类来说，情况就不是这样了，嵌套类意味着： 不需要一个外部类对象来创建嵌套类对象。 无法从嵌套类对象内部访问非 static 的外部类对象。 从另一方面来看，嵌套类和普通内部类还有些不同。普通内部类的字段和方法，只能放在类的外部层次中，所以普通内部类中不能有 static 数据、static 字段，也不能包含嵌套类，但是嵌套类中可以包含所有这些内容： 可以看到我们在 main() 中并不需要创建 NestedClass 对象就能直接创建 static 的内部类对象。 6. 接口中的类 嵌套类可以是接口的一部分，放到接口中的任何类都会自动成为 public 和 static 的，因为类是 static 的，所以被嵌套的类只是放在了这个接口的命名空间内，甚至可以在内部类内实现包围它的这个接口，就像这样： "},{"title":"Java方法调用动态绑定（多态性）与继承详解","date":"2023-10-23T13:06:00.000Z","url":"/posts/2297.html","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 本文介绍了 Java 的多态性，并深入分析 Java 中的动态绑定方法机制。 1. 方法调用绑定 我们首先来看下面这个例子： 在 main() 方法中，我们将 Piano 引用传递给了 tune()，且不需要任何强制类型转换。这是因为 Instrument 中的接口必定存在于 Piano 中，因为 Piano 继承了 Instrument。从 Piano 向上转型到 Instrument 可以“缩小”该接口，但不会小于 Instrument 的完整接口。 那么编译器怎么可能知道这个 Instrument 引用在这里指的是 Piano，而不是 Guitar？为了更深入地了解这个问题，有必要研究一下绑定（binding）这个问题。 将一个方法调用和一个方法体关联起来的动作称为绑定。在程序运行之前执行绑定（如果存在编译器和链接器的话，由它们来实现），称为前期绑定。你之前可能没有听说过这个术语，因为在面向过程语言中默认就是前期绑定的。例如，在 C 语言中只有一种方法调用，那就是前期绑定。 解决这个问题的方案称为后期绑定，这意味着绑定发生在运行时，并基于对象的类型。后期绑定也称为动态绑定或运行时绑定，当一种语言实现后期绑定时，必须有某种机制在运行时来确定对象的类型，并调用恰当的方法。也就是说，编译器仍然不知道对象的类型，但方法调用机制能找到并调用正确的方法体。 Java 中的所有方法绑定都是后期绑定，除非方法是 static 或 final 的（private 方法隐式为 final）。这意味着通常不需要你来决定是否要执行后期绑定，因为它会自动发生。 2. 尝试重写Private方法 看一下下面这段代码： 可能会很自然地认为输出应该为 Public f()，但 private 方法自动就是 final 的，并且对子类也是隐藏的，所以 Derived 的 f() 在这里是一个全新的方法，它甚至没有重载，因为 f() 的基类版本在 Derived 中是不可见的。 这样的结果就是，只有非 private 的方法可以被重写，但要注意重写 private 方法的假象，它不会产生编译器警告，但也不会执行你可能期望的操作，如果使用了 @Override 注解，那么这个问题就会被检测出来。 3. 字段访问与静态方法的多态 现在你可能会开始认为一切都可以多态地发生，但是，只有普通的方法调用可以是多态的。例如，如果直接访问一个字段，则该访问会在编译时解析： 当 Sub 对象向上转型为 Super 引用时，任何字段访问都会被编译器解析，因此不是多态的。在此示例中，Super.x 和 Sub.x 被分配了不同的存储空间，因此，Sub 实际上包含两个被称为 x 的字段：它自己的字段和它从 Super 继承的字段。然而，当你在 Sub 中引用 x 时，Super 版本并不是默认的那个，要获得 Super 的字段必须明确地使用 super.x。 现在我们再来看一下静态方法，如果一个方法是静态的，那它的行为就不会是多态的，因为静态方法与类相关联，而不是与单个对象相关联： 4. 构造器内部的多态方法行为 构造器调用的层次结构带来了一个难题，对于正在构造的对象，如果在构造器中调用它的动态绑定方法，会发生什么？ 在普通方法内部，动态绑定调用是在运行时解析的，这是因为对象不知道它是属于该方法所在的类还是其子类。如果在构造器内调用动态绑定方法，就会用到该方法被重写后的定义。但是，这个调用的效果可能相当出乎意料，因为这个被重写的方法是在对象（即子类对象）完全构造之前被调用的，因为是从外到内（即从基类到子类）执行构造器的，这可能会带来一些难以发现的错误。如下面这段代码所示： A.f() 是为重写而设计的，这个重写发生在 B 中，但是在 A 的构造器调用了这个方法，而这个调用实际上是对 B.f() 的调用。输出显示，当 A 的构造器调用 f() 时，B.x 的值甚至不是默认的初始值1，而是0。 因此类的完整初始化过程如下： 在发生任何其他事情之前，为对象分配的存储空间会先被初始化为二进制零。 如前面所述的那样调用基类的构造器，此时被重写的 f() 方法会被调用（是的，这发生在 B 构造器被调用之前），由于第1步的缘故，此时会发现 B.x 值为零。 按声明的顺序来初始化成员。 执行子类构造器的主体代码。 这样做有一个好处：一切至少都会初始化为零（或对于特定数据类型来说，是任何与零等价的值），而不仅仅是被视为垃圾。这包括通过组合嵌入在类中的对象引用，这些引用默认为 null。因此，如果忘记初始化该引用，在运行时就会出现异常。 因此，编写构造器时有一个很好的准则：用尽可能少的操作使对象逬入正常状态，如果可以避免的话，请不要调用此类中的任何其他方法。只有基类中的 final 方法可以在构造器中安全调用（这也适用于 private 方法，它们默认就是 final 的）这些方法不能被重写，因此不会产生这种令人惊讶的问题。 5. 用继承与组合进行设计 一旦学习了多态，似乎一切就都应该被继承，因为多态是一个如此巧妙的工具，但其实这会给你的设计増加负担。更好的方法是先选择组合，尤其是在不清楚到底使用哪种方法时。组合不会强制我们的程序设计使用继承层次结构，组合也更加灵活，因为在使用组合时可以动态选择类型（以及随之而来的行为），而继承则要求在编译时就知道确切的类型。以下示例说明了这一点： 6. 向下转型与反射 因为向上转型（在继承层次结构中向上移动）会丢失特定类型的信息，所以我们自然就可以通过向下转型（downcast）来重新获取类型信息，即在继承层次结构中向下移动。 我们知道向上转型总是安全的，因为基类不可能有比子类更多的接口。因此，通过基类接口发送的每条消息都能保证被子类接受。 然而对于向下转型来说，我们并不知道这些，在 Java 中，每个转型都会被检查。因此，即使看起来只是在执行一个普通的带括号的强制转型，但在运行时会检查此强制转型，以确保它实际上是你期望的类型，如果不是，则会抛出一个 ClassCastException 错误。这种在运行时检查类型的行为是 Java 反射（reflection）的一部分。以下示例里展示了一些基本的反射行为： 如果想访问 MoreUseful 对象的扩展接口，可以尝试向下转型，如果类型正确就会成功。否则会得到一个 ClassCastException 异常。其实你无须为此异常编写任何特殊代码，因为它表示一个程序员犯的错误，它可能发生在程序中的任何位置。"},{"title":"Java类中字段及子类初始化过程详解","date":"2023-10-22T07:16:00.000Z","url":"/posts/54920.html","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 本文介绍了 Java 类中不同类型的字段以及子类的初始化过程。 1. 初始化顺序 类中的变量定义顺序决定了初始化的顺序，即使分散到方法定义之间，变量定义仍然会在任何方法（包括构造器）调用之前就被初始化，看下面这段代码： 在类 B 中，A 对象的定义分散到各个方法之间，但他们均在构造器执行前被初始化完成，其中有一个对象引用在构造器内被重新初始化。 a3 引用被初始化了两次：一次在构造器调用之前，另一次在构造器调用期间（第一个对象被丢弃了，因此稍后可能会被垃圾收集器回收）。 2. 静态数据的初始化 无论创建了多少对象，静态数据都只有一份存储空间。来看下面的代码： 静态字段 a2 和 a3 的创建在 a1 字段之前，且仅在第一个 B 对象创建时被初始化，之后这些静态对象不会被重新初始化。 因此初始化的顺序是从静态字段开始，然后是非静态字段。例如要执行静态的 main 方法，必须先加载 StaticInitialization 类，然后初始化他的静态字段 b2，这就导致类 B 被加载，而类 B 中包含静态的类 A 的对象，因此 A 也被加载，所以这个程序中所有的类都在 main 方法开始执行前被加载。 现在总结一下对象创建的过程，假设有一个名为 A 的类： 尽管没有显式使用 static 关键字，但构造器实际上也是静态方法。因此，第一次创建类型为 A 的对象时，或者第一次访问类 A 的静态方法或静态字段时，Java 解释器会搜索类路径来定位 A.class 文件。 当 A.class 被加载后（这将创建一个 Class 对象），它的所有静态初始化工作都会执行。因此，静态初始化只在 Class 对象首次加载时发生一次。 当使用 new A() 创建对象时，构建过程首先会在堆上为 A 对象分配足够的存储空间。 这块存储空间会被淸空，然后自动将该 A 对象中的所有基本类型设置为其默认值（数值类型的默认值是0，boolean 和 char 则是和0等价的对应值），而引用会被设置为 null。 执行所有出现在字段定义处的初始化操作。 执行构造器。这实际上可能涉及相当多的动作，尤其是在涉及继承时。 3. 显式的静态初始化 Java 允许在一个类里将多个静态初始化语句放在一个特殊的“静态子句”里（有时称为静态块）： 尽管看起来有点像一个方法，但它只是在 static 关键字后加了一段代码，这段代码和其他静态初始化语句一样，只执行一次：第一次创建该类的对象时，或第一次访问该类的静态成员时（即使从未创建过该类的对象）。 4. 子类初始化 当创建子类对象时，它里面包含了一个基类的子对象。正确初始化基类的子对象至关重要，我们只有一种方法可以保证这一点：在子类构造器中调用基类构造器来执行初始化，它具有执行基类初始化所需的全部信息和权限。Java 会自动在子类构造器中插入对基类无参构造器的调用： 可以看到构造过程是从基类“向外”逬行的，因此基类在子类构造器可以访问它之前就被初始化了。即使没有为 BaseNonParamsConstructor 创建构造器，编译器也会为它合成一个可以调用基类构造器的无参构造器。 如果基类没有无参构造器，或者如果你必须要调用具有参数的基类构造器，那么就要使用 super 关键字和相应的参数列表，来显式调用基类构造器： 5. 子类及静态数据初始化 了解整个初始化的过程，包括继承，有助于我们全面了解所发生的事情，现在我们来看一个综合示例： 当你运行这个文件时，首先会尝试访问静态方法 Beetle.main()，所以加载器会去 Beetle.class 文件中找到 Beetle 类的编译代码，在加载它的代码时，加载器注意到有一个基类 Insect，然后它就会去加载基类。无论是否创建该基类的对象，都会发生这种情况。 如果基类又有自己的基类，那么第二个基类也将被加载，以此类推。接下来，会执行根基类（本例中为 Insect）中的静态初始化，然后是下一个子类，以此类推。这很重要，因为子类的静态初始化可能依赖于基类成员的正确初始化。"},{"title":"Java对象的相等判定问题与equals方法详解","date":"2023-10-21T08:06:00.000Z","url":"/posts/43228.html","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 本文介绍了 Java 中不同类的相等判定规则以及不同类中 equals 方法的效果。 1. Integer 首先我们来看以下这段代码的运行结果： 对于参数值127，== 操作符只在使用 new 创建对象的方式上判定两个数为 false，这是因为操作符 ==/!= 比较的是对象的引用，虽然参与比较的两个引用包含的内容相同，但他们指向了内存中的不同对象。注意这种定义方式在 Java9 之后已经被废弃，因为其效率远不如使用 valueOf 方法。 对于参数值128，可以发现所有创建方式创建的两个相同值的 Integer 对象在 == 操作符的判定下均为 false，这是因为出于效率的原因，Integer 会通过享元模式来缓存范围在-128~127内的对象，因此多次调用 Integer.valueOf(127) 生成的其实是同一个对象，而在此范围之外的值则不会这样，即每次调用 Integer.valueOf(128) 返回的都是不同的对象。 因此在使用 Integer 的时候如果需要比较值是否相等应该只是用 equals 方法。 2. Double 我们再来看下面这段代码： 理论上浮点数的比较应该是很严格的，即两个数值之间即使有极小的不同也应该不相等。 例如0和 Double.MIN_VALUE 相比较并不相等，但是 Double.MAX_VALUE 减去一百万倍的 Double.MIN_VALUE 却仍等于 Double.MIN_VALUE，这是因为当一个非常大的数值减去一个相对较小的数值时，非常大的数值并不会发生显著变化，这叫做舍入误差，误差的产生原因是因为机器不能存储足够的信息来表示一个大数值的微小变化。 3. 自定义类 现在你是不是以为在比较对象内容是否相等的情况下都一律用 equals 函数即可？但是并没有这么简单。看下面这段代码： 我们创建了两个类 A 的对象，且值相等，但是 equals 方法返回的结果为 false，这是因为 equals 方法的默认行为是比较引用，如果想比较内容必须像类 B 那样重写 equals 方法。 因此实际上其实是大多数标准库会重写 equals 方法来比较对象的内容而不是他们的引用。"},{"title":"算法竞赛Java数据结构与算法类详解","date":"2023-10-20T08:39:00.000Z","url":"/posts/23459.html","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 本文介绍了 Java 中常用的数据结构与算法类。 1. String/StringBuffer/StringBuilder String 类即字符串，在 Java 中 String 类是不可改变的，如果修改 String 对象，那么其实是开一个新的空间保存，而原空间中的字符串还存在于内存中。String 类的用法如下： 当需要对字符串进行频繁修改时，要用 StringBuffer 或 StringBuilder 类。这两个类的对象能够被多次修改，并且不产生新的未使用对象。 StringBuffer 和 StringBuilder 之间最大的不同在于 StringBuilder 的方法不是线程安全的（不能同步访问）。但是 StringBuilder 相较于 StringBuffer 有速度优势，所以多数情况下建议使用 StringBuilder 类。以 StringBuilder 为例，用法如下： 2. Arrays Arrays 类定义在 java.util 包中，该类能够方便地操作数组，用法如下： 3. ArrayList Java 的集合框架主要包括两种类型的容器： 集合（Collection）：存储一个元素集合。 图（Map）：存储键/值对映射。 其中，Collection 接口又包含了 List 接口和 Set 接口两大分支，前者为允许存在重复元素的有序集合，后者为不允许存在重复元素的无序集合；Map 是一个映射接口，其中的每一个元素包含一个 key 和对应的 value。 ArrayList 类实现了 List 接口，继承自 AbstractList 类，位于 java.util 包中（迭代器也位于该包中），可以用于动态地调整存储在其中的元素的大小。ArrayList 类是基于数组的数据结构，它提供了一组用于操作元素的方法，包括添加、删除、插入、搜索和排序等。 ArrayList 用法如下： 4. LinkedList Java 的 LinkedList 类是一个实现了 List 接口和 Deque 接口的双向链表。它也可以被当作堆栈、队列或双端队列进行操作。 与 ArrayList 相比，LinkedList 的增加和删除的操作效率更高，而查找和修改的操作效率较低。因此需要频繁访问列表中的某一个元素且只需要在列表末尾进行添加和删除元素操作推荐使用 ArrayList，而需要频繁的在列表开头、中间、末尾等位置进行添加和删除元素操作推荐使用 LinkedList。 LinkedList 的定义与遍历方式与 ArrayList 相同，且上述的 ArrayList 常用方法在 LinkedList 中均可使用，额外添加的部分方法如下： 5. HashSet/TreeSet Java 的 HashSet 类是一个实现了 Set 接口的集合类，它使用 HashMap 作为基础，因此其特性和 HashMap 类似： 不重复：是一个没有重复元素的集合。 无序：它不保证元素的顺序。 允许 null：允许使用 null 元素。 非同步：即不是线程安全的，如果多个线程尝试同时修改 HashSet，则最终结果是不确定的。 必须在多线程访问时显式同步对 HashSet 的并发访问。 HashSet 用法如下： TreeSet 与 HashSet 的用法基本一致，区别在于 TreeSet 中的元素是有序的： 6. HashMap/TreeMap Java 的 HashMap 是一个实现了 Map 接口的哈希表，它存储的内容是键值对（key-value）映射。HashMap 的特性如下： 键值对存储：HashMap 是以 key-value 存储形式存在，主要用来存放键值对。 非同步：HashMap 的实现不是同步的，这意味着它不是线程安全的。 允许 null：它的 key、value 都可以为 null。 无序：此外，HashMap 中的映射不是有序的。 HashMap 的底层数据结构为： Java8 之前：HashMap 由数组+链表组成，数组是 HashMap 的主体，链表则主要用于解决哈希碰撞。 Java8 之后：在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）并且当前数组的长度大于64时，此时此索引位置上的所有数据改为使用红黑树存储。 HashMap 的保存顺序与插入顺序不同，这是因为 HashMap 使用了一种非常快速的算法，该算法会控制顺序。LinkedHashMap 则是按照插入顺序来保存键值对，同时保留了 HashMap 的查找速度。 HashMap 用法如下： 与 Set 同理，TreeMap 与 HashMap 的用法基本一致，区别在于 TreeMap 中的元素是根据键的升序排好序的： 7. Stack 栈（Stack）是一个后进先出（LIFO）的集合，它有时被称作下推栈，因为不管我们最后压入的是什么，它都会第一个弹出。 Java 1.0 就提供了 Stack 类，结果这个类的设计非常糟糕，不过因为要向后兼容，所以我们永远也无法摆脱 Java 过去的设计错误了。Java 6 加入了 ArrayDeque，提供了直接实现栈功能的方法： 8. Queue/PriorityQueue 队列（Queue）是一个典型的先进先岀（FIFO）的集合。换言之，我们在一端放入在另一端拿出来，放入的顺序和取出的顺序是一样的。队列常用来将对象从程序的一个区域可靠地转移到另一个区域，队列在并发编程中特别重要，因为它们可以安全地将对象从一个任务转移到另一个任务。 LinkedList 实现了 Queue 接口，提供了支持队列行为的方法，因此 LinkedList 可以作为 Queue 的一种实现来使用，通过将 LinkedList 向上转型为 Queue，下面这个示例使用了 Queue 接口特有的方法： 其中： offer() 是 Queue 特有的方法之一，负责在队列尾部插入一个元素，如果无法插入则返回 false。 peek() 和 element() 都会返回队头元素，不会将其删除，但是如果队列为空，peek() 返回 null，而 element() 抛出 NoSuchElementException 异常。 poll() 和 remove() 会将队头元素删除，同时返回该元素，但是如果队列为空，poll() 返回 null，而 remove() 抛出 NoSuchElementException 异常。 优先级队列（Priority Queue）是指下一个要拿出的元素是需求最强烈的元素（最高优先级）。Java 5 中添加了 PriorityQueue，为这种行为提供了一个自动化的实现。 PriorityQueue 默认的排序方法使用的是对象在队列中的自然顺序，但我们可以通过提供自己的 Comparator 来修改顺序： 9. 深入Iterator与Collection Collection 是所有序列集合共同的根接口，可以认为它是一个为表示其他接口之间的共性而出现的附属接口。支持使用这样一个接口的理由是，它可以创建更通用的代码，通过面向接口而不是面向实现来编写代码，我们的代码可以应用于更多对象类型。因此，如果我们编写一个以 Collection 为参数的方法，那么该方法可以应用于任何实现了 Collection 的类型。 在 Java 中，看上去遵循 C++ 的方式比较明智，即使用迭代器而非 Collection 来表示集合之间的共性。但是，因为在 Java 中实现 Collection 也就意味着提供了 iterator() 方法，所以这两种方式其实是绑在一起了： 10. 自定义对象数组排序 我们需要让进行排序的类实现 Comparable 接口，重写其中的 compareTo() 方法，在其中定义排序规则，那么就可以直接调用 java.util.Arrays.sort() 来排序对象数组： "},{"title":"Django Channels、WS协议及同步与异步详解","date":"2023-10-11T08:28:00.000Z","url":"/posts/45720.html","tags":[["Python","/tags/Python/"]],"categories":[["Python","/categories/Python/"]],"content":" 介绍同步与异步、WebSocket 协议以及 Django Channels 三者的概念及其联系，并结合代码示例进行讲解。 1. 同步与异步 在 Django 中，同步和异步主要涉及到请求处理的方式。这两种方式的主要区别在于它们如何处理多个并发请求： 同步（Synchronous）：在同步模式下，Django 会为每个请求创建一个单独的线程或进程。这意味着，如果一个请求正在等待响应（例如，等待数据库查询返回结果），那么整个线程或进程将被阻塞，直到响应返回。这可能会导致资源的浪费，因为在等待期间，线程或进程不能做其他任何事情。 异步（Asynchronous）：与同步模式不同，异步模式允许单个线程或进程同时处理多个请求。当一个请求需要等待响应时（例如，等待数据库查询返回结果），线程或进程可以切换到另一个请求，继续执行其他任务，而不是被阻塞。这样可以更有效地利用系统资源，提高并发处理能力。 Django 3.1 版本开始引入了对异步视图和中间件的支持，这意味着你可以编写异步的视图函数，这些函数可以使用 Python 的 async 和 await 关键字进行定义。这使得 Django 可以更好地处理 I/O 密集型任务，如 HTTP 请求、数据库操作和文件读写等。 然而需要注意的是，并非所有的 Django 组件都支持异步操作。例如，Django 的 ORM（对象关系映射）目前仍然是同步的，这意味着你不能在异步视图或中间件中直接使用它。如果你需要在异步代码中执行数据库操作，你需要使用 Django 提供的 sync_to_async 或 async_to_sync 函数来确保数据库操作在同步环境中执行。 总的来说，同步和异步各有优势和适用场景。对于 CPU 密集型任务，同步模式可能更合适；而对于 I/O 密集型任务，异步模式可能会带来更好的性能。在实际开发中，你可能需要根据应用的具体需求和性能要求来选择使用同步还是异步。 （1）同步代码样例 现在我们来看一些同步与异步的样例，首先是同步视图，在下面这个例子中，当请求到达 sync_view 时，Django 将等待视图函数完成后才会处理下一个请求： 下面这个例子在视图中同步地从数据库获取所有的博客对象，然后将它们传递给模板： （2）异步代码样例 在下面这个例子中，async_view 是一个异步视图。当请求到达这个视图时，Django 可以在等待 asyncio.sleep(1) 完成时处理其他请求： 请注意，要使用异步视图，你需要确保你的 Django 项目正在运行在支持异步的 ASGI 服务器上，而不是传统的 WSGI 服务器。此外，你的中间件和任何你在视图中调用的代码也必须支持异步。否则，你可能会遇到问题。如果你的代码库主要是同步的，那么最好坚持使用同步视图。如果你正在编写新的、主要使用 Python 的异步库的代码，那么异步视图可能会很有用。请记住，混合使用同步和异步代码可以很复杂，需要谨慎对待。 再来看下面的例子，我们创建了一个新的 get_blogs 异步函数，它使用 run_in_executor 方法在一个单独的线程中运行数据库查询。这允许 Django 在等待数据库查询完成时处理其他请求： 请注意，虽然这个示例展示了如何在 Django 视图中使用异步代码操作数据库，但是 Django 的数据库层目前还不支持原生的异步操作。因此，在实践中，你可能需要使用像 asgiref.sync_to_async 这样的工具来安全地在异步视图中执行同步数据库操作。同时，你也需要确保你的数据库驱动程序和数据库服务器能够处理并发连接。否则，你可能会遇到性能问题或错误。如果你不确定如何正确地使用异步代码，那么最好使用同步视图和同步数据库操作。 2. WebSocket WebSocket 是一种用于在 Web 和移动应用程序之间进行实时通信的新标准。WebSocket 设计为在 Web 浏览器和 Web 服务器之间实现，但也可以由客户端或服务器应用程序使用。WebSocket 是一种提供单个 TCP 连接上的全双工通信通道的协议，可以实现服务器和客户端之间的实时交互。 WebSocket 与 HTTP 不同，其主要区别如下： 通信方式：HTTP 是单向的，客户端发送请求，服务器发送响应。而 WebSocket 是双向的，在客户端-服务器通信的场景中使用的全双工协议，即客户端和服务器可以同时发送和接收数据。 连接：HTTP 每次请求都需要重新建立连接，而 WebSocket 使用长连接实现数据实时推送。一旦通信链接建立和连接打开后，消息交换将以双向模式进行，客户端-服务器之间的连接会持久存在。 数据传输：HTTP 协议中的数据传输是文本格式的，而 WebSocket 可以传输文本和二进制数据。 性能：由于 HTTP 的每次请求都需要建立连接和断开连接，而 WebSocket 可以在一次连接上进行多次通信，因此 WebSocket 在性能上比 HTTP 有优势。 应用场景：HTTP 主要用于客户端和服务器之间的请求和响应，如浏览器请求网页和服务器返回网页的 HTML 文件。WebSocket 可以实现双向通信，常常用于实时通信场景。 协议头：HTTP 协议头的大小从200字节到2KB不等，常见大小是700-800字节。而 WebSocket 协议头相对较小，这使得其在高频率、小数据量的通信场景下更有优势。 状态：HTTP 是无状态协议，而 WebSocket 是有状态协议。这意味着客户端和服务器之间的连接将保持活动状态，直到被任何一方（客户端或服务器）终止。 两种协议都位于 OSI 模型的第七层，并依赖于第四层的 TCP。尽管它们是不同的，但 RFC 6455 指出，WebSocket 旨在通过 HTTP 端口443和80工作，并支持 HTTP 代理和中介，从而使其与 HTTP 兼容。为了实现兼容性，WebSocket 握手使用 HTTP Upgrade 头从 HTTP 协议切换到 WebSocket 协议。 WebSocket 协议使得 Web 浏览器（或其他客户端应用程序）和 Web 服务器之间可以在不需要客户端请求的情况下发送内容，以及在保持连接打开的同时传递消息。这样，客户端和服务器之间可以进行双向持续的对话。通信通常是通过 TCP 端口号443（或80，如果是非安全连接）进行的，这对于使用防火墙阻止非 Web Internet 连接的环境是有利的。 在 Django 中实现 WebSocket，你可以选择使用 channels 或者 dwebsocket。但是，channels 被更广泛地使用，因为它可以完美地集成到 Django 的生态系统中。 3. 在JavaScript中使用WebSocket （1）创建 WebSocket 对象 （2）连接成功时的回调函数 当 WebSocket 连接成功时，onopen 事件会被触发。你可以在这个函数中发送消息到服务器： （3）从服务器接收信息时的回调函数 当从服务器接收到信息时，onmessage 事件会被触发。你可以在这个函数中处理接收到的数据： （4）连接关闭时的回调函数 当连接关闭后，onclose 事件会被触发。你可以在这个函数中处理连接关闭后的逻辑： （5）连接失败时的回调函数 （6）监听窗口关闭事件 当窗口关闭时，主动去关闭 WebSocket 连接，防止连接还没断开就关闭窗口，这样服务端会抛异常： 4. Django Channels Django Channels 是一个开源框架，它扩展了 Django 的功能，使得 Django 不仅可以处理 HTTP，还可以处理需要长时间连接的协议，如 WebSocket、MQTT（消息队列遥测传输）、聊天协议、广播等实时应用。 Channels 允许 Django 项目支持“长连接”，它用 ASGI 替换了 Django 的默认 WSGI。ASGI（Asynchronous Server Gateway Interface）为异步 Python Web 服务器和应用程序提供了一个接口，同时支持 WSGI 提供的所有功能。 Channels 保留了 Django 的同步行为，并添加了一层异步协议，允许用户编写完全同步、异步或两者混合的视图（Views）。Django Channels 提供了一种通信系统，叫做 Channel Layer，它可以让多个 Consumer 实例之间互相通信，以及与外部 Django 程序实现互通。 Channel Layer 主要包括两种抽象概念：Channel 和 Group。Channel 是一个发送消息的通道，每个 Channel 都有一个名称，拥有这个名称的人都可以往 Channel 里面发送消息。Group 是多个 Channel 的集合，每个 Group 都有一个名称，拥有这个名称的人都可以往这个 Group 里添加/删除 Channel，也可以往 Group 里发送消息。Group 内的所有 Channel 都可以收到，但是不能给 Group 内的具体某个 Channel 发送消息。使用 Django Channels 可以实现一些实时通讯的功能，如在线聊天室、游戏、通知等。 Consumer 是 Channels 的基本单位，相当于 Django 的视图，它是一个事件驱动的类，可以处理不同类型的事件，如连接、断开、接收消息等，支持同步和异步应用程序。 现在我们来看一下 Django Channels 的样例，首先需要安装 Channels 和 Channels Redis： 然后需要在你的项目设置中（settings.py 文件）配置 CHANNEL_LAYERS 需要添加 channels 到你的 INSTALLED_APPS 列表，并设置 ASGI_APPLICATION 和 CHANNEL_LAYERS。例如： 在这个例子中，我们假设你正在本地运行一个 Redis 服务器，它监听在6379端口。如果你的 Redis 服务器在其他地方或者使用了不同的端口，你需要更新 hosts 设置。 接下来你需要在 Django App 的目录下创建一个路由文件 routing.py，作用相当于 HTTP 的 urls，并在其中定义你的 WebSocket 路由。我们先创建出来： 此外，你还需要运行一个兼容的 ASGI 服务器，如 Daphne 或 Uvicorn。我们安装 Daphne： 输入 daphne 命令查看是否可用，如果不可用说明应该是没有配置环境变量，按如下方式修改环境变量（需要重启系统）： 为了在 Django 项目中使用 Daphne，你需要确保你的项目已经配置为使用 ASGI 而不是 WSGI。这通常意味着你需要在你的项目中创建一个 asgi.py 文件，并在你的设置文件中设置 ASGI_APPLICATION 变量（之前已经设置好了）。 现在我们配置 djangoapp/djangoapp/asgi.py 文件： 现在我们在 Django App 目录下创建 consumers.py： 在这个例子中，我们创建了一个 ChatConsumer 类，它是一个异步的 WebSocket Consumer，这个类继承自 AsyncWebsocketConsumer，这是 Django Channels 提供的一个基础类。 其中的主要函数说明如下： async def connect(self)：当一个 WebSocket 连接打开时，这个方法会被调用。在这个方法中，我们从 URL 路由中获取房间名，并将其保存在 self.room_name 中。然后，我们创建一个房间组名，并将其保存在 self.room_group_name 中。然后，我们将当前的 Channel 添加到房间组中。最后，我们接受 WebSocket 连接。 async def disconnect(self, close_code)：当一个 WebSocket 连接关闭时，这个方法会被调用。在这个方法中，我们将当前的 Channel 从房间组中移除。 async def receive(self, text_data)：当从 WebSocket 接收到消息时，这个方法会被调用。在这个方法中，我们首先将接收到的文本数据解析为 JSON，然后我们从 JSON 数据中获取消息，并将其发送到房间组中。 async def chat_message(self, event)：这是一个自定义的事件处理器方法，当从房间组接收到类型为 chat_message 的事件时，这个方法会被调用。在这个方法中，我们首先从事件中获取消息，然后我们将消息发送回 WebSocket。 总的来说，这段代码的功能为：当用户连接到 WebSocket 时，他们会被添加到一个名为 chat_&#123;room_name&#125; 的组中。当他们发送消息时，这个消息会被广播到他们所在的组中的所有其他用户。当他们接收到组中的消息时，这个消息会被发送回他们的 WebSocket，即实现了一个简单的聊天室功能。 最后我们定义一下 Consumer 的路由，以下代码将 URL 路径 ws/chat/&#123;room_name&#125;/ 映射到我们的 ChatConsumer。这意味着当用户连接到这个路径的 WebSocket 时，他们会被连接到聊天室： "},{"title":"Django使用SMTP发送邮件教程","date":"2023-10-08T07:37:00.000Z","url":"/posts/60606.html","tags":[["Python","/tags/Python/"]],"categories":[["Python","/categories/Python/"]],"content":" 介绍 Django 中如何使用 SMTP 协议发送邮件。 1. SMTP介绍 SMTP（Simple Mail Transfer Protocol）即简单邮件传输协议，它是一组用于由源地址到目的地址传送邮件的规则，由它来控制信件的中转方式。SMTP 协议属于 TCP/IP 协议簇，它帮助每台计算机在发送或中转信件时找到下一个目的地。通过 SMTP 协议所指定的服务器，就可以把 E-mail 寄到收信人的服务器上了，整个过程只要几分钟。SMTP 服务器则是遵循 SMTP 协议的发送邮件服务器，用来发送或中转发出的电子邮件。 一个 SMTP 服务器总是有一个独特的地址，和一个用于发送邮件的特定端口，在大多数情况下是587。本文以使用 QQ 邮箱为例，因此我们将使用的地址是 smtp.qq.com，端口号是587。 2. 申请邮箱授权码 首先我们需要启用 QQ 邮箱的 SMTP 服务，并申请一个授权码，打开邮箱网页，然后在“设置-账号”选项卡中找到 SMTP 服务部分，点击开启服务，然后完成相关的身份验证后会收到一串授权码。 3. Django发送邮件 首先在项目的 settings.py 文件中添加配置信息： 配置参数说明如下： EMAIL_BACKEND：声明了我们的 Django 项目将用于连接 SMTP 服务器的后端。这个变量指向 smtp.EmailBackend 类，该类接收发送邮件所需的所有参数。虽然这个类是默认的 EMAIL_BACKEND，但在 Django 的设置中明确定义被认为是一个好的做法。 EMAIL_HOST：将要使用的 SMTP 服务器域，取决于你的电子邮件提供商。常用的有：smtp.qq.com、smtp.163.com、smtp.gmail.com 等。 EMAIL_PORT：SMTP 服务器端口号，587是大多数 SMTP 服务器的默认端口，对于个人电子邮件提供商来说，这一点仍然适用。 EMAIL_USE_TLS：TLS（Transport Layer Security，传输层安全协议），用于在两个应用程序之间提供保密性和数据完整性。在此处用于加密网络应用程序（Django）和服务器（SMTP 服务器）之间的通信。 EMAIL_HOST_USER：主机用户（发件人邮箱），设置是你的个人电子邮件地址。 EMAIL_HOST_PASSWORD：发件人的授权码，即从你的电子邮件帐户获得的应用程序密码。 RECIPIENT_ADDRESS：收件人邮箱列表。 现在假设我们的前端发来如下请求： 然后我们需要在后端接收 message，并将其通过邮件（django.core.mail.send_mail 函数）发送给自己（即 settings 中的 EMAIL_HOST_USER 为自己的邮箱，RECIPIENT_ADDRESS 列表中也仅有一个自己的邮箱）： "},{"title":"Django学习笔记-实现聊天系统","date":"2023-10-04T07:08:00.000Z","url":"/posts/54938.html","tags":[["Python","/tags/Python/"]],"categories":[["Python","/categories/Python/"]],"content":" 本节内容是通过 WebSocket 实现多人模式中的在线聊天系统。 1. 实现聊天系统前端界面 聊天系统整体可以分为两部分：输入框与历史记录。 我们需要先修改一下之前代码中的一个小 BUG，当在一个窗口中按 Q 时，另一个窗口中点击鼠标左键也能攻击，因为按下按键的事件被所有窗口都捕捉到了，这是不合理的。 我们之前监听的对象是 window，每个地图是一个 canvas 元素，因此我们可以绑定到 canvas 对象上。由于不是所有对象都能添加绑定事件的，因此我们还需要对 canvas 做一个修改，需要添加 tabindex 参数并将其聚焦后才能监听事件，首先在 GameMap 类中修改一下 canvas 对象： 在 Player 类中修改： 聊天的前端界面需要创建一个新的文件，我们在 ~/djangoapp/game/static/js/src/playground 目录下创建一个 chat_field 目录，并进入该目录创建 zbase.js 文件： 然后在 AcGamePlayground 类中创建出来： 现在在 Player 类中即可监听事件： 然后我们还需要实现一下聊天区的 CSS 样式（在 ~/djangoapp/game/static/css 目录的 game.css 文件中）： 现在我们实现在历史记录区域里添加新消息的功能： 2. 实现后端同步函数 我们先在 WebSocket 前端实现发送和接收消息的函数： 然后实现后端代码： 最后在前端的 ChatField 类中调用一下发送消息的函数即可： 上一章：Django学习笔记-实现联机对战（下）。 下一章：无。"},{"title":"Django、Nginx、uWSGI详解及配置示例","date":"2023-10-03T02:25:00.000Z","url":"/posts/65222.html","tags":[["Python","/tags/Python/"]],"categories":[["Python","/categories/Python/"]],"content":" 介绍 Django、Nginx 以及 uWSGI 三者的概念及其联系与区别，并讲解如何配置。 一、Django、Nginx、uWSGI的概念、联系与区别 Django、Nginx 和 uWSGI 都是用于构建和运行 Web 应用程序的软件，这三个软件的概念如下： Django：Django 是一个基于 Python 的开源 Web 框架，它提供了一套完整的工具和组件，可以帮助开发人员快速构建 Web 应用程序。Django 遵循了 MVC（模型-视图-控制器）的设计模式，将业务逻辑、数据模型和用户界面分离，提高了代码的可读性和可维护性。Django 还支持多种数据库、缓存、模板引擎、表单验证、国际化、安全性等特性，使得开发者可以专注于业务需求，而不用担心底层的细节。Django 框架主要负责处理业务逻辑和生成结果给 Web 服务器，再由 Web 服务器返回给浏览器。而 Web 框架和 Web 服务器之间的通信需要遵循一套规范，这个规范就是 WSGI。 Nginx：Nginx 是一个高性能的 HTTP 和反向代理 Web 服务器，它可以处理来自客户端（例如浏览器）的 HTTP 请求，并将其转发给后端的 Web 应用程序或其他服务器。Nginx 具有高并发、低内存占用、负载均衡、静态文件缓存等优点，可以提高 Web 应用程序的响应速度和可靠性。如果你有多个 Web 服务器，你可以使用 Nginx 来做负载均衡，根据某些规则将不同的请求分发到不同的 Web 服务器上去。 uWSGI：uWSGI 是一个实现了 WSGI 协议、uwsgi 协议和 HTTP 协议的 Web 服务器接口，它可以在 Web 服务器和 Web 应用程序之间提供接口，使得它们可以相互通信。WSGI（Web Server Gateway Interface）是一种 Python 用于 Web 开发的标准接口，它定义了 Web 服务器如何调用 Web 应用程序，并将结果返回给客户端，在生产环境中使用 WSGI 作为 Python Web 的服务器。uwsgi 是 uWSGI 程序实现的一个私有协议，它采用二进制格式传输数据，比 HTTP 协议更高效。uWSGI 是一个应用服务器，它可以将客户端请求转发给 Django 等 Web 应用程序进行处理。 Django、Nginx、uWSGI 之间的联系和区别主要体现在以下几个方面： 角色：Django 是一个 Web 框架，负责处理业务逻辑和生成响应内容；Nginx 是一个 Web 服务器，负责接收和转发 HTTP 请求；uWSGI 是一个 Web 服务器接口，负责将 HTTP 请求转换为 WSGI 请求，并调用 Django 处理。 协议：Django 遵循 WSGI 协议，与 uWSGI 进行通信；Nginx 遵循 HTTP 协议，与客户端和 uWSGI 进行通信；uWSGI 支持多种协议，包括 HTTP、uwsgi 和 WSGI。 性能：Django 本身不是一个高性能的 Web 框架，它需要借助其他软件来提高效率；Nginx 是一个高性能的 Web 服务器，它可以处理大量的并发请求，并缓存静态文件；uWSGI 是一个高效的 Web 服务器接口，它可以利用 uwsgi 协议减少数据传输的开销。 配置：Django 需要在 settings.py 文件中配置数据库、中间件、应用等信息；Nginx 需要在 nginx.conf 文件中配置监听端口、反向代理规则、静态文件路径等信息；uWSGI 需要在 uwsgi.ini 文件中配置项目路径、端口号、进程数等信息。 总结一下： Nginx：HTTP 服务器，反向代理服务器。 uWSGI：应用服务器，或者更精确地说是 WSGI 应用容器。 Django：WSGI 应用程序（框架）。 二、Nginx正向代理和反向代理的区别 正向代理和反向代理是两种不同的代理模式，它们的区别主要在于代理的对象和目的不同： 正向代理：指客户端（如浏览器）通过代理服务器来访问目标服务器，目的是为了隐藏客户端的真实身份或者突破访问限制。正向代理的特点是客户端知道目标服务器的地址，而目标服务器不知道客户端的地址。例如，如果你想访问某个国外的网站，但是由于网络封锁或者速度慢，你可以通过一个正向代理服务器来转发你的请求，这样就可以提高访问效率或者绕过限制。 反向代理：指客户端（如浏览器）直接访问代理服务器，然后代理服务器再转发请求给目标服务器，目的是为了提高目标服务器的性能或者安全性。反向代理的特点是客户端不知道目标服务器的地址，而目标服务器知道代理服务器的地址。例如，如果你想访问某个网站，但是这个网站有多台后端服务器提供服务，你可以通过一个反向代理服务器来分发你的请求，这样就可以实现负载均衡或者缓存等功能。 总之，正向代理和反向代理的区别就是看你是站在客户端的角度还是目标服务器的角度。正向代理是为了满足客户端的需求，而反向代理是为了满足目标服务器的需求。 三、Nginx与uWSGI的配置文件示例 要在 Django 应用程序中使用 Nginx 和 uWSGI，你需要做以下几个步骤： 安装 Nginx 和 uWSGI，你可以使用 apt 或 pip 命令来安装它们： 配置 uWSGI，你需要创建一个 ini 文件，指定你的项目目录、模块、端口、进程、日志等信息。 配置 Nginx，你需要创建一个 conf 文件，指定你的监听端口、服务器名、静态文件路径、反向代理规则等信息。 启动 uWSGI 和 Nginx，你可以使用 systemctl/service 或 uwsgi 命令来启动它们。 Nginx 配置文件的位置一般是在 /etc/nginx/nginx.conf，它用来定义 Nginx 服务器的基本参数。Nginx 配置文件的语法格式是由多个块组成，每个块用花括号 &#123;&#125; 包围，每个指令用分号 ; 结束。例如，一个简单的 Nginx 配置文件可以写成这样： uWSGI 配置文件的位置可以自己指定，一般放在项目目录下，假设我们在项目根目录下的 scripts/uwsgi.ini 文件中。uWSGI 配置文件的语法格式是由多个节组成，每个节用方括号 [] 包围，每个指令用等号 = 赋值。例如，一个简单的 uWSGI 配置文件可以写成这样： "},{"title":"Django学习笔记-实现联机对战（下）","date":"2023-09-25T12:19:00.000Z","url":"/posts/60167.html","tags":[["Python","/tags/Python/"]],"categories":[["Python","/categories/Python/"]],"content":" 本节内容是通过 Django Channels 框架使用 WebSocket 协议实现多人模式中的同步移动，攻击以及被击中判定函数。 此外实现房间内玩家数提示板、技能冷却时间以及闪现技能。 1. 编写移动同步函数move_to 与上一章中的 create_player 同步函数相似，移动函数的同步也需要在前端实现 send_move_to 和 receive_move_to 函数。我们修改 MultiPlayerSocket 类（在目录 ~/djangoapp/game/static/js/src/playground/socket/multiplayer 中）： 然后修改一下后端通信代码（~/djangoapp/game/consumers/multiplayer 目录中的 index.py 文件）： 最后我们还需要调用函数，首先我们需要在 AcGamePlayground 类中记录下游戏模式 mode： 然后在 Player 类中进行修改，当为多人模式时，需要广播发送 move_to 信号： 现在即可实现多名玩家的同步移动。当 A 窗口中的玩家移动时，首先该窗口（Player 类）的监听函数会控制该玩家自身进行移动，接着判定为多人模式，因此再调用 MultiPlayerSocket 类中的 send_move_to 函数向服务器发送信息（通过 WebSocket 向服务器发送一个事件），接着服务器端（~/djangoapp/game/consumers/multiplayer/index.py 文件中）的 receive 函数会接收到信息，发现事件 event 为 move_to，就会调用 move_to 函数，该函数会向这个房间中的其他所有玩家群发消息，每个窗口都会在前端（MultiPlayerSocket 类中）的 receive 函数接收到信息，通过事件路由到 receive_move_to 函数，该函数就会通过 uuid 调用每名玩家的 move_to 函数。 2. 编写攻击同步函数shoot_fireball 由于发射的火球是会消失的，因此需要先将每名玩家发射的火球存下来，此外我们实现一个根据火球的 uuid 删除火球的函数，在 Player 类中进行修改： 由于火球在 Player 中存了一份，因此我们在删除火球前需要将它从 Player 的 fire_balls 中删掉。且由于 FireBall 类中的 update 函数过于臃肿，可以先将其分成 update_move 以及 update_attack，我们修改 FireBall 类： 然后我们在 MultiPlayerSocket 类中实现 send_shoot_fireball 和 receive_shoot_fireball 函数： 现在我们需要实现后端函数： 最后是在 Player 类中调用函数： 3. 编写击中判定同步函数attack 我们需要统一攻击这个动作，由一个窗口来唯一判断是否击中，若击中则广播给其他窗口，因此窗口中看到其他玩家发射的火球仅为动画，不应该有击中判定。我们先在 FireBall 类中进行修改： 每名玩家还需要有一个函数 receive_attack 表示接收到被攻击的信息： 我们假设发射火球的玩家为 attacker，被击中的玩家为 attackee，被击中者的位置也是由攻击者的窗口决定的，且火球在击中其他玩家后在其他玩家的窗口也应该消失，因此还需要传火球的 uuid。我们在 MultiPlayerSocket 类中实现 send_attack 与 receive_attack 函数： 然后实现后端函数如下： 最后需要在火球 FireBall 类中调用攻击判定的同步函数： 4. 优化改进（玩家提示板、技能CD） 我们限制在房间人数还没到3个时玩家不能移动，需要在 AcGamePlayground 类中添加一个状态机 state，一共有三种状态：waiting、fighting、over，且每个窗口的状态是独立的，提示板会在之后进行实现： 接下来我们实现一个提示板，显示当前房间有多少名玩家在等待，在 ~/djangoapp/game/static/js/src/playground 目录下新建 notice_board 目录，然后进入该目录创建 zbase.js 文件如下： 每次有玩家创建时就将 player_count 的数量加一，当玩家数量大于等于3时将游戏状态转换成 Fighting，且设置除了在 Fighting 状态下点击鼠标或按下按键才有效果，否则无效。在 Player 类中进行修改： 现在对局一开始就能攻击，显然不太合适，因此还需要设定在游戏刚开始的前若干秒无法攻击，即技能冷却。每个窗口只有自己才有技能冷却，也就是只能看到自己的冷却时间。现在我们给火球技能设置一秒的冷却时间，在 Player 类中进行修改： 我们还不知道技能什么时候冷却好，因此还需要加上一个技能图标与 CD 提示，可以模仿其他 MOBA 类游戏，在技能图标上添加一层 CD 涂层即可。假设我们的技能图标资源存放在 ~/djangoapp/game/static/image/playground 目录下，那么我们在 Player 类中渲染技能图标： 5. 闪现技能 闪现技能的实现很简单，整体参考之前的火球技能即可，我们先实现单机模式下的闪现技能，在 Player 类中实现： 然后我们还需要将闪现技能在多人模式中进行同步，原理和移动的同步是一样的，先在 MultiPlayerSocket 类中实现前端函数： 然后实现一下后端，在 ~/djangoapp/game/consumers/multiplayer/index.py 文件中实现： 最后在 Player 类中调用一下广播闪现技能的函数即可： 上一章：Django学习笔记-实现联机对战（上）。 下一章：Django学习笔记-实现聊天系统。"},{"title":"Python子进程管理与进程信息获取","date":"2023-09-18T02:54:00.000Z","url":"/posts/31812.html","tags":[["Python","/tags/Python/"]],"categories":[["Python","/categories/Python/"]],"content":" 介绍 Python 中的子进程模块 subprocess、线程池模块 ThreadPoolExecutor 以及系统信息获取模块 Psutil 的用法。 1. Python子进程模块subprocess subprocess 模块允许我们启动一个新进程，并连接到它们的输入/输出/错误管道，从而获取返回值。 （1）run 方法 首先我们来看看 run 方法的使用，该方法的参数如下： args：表示要执行的命令。必须是一个字符串或字符串参数列表。 stdin、stdout 和 stderr：子进程的标准输入、输出和错误。其值可以是 subprocess.PIPE、subprocess.DEVNULL、一个已经存在的文件描述符、已经打开的文件对象或者 None。subprocess.PIPE 表示为子进程创建新的管道。subprocess.DEVNULL 表示使用 os.devnull。默认使用的是 None，表示什么都不做。另外，stderr 可以合并到 stdout 里一起输出。 timeout：设置命令超时时间。如果命令执行时间超时，子进程将被杀死，并抛出 TimeoutExpired 异常。 check：如果该参数设置为 True，并且进程退出状态码不是 0，则抛出 CalledProcessError 异常。 encoding：如果指定了该参数，则 stdin、stdout 和 stderr 可以接收字符串数据，并以该编码方式编码。否则只接收 bytes 类型的数据。 shell：如果该参数为 True，将通过操作系统的 Shell 执行指定的命令。 capture_output：如果 capture_output = True，则将捕获 stdout 和 stderr，调用时内部的 Popen 对象将自动使用 stdout = PIPE 和 stderr = PIPE 创建标准输出和标准错误对象；传递 stdout 和 stderr 参数时不能同时传递 capture_output 参数。如果希望捕获并将两个 stream 合并为一个，使用 stdout = PIPE 和 stderr = STDOUT。 下面我们来看一个例子，run 方法调用方式返回 CompletedProcess 实例： 其中，Python子进程测试程序1.py 内容如下： （2）Popen 方法 Popen 是 subprocess 的核心，子进程的创建和管理都靠它处理。 Popen 方法的参数如下： args：Shell 命令，可以是字符串或者序列类型（如：列表、元组）。 bufsize：缓冲区大小。当创建标准流的管道对象时使用，默认为 -1。0 表示不使用缓冲区，1 表示行缓冲，仅当 universal_newlines = True 时可用，也就是文本模式。正数表示缓冲区大小，负数表示使用系统默认的缓冲区大小。 stdin、stdout、stderr：分别表示程序的标准输入、输出、错误句柄。 preexec_fn：只在 Unix 平台下有效，用于指定一个可执行对象（callable object），它将在子进程运行之前被调用。 shell：如果该参数为 True，将通过操作系统的 Shell 执行指定的命令。 cwd：用于设置子进程的当前目录。 env：用于指定子进程的环境变量。如果 env = None，子进程的环境变量将从父进程中继承。 该方法会创建一个 Popen 对象， Popen 对象有以下几种方法： poll()：检查进程是否终止，如果终止返回 returncode，否则返回 None。 wait(timeout)：等待子进程终止。 communicate(input=None, timeout=None)：和子进程交互，向子进程发送和读取数据。将 input 指定数据发送到 stdin；从 stdout 和 stderr 读取数据，直到到达文件末尾，等待进程终止。所以，返回值是一个元组：(stdout_data, stderr_data)。如果 timeout 时间内子进程不结束，则会抛出 TimeoutExpired 异常。其中需要注意的是，捕获异常之后，可以再次调用该函数，因为子进程并没有被 KILL。因此，如果超时结束程序的话，需要现正确 KILL 子进程。 send_signal(singnal)：发送信号到子进程。 terminate()：停止子进程，也就是发送 SIGTERM 信号到子进程。 kill()：杀死子进程，发送 SIGKILL 信号到子进程。 Popen 方法的样例如下： 其中，Python子进程测试程序2.py 内容如下： 现在我们来看一下 communicate 的用法，我们将测试程序修改为运行时间超过一秒： 现在的 Python子进程测试程序2.py 内容如下： 2. ThreadPoolExecutor线程池 concurrent.futures 模块是 Python3.2 中引入的新模块，用于支持异步执行，以及在多核 CPU 和网络 I/O 中进行高效的并发编程。线程池的基类是 concurrent.futures 模块中的 Executor，Executor 提供了两个子类，即 ThreadPoolExecutor 和 ProcessPoolExecutor ，简化了跨平台异步编程的实现。其中 ThreadPoolExecutor 用于创建线程池，而 ProcessPoolExecutor 用于创建进程池。如果使用线程池/进程池来管理并发编程，那么只要将相应的 Task 函数提交给线程池/进程池，剩下的事情就由线程池/进程池来搞定。 首先，让我们先来理解多进程和多线程两种并发编程的方式： 多进程：当通过多进程来实现并发编程时，程序会将任务分配给多个进程，这些进程可以在不同的 CPU 上同时运行。进程之间是独立的，各自有自己的内存空间等，可以实现真正的并行执行。不过，进程之间的通信比较耗时，需要使用 IPC（进程间通信）机制，而且进程之间的切换比线程之间的切换耗时，所以创建进程的代价较高。 多线程：当通过多线程来实现并发编程时，程序会将任务分配给多个线程，这些线程可以在同一个进程中的不同 CPU 核上同时运行。线程之间共享进程的内存空间，因此开销比较小。但是需要注意，在 Python 解释器中，线程是无法实现真正的并行执行，因为 Python 有 GIL（全局解释器锁），它确保同时只有一个线程运行 Python 代码。因此，一个 Python 进程中的多个线程并不能并行执行，在使用多线程编程时不能完全利用多核 CPU。 ThreadPoolExecutor 创建一个线程池，任务可以提交到这个线程池中执行。ThreadPoolExecutor 比 ProcessPoolExecutor 更容易使用，且没有像进程那样的开销。它可以让我们在一个 Python 解释器中进行跨线程异步编程，因为它规避了 GIL。 Exectuor 提供了如下常用方法： submit(fn, *args, **kwargs)：将 fn 函数提交给线程池。*args 代表传给 fn 函数的参数，**kwargs 代表以关键字参数的形式为 fn 函数传入参数。 map(func, *iterables, timeout=None, chunksize=1)：该函数类似于全局函数 map(func, *iterables)，只是该函数将会启动多个线程，以异步方式立即对 iterables 执行 map 处理。 shutdown(wait=True)：关闭线程池。 程序将 fn 函数 submit 给线程池后，submit 方法会返回一个 Future 对象，Future 类主要用于获取线程任务函数的返回值。由于线程任务会在新线程中以异步方式执行，因此，线程执行的函数相当于一个“将来完成”的任务，所以 Python 使用 Future 来代表。 Future 对象提供了如下方法： cancel()：取消该 Future 代表的线程任务。如果该任务正在执行，不可取消，则该方法返回 False；否则，程序会取消该任务，并返回 True。 cancelled()：返回 Future 代表的线程任务是否被成功取消。 running()：如果该 Future 代表的线程任务正在执行、不可被取消，该方法返回 True。 done()：如果该 Funture 代表的线程任务被成功取消或执行完成，则该方法返回 True。 result(timeout=None)：获取该 Future 代表的线程任务最后返回的结果。如果 Future 代表的线程任务还未完成，该方法将会阻塞当前线程，其中 timeout 参数指定最多阻塞多少秒。 exception(timeout=None)：获取该 Future 代表的线程任务所引发的异常。如果该任务成功完成，没有异常，则该方法返回 None。 add_done_callback(fn)：为该 Future 代表的线程任务注册一个“回调函数”，当该任务成功完成时，程序会自动触发该 fn 函数。 在用完一个线程池后，应该调用该线程池的 shutdown() 方法，该方法将启动线程池的关闭序列。调用 shutdown() 方法后的线程池不再接收新任务，但会将以前所有的已提交任务执行完成。当线程池中的所有任务都执行完成后，该线程池中的所有线程都会死亡。 使用线程池来执行线程任务的步骤如下： 调用 ThreadPoolExecutor 类的构造器创建一个线程池。 定义一个普通函数作为线程任务。 调用 ThreadPoolExecutor 对象的 submit() 方法来提交线程任务。 当不想提交任何任务时，调用 ThreadPoolExecutor 对象的 shutdown() 方法来关闭线程池。 下面我们来看一个例子： 3. 系统信息获取模块Psutil 现在可能会有人在想那我们如何获取子进程/线程在运行时的时间开销或者内存占用等信息呢？Python 有一个第三方模块 psutil，专门用来获取操作系统以及硬件相关的信息，比如：CPU、磁盘、网络、内存等等。 首先我们需要安装 psutil，直接通过 pip 命令安装即可： （1）查看 CPU 相关信息： （2）查看内存及磁盘相关信息： （3）查看网络相关信息： （4）查看进程相关信息： 现在我们使用 psutil 模块实现获取 ThreadPoolExecutor 线程任务运行的时间与内存占用信息： 其中，Python子进程测试程序2.py 内容如下： "},{"title":"Web学习笔记-React（Redux）","date":"2023-09-11T02:56:00.000Z","url":"/posts/41844.html","tags":[["Web","/tags/Web/"]],"categories":[["Web","/categories/Web/"]],"content":" 本文记录 React 的学习过程，内容为 Redux。 之前我们提到过一个问题，就是如果两个兄弟组件要访问对方的数据，需要将数据存放到最近公共祖先上，这样当 DOM 树很复杂时就很麻烦。Redux 就是在整个 DOM 树之外拿出一个地方，用来存储一些全局的值。 1. Redux基本概念 Redux 会将 state 维护成一个树结构，每个节点存一个值，使用一个函数 reducer 维护每个值。Redux 用字典存储子节点，一般被称为 store。 我们如果想修改树里面的某一个值，会将整个树重新计算一遍。我们会使用 dispatch 函数，他会递归调用每个 reducer 函数。此外还需要传入一个对象参数，表示需要对哪个节点进行操作，其中有个属性 type，我们会给每个节点定义一个唯一的 type。 Redux 的基本概念总结如下： store：存储树结构。 state：维护的数据，一般维护成树的结构。 reducer：对 state 进行更新的函数，每个 state 绑定一个 reducer。传入两个参数：当前 state 和 action，返回新 state。 action：一个普通对象，存储 reducer 的传入参数，一般描述对 state 的更新类型，其中的 type 属性表示要修改的节点。 dispatch：传入一个参数 action，对整棵 state 树操作一遍，即递归调用整棵树的所有 reducer 函数。 我们先创建一个 Redux 项目 redux-app： 进入项目根目录，安装相关的模块： 假设现在我们只维护一个 state，我们构建一个最简单的朴素版 Redux（和 React 无关）： 现在来看看如何维护两个节点： 控制台输出如下： f_all 也可以不用自己手写实现，可以使用 combineReducers 实现： 2. React-Redux基本概念 现在我们来看一下 Redux 如何与 React 组合起来。我们需要用 Provider 将我们的整个项目包含起来。React-Redux基本概念如下： Provider 组件：用来包裹整个项目，其 store 属性用来存储 Redux 的 store 对象。 connect(mapStateToProps, mapDispatchToProps) 函数：用来将 store 与组件关联起来，该函数会返回一个函数，返回的函数可以将组件作为输入参数，然后返回一个新的组件，这个新的组件会将 state 的值绑定到组件的 props 属性上。 mapStateToProps：每次 store 中的状态更新后调用一次，用来更新组件中的值，即将 store 中 state 的值绑定到组件的 props 属性上。 mapDispatchToProps：组件创建时调用一次，用来将 store 的 dispatch 函数传入组件，即将 dispatch 函数映射到组件的 props 属性上。 为了方便展示，我们定义三个组件：App、Number（state 从0开始）、String（state 从空串开始）。 App 代码如下： 然后我们在 index.js 中实现 React-Redux： 现在我们来看一下如何在 Number 和 String 组件中调用他们的 state 值，需要用到一个 API：connect，以 Number 为例： 现在我们再来看一下如何修改 state 的值，需要定义 mapDispatchToProps 对象将 dispatch 映射到 props 里，假设我们要在 Number 中操作 String 里的 state： 同理我们实现在 String 中操作 Number 中的 state： 上一章：Web学习笔记-React（路由）。 下一章：无。"},{"title":"Web学习笔记-React（路由）","date":"2023-09-10T10:22:00.000Z","url":"/posts/25510.html","tags":[["Web","/tags/Web/"]],"categories":[["Web","/categories/Web/"]],"content":" 本文记录 React 的学习过程，内容为路由。 本节内容是如何将页面和 URL 一一对应起来，并实现前端渲染。 1. Web分类 Web 页面可以分为两大类： 静态页面：页面里的数据是写死的，即整个文件存放在服务器上，当用户访问 URL 时，服务器原封不动地将页面信息传给前端。 动态页面：页面里的数据是动态填充的，即服务器上存的是页面的模板，数据是存到数据库里的，当用户打开页面时，会动态将这个页面拼接起来。现在一般都是动态页面。 后端渲染：数据在后端填充，即模板与数据的拼接操作是在服务器端进行的。客户端向服务器端发送 URL，服务器端返回拼接好的页面。 前端渲染：数据在前端填充，即模板与数据的拼接操作是在用户的浏览器进行的。第一次打开页面时，客户端向服务器端发送 URL，服务器端返回所有页面的模板，渲染的时候根据当前需要哪些数据再向服务器端请求数据；第二次打开页面时，直接用 JS 刷新当前页面，不一定会向后端发送请求。 2. Route组件 Route 组件可以让我们的前端页面也可以和 URL 唯一对应起来，使得前端渲染的模式看起来假装和后端渲染是一样的。 我们创建一个新的项目 route-app，然后用 VS Code 打开项目： 配置一下环境： VS Code 安装插件：Auto Import - ES6, TS, JSX, TSX 安装 Route 组件（在项目根目录下安装，安装好后重启一下 VS Code）：npm i react-router-dom 安装 Bootstrap：npm i bootstrap Route 组件介绍： BrowserRouter：所有需要路由的组件，都要包裹在 BrowserRouter 组件内； Link：跳转到某个链接（但是没有向后端发请求），to 属性表示跳转到的链接； Routes：类似于 C++ 中的 switch，但是只匹配第一个路径，即从前往后看每个 Route，判断当前链接是否等于 Route 中的链接，如果是则渲染 Route 中的组件，之后的就不继续往下判断了； Route：路由，path 属性表示路径，element 属性表示路由到的内容（组件）。 我们先创建好我们项目的根组件 App、导航栏 NavBar，以及多个子页面的组件：Home、Linux、Django、Web、NotFound。 NavBar 代码如下： App 代码如下： Home、Linux、Django、Web、NotFound 代码类似，只展示一个： 现在我们根据 URL 来渲染页面，注意此时还是属于后端渲染，每次都会重新加载页面，我们修改 App： 现在我们用 Link 替换 NavBar 中的链接标签 a，这样就变为了前端渲染： 3. URL中传递参数 当网站的页面数量很多的时候，我们肯定不可能去写那么多个 Route。 假设我们现在有几篇 Web 讲义，第 $i$ 篇的路由链接为：/web/content/i： 我们先实现一下讲义内容的组件 WebContent： 然后在 App 中写一下路由（我们不能写多个 &lt;Route path='/web/content/i' element=&#123;&lt;WebContent /&gt;&#125; /&gt;，而是用 :xxx）： 现在我们如何在 WebContent 中获取 :chapter 参数呢？先看一下函数组件获取参数的方式，可以直接用 useParams 函数获取参数： 如果是类组件的话就需要先套一层函数组件，然后把 useParams 函数作为参数传给自己： 4. Search Params传递参数 如果网站链接形式为：/web/content?chapter=3，这样的链接也可以获取参数。 我们先改一下 Web 中的链接形式： 然后在 WebContent 中获取链接的参数： 函数组件的写法如下： 5. 重定向 当打开一个不存在的链接时应该重定向到 404 Not Found，我们先将这个路由定义出来：&lt;Route path='/404' element=&#123;&lt;NotFound /&gt;&#125; /&gt;。 使用 Navigate 组件可以重定向，我们可以使用通配符 * 匹配其余的所有路径，然后将其重定向到 /404 页面即可： 6. 嵌套路由 假设 Linux 组件中有两个子模块 Homework 和 Terminal，我们可以在 App 中创建嵌套路由： 但是现在执行网页 /linux/homework 时不会渲染出子路由的内容，我们需要在父组件中添加 &lt;Outlet /&gt; 组件，用来填充子组件的内容： 上一章：Web学习笔记-React（组合Components）。 下一章：Web学习笔记-React（Redux）。"},{"title":"Web学习笔记-React（组合Components）","date":"2023-09-09T09:38:00.000Z","url":"/posts/18290.html","tags":[["Web","/tags/Web/"]],"categories":[["Web","/categories/Web/"]],"content":" 本文记录 React 的学习过程，内容为组合 Components。 本节内容是组件与组件之间的组合，例如用不同组件构成 DOM 树，以及给不同的组件传递数据或者调用不同组件的方法，还有不同组件的生命周期。 1. 创建父组件 我们还是继续在之前的 Box 组件上进行操作，首先创建一个 Boxes 组件，其中包含一系列 Box 组件。 在 components 目录中创建 boxes.jsx： 然后修改一下 index.js： 现在我们在 Boxes 中加入多个 Box，当一个组件中包含多个并列元素的时候，需要用一个标签将他们括起来，可以使用 React 中的一个虚拟标签 &lt;React.Fragment&gt;： 为了方便也可以用一个数组来表示，将 Box 的信息存到 state 里，由于 React 组件如果有若干个儿子那么他们的 key 需要不一样，因此还需要存一个唯一的 id： 2. 从上往下传递数据 通过 this.props 属性可以从上到下传递数据。例如我们在 Boxes 中传递 x： 可以在 Box 中输出信息 console.log(this.props); 查看内容。 修改 Box 中的 x： 3. 传递子节点 可以将标签写成 &lt;Box&gt;&lt;/Box&gt; 的形式，然后在标签中添加子标签： 这样 this.props 中会多一个属性 children，可以使用 [] 单独访问某个子标签。我们可以将这个传过来的值定义在任何地方，例如可以放到每个 Box 组件的最上方： 4. 从下往上调用函数 父元素可以通过 this.props 向子元素传递信息，子元素也可以使用函数向父元素传递信息。假设我们需要实现通过点击删除按钮删除某个 Box，其信息保存在 Boxes 的 state 中，但是我们点击触发事件是在 Box 中（注意：每个组件的 this.state 只能在组件内部修改，不能在其他组件内修改）。 我们可以在父元素中定义好函数，然后将函数传给子元素： 这样子元素就能调用函数对父元素进行操作了： 现在我们在 Boxes 中实现一个 Reset 按钮实现清空所有 Box 的 x： 在控制台观察时可以发现点击 Reset 按钮后 x 确实置零了，但是 Box 显示出来的 x 并没有改变，这是因为 state 值不能在外部修改，因此我们可以将 Box 中的 state 删掉，需要在该组件中渲染外面的 state 的值。 每个维护的数据仅能保存在一个 this.state 中，不要直接修改 this.state 的值，因为 setState 函数可能会将修改覆盖掉。 修改 Boxes，将之前 Box 中操作 state 的函数转移过来： 然后修改 Box，将 this.state 替换成父组件传递过来的 props： 5. 兄弟组件间传递消息 如果组件的结构关系更为复杂，那么就需要将多个组件共用的数据存放到最近公共祖先的 this.state 中。 我们创建一个 App 组件，其包含两个子组件 NavBar（导航栏）和 Boxes，这两个组件为兄弟组件。 首先是 navbar.jsx： 然后是 app.jsx： 现在假设我们要在 NavBar 中存放 Boxes 中有几个 Box 的信息，那么只能把信息放到这两个组件的最近公共祖先 App 中。 我们将 Boxes 中与 state 有关的内容都移到 App 中： 移动后的 App 如下： 现在即可在 NavBar 中读取 Boxes 的长度信息了： 6. 无状态函数组件 当组件中没有用到 this.state 时，可以简写为无状态的函数组件。类相对于函数最大的好处就是可以很方便地维护状态（局部变量）。 无状态函数组件（Stateless Funtion Component），输入 sfc 即可自动补全出来。函数组件相当于只有 render 函数的类组件。注意：函数的传入参数为 props 对象： 7. 组件的生命周期 Mount 周期（挂载，表示对象被创建出来），执行顺序（按顺序执行三个函数）：constructor() -&gt; render() -&gt; componentDidMount() Update 周期（修改，例如点击按钮），执行顺序：render() -&gt; componentDidUpdate() Unmount 周期（删除），执行顺序：componentWillUnmount() 其中，componentDidUpdate 函数有两个参数，分别表示更新前的 props 和 state： 输出的 state 内容如下： 上一章：Web学习笔记-React（配置环境、ES6语法补充、Components）。 下一章：Web学习笔记-React（路由）。"},{"title":"Django学习笔记-实现联机对战（上）","date":"2023-08-27T09:42:00.000Z","url":"/posts/31494.html","tags":[["Python","/tags/Python/"]],"categories":[["Python","/categories/Python/"]],"content":" 本节内容是通过 Django Channels 框架使用 WebSocket 协议实现多人模式中的同步创建玩家函数。 1. 统一长度单位 多人模式中每个玩家所看到的地图相对来说应该是一样的，因此需要固定地图的长宽比，一般固定为16:9。我们需要在游戏窗口的长宽中取最小值，然后将地图渲染为16:9的大小。 我们在 AcGamePlayground 类中实现一个 resize 函数用于将长宽比调整为16:9并且达到最大： 现在需要将窗口大小的修改效果作用到黑色背景上，因此我们在 GameMap 类中也实现一个 resize 函数用于修改背景大小： 我们修改一下 game.css 文件，添加以下内容，实现将地图居中： 现在我们还需要修改地图里面的目标，一共有三种分别是玩家、火球、被击中的粒子效果。 首先修改一下 AcGamePlayground 类中的玩家初始化代码： 然后我们修改 Player 类，将所有绝对变量替换为相对变量： 然后修改 FireBall 类，只需要修改 eps 以及 render 函数即可： 最后修改 Particle 类，同样也是只需要修改 eps 以及 render 函数即可： 2. 增加联机对战模式 我们先修改 AcGameMenu 类，实现多人模式按钮的逻辑： 然后修改 AcGamePlayground 类，区分两种模式，且需要进一步区分玩家类别，之前使用 True/False 表示是否是玩家本人，现在可以用字符串区分玩家本人（me）、其他玩家（enemy）以及人机（robot）： 然后还需要修改一下 Player 类，将原本的 this.is_me 判断进行修改： 3. 配置Django Channels 假设有三名玩家编号为1、2、3进行多人游戏，那么每个玩家都有自己的一个窗口，且窗口中都能看到三名玩家。如果当前玩家1、2在进行游戏，3加入了游戏，那么需要告诉1、2两名玩家3来了，且还要告诉3当前已经有玩家1、2了。 要实现这一点，可以通过一个中心服务器 Server（可以就是自己租的云服务器），即3向服务器发送他来了，服务器给1、2发送消息，且服务器给3发送消息说之前已经有1、2两名玩家了。因此服务器中需要存储每个地图中的玩家信息，用于完成第一个同步事件：生成玩家事件。 我们之后一共需要实现四个同步函数：create_player、move_to、shoot_fireball、attack。前三个函数顾名思义，最后的 attack 函数是因为服务器存在延迟，比如3发射一个火球在本地看打中了1，但是由于延迟在1那边可能是没被打中的。 攻击判断是一个权衡问题，一般的游戏都是选择在本地进行攻击判断，而不是云服务器，即以发起攻击的玩家窗口进行判断，如果击中了则通过 attack 函数在服务器上广播信息。 在此之前我们使用的是 HTTP 协议，该协议为单向的，即客户端需要先向服务器请求信息后服务器才会返回信息，而服务器是不会主动向客户端发送信息的。 因此此处我们需要使用 WebSocket 协议（WS），同理该协议也有对应的加密协议 WSS，Django Channels 即为 Django 支持 WSS 协议的一种实现方式。 （1）安装 channels_redis： （2）配置 djangoapp/djangoapp/asgi.py 文件： 文件内容如下（注意 djangoapp 需要改成自己项目的名称）： （3）配置 djangoapp/djangoapp/settings.py 文件： 在 INSTALLED_APPS 中添加 channels，添加后如下所示（注意 djangoapp 需要改成自己项目的名称）： 然后在文件末尾添加： （4）配置 game/routing.py 文件： 这一部分的作用相当于 HTTP 的 urls，文件内容如下： （5）编写 game/consumers，这一部分的作用相当于 HTTP 的 views。 在 game 目录下创建 consumers 目录，然后进入该目录，先创建好 __init__.py 文件。由于我们未来会使用 WSS 协议支持联机对战和聊天室，因此我们需要再创建两个目录，先创建 multiplayer 目录，进入该目录创建 __init__.py 文件，然后编写 index.py： （6）启动 django_channels： 首先安装 daphne： 输入 daphne 查看是否可用，如果不可用说明应该是没有配置环境变量，按如下方式修改环境变量（需要重启系统）： HTTP 有 uwsgi 启动服务，WS 同样也需要启动，使用的是 asgi，在 ~/djangoapp 目录下执行（注意 djangoapp 需要改成自己项目的名称）： 项目进行到这里已经需要启动多个服务了，顺序是： 4. 前端创建连接 前端（Playground）需要跟后端（WS）连接，我们需要在每个客户端中建立一个和服务器的连接，一般是使用 Web Socket 连接。 首先配置一下 /djangoapp/game 目录下的路由 routing.py： 我们在 /djangoapp/game/static/js/src/playground 目录下创建 socket 目录，Socket 也分为两个模块，分别为联机模式和聊天室，还是先实现联机模式，在 socket 目录中创建 multiplayer 目录，进入该目录后创建 zbase.js： 当前端创建连接，即执行 new WebSocket 时，会调用 consumers 的 MultiPlayer 类（之后直接称为 MultiPlayer）中的 connect 函数；当前端断开连接（刷新或者关闭页面）时，会调用 disconnect 函数；receive 函数用于接收前端向后端发送的请求。 由于服务器需要向多个客户端群发消息，Django Channels 中有一个概念叫做组（group），可以将多个不同的连接放到同一个组里，可以使用相关函数统一操作组里的连接，例如 group_send 群发消息。 现在在 AcGamePlayground 类中添加我们刚实现的 MultiPlayerSocket： 然后我们打包静态文件并同步到发行版本，复习一下： 重启一下服务，现在我们每次都需要重启两个服务，即 HTTPS 和 WSS： 打开游戏进入多人模式，即可在后端的 WSS 服务看到输出信息。 5. 前端发送请求 有玩家进来时，需要两个函数，一个是实现客户端向服务器发送 create_player 请求，另一个是实现服务器从客户端接收请求的功能。 先实现发送请求功能，在 MultiPlayerSocket 类中编写 send_create_player 函数： 然后在 AcGamePlayground 类中调用： 没有修改过后端代码，因此不需要重启服务，直接重新打包一下静态文件即可，然后进入多人模式即可看到后端的输出信息，说明现在连接就已经创建成功了。 6. 编写同步函数create_player 我们每一个地图的所有信息都会有一个备份，比如1号玩家在2、3号窗口都会有一个备份，在不同的地图里我们需要能够判断出来谁是谁，比如在1号窗口中玩家1击中了玩家2，那么把信息发送到第二个窗口后，需要知道谁是2。 因此我们需要给所有信息一个唯一的编号（可以使用一个随机的八位数，如果怕重复可以使用更多位数），使得我们未来知道需要同步哪些东西。在 AcGameObject 类中进行修改： 现在又有一个问题，每次有一名新玩家进入时都会创建若干个编号，但是和之前其它窗口中的编号不一致，我们需要用通信的方式将他们保持一致，原则为谁创建的对象就用谁那边产生的编号，比如1号窗口创建了1号玩家，那么其它玩家窗口中的1号玩家的编号就由1号窗口发送过来。 由于某个客户端（假设是1号窗口）向服务器发送消息后服务器会转播给所有客户端，也就是会发给自己（1号窗口），这种情况1号窗口应该要 Pass 掉这条信息，因此我们需要判断信息是哪个客户端发的。这边我们就可以用每个玩家的唯一编号，这样就可以保证每个窗口不一样，我们在 AcGamePlayground 中修改： 现在客户端向服务器端发送消息的时候就要带上自己的 uuid： 现在再测试一下即可在后端看到服务器接收到的客户端信息。 接下来我们需要同步 create_player 这个事件，即当有新玩家来的时候，在所有窗口里创建这个玩家，同时将已有的玩家渲染到当前窗口里。 首先我们需要在服务器端存下每一个游戏房间的信息，可以存在 Redis 里，我们可以用房间（room）的概念。每个房间有人数上限，这是一个通用配置，可以写在 settings.py 里，我们往该文件里添加一行： 然后修改后端文件（/djangoapp/game/consumers/multiplayer 目录中的 index.py）： 现在我们需要在 AcGamePlayground 中传送用户名和头像参数： 然后在 MultiPlayerSocket 中接收信息并向服务器后端发送请求： 现在编写后端接收到信息后的处理逻辑，接收到创建玩家的信息后需要将该玩家添加到房间中，并给房间中其他连接群发消息，组内各个连接接收到消息后再发送给前端即可： 现在我们需要在前端处理接收 WSS 协议的信息，修改一下 MultiPlayerSocket 类： 现在我们多开窗口即可看到多名玩家同步到一个地图上了。 上一章：Django学习笔记-AcApp端授权AcWing一键登录。 下一章：Django学习笔记-实现联机对战（下）。"},{"title":"Django学习笔记-AcApp端授权AcWing一键登录","date":"2023-08-26T11:59:00.000Z","url":"/posts/57210.html","tags":[["Python","/tags/Python/"]],"categories":[["Python","/categories/Python/"]],"content":" 本节内容是通过 OAuth2 实现 AcApp 端的 AcWing 一键登录功能。 AcApp 端使用 AcWing 一键授权登录的流程与之前网页端的流程一样，只有申请授权码这一步有一点细微的差别。 我们在打开 AcApp 应用之后会自动向 AcWing 请求账号登录，客户端会向后端服务器请求一些参数，然后后端服务器向 AcWing 请求授权码，然后 AcWing 在接到请求之后会询问用户是否要授权登录，如果用户同意了那么 AcWing 会给客户端发送一个授权码，客户端可以通过授权码加上自己的身份信息向 AcWing 服务器请求自己的授权令牌 access_token 和用户的 openid，最后客户端在拿到令牌和 ID 后即可向 AcWing 服务器请求用户的用户名和头像等信息。 在网页端授权登录时我们使用的方法是通过 URL 的方式重定向到某一个链接里申请授权码，而这次的 AcApp 不是通过链接，而是通过 AcWing 的一个 API 申请，请求授权码的 API： 参数说明： appid：应用的唯一 ID，可以在 AcWing 编辑 AcApp 的界面里看到； redirect_uri：接收授权码的地址，表示 AcWing 端要将授权码返回到哪个链接，需要对链接进行编码：Python3 中使用 urllib.parse.quote；Java 中使用 URLEncoder.encode； scope：申请授权的范围，目前只需填 userinfo； state：用于判断请求和回调的一致性，授权成功后原样返回该参数值，即接收授权码的地址需要判断是否是 AcWing 发来的请求（判断收到的 state 与发送出去的 state 是否相同），如果不是直接 Pass。该参数可用于防止 CSRF 攻击（跨站请求伪造攻击），建议第三方带上该参数，可设置为简单的随机数（如果是将第三方授权登录绑定到现有账号上，那么推荐用 随机数 + user_id 作为 state 的值，可以有效防止CSRF攻击）。此处 state 可以存到 Redis 中，设置两小时有效期； callback：redirect_uri 返回后的回调函数，即接受 receive_code 函数向前端返回的信息。 用户同意授权后，会将 code 和 state 传递给 redirect_uri。 如果用户拒绝授权，则将会收到如下错误码： 我们在 game/views/settings/acwing/acapp 目录中将之前网页端的 apply_code.py 与 receive_code.py 复制过来，然后对 apply_code.py 进行一点小修改，这次不是返回一个链接，而是返回四个参数： 进入 game/urls/settings/acwing 修改一下路由： 现在访问  即可看到返回内容。 然后我们修改一下 receive_code.py： 接着我们修改前端文件，也就是 game/static/js/src/settings 目录中的 Settings 类： 注意，如果遇到跨域问题：Access to XMLHttpRequest at 'XXX'，大概率是某个文件的内容写错了，可以检查 uWSGI 启动后的报错内容修改代码。 上一章：Django学习笔记-VS Code本地运行项目。 下一章：Django学习笔记-实现联机对战（上）。"},{"title":"DeepLabV3Plus核心代码详解","date":"2023-08-19T07:49:00.000Z","url":"/posts/21781.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 本文记录 DeepLabV3+ 论文（Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation）的阅读笔记。 1. Atrous Spatial Pyramid Pooling ASPP（Atrous Spatial Pyramid Pooling），空洞空间金字塔池化。简单理解就是个至尊版池化层，其目的与普通的池化层一致，尽可能地去提取特征。 ASPP 本质上由一个 1×1 的卷积层、三个 3×3 的空洞卷积层 ASPP Conv 以及一个全局池化层 ASPP Pooling。五个模块输出的特征图尺寸都与输入相同，因此最后将它们在通道维度上 Concat 起来然后再通过一个 1×1 的卷积层降维得到 ASPP 的输出。 1.1 ASPP Conv 空洞卷积层与一般卷积层之间的差别在于膨胀率（dilation rate），膨胀率控制的是卷积时的 padding 以及 dilation。ASPP 应用了多个不同膨胀率的并行空洞卷积。通过不同的填充与膨胀，可以在不改变输出图像尺寸的情况下获取不同尺度的感受野，提取多尺度的信息。注意卷积核尺寸始终保持 3×3 不变： 1.2 ASPP Pooling ASPP Pooling 首先是一个 AdaptiveAvgPool2d 层。所谓自适应均值池化，其自适应的地方在于不需要指定 kernel size 和 stride，只需指定最后的输出尺寸（此处为 1×1）。通过将各通道的特征图尺寸分别压缩至 1×1，从而提取各通道的特征，进而获取全局的特征。然后是一个 1×1 的卷积层，对上一步获取的特征进行进一步的提取，并降维。需要注意的是，在 ASPP Pooling 的网络结构部分，只是对特征进行了提取；而在 forward 方法中，除了顺序执行网络的各层外，最终还将特征图从 1×1 上采样回原来的尺寸： 1.3 ASPP 我们将以上模块进行组合即可构建完整的 ASPP 模块： 2. 空洞卷积ResNet 本文以 ResNet50、101、152 为例，构建加入了空洞卷积的 ResNet 网络。ResNet 网络一共有四大层，我们记为 layer1、layer2、layer3 以及 layer4，默认情况下输出的 Feature Map 宽高尺寸比原图像小32倍，我们可以在第2~4层使用空洞卷积，假设在最后一层使用了空洞卷积，那么最后输出的 Feature Map 宽高尺寸比原图像小16倍： 3. IntermediateLayerGetter 我们需要用到 ResNet 特征提取层提取出的特征，其中有浅层特征（layer1 的输出）与深层特征（layer4 的输出），两者的通道维度我们分别记作 low_level_channels 与 in_channels，越深层的特征蕴含的语义信息越丰富，但是目标的位置信息更为模糊，小目标的特征可能还会丢失。因此我们需要记录下这些特征，实现 IntermediateLayerGetter： 4. DeepLabV3Plus ResNet 与 ASPP 均为 DeepLabV3 的 Encoder 部分，现在先介绍一下 Decoder 部分 DeepLabHeadV3Plus。 ResNet 提取出的浅层特征与深层特征的通道维度我们分别记作 low_level_channels 与 in_channels。先对浅层特征做一个投影，降低通道维度，将投影后的特征记作 low_level_feature；接着将深层特征通过 ASPP，并上采样到与 low_level_feature 相同的尺寸，将该特征记作 output_feature；最后将这两个特征在通道维度上 Concat 起来后通过分类头将通道维度映射为分类数量： 空间金字塔池化模块能够通过使用滤波器（卷积）或池化操作以多种比率（rate）和多个有效感受野（fields-of-view）探测传入特征，对多尺度的上下文信息进行编码；Encoder-Decoder 结构的模块能够通过逐渐恢复空间信息来捕获更清晰的对象边界。DeepLabV3Plus 结合了这两者的优点，通过添加了一个简单而有效的解码器模块来扩展 DeepLabv3，以细化分割结果，尤其是沿对象边界的分割结果。 此外还可将深度可分离卷积应用于 ASPP 和解码器模块，从而形成更快、更强的编码器-解码器网络，该部分代码在下一节中介绍。 最后我们组合所有的模块即可构建完整的 DeepLabV3Plus，注意最后需要对 Decoder 的分类头输出进行上采样，恢复到原始的输入图像大小： 5. AtrousSeparableConvolution 深度可分离卷积，将标准卷积分解为 Depth-wise 卷积与 Point-wise 卷积，大大降低了计算复杂度。我们在其中结合空洞卷积即可构建空洞可分离卷积： "},{"title":"KMeans聚类与PCA主成分分析","date":"2023-08-09T02:03:00.000Z","url":"/posts/23991.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 介绍二维数据与高维数据的 K-Means 聚类算法以及高维数据的 PCA 主成分分析方法。 1. 二维数据K-Means聚类 2. 主成分分析PCA PCA（Principal Component Analysis）是一种常见的数据分析方式，常用于高维数据的降维，可用于提取数据的主要特征分量。PCA 通常用于降低大型数据集的维数，使数据集中的指标数量变少，并且保留原数据集中指标的大部分信息。总而言之：减少数据指标数量，保留尽可能多的信息。 PCA 优点在于数据降维，便于提取数据的主要特征，使得数据更容易使用，减少计算开销，去除噪音等；PCA 适用于结构化数据，不仅能将数据压缩，也使得降维之后的数据特征相互独立。PCA 缺点在于不一定需要，有可能损失有用信息，只针对训练集保留主要信息，可能造成过拟合。 PCA 的步骤主要分为五步：标准化连续初始变量的范围、计算协方差矩阵以识别相关性、计算协方差矩阵的特征向量和特征值以识别主成分、创建特征向量来决定保留那些主成分、沿主成分轴重铸数据。 我们随机生成具有 y = 0.5x 的二维数据，并在 Y 轴方向上添加少量随机噪音。通过使用 PCA 法拟合数据，其中最重要的是分量和解释方差，对于这些数字的概念，让我们将其可视化为输入数据上的向量，使用分量来定义向量的方向，使用解释方差来定义向量的平方长度。 这些向量代表数据的主轴，向量的长度表明该轴在描述数据分布方面的重要性，更准确的说，它是投影时数据方差的度量到那个轴。每个数据点在主轴上的投影是数据的主成分。这种从数据轴到主轴的变换被称为 Affine transformation，基本上由平移，旋转和均匀缩放组成。 代码如下： 3. 高维数据PCA后聚类 4. 高维数据聚类并计算与中心的相似度 "},{"title":"分割图像的着色与相似度匹配","date":"2023-08-08T06:14:00.000Z","url":"/posts/21944.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 介绍图像分割后产生的 Mask 灰度图的着色以及相似度匹配计算。 1. 分割图像着色 以 SAM 分割为例，我们分割出来产生的 masks 为一个 List，长度为分割出来的类别数，List 中的每个元素为一个 Dict，记录了分割目标的面积、边界框等信息，其中的 segmentation 字段为分割出来的二值图，宽高与原图一致，目标像素点为 True，否则为 False。 我们实现两种方式分别对分割出来的 masks 以及保存下来的若干张分割图像进行合并与上色。灰度图像和伪彩色图像都对应一个索引表，这个索引表又叫调色板。图像的像素值就是索引，灰度图的索引表为： 像素值 R G B 0 0 0 0 1 1 1 1 2 2 2 2 3 3 3 3 ... ... ... ... 255 255 255 255 索引表不同的像素值对应的 RGB 值就是该像素的颜色，灰度图像的索引表中的 RGB 值都与像素值相同。同理，只要修改这些 RGB 数值，就可以显示伪彩色图像了。注意调色板的索引从0-255，因此，调色板的每个索引对应的 RGB 值都要进行设置。 代码如下： 2. 分割图相似度匹配 区域相似度（Region Similarity）：为了测量基于区域的分割相似度，即错误像素的数量，我们使用 Jaccard 索引 𝒥 表示， 𝒥 定义为预测的分割输出 Mask 和真值 Mask 之间的交并比 IoU（Intersection over Union），Jaccard 索引提供了关于错误分类像素的直观的信息。 边沿精度（Contour Accuracy）：边沿精度即计算 F-score，F-score 评估的是预测 Mask 的边界是否与真值 Mask 的边界对应。首先应提取预测 Mask 和真值 Mask 的边界元素坐标，将边界上的元素置为 True，非边界的元素置为 False。F-score 被定义为精度和召回率的调和平均数。 精度（Precision，P，也称查准率）：分母应是预测 Mask 的边界元素总数，分子则是在预测 Mask 为边界的那些元素中真正属于真值的。换句话说，预测 Mask 假设有100个元素为边界元素，但实际上可能只有70个存在于真值图的对应位置上，即70个真值的正样本被正确（True）预测为 Positive，属于 True Positive（TP），所以此时的查准率为70%，剩下的30个元素是错误（False）预测为 Positive，属于 False Positive（FP）。 召回率（Recall，R，也称查全率）：分母是真值 Mask 的边界元素总数，分子表示多少个本质的正样本被预测出来。例如真值 Mask 的边界有140个元素，但实际的预测 Mask 中只有70个真值的正样本被正确（True）预测为 Positive（TP），还有70个被错误（False）预测为 Negative（False Negative），那么此时的 Recall 为50%。 J &amp; F 指标的计算代码如下： "},{"title":"DeAOT视频追踪论文阅读笔记","date":"2023-08-08T05:58:00.000Z","url":"/posts/6828.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 本文记录 DeAOT 视频追踪论文的阅读笔记。 涉及的相关知识点为：AOT（Associating Objects with Transformers for Video Object Segmentation）、DeAOT（Decoupling Features in Hierarchical Propagation for Video Object Segmentation）、FPN（Feature Pyramid Networks for Object Detection）、Depth-wise Convolution、DropPath、GroupNorm。 1. 相关知识 1.1 深度可分离卷积 Depth-wise（DW）卷积与 Point-wise（PW）卷积，合起来被称作 Depth-wise Separable Convolution（深度可分离卷积），该结构和常规卷积操作类似，可用来提取特征，但相比于常规卷积操作，其参数量和运算成本较低。所以在一些轻量级网络中会碰到这种结构，如 MobileNet。 Depth-wise Convolution 的一个卷积核负责一个通道，即一个通道只被一个单通道的卷积核卷积，而常规卷积每个卷积核是同时操作输入图片的每个通道，即每个卷积核的通道数与图片的通道数相同。 Depth-wise Convolution 完成后的 Feature Map 数量与输入层的通道数相同，无法扩展 Feature Map。而且这种运算对输入层的每个通道独立进行卷积运算，没有有效地利用不同通道在相同空间位置上的特征信息。因此需要Point-wise Convolution 来将这些 Feature Map 进行组合生成新的 Feature Map。 Depth-wise Convolution 代码如下： Point-wise Convolution 的运算与常规卷积运算非常相似，它的卷积核的尺寸为 1 × 1 × M，M 为输入图像的通道数。所以这里的卷积运算会将上一步的 Feature Map 在深度方向上进行加权组合，生成新的 Feature Map。有几个卷积核就有几个输出 Feature Map。 Point-wise Convolution 代码如下： Depth-wise 卷积对每个输入通道独立执行空间卷积，而 Point-wise 卷积用于组合 Depth-wise 卷积的输出。将两者组合起来即为深度可分离卷积神经网络： 1.2 DropPath DropPath 是一种针对分支网络而提出的网络正则化方法，作用是将深度学习网络中的多分支结构随机删除。DropPath 作用的是网络分支，而 DropOut 作用的是 Feature Map，DropConnect 作用的是参数。 简单来说，DropPath 的输出是随机将一个 batch 中所有的神经元均设置为0；而在 DropOut 中，是在每个 batch 中随机选择神经元设置为0。 DropPath 代码如下： 1.3 Group Normalization BN 全名是 Batch Normalization，见名知意，其是一种归一化方式，而且是以 batch 的维度做归一化，那么问题就来了，此归一化方式如果使用过小的 batch size 会导致其性能下降，一般来说每个 GPU 上的 batch size 设为32最合适，但是对于一些其他深度学习任务 batch size 往往只有1或2，比如目标检测，图像分割，视频分类上，输入的图像数据很大，较大的 batch size 显存吃不消。 另外，Batch Normalization 是在 batch 这个维度上做 Normalization，但是这个维度并不是固定不变的，比如训练和测试时一般不一样，一般都是训练的时候在训练集上通过滑动平均预先计算好平均（mean），和方差（variance）参数，在测试的时候，不再计算这些值，而是直接调用这些预计算好的参数来用。但是，当训练数据和测试数据分布有差别时，训练机上预计算好的数据并不能代表测试数据，这就导致在训练、验证、测试这三个阶段存在 inconsistency（不一致性）。 Group Normalization（GN）首先将 channel 分为许多组（group），对每一组做归一化，即先将 feature 的维度由 [N, C, H, W] reshape 为 [N, G, C/G, H, W]，归一化的维度为 [C/G, H, W]。事实上，GN 的极端情况就是 LN（Layer Normalization）和 IN（Instance Normalization），分别对应 G = 1 和 G = C，作者在论文中给出 G 设为32较好。 Group Normalization 代码如下： 1.4 FPN特征金字塔网络 目标的多尺度一直是目标检测算法极为棘手的问题。像 Fast R-CNN，YOLO 这些只是利用深层网络进行检测的算法，是很难把小目标物体检测好的。因为小目标物体本身的像素就比较少，随着下采样的累积，它的特征更容易被丢失。 特征金字塔网络（Feature Pyramid Network，FPN）是一个在特征尺度的金字塔操作，它是通过将自底向上（Bottom-up）和自顶向下（Top-down）的特征图进行融合来实现特征金字塔操作的。FPN 提供的是一个特征融合的机制，并没有引入太多的参数，实现了以增加极小计算代价的情况下提升对多尺度目标的检测能力。 自底向上即是卷积网络的前向过程，我们可以选择不同的骨干网络，例如 ResNet-50 或者 ResNet-101。前向网络的返回值依次是 C2、C3、C4、C5，是每次池化之后得到的 Feature Map。 通过自底向上路径，FPN 得到了四组 Feature Map。浅层的 Feature Map，例如 C2 含有更多的底层信息（纹理，颜色等），而深层的 Feature Map 如 C5 含有更多的语义信息。为了将这四组倾向不同特征的 Feature Map 组合起来，FPN 使用了自顶向下及横向连接的策略，最终得到 P2、P3、P4、P5 四个输出。 最后，FPN 在 P2、P3、P4、P5 之后均接了一个 3*3 Conv 操作，该卷积操作是为了减轻上采样的混叠效应（aliasing effect）。 FPN 和 U-Net 最大的不同是它的多个层级的都会有各自的输出层，而每个输出层都有不同尺度的感受野。一个比较粗暴的方式是每一层都预测所有的样本，而另一个更好的选择是根据一些可能存在的先验知识选择一个最好的层。 FPN 代码如下： 2. 半监督VOS与AOT模型 2.1 VOS与AOT简介 视频对象分割（VOS）旨在识别和分割给定视频中的一个或多个感兴趣的对象，半监督 VOS 需要算法在给定一帧或多帧的对象注释掩码的情况下跟踪和分割整个视频序列中的对象。 此前最先进的方法学习用单个正目标解码特征，因此必须在多目标场景下单独匹配和分割每个目标，消耗多倍的计算资源。我们提出 Associating Objects with Transformers（AOT）方法来统一匹配和解码多个对象。AOT 采用 Identification 机制将多个目标关联到同一高维嵌入空间中。因此可以像处理单个对象一样高效地同时处理多个对象的匹配和分割解码。 AOT 方法将分层传播引入到 VOS 中。分层传播可以逐渐将 ID 信息从过去的帧传播到当前帧，并将当前帧的特征从 object-agnostic（对象不可知）转移到 object-specific（对象特定）。 2.2 ID 机制 ID 机制为每个目标分配唯一的 ID 信息，并将任意数量（要求小于预定义的大量）目标的 mask 嵌入到同一高维空间中。因此，网络可以学习所有目标之间的关联或相关性。此外，可以利用分配的 ID 信息直接解码多对象分割。 我们初始化一个身份库（ID Bank），其中存储 M 个具有 C 维的识别向量。为了嵌入多个不同的目标掩码，每个目标将被随机分配一个不同的识别向量。 2.3 Long Short-Term Transformer（LSTT） 本文设计长短期 Transformer（LSTT）用于构建分层对象匹配和传播。每个 LSTT 块都利用长期注意力来匹配第一帧的嵌入，并利用短期注意力来匹配多个附近帧的嵌入。与仅利用一个注意力层的方法相比，我们发现分层注意力结构在关联多个对象方面更有效。 LSTT 首先采用自注意力层，负责学习当前帧内目标之间的关联或相关性。此外，LSTT 还引入了长期注意力和短期注意力，前者用于聚合来自长期记忆帧的目标信息，后者能够从邻近的短期帧学习时间平滑性。所有注意力模块都是以多头注意力的形式实现的，即多个注意力模块后跟串联和线性投影。 长期注意力负责将目标的信息从过去的记忆帧（包含参考帧和存储的预测帧）聚合到当前帧。由于当前帧和过去帧之间的时间间隔是可变的并且可以是长期的，因此时间平滑性难以保证。因此，长期注意力采用非局部注意力。 短期注意力用于聚合每个当前帧位置的时空邻域中的信息。直观上，多个连续视频帧之间的图像变化始终是平滑且连续的。因此，连续帧中的目标匹配和传播可以限制在小的时空邻域内，从而比非局部过程具有更好的效率。 3. DeAOT 本文重点是为半监督视频对象分割（VOS）开发一种更有效的分层传播方法。在 AOT 方法中 object-specific 信息的增加将不可避免地导致深层传播层中 object-agnostic 的视觉信息的丢失。为了解决这样的问题并进一步促进视觉嵌入的学习，本文提出了一种分层传播中的解耦特征（DeAOT）方法。DeAOT 通过在两个独立的分支中处理 object-agnostic 和 object-specific 的嵌入来解耦它们的分层传播。其次，为了补偿双分支传播的额外计算，设计了一种用于构造分层传播的有效模块 GPM（门控传播模块），它是通过单头注意力精心设计的。 3.1 分层双分支传播 DeAOT 在两个并行分支中传播对象的视觉特征（visual features）和掩码（mask features）特征。具体来说，视觉分支负责匹配对象、收集过去的视觉信息并提炼对象特征。为了重新识别对象，ID 分支重用视觉分支计算的匹配图（注意力图），将 ID 嵌入（由 AOT 中的 ID 机制编码）从过去的帧传播到当前帧。两个分支共享相同的具有 L 个传播层的层次结构。 3.2 门控传播模块GPM 门控传播函数首先通过使用条件门来增强基于注意力的传播，此外，我们利用 Depth-wise 卷积以轻量级方式增强局部空间上下文的建模。 门控传播模块由三种门控传播组成：自传播、长期传播、短期传播。与 LSTT 相比，GPM 去掉了前馈模块，进一步节省了计算量和参数。所有传播过程都采用门控传播函数。"},{"title":"Django学习笔记-VS Code本地运行项目","date":"2023-07-06T05:19:00.000Z","url":"/posts/56617.html","tags":[["Python","/tags/Python/"]],"categories":[["Python","/categories/Python/"]],"content":" 截止到上一章节，我们的项目一直是部署在云服务器上，包括编写代码以及调试运行也是在云服务器上，现在我们尝试将其放回 Windows 本地环境运行。 1. 将项目传到本地 这一步可以使用 Git 也可以使用 SCP，由于之前项目上传在 AcGit 上，只能在 AC Terminal 中拉取，因此使用 SCP 远程传输： 2. 虚拟环境配置 我们需要先配置一个和云服务上相同的虚拟环境，首先在 VS Code 中打开该项目并且打开终端，在项目根目录下创建虚拟环境： 虚拟环境创建成功之后，一般不会自动启用，所以需要启用它，进入 venv/Scripts 目录运行脚本 Activate.ps1： 此时在 VS Code 中可以看到命令行首部多了 (venv)，说明已进入虚拟环境，然后在右下角选择虚拟环境中的解释器即可。 现在我们在虚拟环境中安装所需的环境： 3. 修改项目相关文件 首先修改一下我们的打包脚本 compress_game_js.sh： 然后打开 Git Bash，进入 scripts 文件夹即可运行脚本： 然后将 djangoapp/settings.py 中的 Redis 配置删除，即删除以下这段话： 同样在该文件中需要将 localhost 添加到 ALLOWED_HOSTS 中： 然后将 AcWing 一键授权登录的相关文件夹删除：game/views/settings/acwing、game/urls/settings/acwing，然后修改 urls/settings/index.py 中的路由： 修改 AcGame 类，去掉 AcWingOS API，且改为非模块化引入 JS 方式，即去掉 export 关键字： 同时前端的 web.html 文件也需要修改： 最后修改 Settings 类，去掉 AcWing 一键登录功能，且修改 ajax 请求的地址： 顺带还需要修改一下 game.css： 到此已经可以在本地运行该项目了，我们也可以根据 Django 学习笔记-实现联机对战（上）和 Django 学习笔记-实现联机对战（下）统一长度单位以及添加闪现技能与技能冷却，因为这些部分与云端内容无关。 最后我们将 .git 文件夹删除，重新上传至 Github（先创建一个仓库 Small_Ball_Fight）： 上一章：Django学习笔记-Web端授权AcWing一键登录。 下一章：Django学习笔记-AcApp端授权AcWing一键登录。"},{"title":"Django学习笔记-Web端授权AcWing一键登录","date":"2023-06-27T15:19:00.000Z","url":"/posts/54821.html","tags":[["Python","/tags/Python/"]],"categories":[["Python","/categories/Python/"]],"content":" 本节内容是通过 OAuth2 实现网页端的 AcWing 一键登录功能。 1. 在Django中集成Redis Redis 为内存数据库，目前我们使用的是 Django 自带的数据库 SQLite，且能够很容易地迁移到 MySQL，这些数据库的效率不如 Redis，其特点为： Redis 存的内容为 key, value 对，而其它数据库存的是若干张表，每张表为若干条目； Redis 为单线程的，不会出现读写冲突。 首先我们需要先安装 Redis： 接着配置一下 Django 的缓存机制，将下面这段代码复制到 settings.py 中： 然后启动 redis-server，启动后可以使用 top 命令查看 redis-server 是否运行： 此时在项目根目录执行 python3 manage.py shell 进入 IPython 交互界面，输入以下代码测试一下 Redis： 注意如果出现报错：ConnectionError: Error 111 connecting to 127.0.0.1:6379. Connection refused. 说明 redis-server 没有启动。 2. 申请授权码 一键授权登录的流程是用户点击一键登录后弹出确认授权的页面，用户确认后就自动创建一个新的账号，且登录到网站里。 具体交互流程为：用户点击按钮后向网站服务器端（Web）发起申请，请求用 AcWing 账号登录，然后 Web 将自己的 AppID 报给 AcWing，AcWing 给用户返回一个页面询问用户是否要授权给刚刚的网站，如果用户同意，那么 AcWing 会将一个授权码 code（两小时有效期）发给 Web，Web 接到授权码后再加上自己的身份信息 AppSecret 以及 AppID 向 AcWing 申请一个授权令牌 access-token（两小时有效期）和用户的 openid（唯一辨别用户），Web 拿到令牌和用户 ID 后即可向 AcWing 申请用户的用户名和头像。 由于我们需要记录每个用户的 openid，因此我们需要在数据库（game/models/player/）的 Player 类中添加一个信息： 在根目录下更新一下数据库： 申请授权码的请求地址：。 请求方法为 GET，参考示例： 参数说明： appid：应用的唯一 ID，可以在 AcWing 编辑 AcApp 的界面里看到； redirect_uri：接收授权码的地址，表示 AcWing 端要将授权码返回到哪个链接，需要对链接进行编码：Python3 中使用 urllib.parse.quote；Java 中使用 URLEncoder.encode； scope：申请授权的范围，目前只需填 userinfo； state：用于判断请求和回调的一致性，授权成功后原样返回该参数值，即接收授权码的地址需要判断是否是 AcWing 发来的请求（判断收到的 state 与发送出去的 state 是否相同），如果不是直接 Pass。该参数可用于防止 CSRF 攻击（跨站请求伪造攻击），建议第三方带上该参数，可设置为简单的随机数（如果是将第三方授权登录绑定到现有账号上，那么推荐用 随机数 + user_id 作为 state 的值，可以有效防止 CSRF 攻击）。此处 state 可以存到 Redis 中，设置两小时有效期。 用户同意授权后会重定向到 redirect_uri，返回参数为 code 和 state。链接格式如下： 如果用户拒绝授权，则不会发生重定向。 我们在 game/views/settings 目录中创建一个 acwing 目录表示 AcWing 授权登录，然后在该目录中先创建一个 __init__.py，再创建两个子目录 web 和 acapp 分别表示 Web 端的 AcWing 一键登录以及 AcApp 端的 AcWing 一键登录，最后同样在这两个子目录中创建 __init__.py 文件。 在 web 目录下创建申请授权码的 API apply_code.py： 接着创建接收授权码的 API receive_code.py： 然后在 game/urls/settings 目录中也创建一个 acwing 目录，在该目录中创建 __init__.py 和 index.py，index.py 内容如下： 然后需要将该目录 include 进 settings 目录的 index.py 中： 重启项目后即可访问 ;公网IP&gt;/settings/acwing/web/apply_code/ 以及 ;公网IP&gt;/settings/acwing/web/receive_code/ 测试效果。 现在来看看 urllib.parse.quote 的作用，它能够将链接重新编码，替换掉原本的部分特殊字符防止出现 BUG： 现在我们完善一下 apply_code.py 的内容： 然后修改一下前端代码（Settings 类）： 3. 申请授权令牌和用户ID 请求地址：。 请求方法为 GET，参考示例： 参数说明： appid：应用的唯一 ID，可以在 AcWing 编辑 AcApp 的界面里看到； secret：应用的秘钥，可以在 AcWing 编辑 AcApp 的界面里看到； code：上一步中获取的授权码。 申请成功的返回内容示例： 申请失败的返回内容示例： 返回参数说明： access_token：授权令牌，有效期2小时； expires_in：授权令牌还有多久过期，单位（秒）； refresh_token：用于刷新 access_token 的令牌，有效期30天； openid：用户的 ID。每个 AcWing 用户在每个 AcApp 中授权的 openid 是唯一的，可用于识别用户； scope：用户授权的范围。目前范围为 userinfo，包括用户名、头像。 现在我们点击 AcWing 一键登录按钮即可跳出请求授权的页面，AcWing 会向 receive_code 函数发送 code 以及 state，现在我们完善 receive_code.py 接到授权后的操作： 现在我们点击一键登录按钮即可在后台看到接收到的授权令牌内容。 4. 申请用户信息 请求地址：。 请求方法为 GET，参考示例： 参数说明： access_token：上一步中获取的授权令牌； openid：上一步中获取的用户 openid。 申请成功的返回内容示例： 申请失败的返回内容示例： 现在我们用授权令牌向 AcWing 申请用户的用户名和头像，进一步完善 receive_code.py： 至此 Web 端的 AcWing 一键登录已经实现。 上一章：Django学习笔记-用户名密码登录。 下一章：Django学习笔记-VS Code本地运行项目。"},{"title":"Django学习笔记-用户名密码登录","date":"2023-06-27T07:48:00.000Z","url":"/posts/6653.html","tags":[["Python","/tags/Python/"]],"categories":[["Python","/categories/Python/"]],"content":" 本节内容是使用 Django 实现用户的注册与登录等后端功能。 1. 扩充Django数据库 首先我们先在 settings.py 中修改：DEBUG = True，否则如果服务器端的代码报错时前端不会显示报错详细信息。 Django 自带一个账号系统，在网址后面添加后缀 /admin 即可访问管理员界面，登录之前的超级管理员账号，进入管理员界面后其中的 Users 界面就是自带的账号系统。 其中可以填写每个用户的姓名和邮箱，Active 表示用户是否被封，不勾选即无法登录；Staff status 表示用户是否能进入到后台管理页面；Superuser status 表示用户是否具有超级管理员的权限。 这个自带的账号系统不能满足我们的需求，例如需要存储用户的头像，该数据库就需要进行扩充。 我们要在 djangoapp/game/models 目录下创建我们所需的表，即创建类似之前提到的 Users，我们在该目录中新建一个 player 目录，然后进入该目录，记得需要先创建一个 __init__.py 文件。 接着我们创建 player.py 用来存储 player 这个数据表的信息，创建类时需要继承基类 django.db.models.Model（开发时有个小 Tips，如果忘记一些关键字怎么写可以在项目根目录执行 python3 manage.py shell，进入 IPython 交互功能，然后输入代码即具有补全功能）： 创建好数据表后如果希望让我们的数据表出现在管理员界面，需要将它注册到管理员页面，在 game 目录下可以看到一个 admin.py 文件，对其进行修改： 每次对数据表的定义更新后都需要执行以下两句指令（在项目根目录）： 然后我们重启一下项目：uwsgi --ini scripts/uwsgi.ini，即可看到新创建的 Player 表，在右上角的 ADD PLAYER 选项中即可添加 Player。 从这个例子即可看出数据库中的 Table 对应 Django 中的 Class，Table 中的每一条数据就对应 Class 中的每一个对象。 2. 实现获取用户信息 假设我们将网页刷新后，Clinet 先向后台服务器（Server）发送一个请求获得当前玩家的信息 getinfo，后台一种是返回用户名和头像，还有一种是返回未登录，现在我们先默认每次都返回玩家的用户名和头像，每次写一个函数时需要写三个部分：views 表示调用数据库的逻辑、urls 表示路由、js 实现调用。 由于有多个前端，因此后端在接收请求的时候需要知道是哪个前端，需要对 AcGame 类进行修改，让其多传入一个参数 acwingos，如果在 AcWing 打开该项目则会传入该参数： 进入 views 目录，我们将所有用户的信息全部放到 settings 目录中，在 settings 目录创建一个 getinfo.py 文件，内容如下： 然后进入 urls/settings 目录，修改 index.py 文件： 现在我们即可访问 ;项目IP地址&gt;/settings/getinfo/ 查看效果。 最后我们需要在游戏的菜单界面之前添加一个界面判断用户是否登录，先将 AcGameMenu 类在初始化操作中 hide，然后进入 /djangoapp/game/static/js/src/settings 目录中创建 zbase.js： 然后在 AcGame 类中创建 Settings 对象： 此时访问网站即可看到控制台的输出信息。 此时项目的执行顺序为：创建 Settings 类的对象时先执行构造函数，接着会执行 start 中的 getinfo 函数，会向后端（）发一个请求，然后路由会找到 views.settings.getinfo 文件中的 getinfo 函数，然后判断 platform 为 WEB，最后通过 getinfo_web 返回用户名和用户头像到 resp 中。 现在我们来判断用户是否登录，修改 views/settings 目录中的 getinfo.py： 然后我们需要将用户的信息存下来，将头像渲染到小球里，首先在 js/src/settings 目录中修改 zbase.js： 然后在 Player 类中渲染用户的头像： 3. 渲染登录与注册界面 首先我们下载 AcWing 一键登录所需要的图标资源，进入 static/image/settings 目录执行以下指令： 然后完善 Settings 类渲染登录与注册界面并且实现两个界面的跳转功能： game.css 内容如下： 4. 实现登录与登出功能 实现登录与登出功能我们同样要编写 views、urls 以及 js 文件，首先在 game/views/settings 目录下创建 login.py 文件： 然后进入 game/urls/settings 目录修改 index.py： 此时重启一下项目后访问 ;公网IP&gt;/settings/login/ 即可看到返回结果。 现在我们实现一下登出函数，在 game/views/settings 目录下创建 logout.py 文件： 然后进入 game/urls/settings 目录修改 index.py： 此时重启一下项目后在登录状态下访问 ;公网IP&gt;/settings/logout/ 即可登出。 然后我们在 Settings 类中实现登录登出： 其中登出函数 logout_on_remote 需要在 AcGameMenu 类中调用： 5. 实现注册功能 首先在 game/views/settings 目录下创建 register.py 文件： 然后进入 game/urls/settings 目录修改 index.py： 此时重启一下项目后访问 ;公网IP&gt;/settings/register/ 即可看到返回结果。 最后修改前端 Settings 类： 6. 修改获取用户信息 之前我们是默认返回第一个用户的信息，我们对 game/views/settings 目录下的 getinfo.py 文件进行修改： 上一章：Django学习笔记-部署Nginx与对接AcApp。 下一章：Django学习笔记-Web端授权AcWing一键登录。"},{"title":"Django学习笔记-部署Nginx与对接AcApp","date":"2023-06-25T15:10:00.000Z","url":"/posts/13510.html","tags":[["Python","/tags/Python/"]],"categories":[["Python","/categories/Python/"]],"content":" 本节内容是通过 Nginx 与 uWSGI 将项目部署至 AcWing 平台上，同时修复在 AcApp 小窗口上存在的部分 BUG。 现在我们需要将之前在网页上运行的项目部署至 AcWing 上，让其前后端分离，一个后端可以对应多个前端。 如果要将网站修改为 HTTPS 协议，需要先购买一个域名，然后申请证书，还需要进行备案，非常麻烦。在 AcWing 上线 App 已具备域名和证书。 1. 增加容器的映射端口80与443 由于创建后的容器不方便增加新的端口映射，因此我们先将原容器保存成镜像后再生成一个新的容器。 首先需要将容器中正在运行的任务全部关闭，然后登录运行容器的服务器，执行以下命令： 此时如果无法远程连接容器可以按照 Django 学习笔记-配置 Docker、Git 环境与项目创建文章创建一个 hosts.allow 文件。 接着去云服务器的控制台，在安全组配置中开放80和443端口。 2. 安装Nginx （1）安装依赖包（安装均在根用户 root 下进行）： （2）安装 Nginx： （3）查看版本号： （4）启动 Nginx 以及查看是否在运行中： 如果报错：System has not been booted with systemd as init system (PID 1). Can't operate. Failed to connect to bus: Host is down，可以参考这篇文章：System has not been booted with systemd as init system (PID 1). Can‘t operate. 问题解决方法，简而言之就是用以下指令代替： 3. 写入AcWing配置信息 配置信息在 AcWing 平台上可以查看，按以下步骤将信息复制到项目的配置文件中即可： 将 nginx.conf 中的内容写入服务器 /etc/nginx/nginx.conf 文件中。如果 Django 项目路径与配置文件中不同，注意修改路径。 将 acapp.key 中的内容写入服务器 /etc/nginx/cert/acapp.key 文件中（cert 目录需要自己创建）。 将 acapp.pem 中的内容写入服务器 /etc/nginx/cert/acapp.pem 文件中。 然后启动 Nginx 服务： 如果启动不成功可以重新加载一下 Nginx 的配置文件即可看到错误在哪： 4. 修改Django项目的配置 打开 settings.py 文件： 将分配的域名添加到 ALLOWED_HOSTS 列表中。注意只需要添加 https:// 后面的部分。 令 DEBUG = False。 归档 static 文件： 在项目根目录下执行：python3 manage.py collectstatic，执行完后可以看到生成了一份 game 中的 static 目录。 5. 配置uWSGI WSGI（Web Server Gateway Interface）是为 Python 语言定义的 Web 服务器和 Web 应用程序或框架之间的一种简单而通用的接口。网关（Gateway）的作用就是在协议之间进行转换。很多框架都自带了 WSGI Server，比如 Flask、Django 等。当然性能都不好，自带的 Web Server 更多的是测试用途，发布时则使用生产环境的 WSGI Server 或者是联合 Nginx 做 uWSGI。 uWSGI 是一个 Web 服务器，它实现了 WSGI、uwsgi、HTTP 等协议。Nginx 中 HttpUwsgiModule 的作用是与 uWSGI 服务器进行交换。 要注意 WSGI、uwsgi、uWSGI 这三个概念的区分： WSGI 的内容之前已经讲过了，是一种通信协议。 uwsgi 同 WSGI 一样是一种通信协议。 uWSGI 则是实现了 uwsgi 和 WSGI 两种协议的 Web 服务器。 uwsgi 协议是一个 uWSGI 服务器自有的协议，它用于定义传输信息的类型（type of information），每一个 uwsgi packet 前 4byte 为传输信息类型描述，它与 WSGI 相比是两样东西。 为什么有了 uWSGI 还需要 Nginx？因为 Nginx 具备优秀的静态内容处理能力，然后将动态内容转发给 uWSGI 服务器，这样可以达到很好的客户端响应。 现在用户通过80/443端口访问 Nginx，而 Nginx 与 Django 项目之间还需要一个桥梁就是 uWSGI。 在 Django 项目中添加 uWSGI 的配置文件：scripts/uwsgi.ini，内容如下： 由于使用 uwsgi 命令启动项目后原来的公网 IP 就无法访问了，因此需要对 web.html 中 CSS、JS 的链接地址进行修改： 然后启动 uwsgi 服务（代替 python3 manage.py runserver 0.0.0.0:8000）： 在 AcWing 填写完应用的剩余信息即可发布。 6. 部分BUG修复 在 AcWing 中打开应用时可以发现鼠标右键点击的位置不对，这是因为之前我们默认游戏的画布在左上角，而开启小窗口后画布就不在左上角了，我们移动小球时用的 e.clientX/Y 表示的是整个屏幕的坐标，而小球的位置坐标本身是画布中的相对坐标。 Canvas 中的 getBoundingClientRect 函数可以获得当前视窗在浏览器中的位置以及自身占据的空间的大小，left 表示窗口左侧边框距离浏览器视窗左侧的距离，top 表示窗口顶侧边框距离浏览器视窗顶侧的距离，right 表示窗口右侧边框距离浏览器视窗左侧的距离，bottom 表示窗口底侧边框距离浏览器视窗顶侧的距离： 因此我们将鼠标点击的坐标分别减去 left 和 top 即可映射到当前视窗内，在 Player 类中进行修改： 注意现在我们的项目为发行版本，修改完静态文件且打包完后还需要去项目根目录执行：python3 manage.py collectstatic，此时会问你是否要覆盖，输入 yes 即可。我们同样也可以修改之前的打包脚本 compress_game_js.sh，在其后面添加一行：echo yes | python3 manage.py collectstatic。 由于游戏有菜单界面，应该在从菜单进入游戏后才开始计算窗口大小，因此我们需要把 AcGamePlayground 的初始化放在 show 函数中执行： 最后我们需要对菜单页面进行修改，在小窗口中的菜单页面不太美观，需要让其适应窗口大小，应该用小窗口的相对距离来表示，对 game.css 进行修改： 上一章：Django学习笔记-创建游戏界面。 下一章：Django学习笔记-用户名密码登录。"},{"title":"Django学习笔记-创建游戏界面","date":"2023-06-14T12:50:00.000Z","url":"/posts/4217.html","tags":[["Python","/tags/Python/"]],"categories":[["Python","/categories/Python/"]],"content":" 本节内容是游戏界面的设计，包括各个目标的绘制、角色的移动与攻击、AI 敌人的设计等部分。 1. 模块化引入JS变量 首先我们需要对之前的代码进行一点小修改，在 web.html 中使用 &lt;script&gt; 会导致定义的所有 Class（例如 AcGame）都会变成网页的全局变量，当引入多个 JS 文件后网页可能会出现重名变量导致冲突，我们最好做一个模块化，当我们需要某个名称的时候，我们只将这一个名称引入进来： 这时候我们刷新网页会看到报错：Uncaught SyntaxError: The requested module '/static/js/dist/game.js' does not provide an export named 'AcGame'，表示如果我们想加载 AcGame 的话需要在这个类前面加一个关键字 export： 此时再次刷新网页即可看到报错消失。 2. 实现物体运动基类 在游戏中物体的运动效果是通过不断刷新界面实现的，浏览器每秒刷新60次（即60帧），每一帧都是一张图片，因此我们需要先实现一个能够每一帧都调用对象的刷新函数的基类（简易游戏引擎）。 我们在 static/js/src/playground/ 目录下创建一个 ac_game_object 目录，然后进入该目录创建 zbase.js。 这个类一般有三个函数，函数 start 在开始时执行一次，用于创建对象时初始化对象的颜色、分值、昵称等信息（从服务器端加载出来），函数 update 每一帧都会执行一次，函数 destroy 表示删除当前对象： 然后我们需要实现每一帧循环渲染一遍全局数组中的对象，JS 提供了一个 API：requestAnimationFrame()，该函数会在一秒钟调用60次，也就是将一秒钟分成60等份，在下一帧时执行一遍函数，我们可以定义一个函数 AC_GAME_ANIMATION 表示每帧需要执行的操作： 3. Canvas绘制游戏画面 接下来我们需要创建游戏画面，在 playground 目录下创建一个 game_map 目录，然后创建 zbase.js： 在 playground 目录下的 zbase.js 中将游戏画面对象创建出来即可： 现在浏览器中的 Canvas 是没有颜色的，我们需要将它渲染出来，即在 GameMap 类中添加一个渲染函数 render： 4. 创建游戏角色及实现移动效果 在 playground 目录下创建一个 player 目录，然后创建 zbase.js（角色也是一个游戏对象，因此也要从 AcGameObject 类中扩展出来），角色需要传入中心坐标 x, y、半径 radius、颜色 color、每秒移动的距离百分比 speed、是否为自己 is_me（因为未来在联机的时候自己和敌人的操作方式是不一样的，敌人的操作是通过网络传过来的）： 然后我们修改 AcGamePlayground 类将自己创建出来试试效果： 接下来我们实现小球的移动，我们需要给每个小球设置一个 X 轴方向的速度和 Y 轴方向的速度，在每次刷新对象的时候更新一下小球的坐标即可： 现在我们还需要实现小球移动到鼠标右键点击的位置，如下图所示，假设从 (x, y) 移动到 (tx, ty)，移动的距离即为两点间的欧几里得距离，atan2(y, x) 函数可以求出方向角 θ，我们将其移动方向归一化视为一个单位圆，那么 X 轴方向的速度即为 1 * cos(θ)，Y 轴方向的速度即为 1 * sin(θ)。 5. 创建角色技能 在 playground 目录下创建一个 skill 目录，我们先实现火球技能，在 skill 目录下再创建 fireball 目录，然后创建 zbase.js： 然后我们要实现玩家选中某个技能后点击鼠标能够使用该技能，获取按键信息时不能用 Canvas，因为不能聚焦，可以用 Window 来获取，每个按键的编号可以在网上查找对照表获得： 6. 创建敌人及实现简单AI 首先我们在 AcGamePlayground 类中添加5名敌人： 然后我们需要让敌人移动起来，可以设定一个随机的目的地，然后移动到该目的地时再随机一个新的目的地，在 Player 中进行相应的修改： 7. 火球碰撞检测 两个圆的相交检测很简单，只需要判断两个圆的圆心距离是否小于两圆的半径之和即可，我们在 FireBall 类中进行碰撞检测： 接着我们需要实现玩家被攻击时的效果，被攻击时有向后的击退效果，且击退过程中玩家无法移动，被攻击后血量减少（使用小球半径表示血量，即被攻击后小球半径变小），血量越少移动速度越快，我们在 Player 类中进行修改： 8. 实现被攻击时的粒子效果 我们可以设计一个当被攻击时向前爆出若干小球的粒子效果，在 playground 目录下创建一个 particle 目录，然后创建 zbase.js： 然后在 Player 类的被击中函数中创建若干粒子： 9. 敌人随机颜色 最后我们随机生成每个敌人的颜色，在 Playground 类中进行修改： 10. 敌人自动攻击 我们可以设置敌人随机向玩家射击，但是开局前几秒限制敌人射击，修改 Player 类如下： 上一章：Django学习笔记-创建菜单界面。 下一章：Django学习笔记-部署Nginx与对接AcApp。"},{"title":"Django学习笔记-创建菜单界面","date":"2023-06-10T12:04:00.000Z","url":"/posts/602.html","tags":[["Python","/tags/Python/"]],"categories":[["Python","/categories/Python/"]],"content":" 本节内容是项目结构的划分与游戏菜单界面的设计。 1. 项目总体设计 （1）系统设计 menu：菜单页面； playground：游戏界面； settings：设置界面。 （2）文件结构 templates：管理 HTML 文件； urls：管理路由，即链接与函数的对应关系； views：管理 HTTP 函数； models：管理数据库数据； static：管理静态文件，比如： css：对象的格式，比如位置、长宽、颜色、背景、字体大小等； js：对象的逻辑，比如对象的创建与销毁、事件函数、移动、变色等； image：图片； audio：声音； …… consumers：管理 WebSocket 函数。 （3）素材地址 背景图片： 下载方式：wget --output-document=自定义图片名称 图片地址； 本地上传：scp -P 20000 .\\xxx.jpeg asanosaki@&lt;公网IP&gt;:。 jQuery 库： &lt;link rel=&quot;stylesheet&quot; href=&quot;;公网IP&gt;:8000/static/css/jquery-ui.min.css&quot;&gt;； &lt;script src=&quot;;公网IP&gt;:8000/static/js/jquery-3.6.1.min.js&quot;&gt;&lt;/script&gt;。 2. 全局设置与项目结构创建 为了后期方便项目的维护管理，首先先将 game 中的 urls.py、models.py、views.py 删除，将其创建为一个目录，并在这三个目录下创建好 __init__.py 文件，注释掉上一节中在总 URL 文件中的配置信息，然后创建好 static 目录。 接着进行一些项目的全局设置，首先设置一下项目的时区，打开 ~/djangoapp/djangoapp/settings.py，修改 TIME_ZONE 和 USE_TZ： 如果 USE_TZ 设置为 True 时，Django 会使用系统默认设置的时区，即 America/Chicago，此时的 TIME_ZONE 不管有没有设置都不起作用。 注意：由于我们是在云服务器上开发的，如果是 Windows 则设置 TIME_ZONE 是无效的，Django 会使用本机的时间。 然后将自己创建的 App 加载进来，找到 INSTALLED_APPS，将 game/apps.py 添加进来： 找到 STATIC_URL = 'static/'，在其附近添加几行： 我们在 game/static/ 中创建 image/menu/ 目录用来存放菜单界面的背景，将图片 background.png 放入该目录中即可在自己的网址上访问这张图片：;公网IP&gt;:8000/static/image/menu/background.png。 一般 CSS 文件只需要一个就行，因此只需要在 game/static/css/ 中创建一个 game.css 文件即可。JS 文件最后一般会有很多，因此在 game/static/js/ 中创建两个目录：dist 和 src，分别表示最终合并在一起生成的 js 文件以及许多 js 源文件，我们可以写一个脚本完成合并操作，在 ~/djangoapp/ 目录下创建一个 scripts 目录，然后在该目录中编写一个 compress_game_js.sh 文件： 添加可执行权限： 我们执行一下该脚本：./compress_game_js.sh，可以看到 ~/djangoapp/game/static/js/dist/ 中多了一个 game.js 文件。 此时我们先将代码上传至 Git： 3. 创建HTML 我们先在 templates 目录下创建三个目录：menu、playground、settings。由于项目是前后端分离的，最后可以放在多种不同的终端上运行，因此我们再建一个 multiends 目录。 在 static/src 目录下也创建三个目录：menu、playground、settings，然后再创建一个文件 zbase.js，表示总文件，由于打包工具是按照字典序打包的，该文件中为总的 Class，会调用前面三个目录中的 Class，JS 在调用之前必须定义好，因此在打包时需要保证前面三个目录中的文件在 zbase.js 之前被打包，因此加一个字典序最大的字母在首部。 在 zbase.js 中定义好总类： 在 templates/multiends/ 中创建 web.html，文件内容如下： 4. 创建Views与更新URL 我们先在 views 目录下创建三个目录：menu、playground、settings，这三个目录都是存放 Python 文件的，因此每个目录下都需要一个 __init__.py 文件。 然后再在 views 目录下创建一个 index.py 作为总函数，该函数只会在 Web 端被调用，主要作用是用来返回上一节中的 HTML 文件的： 现在我们开始写路由，先在 urls 目录下创建三个目录：menu、playground、settings，在每个目录下都创建一个 __init__.py 文件。接着同样创建一个 index.py，用来将所有该路径下其它目录中的路径 Include 进来，可以参考总 URL 文件编写。 先在三个目录中也创建好 index.py，内容如下： 此前在 views 目录中实现的 index.py 为总函数，即整个项目只有一个主链接，因此 urls 目录中的 index.py 也要将其 Include 进来： 最后修改一下总 URL 文件（~/djangoapp/djangoapp/ 中的 urls.py）： 此时梳理一下路由顺序：首先会进入到 ~/djangoapp/djangoapp/ 中的 urls.py 中，然后发现浏览器链接中没有带后缀（例如 admin/），所以会进到 game.urls.index 中，由于没有后缀因此又调用第一个路由，也就是直接调用 game.views.index 中的 index 函数，该函数会渲染 multiends/web.html 里面的内容。 Tips：浏览器中按 F12 打开控制台后如果看到报错：The Cross-Origin-Opener-Policy header has been ignored, because the URL's origin was untrustworthy. It was defined either in the final response or a redirect.，表示出现了跨域问题，在 settings.py 中添加如下代码即可： 最后将代码上传至 Git： 5. 创建菜单 我们需要创建一个菜单对象，首先在 static/js/src/menu/ 目录中创建一个 zbase.js 文件，内容如下： 然后在 game.css 中定义相应的样式： 最后需要更新 static/js/src/ 中的 zbase.js 文件： 此时即可在页面中看到自己的背景图片（注意更新完 JS 文件后都需要执行一下打包脚本 compress_game_js.sh）。 Tips：如果修改完 CSS 文件后网页没有变化说明页面把 CSS 文件缓存下来了，可以按 F12 打开控制台，在 Network 选项卡中勾选上 Disable cache。 现在我们在菜单界面添加几个按钮，首先修改菜单目录中的 zbase.js 文件（修改完记得运行打包脚本）： 然后修改 game.css 设置一下按钮的样式： 现在我们实现点击按钮切换页面的功能，需要给每个按钮绑定一个函数，我们可以定义一个 start 函数放在构造函数中，表示对象被创建出来时需要执行的一些初始化操作，即在 start 中可以绑定一些事件。 首先我们先把游戏界面的简易版本写出来，在 static/js/src/playground/ 目录中创建一个 zbase.js 文件，内容如下： 然后更新一下总的 zbase.js 文件： 最后在菜单界面中设置一下按钮点击的响应操作，点击单人模式按钮即可跳转到 AcGamePlayground 界面： 最后将代码上传至 Git： 上一章：Django学习笔记-概述与项目环境配置。 下一章：Django学习笔记-创建游戏界面。"},{"title":"Django学习笔记-概述与项目环境配置","date":"2023-06-08T09:16:00.000Z","url":"/posts/26150.html","tags":[["Python","/tags/Python/"]],"categories":[["Python","/categories/Python/"]],"content":" 本文记录 Django 的学习过程。 Django 官方文档：Django Web Docs。 本节内容是 Django 框架的介绍以及本次开发的小游戏项目的环境配置与初始化。 1. Django概述 Django 是后起之秀，近些年越来越流行，Youtube（月活20亿+）、Instagram（月活10亿+）等公司采用了 Django 框架。可以作为 Web、App、小程序、AcWing 云端 App（AC APP）等各种项目的后端。 Django 优势： 开发效率高，生态完善，有官方社区长期支持。 运行效率高（常见误区：Python 运行效率低，所以 Python 写的应用运行效率低）。 项目运行效率瓶颈有很多，比如：数据库查询、网络带宽/延迟、硬盘读写速度等，这些与框架关系不大。 计算密集型的模块可以用 C/C++ 实现，然后编译成动态链接库再 import 进来。 计算密集型的微服务可以通过 thrift 等工具对接，微服务的 Server 端代码可以用 C/C++ 语言实现。 有很多工具可以将 Python 代码翻译成 C/C++，比如 Cython、Pypy。AcWing 题库中的不少题目，会发现 Python3 比 Java 还快一些。 既适合大公司，也适合个人开发者，平均开发一个 Web/AC App 只需要半个月至一个月。 2. 开发环境介绍 完全无需配置本地环境。利用 AC Terminal 直接在云端开发，使用工具：vim、tmux 等。不推荐在本地开发。 本项目会涉及多台服务器间的网络通信，如果在本地开发，未来不方便调试和部署。 在本地开发无法统一开发环境，部分 Python 包在 Windows 系统上安装困难。 需要租一台具有公网 IP 的云服务器，并安装 Docker。 服务器配置无要求。 后期可以利用 Docker 随意迁移。 在 AC Terminal 的 /var/lib/acwing/docker/images/ 目录下给大家提供统一的课程 Docker 镜像（也会讲解如何自己配置 Django 开发环境）。 标准化开发环境，避免未来出现软件版本不兼容。 省去配环境的环节。 使用 AC Git 管理项目代码。 方便回滚代码。 3. 配置Docker环境 首先拉取一个 Ubuntu 镜像： 创建容器后进入容器配置基本环境并创建用户： 然后去云服务器官网放行20000端口和8000端口，以阿里云为例，协议类型选 TCP，端口范围分别为 目的: 8000/8000 与 目的: 20000/20000，授权对象都为 源: 0.0.0.0/0。 通过 SSH 远程连接容器： 如果出现 kex_exchange_identification: read: Connection reset by peer 报错，就去进入容器的根用户下编辑 hosts.allow 文件： 在文件中添加一行：sshd: ALL，然后重启 SSH 服务：service ssh restart。 进入容器根用户安装剩余的环境： 在命令行查看 Django 版本： 4. Django项目创建 通过以下命令创建一个 Django 项目： 将项目上传至 Git（注意需要先在容器中生成公钥，并在 Git 中添加公钥）： 尝试启动一下项目： 这时候访问一下网址：;云服务器的公网IP&gt;:8000/，会看到提示：Invalid HTTP_HOST header: '&lt;云服务器的公网IP&gt;:8000'. You may need to add '&lt;云服务器的公网IP&gt;' to ALLOWED_HOSTS.，这是因为 Django 是个很安全的框架，会自动屏蔽很多可疑的访问，我们需要将公网 IP 添加到 settings.py 文件的 ALLOWED_HOSTS 中： Tips：如果找不到 ALLOWED_HOSTS 可以使用 ag 命令查找： 现在即可成功访问网址。 此时会看到项目文件夹下出现了一个 __pycache__ 目录，这个是预编译好的一些文件，用于加速 Python 运行，我们在往 Git 上传代码时最好不要上传这些中间文件，我们可以在仓库的根目录下添加一个 .gitignore 文件，文件内容如下： 最后上传至 Git： 5. Django App创建 在上一节中启动的页面为 Django 的默认页面，我们在开发时需要创建一个新的 App 写自己的页面，首先通过以下指令创建一个名为 game 的 App，创建好后当前目录下会生成一个名为 game 的目录： 此时启动 runserver 指令时会发现出现了报错：You have 18 unapplied migration(s). Your project may not work properly ......，原因是有一部分的数据库修改还没有同步到数据库里，运行以下指令同步数据库的修改： 打开 ;公网IP&gt;:8000/admin/ 页面可以看到 Django 自带的管理员页面，我们可以创建管理员用户（假设用户名和密码都为 admin）： 然后即可登录管理员账户进入管理员界面。 我们再回过头来进入之前创建的 App 中，能看到有几个比较重要的文件：models.py、views.py，我们先手动创建剩余的比较重要的文件： 接下来整个项目需要操作的就这四个文件，其中 models.py 存储各种数据结构，views.py 存储视图（函数），例如每点一次按钮都要调用一次服务器端的函数，urls.py 是一个路由，用户访问某个功能（页面）时传的是一个地址（URL），服务器端拿到这个地址后需要做一个路由，查看调用的是哪个函数，templates 存储 HTML 文件。 首先在 views.py 中写一个简单的函数： 然后参考 ~/djangoapp/djangoapp/urls.py（总 URL 文件）编写 game 中的 urls.py 文件： 然后我们需要更新一下总 URL 文件（~/djangoapp/djangoapp/urls.py）： 此时我们在项目根目录下启动项目：python3 manage.py runserver 0.0.0.0:8000，然后打开网址 ;公网IP&gt;:8000/game/ 即可看到我们的网页，最后更新一下 Git 即可。 6. 更改Django项目名称 我们以 settings.py 文件为例，假设项目的目录结构为：OLD_NAME/OLD_NAME/settings.py，项目根目录的名称可以随意改变，即我们可以先改为 NEW_NAME/OLD_NAME/...，然后我们再将第二级 OLD_NAME 目录改为 NEW_NAME，修改完第二级目录的名字后我们还需要改以下几个文件： settings.py manage.py asgi.py wsgi.py 上一章：无。 下一章：Django学习笔记-创建菜单界面。"},{"title":"Kaggle项目实战","date":"2023-05-30T01:45:00.000Z","url":"/posts/6211.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 记录 Kaggle 中的一些经典竞赛，也当做自己的练手小项目。 1. 实战Kaggle比赛：预测房价 "},{"title":"Python遥感常用模块Rasterio与Rioxarray教程","date":"2023-05-29T07:52:00.000Z","url":"/posts/9681.html","tags":[["Python","/tags/Python/"]],"categories":[["Python","/categories/Python/"]],"content":" rasterio 是一个很多模块是基于 GDAL 的 Python 包，可用于处理地理空间栅格数据，例如 GeoTIFF 文件。xarray 是一个为数组提供标签，例如尺寸、坐标和其他特定属性的 Python 包，它使大维数组的工作更加直观。rioxarray 结合了 rasterio 的功能和 xarray 的所有优点。 1. Rasterio与Rioxarray安装 首先安装 Rasterio 模块，（本人使用 conda 安装时遇到过报错 ImportError: cannot import name 'CRS' from 'pyproj' (unknown location)，是由于 pyproj 模块安装不全，因此建议采用后面的离线安装方式或者之后遇到问题时删除 pyproj 模块后再离线安装该模块）： 如果安装失败可以采用离线安装的方式，Rasterio 依赖很多第三方库，所以比较麻烦，按下面的顺序依次安装即可，可以尝试使用 pip 安装或者下载 .whl 文件离线安装（注意对上 Python 版本）： 各个模块的链接：Pyproj、Shapely、GDAL、Fiona、Rasterio。 离线安装指令： 在 Python 中使用 Anaconda 安装 rioxarray 包时，首先需要安装 GDAL 和 rasterio，然后再安装 rioxarray： 2. 使用教程 （1）使用 Rioxarray 读取并展示图像： 也可以用另一种形式展示（注意如果使用 Rasterio 读取图像则无法使用该方式展示图像）： （2）使用 Rasterio 读取图像： （3）转换为 Tensor 类型： （4）将 TIFF 图像逐像素提取出数据构建 CSV 文件： "},{"title":"Python绘图模块Plotly教程","date":"2023-05-29T07:42:00.000Z","url":"/posts/31783.html","tags":[["Python","/tags/Python/"]],"categories":[["Python","/categories/Python/"]],"content":" Plotly 是一个快速完善并崛起的交互式的、开源的绘图库库，Python 库则是它的一个重要分支。现已支持超过40种独特的图表类型，涵盖了广泛的统计、金融、地理、科学和三维用例。 1. Plotly安装 Python 中可以使用 pip 或者 conda 安装 Plotly： 2. Plotly绘图教程 2.1 折线图与散点图 折线图不仅可以表示数量的多少，而且可以反映同一事物在不同时间里的发展变化的情况，易于显示数据变化趋势，可以直观地反映这种变化以及各组之间的差别。 2.2 饼图 饼图用于强调各项数据占总体的占比，强调个体和整体的比较。 2.3 直方图 直方图虽然也和条形图一样通过矩形的长度表示数值，但他的宽度一般用于表示各组的组距，因此其高度与宽度均有意义，适合展示大量数据集的统计结果，直方图的表示的数据通常是连续排列，而柱状图则是分开排列。 可设置 barmode 参数实现多个直方图覆盖的效果： 2.4 条形图 条形图用于比较各组数据的差异性，强调进行个体间的比较。 2.5 热力图 热力图是一种特殊的图表，它是一种通过对色块着色来显示数据的统计图表，在绘图时，需要指定每个颜色映射的规则（一般以颜色的强度或色调为标准）；比如颜色越深的表示数值越大、程度越深或者颜色越浅的数值越大、程度越深。热力图适合用于查看总体的情况、观察特殊值或者显示多个变量之间的差异性、检测它们之间是否存在相关性等等。 2.6 导出图像到本地 首先我们需要安装两个依赖项：orca 和 psutil，orca 在 PyPi 存储库中不可用，因此需要使用 conda 安装： 或者直接安装 kaleido 模块： 安装完成后即可使用 Plotly 的 io 库导出图像（格式可以是 SVG、JPG、PNG等）： "},{"title":"D2L学习笔记-注意力机制","date":"2023-05-21T09:57:00.000Z","url":"/posts/39408.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 李沐动手学深度学习（PyTorch）课程学习笔记第十章：注意力机制。 1. 注意力提示 注意力机制（Attention Mechanism）是人们在机器学习模型中嵌入的一种特殊结构，用来自动学习和计算输入数据对输出数据的贡献大小。 非自主性提示是基于环境中物体的突出性和易见性。想象一下，假如我们面前有五个物品：一份报纸、一篇研究论文、一杯咖啡、一本笔记本和一本书，所有纸制品都是黑白印刷的，但咖啡杯是红色的。换句话说，这个咖啡杯在这种视觉环境中是突出和显眼的，不由自主地引起人们的注意，所以我们会把视力最敏锐的地方放到咖啡上。喝咖啡后，我们会变得兴奋并想读书，所以转过头，重新聚焦眼睛，然后看看书，与咖啡杯是由于突出性导致的选择不同，此时选择书是受到了认知和意识的控制，因此注意力在基于自主性提示去辅助选择时将更为谨慎。受试者的主观意愿推动，选择的力量也就更强大。自主性的与非自主性的注意力提示解释了人类的注意力的方式，下面来看看如何通过这两种注意力提示，用神经网络来设计注意力机制的框架。 首先，考虑一个相对简单的状况，即只使用非自主性提示。要想将选择偏向于感官输入，则可以简单地使用参数化的全连接层，甚至是非参数化的最大汇聚层或平均汇聚层。 因此，“是否包含自主性提示”将注意力机制与全连接层或汇聚层区别开来。在注意力机制的背景下，自主性提示被称为查询（query）。给定任何查询，注意力机制通过注意力汇聚（attention pooling）将选择引导至感官输入（sensory inputs，例如中间特征表示）。在注意力机制中，这些感官输入被称为值（value）。更通俗的解释，每个值都与一个键（key）配对，这可以想象为感官输入的非自主提示。可以通过设计注意力汇聚的方式，便于给定的查询（自主性提示）与键（非自主性提示）进行匹配，这将引导得出最匹配的值（感官输入）。 平均汇聚层可以被视为输入的加权平均值，其中各输入的权重是一样的。实际上，注意力汇聚得到的是加权平均的总和值，其中权重是在给定的查询和不同的键之间计算得出的。为了可视化注意力权重，需要定义一个 show_heatmaps 函数，其输入 matrices 的形状是 (要显示的行数, 要显示的列数, 查询的数目, 键的数目)。下面使用一个简单的例子进行演示，在本例子中，仅当查询和键相同时，注意力权重为1，否则为0： 此外可以使用 Plotly 绘制热力图： 2. 注意力汇聚：Nadaraya-Watson核回归 上节介绍了框架下的注意力机制的主要成分：查询（自主提示）和键（非自主提示）之间的交互形成了注意力汇聚；注意力汇聚有选择地聚合了值（感官输入）以生成最终的输出。本节将介绍注意力汇聚的更多细节，以便从宏观上了解注意力机制在实践中的运作方式。具体来说，1964年提出的 Nadaraya-Watson 核回归模型是一个简单但完整的例子，可以用于演示具有注意力机制的机器学习，其理论介绍可见：注意力汇聚：Nadaraya-Watson核回归。 首先生成一个非线性函数的人工数据集： 函数 plot_kernel_reg 将绘制所有的训练样本（样本由圆圈表示），不带噪声项的真实数据生成函数（标记为 Truth），以及学习得到的预测函数（标记为 Pred）。先使用最简单的估计器来解决回归问题，即基于平均汇聚来计算所有训练样本输出值的平均值： 显然，平均汇聚忽略了输入，Nadaraya-Watson 核回归根据输入的位置对输出进行加权，是一个非参数模型。接下来，我们将基于这个非参数的注意力汇聚模型来绘制预测结果。从绘制的结果会发现新的模型预测线是平滑的，并且比平均汇聚的预测更接近真实。 非参数的 Nadaraya-Watson 核回归具有一致性（consistency）的优点：如果有足够的数据，此模型会收敛到最优结果。尽管如此，我们还是可以轻松地将可学习的参数集成到注意力汇聚中。 为了更有效地计算小批量数据的注意力，我们可以利用深度学习开发框架中提供的批量矩阵乘法： 在注意力机制的背景中，我们可以使用小批量矩阵乘法来计算小批量数据中的加权平均值： 定义 Nadaraya-Watson 核回归的带参数版本为： 接下来，将训练数据集变换为键和值用于训练注意力模型。在带参数的注意力汇聚模型中，任何一个训练样本的输入都会和除自己以外的所有训练样本的“键-值”对进行计算，从而得到其对应的预测输出： 训练带参数的注意力汇聚模型时，使用平方损失函数和随机梯度下降： 训练完带参数的注意力汇聚模型后可以发现：在尝试拟合带噪声的训练数据时，预测结果绘制的线不如之前非参数模型的平滑，因为与非参数的注意力汇聚模型相比，带参数的模型加入可学习的参数后，曲线在注意力权重较大的区域变得更不平滑。 3. 注意力评分函数 在上一节中使用了高斯核来对查询和键之间的关系建模。高斯核的指数部分可以视为注意力评分函数（attention scoring function），简称评分函数（scoring function），然后把这个函数的输出结果输入到 Softmax 函数中进行运算。通过上述步骤，将得到与键对应的值的概率分布（即注意力权重）。最后，注意力汇聚的输出就是基于这些注意力权重的值的加权和。 选择不同的注意力评分函数会导致不同的注意力汇聚操作。本节将介绍两个流行的评分函数，稍后将用他们来实现更复杂的注意力机制。 正如上面提到的，Softmax 操作用于输出一个概率分布作为注意力权重。在某些情况下，并非所有的值都应该被纳入到注意力汇聚中。例如，为了在机器翻译中高效处理小批量数据集，某些文本序列被填充了没有意义的特殊词元。为了仅将有意义的词元作为值来获取注意力汇聚，可以指定一个有效序列长度（即词元的个数），以便在计算 Softmax 时过滤掉超出指定范围的位置。下面的 masked_softmax 函数实现了这样的掩蔽 Softmax 操作（masked softmax operation），其中任何超出有效长度的位置都被掩蔽并置为0。 为了演示此函数是如何工作的，考虑由两个2*4矩阵表示的样本，这两个样本的有效长度分别为2和3。经过掩蔽 Softmax 操作，超出有效长度的值都被掩蔽为0： 同样，也可以使用二维张量，为矩阵样本中的每一行指定有效长度： 接下来将介绍加性注意力与缩放点积注意力，其理论分析可见：注意力评分函数。 一般来说，当查询和键是不同长度的矢量时，可以使用加性注意力作为评分函数。将查询和键连结起来后输入到一个多层感知机（MLP）中，感知机包含一个隐藏层，其隐藏单元数是一个超参数。通过使用 tanh 作为激活函数，并且禁用偏置项： 尽管加性注意力包含了可学习的参数，但由于本例子中每个键都是相同的，所以注意力权重是均匀的，由指定的有效长度决定： 使用点积可以得到计算效率更高的评分函数，但是点积操作要求查询和键具有相同的长度，为了演示 DotProductAttention 类，我们使用与先前加性注意力例子中相同的键、值和有效长度。对于点积操作，我们令查询的特征维度与键的特征维度大小相同： 与加性注意力演示相同，由于键包含的是相同的元素，而这些元素无法通过任何查询进行区分，因此获得了均匀的注意力权重。 4. Bahdanau注意力（使用注意力的seq2seq） Bahdanau 注意力模型的原理可见：Bahdanau 注意力。 下面看看如何定义 Bahdanau 注意力，实现循环神经网络编码器-解码器。其实，我们只需重新定义解码器即可。为了更方便地显示学习的注意力权重，以下 AttentionDecoder 类定义了带有注意力机制解码器的基本接口： 接下来，让我们在接下来的 Seq2SeqAttentionDecoder 类中实现带有 Bahdanau 注意力的循环神经网络解码器。首先，初始化解码器的状态，需要下面的输入： 编码器在所有时间步的最终层隐状态，将作为注意力的键和值； 上一时间步的编码器全层隐状态，将作为初始化解码器的隐状态； 编码器有效长度（排除在注意力池中填充词元）。 在每个解码时间步骤中，解码器上一个时间步的最终层隐状态将用作查询。因此，注意力输出和输入嵌入都连结为循环神经网络解码器的输入。 接下来，使用包含7个时间步的4个序列输入的小批量测试 Bahdanau 注意力解码器： 我们在这里指定超参数，实例化一个带有 Bahdanau 注意力的编码器和解码器，并对这个模型进行机器翻译训练： 模型训练后，我们用它将几个英语句子翻译成法语并计算它们的 BLEU 分数： 训练结束后，下面通过可视化注意力权重会发现每个查询都会在键值对上分配不同的权重，这说明在每个解码步中，输入序列的不同部分被选择性地聚集在注意力池中： 5. 多头注意力 在实践中，当给定相同的查询、键和值的集合时，我们希望模型可以基于相同的注意力机制学习到不同的行为，然后将不同的行为作为知识组合起来，捕获序列内各种范围的依赖关系（例如，短距离依赖和长距离依赖关系）。因此，允许注意力机制组合使用查询、键和值的不同子空间表示（representation subspaces）可能是有益的。 为此，与其只使用单独一个注意力汇聚，我们可以用独立学习得到的 h 组不同的线性投影（linear projections）来变换查询、键和值。然后，这 h 组变换后的查询、键和值将并行地送到注意力汇聚中。最后，将这 h 个注意力汇聚的输出拼接在一起，并且通过另一个可以学习的线性投影进行变换，以产生最终输出。这种设计被称为多头注意力（multihead attention）。对于 h 个注意力汇聚输出，每一个注意力汇聚都被称作一个头（head）。基于这种设计，每个头都可能会关注输入的不同部分，可以表示比简单加权平均值更复杂的函数。 多头注意力模型的原理可见：多头注意力。 在实现过程中通常选择缩放点积注意力作为每一个注意力头： 为了能够使多个头并行计算，上面的 MultiHeadAttention 类将使用下面定义的两个转置函数。具体来说，transpose_output 函数反转了 transpose_qkv 函数的操作： 下面使用键和值相同的小例子来测试我们编写的 MultiHeadAttention 类。多头注意力输出的形状是 (batch_size, num_queries, num_hiddens)： 6. 自注意力和位置编码 在深度学习中，经常使用卷积神经网络（CNN）或循环神经网络（RNN）对序列进行编码。想象一下，有了注意力机制之后，我们将词元序列输入注意力池化中，以便同一组词元同时充当查询、键和值。具体来说，每个查询都会关注所有的键-值对并生成一个注意力输出。当查询、键和值来自同一组输入时被称为自注意力（self-attention），也被称为内部注意力（intra-attention）。本节将使用自注意力进行序列编码，以及如何使用序列的顺序作为补充信息。 自注意力模型的原理可见：自注意力和位置编码。 下面的代码片段是基于多头注意力对一个张量完成自注意力的计算，张量的形状为 (批量大小, 时间步的数目或词元序列的长度, h)，输出与输入的张量形状相同： 在处理词元序列时，循环神经网络是逐个的重复地处理词元的，而自注意力则因为并行计算而放弃了顺序操作。为了使用序列的顺序信息，通过在输入表示中添加位置编码（positional encoding）来注入绝对的或相对的位置信息。位置编码可以通过学习得到也可以直接固定得到。接下来描述的是基于正弦函数和余弦函数的固定位置编码。 在位置嵌入矩阵 P 中，行代表词元在序列中的位置，列代表位置编码的不同维度。从下面的例子中可以看到位置嵌入矩阵的第6列和第7列的频率高于第8列和第9列。第6列和第7列之间的偏移量（第8列和第9列相同）是由于正弦函数和余弦函数的交替： 通过绘制热力图可以看到，位置编码通过使用三角函数在编码维度上降低频率： 7. Transformer"},{"title":"D2L学习笔记-现代循环神经网络","date":"2023-04-11T04:58:00.000Z","url":"/posts/7592.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 李沐动手学深度学习（PyTorch）课程学习笔记第九章：现代循环神经网络。 1. 门控循环单元（GRU） 在通过时间反向传播中，我们讨论了如何在循环神经网络中计算梯度，以及矩阵连续乘积可以导致梯度消失或梯度爆炸的问题。下面我们简单思考一下这种梯度异常在实践中的意义，我们可能会遇到以下的情况： 早期观测值对预测所有未来观测值具有非常重要的意义。考虑一个极端情况，其中第一个观测值包含一个校验和，目标是在序列的末尾辨别校验和是否正确。在这种情况下，第一个词元的影响至关重要。我们希望有某些机制能够在一个记忆元里存储重要的早期信息。如果没有这样的机制，我们将不得不给这个观测值指定一个非常大的梯度，因为它会影响所有后续的观测值。 一些词元没有相关的观测值。例如，在对网页内容进行情感分析时，可能有一些辅助 HTML 代码与网页传达的情绪无关。我们希望有一些机制来跳过隐状态表示中的此类词元。 序列的各个部分之间存在逻辑中断。例如，书的章节之间可能会有过渡存在，或者证券的熊市和牛市之间可能会有过渡存在。在这种情况下，最好有一种方法来重置我们的内部状态表示。 在学术界已经提出了许多方法来解决这类问题。其中最早的方法是长短期记忆（long-short-term memory，LSTM），我们将在下一节中讨论。门控循环单元（gated recurrent unit，GRU）是一个稍微简化的变体，通常能够提供同等的效果，并且计算的速度明显更快。由于门控循环单元更简单，我们从它开始解读。 门控循环单元与普通的循环神经网络之间的关键区别在于：前者支持隐状态的门控。这意味着模型有专门的机制来确定应该何时更新隐状态，以及应该何时重置隐状态。这些机制是可学习的，并且能够解决了上面列出的问题。例如，如果第一个词元非常重要，模型将学会在第一次观测之后不更新隐状态。同样，模型也可以学会跳过不相关的临时观测。最后，模型还将学会在需要的时候重置隐状态。下面我们将详细讨论各类门控。 我们首先介绍重置门（reset gate）和更新门（update gate）。我们把它们设计成 (0, 1) 区间中的向量，这样我们就可以进行凸组合。重置门允许我们控制可能还想记住的过去状态的数量；更新门将允许我们控制新状态中有多少个是旧状态的副本。 我们从构造这些门控开始。重置门和更新门的输入是由当前时间步的输入和前一时间步的隐状态给出。两个门的输出是由使用 Sigmoid 激活函数的两个全连接层给出。门控循环单元的数学表达详见：门控循环单元（GRU）。 1.1 门控循环单元的从零开始实现 为了更好地理解门控循环单元模型，我们从零开始实现它。首先，我们读取上一章中使用的时间机器数据集： 下一步是初始化模型参数。我们从标准差为0.01的高斯分布中提取权重，并将偏置项设为0，超参数 num_hiddens 定义隐藏单元的数量，实例化与更新门、重置门、候选隐状态和输出层相关的所有权重和偏置： 现在我们将定义隐状态的初始化函数 init_gru_state。与从零开始实现 RNN 中定义的 init_rnn_state 函数一样，此函数返回一个形状为 (批量大小, 隐藏单元个数) 的张量，张量的值全部为零： 现在我们准备定义门控循环单元模型，模型的架构与基本的循环神经网络单元是相同的，只是权重更新公式更为复杂： 训练和预测的工作方式与从零开始实现 RNN 完全相同。训练结束后，我们分别打印输出训练集的困惑度，以及前缀 time traveler 和 traveler 的预测序列上的困惑度： 1.2 门控循环单元的简洁实现 高级 API 包含了前文介绍的所有配置细节，所以我们可以直接实例化门控循环单元模型。这段代码的运行速度要快得多，因为它使用的是编译好的运算符而不是 Python 来处理之前阐述的许多细节： 2. 长短期记忆网络（LSTM） 长期以来，隐变量模型存在着长期信息保存和短期输入缺失的问题。解决这一问题的最早方法之一是长短期存储器（long short-term memory，LSTM）。它有许多与门控循环单元一样的属性。 可以说，长短期记忆网络的设计灵感来自于计算机的逻辑门。长短期记忆网络引入了记忆元（memory cell），或简称为单元（cell）。有些文献认为记忆元是隐状态的一种特殊类型，它们与隐状态具有相同的形状，其设计目的是用于记录附加的信息。为了控制记忆元，我们需要许多门。其中一个门用来从单元中输出条目，我们将其称为输出门（output gate）。另外一个门用来决定何时将数据读入单元，我们将其称为输入门（input gate）。我们还需要一种机制来重置单元的内容，由遗忘门（forget gate）来管理，这种设计的动机与门控循环单元相同，能够通过专用机制决定什么时候记忆或忽略隐状态中的输入。 长短期记忆网络的数学表达详见：长短期记忆网络（LSTM）。 2.1 长短期记忆网络的从零开始实现 现在，我们从零开始实现长短期记忆网络，我们首先加载时光机器数据集： 接下来，我们需要定义和初始化模型参数。如前所述，超参数 num_hiddens 定义隐藏单元的数量。我们按照标准差0.01的高斯分布初始化权重，并将偏置项设为0： 在初始化函数中，长短期记忆网络的隐状态需要返回一个额外的记忆元，单元的值为0，形状为 (批量大小, 隐藏单元数)。因此，我们得到以下的状态初始化： 实际模型的定义与我们前面讨论的一样：提供三个门和一个额外的记忆元。请注意，只有隐状态 H 才会传递到输出层，而记忆元 C 不直接参与输出计算： 让我们通过实例化从零实现 RNN 章节中引入的 RNNModelScratch 类来训练一个长短期记忆网络，就如我们在上一节中所做的一样： 2.2 长短期记忆网络的简洁实现 使用高级 API，我们可以直接实例化 LSTM 模型： 3. 深度循环神经网络 到目前为止，我们只讨论了具有一个单向隐藏层的循环神经网络。其中，隐变量和观测值与具体的函数形式的交互方式是相当随意的。只要交互类型建模具有足够的灵活性，这就不是一个大问题。然而，对一个单层来说，这可能具有相当的挑战性。之前在线性模型中，我们通过添加更多的层来解决这个问题。而在循环神经网络中，我们首先需要确定如何添加更多的层，以及在哪里添加额外的非线性，因此这个问题有点棘手。 事实上，我们可以将多层循环神经网络堆叠在一起，通过对几个简单层的组合，产生了一个灵活的机制。特别是，数据可能与不同层的堆叠有关。例如，我们可能希望保持有关金融市场状况（熊市或牛市）的宏观数据可用，而微观数据只记录较短期的时间动态。 深度循环神经网络的数学表达详见：深度循环神经网络。 实现多层循环神经网络所需的许多逻辑细节在高级 API 中都是现成的。简单起见，我们仅示范使用此类内置函数的实现方式。以长短期记忆网络模型为例，该代码与上一节中使用的代码非常相似，实际上唯一的区别是我们指定了层的数量，而不是使用单一层这个默认值。像往常一样，我们从加载数据集开始： 像选择超参数这类架构决策也跟上一节中的决策非常相似。因为我们有不同的词元，所以输入和输出都选择相同数量，即 vocab_size。隐藏单元的数量仍然是256。唯一的区别是，我们现在通过 num_layers 的值来设定隐藏层数： 最后和上一节一样训练模型看看效果： 4. 双向循环神经网络 在双向循环神经网络中，每个时间步的隐状态由当前时间步的前后数据同时决定，通过反向更新的隐藏层来利用反向时间信息，通常用来对序列抽取特征、填空，而不是预测未来。 双向循环神经网络的数学表达详见：双向循环神经网络。 由于双向循环神经网络使用了过去的和未来的数据，所以我们不能盲目地将这一语言模型应用于任何预测任务。尽管模型产出的困惑度是合理的，该模型预测未来词元的能力却可能存在严重缺陷。我们用下面的示例代码引以为戒，以防在错误的环境中使用它们： 5. 机器翻译与数据集 语言模型是自然语言处理的关键，而机器翻译是语言模型最成功的基准测试。因为机器翻译正是将输入序列转换成输出序列的序列转换模型（sequence transduction）的核心问题。 与语言模型那一节中的语料库是单一语言的语言模型问题存在不同，机器翻译的数据集是由源语言和目标语言的文本序列对组成的。因此，我们需要一种完全不同的方法来预处理机器翻译数据集，而不是复用语言模型的预处理程序。 首先，下载一个由双语句子对组成的“英-法”数据集，数据集中的每一行都是制表符分隔的文本序列对，序列对由英文文本序列和翻译后的法语文本序列组成。请注意，每个文本序列可以是一个句子，也可以是包含多个句子的一个段落。在这个将英语翻译成法语的机器翻译问题中，英语是源语言（source language），法语是目标语言（target language）。 下载数据集后，原始文本数据需要经过几个预处理步骤。例如，我们用空格代替不间断空格（non-breaking space），使用小写字母替换大写字母，并在单词和标点符号之间插入空格： 与之前的字符级词元化不同，在机器翻译中，我们更喜欢单词级词元化（最先进的模型可能使用更高级的词元化技术）。下面的 tokenize_nmt 函数对前 num_examples 个文本序列对进行词元化，其中每个词元要么是一个词，要么是一个标点符号。此函数返回两个词元列表：source 和 target，source[i] 是源语言（这里是英语）第 i 个文本序列的词元列表，target[i] 是目标语言（这里是法语）第 i 个文本序列的词元列表。 让我们绘制每个文本序列所包含的词元数量的直方图。在这个简单的“英-法”数据集中，大多数文本序列的词元数量少于20个： 由于机器翻译数据集由语言对组成，因此我们可以分别为源语言和目标语言构建两个词表。使用单词级词元化时，词表大小将明显大于使用字符级词元化时的词表大小。为了缓解这一问题，这里我们将出现次数少于2次的低频率词元视为相同的未知（&lt;unk&gt;）词元。除此之外，我们还指定了额外的特定词元，例如在小批量时用于将序列填充到相同长度的填充词元（&lt;pad&gt;），以及序列的开始词元（&lt;bos&gt;）和结束词元（&lt;eos&gt;）。这些特殊词元在自然语言处理任务中比较常用。 回想一下，语言模型中的序列样本都有一个固定的长度，无论这个样本是一个句子的一部分还是跨越了多个句子的一个片断。这个固定长度是由语言模型中的 num_steps（时间步数或词元数量）参数指定的。在机器翻译中，每个样本都是由源和目标组成的文本序列对，其中的每个文本序列可能具有不同的长度。 为了提高计算效率，我们仍然可以通过截断（truncation）和填充（padding）方式实现一次只处理一个小批量的文本序列。假设同一个小批量中的每个序列都应该具有相同的长度 num_steps，那么如果文本序列的词元数目少于 num_steps 时，我们将继续在其末尾添加特定的 &lt;pad&gt; 词元，直到其长度达到 num_steps；反之，我们将截断文本序列时，只取其前 num_steps 个词元，并且丢弃剩余的词元。这样，每个文本序列将具有相同的长度，以便以相同形状的小批量进行加载。下面的 truncate_pad 函数将截断或填充文本序列： 现在我们定义一个函数，可以将文本序列转换成小批量数据集用于训练。我们将特定的 &lt;eos&gt; 词元添加到所有序列的末尾，用于表示序列的结束。当模型通过一个词元接一个词元地生成序列进行预测时，生成的 &lt;eos&gt; 词元说明完成了序列输出工作。此外，我们还记录了每个文本序列的长度，统计长度时排除了填充词元，在稍后将要介绍的一些模型会需要这个长度信息。 最后，我们定义 load_data_nmt 函数来返回数据迭代器，以及源语言和目标语言的两种词表： 6. 编码器-解码器架构 正如我们在上一节中所讨论的，机器翻译是序列转换模型的一个核心问题，其输入和输出都是长度可变的序列。为了处理这种类型的输入和输出，我们可以设计一个包含两个主要组件的架构：第一个组件是一个编码器（encoder）：它接受一个长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。第二个组件是解码器（decoder）：它将固定形状的编码状态映射到长度可变的序列。这被称为编码器-解码器（encoder-decoder）架构，示意图可见：编码器-解码器架构。 由于“编码器-解码器”架构是形成后续章节中不同序列转换模型的基础，因此本节将把这个架构转换为接口方便后面的代码实现。 在编码器接口中，我们只指定长度可变的序列作为编码器的输入 X。任何继承这个 Encoder 基类的模型将完成代码实现： 在下面的解码器接口中，我们新增一个 init_state 函数，用于将编码器的输出（enc_outputs）转换为编码后的状态。注意，此步骤可能需要额外的输入，例如输入序列的有效长度。为了逐个地生成长度可变的词元序列，解码器在每个时间步都会将输入（例如在前一时间步生成的词元）和编码后的状态映射成当前时间步的输出词元： 总而言之，“编码器-解码器”架构包含了一个编码器和一个解码器，并且还拥有可选的额外的参数。在前向传播中，编码器的输出用于生成编码状态，这个状态又被解码器作为其输入的一部分： 7. 序列到序列学习（seq2seq） 遵循编码器-解码器架构的设计原则，循环神经网络编码器使用长度可变的序列作为输入，将其转换为固定形状的隐状态。换言之，输入序列的信息被编码到循环神经网络编码器的隐状态中。为了连续生成输出序列的词元，独立的循环神经网络解码器是基于输入序列的编码信息和输出序列已经看见的或者生成的词元来预测下一个词元。在机器翻译中使用两个循环神经网络进行序列到序列学习的图示以及理论介绍可见：序列到序列学习（seq2seq）。 在序列到序列学习中，特定的 &lt;eos&gt; 表示序列结束词元。一旦输出序列生成此词元，模型就会停止预测。在循环神经网络解码器的初始化时间步，有两个特定的设计决定：首先，特定的 &lt;bos&gt; 表示序列开始词元，它是解码器的输入序列的第一个词元。其次，使用循环神经网络编码器最终的隐状态来初始化解码器的隐状态。 从技术上讲，编码器将长度可变的输入序列转换成形状固定的上下文变量，并且将输入序列的信息在该上下文变量中进行编码。 现在，让我们实现循环神经网络编码器。注意，我们使用了嵌入层（embedding layer）来获得输入序列中每个词元的特征向量。嵌入层的权重是一个矩阵，其行数等于输入词表的大小（vocab_size），其列数等于特征向量的维度（embed_size）。另外，本文选择了一个多层门控循环单元来实现编码器。 下面，我们实例化上述编码器的实现：我们使用一个两层门控循环单元编码器，其隐藏单元数为16。给定一小批量的输入序列 X（批量大小为4，时间步为7）。在完成所有时间步后，最后一层的隐状态的输出是一个张量（output 由编码器的循环层返回），其形状为 (时间步数, 批量大小, 隐藏单元数)。 由于这里使用的是门控循环单元，所以在最后一个时间步的多层隐状态的形状是 (隐藏层的数量, 批量大小, 隐藏单元的数量)。如果使用长短期记忆网络，state 中还将包含记忆单元信息。 当实现解码器时，我们直接使用编码器最后一个时间步的隐状态来初始化解码器的隐状态。这就要求使用循环神经网络实现的编码器和解码器具有相同数量的层和隐藏单元。为了进一步包含经过编码的输入序列的信息，上下文变量在所有的时间步与解码器的输入进行拼接（concatenate）。为了预测输出词元的概率分布，在循环神经网络解码器的最后一层使用全连接层来变换隐状态。 下面，我们用与前面提到的编码器中相同的超参数来实例化解码器。如我们所见，解码器的输出形状变为 (批量大小, 时间步数, 词表大小)，其中张量的最后一个维度存储预测的词元分布。 在每个时间步，解码器预测了输出词元的概率分布。类似于语言模型，可以使用 Softmax 来获得分布，并通过计算交叉熵损失函数来进行优化。回想一下我们将特定的填充词元添加到序列的末尾，因此不同长度的序列可以以相同形状的小批量加载。但是，我们应该将填充词元的预测排除在损失函数的计算之外。 为此，我们可以使用下面的 sequence_mask 函数通过零值化屏蔽不相关的项，以便后面任何不相关预测的计算都是与零的乘积，结果都等于零。例如，如果两个序列的有效长度（不包括填充词元）分别为1和2，则第一个序列的第一项和第二个序列的前两项之后的剩余项将被清除为零： 现在，我们可以通过扩展 Softmax 交叉熵损失函数来遮蔽不相关的预测。最初，所有预测词元的掩码都设置为1。一旦给定了有效长度，与填充词元对应的掩码将被设置为0。最后，将所有词元的损失乘以掩码，以过滤掉损失中填充词元产生的不相关预测。 我们可以创建三个相同的序列来进行代码健全性检查，然后分别指定这些序列的有效长度为4、2和0。结果就是，第一个序列的损失应为第二个序列的两倍，而第三个序列的损失应为零。 在下面的循环训练过程中，特定的序列开始词元（&lt;bos&gt;）和原始的输出序列（不包括序列结束词元 &lt;eos&gt;）拼接在一起作为解码器的输入。这被称为强制教学（teacher forcing），因为原始的输出序列（词元的标签）被送入解码器。或者，将来自上一个时间步的预测得到的词元作为解码器的当前输入。 为了采用一个接着一个词元的方式预测输出序列，每个解码器当前时间步的输入都将来自于前一时间步的预测词元。与训练类似，序列开始词元（&lt;bos&gt;）在初始时间步被输入到解码器中，当输出序列的预测遇到序列结束词元（&lt;eos&gt;）时，预测就结束了。 我们可以通过与真实的标签序列进行比较来评估预测序列。虽然BLEU（bilingual evaluation understudy）最先是用于评估机器翻译的结果，但现在它已经被广泛用于测量许多应用的输出序列的质量。BLEU的详细介绍可见：序列到序列学习（seq2seq）。 8. 束搜索 束搜索为预测输出序列的一个算法，详细介绍可见：束搜索。"},{"title":"D2L学习笔记-循环神经网络","date":"2023-04-05T06:04:00.000Z","url":"/posts/11559.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 李沐动手学深度学习（PyTorch）课程学习笔记第八章：循环神经网络。 1. 序列模型 由于涉及较多数学公式，序列模型的讲解可以转至：序列模型。 首先，我们生成一些数据：使用正弦函数和一些可加性噪声来生成序列数据，时间步为 1, 2, ..., 1000： 接下来，我们将这个序列转换为模型的特征-标签（feature-label）对。基于嵌入维度 𝜏，我们将数据映射为数据对 𝑦_𝑡 = 𝑥_𝑡 和 𝐱_𝑡 = [𝑥_&#123;𝑡 - 𝜏&#125;, ...,𝑥_&#123;𝑡 - 1&#125;]，这比我们提供的数据样本少了 𝜏 个，因为我们没有足够的历史记录来描述前 𝜏 个数据样本。一个简单的解决办法是：如果拥有足够长的序列就丢弃这几项；另一个方法是用零填充序列。在这里，我们仅使用前600个特征-标签对进行训练： 在这里，我们使用一个相当简单的架构训练模型：一个拥有两个全连接层的多层感知机，ReLU 激活函数和平方损失： 现在准备训练模型，实现下面的训练代码的方式与前面几章中的循环训练基本相同。因此，我们不会深入探讨太多细节： 由于训练损失很小，因此我们期望模型能有很好的工作效果。让我们看看这在实践中意味着什么。首先是检查模型预测下一个时间步的能力，也就是单步预测（one-step-ahead prediction）： 正如我们所料，单步预测效果不错。即使这些预测的时间步超过了604（n_train + tau），其结果看起来仍然是可信的。然而有一个小问题：如果数据观察序列的时间步只到604，我们需要一步一步地向前迈进，换句话说，我们必须使用我们自己的预测（而不是原始数据）来进行多步预测。让我们看看效果如何： 如上面的例子所示，绿线的预测显然并不理想。经过几个预测步骤之后，预测的结果很快就会衰减到一个常数。为什么这个算法效果这么差呢？事实是由于误差的累积：假设在步骤1之后，我们积累了一些误差，于是步骤2的输入被扰动了，后面的预测误差依此类推。因此误差可能会相当快地偏离真实的观测结果。例如，未来24小时的天气预报往往相当准确，但超过这一点，精度就会迅速下降。我们将在本章及后续章节中讨论如何改进这一点。 基于 k = 1, 4, 16, 64，通过对整个序列预测的计算，让我们更仔细地看一下 k 步预测的困难： 2. 文本预处理 对于序列数据处理问题，我们在上一节中评估了所需的统计工具和预测时面临的挑战。这样的数据存在许多种形式，文本是最常见例子之一。例如，一篇文章可以被简单地看作一串单词序列，甚至是一串字符序列。本节中，我们将解析文本的常见预处理步骤。这些步骤通常包括： 将文本作为字符串加载到内存中。 将字符串拆分为词元（如单词和字符）。 建立一个词表，将拆分的词元映射到数字索引。 将文本转换为数字索引序列，方便模型操作。 首先，我们从 H.G.Well 的时光机器中加载文本。这是一个相当小的语料库，只有30000多个单词，但足够我们小试牛刀，而现实中的文档集合可能会包含数十亿个单词。下面的函数将数据集读取到由多条文本行组成的列表中，其中每条文本行都是一个字符串。为简单起见，我们在这里忽略了标点符号和字母大写： 下面的 tokenize 函数将文本行列表（lines）作为输入，列表中的每个元素是一个文本序列（如一条文本行）。每个文本序列又被拆分成一个词元列表，词元（token）是文本的基本单位。最后，返回一个由词元列表组成的列表，其中的每个词元都是一个字符串（string）： 词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。现在，让我们构建一个字典，通常也叫做词表（vocabulary），用来将字符串类型的词元映射到从0开始的数字索引中。我们先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计，得到的统计结果称之为语料（corpus）。然后根据每个唯一词元的出现频率，为其分配一个数字索引。很少出现的词元通常被移除，这可以降低复杂性。另外，语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元 &lt;unk&gt;。我们可以选择增加一个列表，用于保存那些被保留的词元，例如：填充词元（&lt;pad&gt;）、序列开始词元（&lt;bos&gt;）、序列结束词元（&lt;eos&gt;）： 我们首先使用时光机器数据集作为语料库来构建词表，然后打印前几个高频词元及其索引： 现在，我们可以将每一条文本行转换成一个数字索引列表： 在使用上述函数时，我们将所有功能打包到 load_corpus_time_machine 函数中，该函数返回 corpus（词元索引列表）和 vocab（时光机器语料库的词表）。我们在这里所做的改变是： 为了简化后面章节中的训练，我们使用字符（而不是单词）实现文本词元化； 时光机器数据集中的每个文本行不一定是一个句子或一个段落，还可能是一个单词，因此返回的 corpus 仅处理为单个列表，而不是使用多词元列表构成的一个列表。 3. 语言模型和数据集 由于涉及较多数学公式，语言模型的讲解可以转至：语言模型和数据集。 根据上一节中介绍的时光机器数据集构建词表，并打印前10个最常用的（频率最高的）单词： 正如我们所看到的，最流行的词看起来很无聊，这些词通常被称为停用词（stop words），因此可以被过滤掉。尽管如此，它们本身仍然是有意义的，我们仍然会在模型中使用它们。此外，还有个明显的问题是词频衰减的速度相当地快。例如，最常用单词的词频对比，第10个还不到第1个的1/5。为了更好地理解，我们可以画出的词频图： 通过词频图我们可以发现：词频以一种明确的方式迅速衰减。将前几个单词作为例外消除后，剩余的所有单词大致遵循双对数坐标图上的一条直线。这意味着单词的频率满足齐普夫定律（Zipf’s law）。这告诉我们想要通过计数统计和平滑来建模单词是不可行的，因为这样建模的结果会大大高估尾部单词的频率，也就是所谓的不常用单词。那么其他的词元组合，比如二元语法、三元语法等等，又会如何呢？我们来看看二元语法的频率是否与一元语法的频率表现出相同的行为方式： 这里值得注意：在十个最频繁的词对中，有九个是由两个停用词组成的，只有一个与 the time 有关。我们再进一步看看三元语法的频率是否表现出相同的行为方式： 最后，我们直观地对比三种模型中的词元频率：一元语法、二元语法和三元语法： 由于序列数据本质上是连续的，因此我们在处理数据时需要解决这个问题。在第一节中我们以一种相当特别的方式做到了这一点：当序列变得太长而不能被模型一次性全部处理时，我们可能希望拆分这样的序列方便模型读取。 在介绍该模型之前，我们看一下总体策略。假设我们将使用神经网络来训练语言模型，模型中的网络一次处理具有预定义长度（例如 𝑛 个时间步）的一个小批量序列。现在的问题是如何随机生成一个小批量数据的特征和标签以供读取。 首先，由于文本序列可以是任意长的，例如整本《时光机器》（The Time Machine），于是任意长的序列可以被我们划分为具有相同时间步数的子序列。当训练我们的神经网络时，这样的小批量子序列将被输入到模型中。假设网络一次只处理具有 𝑛 个时间步的子序列，那么可以从指定的起始位置开始截取连续的长度为 𝑛 的子序列，因为我们可以选择任意偏移量来指示初始位置，所以我们有相当大的自由度。 如果我们只选择一个偏移量，那么用于训练网络的、所有可能的子序列的覆盖范围将是有限的。因此，我们可以从随机偏移量开始划分序列，以同时获得覆盖性（coverage）和随机性（randomness）。下面，我们将描述如何实现随机采样（random sampling）和顺序分区（sequential partitioning）策略。 在随机采样中，每个样本都是在原始的长序列上任意捕获的子序列。在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元，因此标签是移位了一个词元的原始序列。 下面的代码每次可以从数据中随机生成一个小批量。在这里，参数 batch_size 指定了每个小批量中子序列样本的数目，参数 num_steps 是每个子序列中预定义的时间步数： 下面我们生成一个从0到34的序列。假设批量大小为2，时间步数为5，这意味着可以生成6个特征-标签子序列对。如果设置小批量大小为2，我们只能得到3个小批量： 在迭代过程中，除了对原始序列可以随机抽样外，我们还可以保证两个相邻的小批量中的子序列在原始序列上也是相邻的。这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为顺序分区： 基于相同的设置，通过顺序分区读取每个小批量的子序列的特征 X 和标签 Y。通过将它们打印出来可以发现：迭代期间来自两个相邻的小批量中的子序列在原始序列中确实是相邻的： 现在，我们将上面的两个采样函数包装到一个类中，以便稍后可以将其用作数据迭代器： 最后，我们定义了一个函数 load_data_time_machine，它同时返回数据迭代器和词表，因此可以与其他带有 load_data 前缀的函数（如 d2l.load_data_fashion_mnist）类似地使用： 4. 循环神经网络 由于涉及较多数学公式，循环神经网络的理论部分可以转至：循环神经网络。 4.1 循环神经网络的从零开始实现 本节将从头开始基于循环神经网络实现字符级语言模型。这样的模型将在 H.G.Wells 的时光机器数据集上训练。和前面上一节中介绍过的一样，我们先读取数据集： 回想一下，在 train_iter 中，每个词元都表示为一个数字索引，将这些索引直接输入神经网络可能会使学习变得困难。我们通常将每个词元表示为更具表现力的特征向量。最简单的表示称为独热编码（one-hot encoding）。 简言之，将每个索引映射为相互不同的单位向量：假设词表中不同词元的数目为N（即 len(vocab)），词元索引的范围为0~N-1。如果词元的索引是整数 i，那么我们将创建一个长度为N的全0向量，并将第 i 处的元素设置为1。此向量是原始词元的一个独热向量。索引为0和2的独热向量如下所示： 我们每次采样的小批量数据形状是二维张量：(批量大小, 时间步数)。one_hot 函数将这样一个小批量数据转换成三维张量，张量的最后一个维度等于词表大小（len(vocab)）。我们经常转换输入的维度，以便获得形状为 (时间步数, 批量大小, 词表大小) 的输出。这将使我们能够更方便地通过最外层的维度，一步一步地更新小批量数据的隐状态： 接下来，我们初始化循环神经网络模型的模型参数。隐藏单元数 num_hiddens 是一个可调的超参数。当训练语言模型时，输入和输出来自相同的词表（输出可以看成多分类问题，即输出表示对每个词元的预测概率）。因此，它们具有相同的维度，即词表的大小： 为了定义循环神经网络模型，我们首先需要一个 init_rnn_state 函数在初始化时返回隐状态。这个函数的返回是一个张量，张量全用0填充，形状为 (批量大小, 隐藏单元数)。在后面的章节中我们将会遇到隐状态包含多个变量的情况，而使用元组可以更容易地处理些： 下面的 rnn 函数定义了如何在一个时间步内计算隐状态和输出。循环神经网络模型通过 inputs 最外层的维度实现循环，以便逐时间步更新小批量数据的隐状态H。此外，这里使用 tanh 函数作为激活函数，当元素在实数上满足均匀分布时，tanh 函数的平均值为0： 定义了所有需要的函数之后，接下来我们创建一个类来包装这些函数，并存储从零开始实现的循环神经网络模型的参数： 让我们检查输出是否具有正确的形状。例如，隐状态的维数是否保持不变： 我们可以看到输出形状是 (时间步数 * 批量大小, 词表大小)，而隐状态形状保持不变，即 (批量大小, 隐藏单元数)。 让我们首先定义预测函数来生成 prefix 之后的新字符，其中的 prefix 是一个用户提供的包含多个字符的字符串。在循环遍历 prefix 中的开始字符时，我们不断地将隐状态传递到下一个时间步，但是不生成任何输出。这被称为预热（warm-up）期，因为在此期间模型会自我更新（例如，更新隐状态），但不会进行预测。预热期结束后，隐状态的值通常比刚开始的初始值更适合预测，从而预测字符并输出它们： 现在我们可以测试 predict 函数。我们将前缀指定为 time traveller，并基于这个前缀生成10个后续字符。鉴于我们还没有训练网络，它会生成荒谬的预测结果： 梯度裁剪的理论可转至：循环神经网络的从零开始实现。 下面我们定义一个函数来裁剪模型的梯度，模型是从零开始实现的模型或由高级 API 构建的模型。我们在此计算了所有模型参数的梯度的范数： 在训练模型之前，让我们定义一个函数在一个迭代周期内训练模型。它与我们训练 Softmax 模型的方式有三个不同之处： 序列数据的不同采样方法（随机采样和顺序分区）将导致隐状态初始化的差异。 我们在更新模型参数之前裁剪梯度。这样的操作的目的是，即使训练过程中某个点上发生了梯度爆炸，也能保证模型不会发散。 我们用困惑度来评价模型。这样的度量确保了不同长度的序列具有可比性。 具体来说，当使用顺序分区时，我们只在每个迭代周期的开始位置初始化隐状态。由于下一个小批量数据中的第 i 个子序列样本与当前第 i 个子序列样本相邻，因此当前小批量数据最后一个样本的隐状态，将用于初始化下一个小批量数据第一个样本的隐状态。这样，存储在隐状态中的序列的历史信息可以在一个迭代周期内流经相邻的子序列。然而，在任何一点隐状态的计算，都依赖于同一迭代周期中前面所有的小批量数据，这使得梯度计算变得复杂。为了降低计算量，在处理任何一个小批量数据之前，我们先分离梯度，使得隐状态的梯度计算总是限制在一个小批量数据的时间步内。 当使用随机抽样时，因为每个样本都是在一个随机位置抽样的，因此需要为每个迭代周期重新初始化隐状态。 循环神经网络模型的训练函数既支持从零开始实现，也可以使用高级 API 来实现。 现在，我们训练循环神经网络模型。因为我们在数据集中只使用了10000个词元，所以模型需要更多的迭代周期来更好地收敛： 4.2 循环神经网络的简洁实现 虽然从零开始实现循环神经网络对了解网络的实现方式具有指导意义，但并不方便。本节将展示如何使用深度学习框架的高级 API 提供的函数更有效地实现相同的语言模型。我们仍然从读取时光机器数据集开始： 高级 API 提供了循环神经网络的实现。我们构造一个具有256个隐藏单元的单隐藏层的循环神经网络层 rnn_layer。事实上，我们还没有讨论多层循环神经网络的意义（这将在深度循环神经网络中介绍）。现在仅需要将多层理解为一层循环神经网络的输出被用作下一层循环神经网络的输入就足够了： 我们使用张量来初始化隐状态，它的形状是 (隐藏层数, 批量大小, 隐藏单元数)： 通过一个隐状态和一个输入，我们就可以用更新后的隐状态计算输出。需要强调的是，rnn_layer 的输出（Y）不涉及输出层的计算：它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入： 我们为一个完整的循环神经网络模型定义了一个 RNNModel 类。注意，rnn_layer 只包含隐藏的循环层，我们还需要创建一个单独的输出层： 在训练模型之前，让我们基于一个具有随机权重的模型进行预测，d2l.predict_ch8 函数与上一节中的 predict 函数相同： 很明显，这种模型根本不能输出好的结果。接下来，我们使用上一节中定义的超参数训练模型： "},{"title":"Python路径操作模块pathlib教程","date":"2023-03-31T02:47:00.000Z","url":"/posts/55.html","tags":[["Python","/tags/Python/"]],"categories":[["Python","/categories/Python/"]],"content":" Python 路径操作新标准：pathlib 模块相较于老式的 os.path 更为简洁易用，本文为该模块的使用教程。 pathlib 库从 Python 3.4 开始，到 Python 3.6 已经比较成熟。如果你的新项目可以直接用 3.6 及以上，建议用 pathlib。相比于老式的 os.path 有几个优势： 老的路径操作函数管理比较混乱，有的是导入 os，有的又是在 os.path 当中，而新的用法统一可以用 pathlib 管理。 老用法在处理不同操作系统 Win、Mac 以及 Linux 之间很吃力。换了操作系统常常要改代码，还经常需要进行一些额外操作。 老用法主要是函数形式，返回的数据类型通常是字符串。但是路径和字符串并不等价，所以在使用 os 操作路径的时候常常还要引入其他类库协助操作。新用法是面向对象，处理起来更灵活方便。 pathlib 简化了很多操作，用起来更轻松。 1. 路径获取 （1）获取当前工作目录 注意：工作目录是在哪个目录下运行你的程序，不是项目目录。 虽然在这里打印出来的很像一个字符串，但实际上得到的是一个 WindowsPath('D:\\Dive into Deep Learning\\src') 对象，如果只想得到字符串表示，不想要 WindowsPath 对象，可以用 str() 转化。 （2）获取用户 Home 目录 （3）获取当前文件路径 （4）获取任意字符串路径 （5）获取绝对路径 （6）获取文件属性 文件属性比如文件大小、创建时间、修改时间等。 2. 路径组成部分 获取路径的组成部分非常方便： .name：文件名，包含后缀名，如果是目录则获取目录名。 .stem：文件名，不包含后缀。 .suffix：后缀，比如 .txt、.png。 .parent：父级目录，相当于 cd ..。 .anchor：锚，目录前面的部分 C:\\ 或者 /。 3. 子路径扫描 （1）扫描某个目录下的所有路径 （2）使用模式匹配（正则表达式）查找目录下的指定文件 （3）检查路径是否符合规则 4. 路径拼接 pathlib 支持用 / 拼接路径，如果用不惯 /，也可以用类似 os.path.join 的方法： 5. 路径测试（判断） 6. 文件操作 （1）创建文件 exist_ok 表示当文件已经存在时，程序的反应。如果为 True，文件存在时，不进行任何操作；如果为 False，则会报 FileExistsError 错误。 （2）创建目录 用 os 创建目录分为两个函数：mkdir() 和 makedirs()。mkdir() 一次只能创建一级目录，makedirs() 可以同时创建多级目录。使用 pathlib 只需要用 path.mkdir() 函数就可以。它提供了 parents 参数，设置为 True 可以创建多级目录，不设置则只能创建一层： （3）删除目录 删除目录非常危险，并且没有提示，一定要谨慎操作。一次只删除一级目录，且当前目录必须为空： （4）删除文件 "},{"title":"D2L学习笔记-计算机视觉","date":"2023-03-10T02:24:00.000Z","url":"/posts/24840.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 李沐动手学深度学习（PyTorch）课程学习笔记第七章：计算机视觉。 1. 图像增广 图像增广在对训练图像进行一系列的随机变化之后，生成相似但不同的训练样本，从而扩大了训练集的规模。此外，应用图像增广的原因是，随机改变训练样本可以减少模型对某些属性的依赖，从而提高模型的泛化能力。例如，我们可以以不同的方式裁剪图像，使感兴趣的对象出现在不同的位置，减少模型对于对象出现位置的依赖。我们还可以调整亮度、颜色等因素来降低模型对颜色的敏感度。 下面的代码有50%的几率使图像向左或向右翻转： 有50%的几率向上或向下翻转，注意，上下翻转图像不如左右图像翻转那样常用，需要根据数据集的特征考虑是否可以将图像上下翻转： 随机裁剪一个面积为原始面积10%到100%的区域，该区域的宽高比从0.5~2之间随机取值。然后，区域的宽度和高度都被缩放到200像素： 我们可以改变图像颜色的四个方面：亮度、对比度、饱和度和色调。在下面的示例中，我们随机更改图像的亮度，随机值为原始图像的50%(1 - 0.5)到150%(1 + 0.5)之间： 在实践中，我们将结合多种图像增广方法。我们可以通过使用一个 Compose 实例来综合上面定义的不同的图像增广方法，并将它们应用到每个图像： 图像增广可以直接作用在图像数据上，也可以在使用 torchvision.datasets 导入数据集的时候通过 transform 参数指定： 2. 微调 微调（fine-tuning）是迁移学习（transfer learning）中的常见技巧，微调包括以下四个步骤： 在源数据集（例如 ImageNet 数据集）上预训练神经网络模型，即源模型。 创建一个新的神经网络模型，即目标模型。这将复制源模型上的所有模型设计及其参数（输出层除外）。我们假定这些模型参数包含从源数据集中学到的知识，这些知识也将适用于目标数据集。我们还假设源模型的输出层与源数据集的标签密切相关；因此不在目标模型中使用该层。 向目标模型添加输出层，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。 在目标数据集（如椅子数据集）上训练目标模型。输出层将从头开始进行训练，而所有其他层的参数将根据源模型的参数进行微调。 当目标数据集比源数据集小得多时，微调有助于提高模型的泛化能力。 我们将在一个 CIFAR10 数据集上微调 ResNet-18 模型。该模型已在 ImageNet 数据集上进行了预训练： 3. 目标检测和边界框 在图像分类任务中，我们假设图像中只有一个主要物体对象，我们只关注如何识别其类别。然而，很多时候图像里有多个我们感兴趣的目标，我们不仅想知道它们的类别，还想得到它们在图像中的具体位置。在计算机视觉里，我们将这类任务称为目标检测（object detection）或目标识别（object recognition）。 下面加载本节将使用的示例图像。图像左边是一只狗，右边是一只猫。它们是这张图像里的两个主要目标： 在目标检测中，我们通常使用边界框（bounding box）来描述对象的空间位置。边界框是矩形的，由矩形左上角的以及右下角的 x 和 y 坐标决定。另一种常用的边界框表示方法是边界框中心的 (x, y) 轴坐标以及框的宽度和高度。 在这里，我们定义在这两种表示法之间进行转换的函数：box_corner_to_center 从两角表示法转换为中心宽度表示法，而 box_center_to_corner 反之亦然。输入参数 boxes 可以是长度为4的张量，也可以是形状为 (N, 4) 的二维张量，其中 N 是边界框的数量。 我们将根据坐标信息定义图像中狗和猫的边界框。图像中坐标的原点是图像的左上角，向右的方向为 x 轴的正方向，向下的方向为 y 轴的正方向： 我们可以将边界框在图中画出，以检查其是否准确。画之前，我们定义一个辅助函数 bbox_to_rect。它将边界框表示成 matplotlib 的边界框格式，在图像上添加边界框之后，我们可以看到两个物体的主要轮廓基本上在两个框内： 4. 目标检测数据集 目标检测领域没有像 MNIST 和 Fashion-MNIST 那样的小数据集。为了快速测试目标检测模型，我们收集并标记了一个小型数据集。首先，我们拍摄了一组香蕉的照片，并生成了1000张不同角度和大小的香蕉图像。然后，我们在一些背景图片的随机位置上放一张香蕉的图像。最后，我们在图片上为这些香蕉标记了边界框。 包含所有图像和 CSV 标签文件的香蕉检测数据集可以直接从互联网下载，通过 read_data_bananas 函数，我们读取香蕉检测数据集的图像和标签。该数据集的 CSV 文件内含目标类别标签和位于左上角和右下角的真实边界框坐标： 以下 BananasDataset 类别将允许我们创建一个自定义 Dataset 实例来加载香蕉检测数据集： 最后，我们定义 load_data_bananas 函数，来为训练集和测试集返回两个数据加载器实例。对于测试集，无须按随机顺序读取它： 让我们读取一个小批量，并打印其中的图像和标签的形状。图像的小批量的形状为：(批量大小, 通道数, 高度, 宽度)，它与我们之前图像分类任务中的相同。标签的小批量的形状为：(批量大小, M, 5)，其中 M 是数据集的任何图像中边界框可能出现的最大数量。 小批量计算虽然高效，但它要求每张图像含有相同数量的边界框，以便放在同一个批量中。通常来说，图像可能拥有不同数量个边界框；因此，在达到 M 之前，边界框少于 M 的图像将被非法边界框填充。这样，每个边界框的标签将被长度为5的数组表示。数组中的第一个元素是边界框中对象的类别，其中-1表示用于填充的非法边界框。数组的其余四个元素是边界框左上角和右下角的 (x, y) 坐标值（值域在0~1之间）。对于香蕉数据集而言，由于每张图像上只有一个边界框，因此 M = 1。 接下来让我们展示10幅带有真实边界框的图像。我们可以看到在所有这些图像中香蕉的旋转角度、大小和位置都有所不同。当然，这只是一个简单的人工数据集，实践中真实世界的数据集通常要复杂得多： 5. 锚框 由于本节难度较大，因此详细分析见：D2L-计算机视觉-锚框。 6. 多尺度目标检测 在上一节中，我们以输入图像的每个像素为中心，生成了多个锚框。基本而言，这些锚框代表了图像不同区域的样本。然而，如果为每个像素都生成的锚框，我们最终可能会得到太多需要计算的锚框。想象一个561*728的输入图像，如果以每个像素为中心生成五个形状不同的锚框，就需要在图像上标记和预测超过200万个锚框（561*728*5）。 6.1 多尺度锚框 减少图像上的锚框数量并不困难。比如，我们可以在输入图像中均匀采样一小部分像素，并以它们为中心生成锚框。此外，在不同尺度下，我们可以生成不同数量和不同大小的锚框。直观地说，比起较大的目标，较小的目标在图像上出现的可能性更多样。例如，1*1、1*2和2*2的目标可以分别以4、2和1种可能的方式出现在2*2的图像上。因此，当使用较小的锚框检测较小的物体时，我们可以采样更多的区域，而对于较大的物体，我们可以采样较少的区域。 为了演示如何在多个尺度下生成锚框，让我们先读取一张图像。它的高度和宽度分别为561和728像素： display_anchors 函数定义如下。我们在特征图（fmap）上生成锚框（anchors），每个单位（像素）作为锚框的中心。由于锚框中的 (x, y) 轴坐标值（anchors）已经被除以特征图（fmap）的宽度和高度，因此这些值介于0和1之间，表示特征图中锚框的相对位置。 由于锚框（anchors）的中心分布于特征图（fmap）上的所有单位，因此这些中心必须根据其相对空间位置在任何输入图像上均匀分布。更具体地说，给定特征图的宽度和高度 fmap_w 和 fmap_h，以下函数将均匀地对任何输入图像中 fmap_h 行和 fmap_w 列中的像素进行采样。以这些均匀采样的像素为中心，将会生成大小为 s（假设列表 s 的长度为1）且宽高比（ratios）不同的锚框： 首先，让我们考虑探测小目标。为了在显示时更容易分辨，在这里具有不同中心的锚框不会重叠：锚框的尺度设置为0.15，特征图的高度和宽度设置为4。我们可以看到，图像上4行和4列的锚框的中心是均匀分布的： 然后，我们将特征图的高度和宽度减小一半，然后使用较大的锚框来检测较大的目标。当尺度设置为0.4时，一些锚框将彼此重叠： 最后，我们进一步将特征图的高度和宽度减小一半，然后将锚框的尺度增加到0.8。此时，锚框的中心即是图像的中心： 6.2 多尺度检测 既然我们已经生成了多尺度的锚框，我们就将使用它们来检测不同尺度下各种大小的目标。下面，我们介绍一种基于 CNN 的多尺度目标检测方法，将在第8节（SSD）中实现。 在某种规模上，假设我们有 c 张形状为 h * w 的特征图。使用上一小节中的方法，我们生成了 hw 组锚框，其中每组都有 a 个中心相同的锚框。例如，在上一小节实验的第一个尺度上，给定10个（通道数量）4 * 4 的特征图，我们生成了16组锚框，每组包含3个中心相同的锚框。接下来，每个锚框都根据真实值边界框来标记了类和偏移量。在当前尺度下，目标检测模型需要预测输入图像上 hw 组锚框类别和偏移量，其中不同组锚框具有不同的中心。 假设此处的 c 张特征图是 CNN 基于输入图像的正向传播算法获得的中间输出。既然每张特征图上都有 hw 个不同的空间位置，那么相同空间位置可以看作含有 c 个单元。根据感受野的定义，特征图在相同空间位置的 c 个单元在输入图像上的感受野相同：它们表征了同一感受野内的输入图像信息。因此，我们可以将特征图在同一空间位置的 c 个单元变换为使用此空间位置生成的 a 个锚框类别和偏移量。本质上，我们用输入图像在某个感受野区域内的信息，来预测输入图像上与该区域位置相近的锚框类别和偏移量。 当不同层的特征图在输入图像上分别拥有不同大小的感受野时，它们可以用于检测不同大小的目标。例如，我们可以设计一个神经网络，其中靠近输出层的特征图单元具有更宽的感受野，这样它们就可以从输入图像中检测到较大的目标。 简言之，我们可以利用深层神经网络在多个层次上对图像进行分层表示，从而实现多尺度目标检测。在第8节我们将通过一个具体的例子来说明它是如何工作的。 7. 区域卷积神经网络（R-CNN）系列 7.1 R-CNN R-CNN 首先从输入图像中选取若干（例如2000个）提议区域（如锚框也是一种选取方法），并标注它们的类别和边界框（如偏移量）。然后，用卷积神经网络对每个提议区域进行前向传播以抽取其特征。接下来，我们用每个提议区域的特征来预测类别和边界框。具体来说，R-CNN 包括以下四个步骤： 对输入图像使用选择性搜索来选取多个高质量的提议区域。这些提议区域通常是在多个尺度下选取的，并具有不同的形状和大小。每个提议区域都将被标注类别和真实边界框； 选择一个预训练的卷积神经网络，并将其在输出层之前截断。将每个提议区域变形为网络需要的输入尺寸，并通过前向传播输出抽取的提议区域特征； 将每个提议区域的特征连同其标注的类别作为一个样本。训练多个支持向量机对目标分类，其中每个支持向量机用来判断样本是否属于某一个类别； 将每个提议区域的特征连同其标注的边界框作为一个样本，训练线性回归模型来预测真实边界框。 尽管 R-CNN 模型通过预训练的卷积神经网络有效地抽取了图像特征，但它的速度很慢。想象一下，我们可能从一张图像中选出上千个提议区域，这需要上千次的卷积神经网络的前向传播来执行目标检测。这种庞大的计算量使得 R-CNN 在现实世界中难以被广泛应用。 7.2 Fast R-CNN R-CNN 的主要性能瓶颈在于，对每个提议区域，卷积神经网络的前向传播是独立的，而没有共享计算。由于这些区域通常有重叠，独立的特征抽取会导致重复的计算。Fast R-CNN 对 R-CNN 的主要改进之一，是仅在整张图像上执行卷积神经网络的前向传播。Fast R-CNN 的主要计算如下： 与 R-CNN 相比，Fast R-CNN 用来提取特征的入卷积神经网络的输入是整个图像，而不是各个提议区域。此外，这个网络通常会参与训练。设输入为一张图像，将卷积神经网络的输出的形状记为 1 * c * h1 * w1； 假设选择性搜索生成了 n 个提议区域。这些形状各异的提议区域在卷积神经网络的输出上分别标出了形状各异的兴趣区域。然后，这些感兴趣的区域需要进一步抽取出形状相同的特征（比如指定高度 h2 和宽度 w2），以便于连结后输出。为了实现这一目标，Fast R-CNN 引入了兴趣区域汇聚层（RoI pooling）：将卷积神经网络的输出和提议区域作为输入，输出连结后的各个提议区域抽取的特征，形状为 n * c * h2 * w2； 通过全连接层将输出形状变换为 n * d，其中超参数 d 取决于模型设计； 预测 n 个提议区域中每个区域的类别和边界框。更具体地说，在预测类别和边界框时，将全连接层的输出分别转换为形状为 n * q（q 是类别的数量）的输出和形状为 n * 4 的输出。其中预测类别时使用 Softmax 回归。 下面，我们演示了兴趣区域汇聚层的计算方法。假设卷积神经网络抽取的特征 X 的高度和宽度都是4，且只有单通道： 让我们进一步假设输入图像的高度和宽度都是40像素，且选择性搜索在此图像上生成了两个提议区域。每个区域由5个元素表示：区域目标类别、左上角和右下角的 (x, y) 坐标： 由于 X 的高和宽是输入图像高和宽的1/10，因此，两个提议区域的坐标先按 spatial_scale 乘以0.1。然后，在 X 上分别标出这两个兴趣区域 X[:, :, 0:3, 0:3] 和 X[:, :, 1:4, 0:4]。最后，在 2 * 2 的兴趣区域汇聚层中，每个兴趣区域被划分为子窗口网格，并进一步抽取相同形状 2 * 2 的特征： 7.3 Faster R-CNN 为了较精确地检测目标结果，Fast R-CNN 模型通常需要在选择性搜索中生成大量的提议区域。Faster R-CNN 提出将选择性搜索替换为区域提议网络（region proposal network），从而减少提议区域的生成数量，并保证目标检测的精度。具体来说，区域提议网络的计算步骤如下： 使用填充为1的 3 * 3 的卷积层变换卷积神经网络的输出，并将输出通道数记为 c。这样，卷积神经网络为图像抽取的特征图中的每个单元均得到一个长度为 c 的新特征； 以特征图的每个像素为中心，生成多个不同大小和宽高比的锚框并标注它们； 使用锚框中心单元长度为 c 的特征，分别预测该锚框的二元类别（含目标还是背景）和边界框； 使用非极大值抑制，从预测类别为目标的预测边界框中移除相似的结果。最终输出的预测边界框即是兴趣区域汇聚层所需的提议区域。 值得一提的是，区域提议网络作为 Faster R-CNN 模型的一部分，是和整个模型一起训练得到的。换句话说，Faster R-CNN 的目标函数不仅包括目标检测中的类别和边界框预测，还包括区域提议网络中锚框的二元类别和边界框预测。作为端到端训练的结果，区域提议网络能够学习到如何生成高质量的提议区域，从而在减少了从数据中学习的提议区域的数量的情况下，仍保持目标检测的精度。 7.4 Mask R-CNN 如果在训练集中还标注了每个目标在图像上的像素级位置，那么 Mask R-CNN 能够有效地利用这些详尽的标注信息进一步提升目标检测的精度。 Mask R-CNN 是基于 Faster R-CNN 修改而来的。具体来说，Mask R-CNN 将兴趣区域汇聚层替换为了兴趣区域对齐层（RoI Align），使用双线性插值（bilinear interpolation）来保留特征图上的空间信息，从而更适于像素级预测。兴趣区域对齐层的输出包含了所有与兴趣区域的形状相同的特征图。它们不仅被用于预测每个兴趣区域的类别和边界框，还通过额外的全卷积网络预测目标的像素级位置。本章的后续章节将更详细地介绍如何使用全卷积网络预测图像中像素级的语义。 8. 单发多框检测（SSD） SSD 模型主要由基础网络组成，其后是几个多尺度特征块。基本网络用于从输入图像中提取特征，因此它可以使用深度卷积神经网络。单发多框检测论文中选用了在分类层之前截断的 VGG，现在也常用 ResNet 替代。我们可以设计基础网络，使它输出的高和宽较大。这样一来，基于该特征图生成的锚框数量较多，可以用来检测尺寸较小的目标。接下来的每个多尺度特征块将上一层提供的特征图的高和宽缩小（如减半），并使特征图中每个单元在输入图像上的感受野变得更广阔。 回想一下在第6节中，通过深度神经网络分层表示图像的多尺度目标检测的设计。由于接近顶部的多尺度特征图较小，但具有较大的感受野，它们适合检测较少但较大的物体。简而言之，通过多尺度特征块，单发多框检测生成不同大小的锚框，并通过预测边界框的类别和偏移量来检测大小不同的目标，因此这是一个多尺度目标检测模型。 8.1 类别预测层与边界框预测层 设目标类别的数量为 q。这样一来，锚框有 q + 1 个类别，其中第0类是背景。在某个尺度下，设特征图的高和宽分别为 h 和 w。如果以其中每个单元为中心生成 a 个锚框，那么我们需要对 hwa 个锚框进行分类。如果使用全连接层作为输出，很容易导致模型参数过多。回忆 NiN 一节介绍的使用卷积层的通道来输出类别预测的方法，单发多框检测采用同样的方法来降低模型复杂度。 具体来说，类别预测层使用一个保持输入高和宽的卷积层。这样一来，输出和输入在特征图宽和高上的空间坐标一一对应。考虑输出和输入同一空间坐标 (x, y)：输出特征图上 (x, y) 坐标的通道里包含了以输入特征图 (x, y) 坐标为中心生成的所有锚框的类别预测。因此输出通道数为 a * (q + 1)。 类别预测层的定义如下： 边界框预测层的设计与类别预测层的设计类似。唯一不同的是，这里需要为每个锚框预测4个偏移量，而不是 q + 1 个类别： 8.2 连结多尺度的预测 单发多框检测使用多尺度特征图来生成锚框并预测其类别和偏移量。在不同的尺度下，特征图的形状或以同一单元为中心的锚框的数量可能会有所不同。因此，不同尺度下预测输出的形状可能会有所不同。 在以下示例中，我们为同一个小批量构建两个不同比例（Y1 和 Y2）的特征图，其中 Y2 的高度和宽度是 Y1 的一半。以类别预测为例，假设 Y1 和 Y2 的每个单元分别生成了5个和3个锚框。进一步假设目标类别的数量为10，对于特征图 Y1 和 Y2，类别预测输出中的通道数分别为 5 * (10 + 1) = 55 和 3 * (10 + 1) = 33，其中任一输出的形状是 (批量大小, 通道数, 高度, 宽度)： 除了批量大小这一维度外，其他三个维度都具有不同的尺寸。为了将这两个预测输出链接起来以提高计算效率，我们将把这些张量转换为更一致的格式。 通道维包含中心相同的锚框的预测结果。我们首先将通道维移到最后一维。因为不同尺度下批量大小仍保持不变，我们可以将预测结果转成二维的 (批量大小, 高 * 宽 * 通道数) 的格式，以方便之后在维度1上的连结。这样一来，尽管 Y1 和 Y2 在通道数、高度和宽度方面具有不同的大小，我们仍然可以在同一个小批量的两个不同尺度上连接这两个预测输出： 8.3 高和宽减半块 高和宽减半块将输入特征图的高度和宽度减半，会扩大每个单元在其输出特征图中的感受野，该模块此前已在 VGG 中使用过： 8.4 基本网络块 基本网络块用于从输入图像中抽取特征。为了计算简洁，我们构造了一个小的基础网络，该网络串联3个高和宽减半块，并逐步将通道数翻倍： 8.5 完整的模型 完整的单发多框检测模型由五个模块组成，每个块生成的特征图既用于生成锚框，又用于预测这些锚框的类别和偏移量。在这五个模块中，第一个是基本网络块，第二个到第四个是高和宽减半块，最后一个模块使用全局最大池化层将高度和宽度都降到1。从技术上讲，第二到第五个区块都是 SSD 中的多尺度特征块： 现在我们为每个块定义前向传播。与图像分类任务不同，此处的输出包括：CNN 特征图 Y、在当前尺度下根据 Y 生成的锚框、预测的这些锚框的类别和偏移量（基于 Y）： 超参数的设置过程可以看：单发多框检测（SSD）。 现在，我们就可以按如下方式定义完整的模型 TinySSD 了： 8.6 训练模型 首先读取数据集和设置超参数： 然后定义损失函数和评价函数，目标检测有两种类型的损失。第一种有关锚框类别的损失：我们可以简单地复用之前图像分类问题里一直使用的交叉熵损失函数来计算；第二种有关正类锚框偏移量的损失：预测偏移量是一个回归问题。但是，对于这个回归问题，我们在这里不使用平方损失，而是使用 L1 范数损失，即预测值和真实值之差的绝对值。掩码变量 bbox_masks 令负类锚框和填充锚框不参与损失的计算。最后，我们将锚框类别和偏移量的损失相加，以获得模型的最终损失函数： 我们可以沿用准确率评价分类结果。由于偏移量使用了 L1 范数损失，我们使用平均绝对误差来（MAE）评价边界框的预测结果。这些预测结果是从生成的锚框及其预测偏移量中获得的： 最后是训练模型，在训练模型时，我们需要在模型的前向传播过程中生成多尺度锚框 anchors，并预测其类别 cls_preds 和偏移量 bbox_preds。然后，我们根据标签信息 label 为生成的锚框标记类别 cls_labels 和偏移量 bbox_labels。最后，我们根据类别和偏移量的预测和标注值计算损失函数： 8.7 预测目标 在预测阶段，我们希望能把图像里面所有我们感兴趣的目标检测出来。在下面，我们读取并调整测试图像的大小，然后将其转成卷积层需要的四维格式。使用 multibox_detection 函数，我们可以根据锚框及其预测偏移量得到预测边界框，然后通过非极大值抑制来移除相似的预测边界框。最后，我们筛选所有置信度不低于0.9的边界框，做为最终输出： 9. 语义分割和数据集 在前几节中讨论的目标检测问题中，我们一直使用方形边界框来标注和预测图像中的目标。本节将探讨语义分割（semantic segmentation）问题，它重点关注于如何将图像分割成属于不同语义类别的区域。与目标检测不同，语义分割可以识别并理解图像中每一个像素的内容：其语义区域的标注和预测是像素级的。与目标检测相比，语义分割标注的像素级的边框显然更加精细。 9.1 图像分割和实例分割 计算机视觉领域还有2个与语义分割相似的重要问题，即图像分割（image segmentation）和实例分割（instance segmentation）。我们在这里将它们同语义分割简单区分一下： 图像分割将图像划分为若干组成区域，这类问题的方法通常利用图像中像素之间的相关性。它在训练时不需要有关图像像素的标签信息，在预测时也无法保证分割出的区域具有我们希望得到的语义。以图像 catdog.jpg 作为输入，图像分割可能会将狗分为两个区域：一个覆盖以黑色为主的嘴和眼睛，另一个覆盖以黄色为主的其余部分身体。 实例分割也叫同时检测并分割（simultaneous detection and segmentation），它研究如何识别图像中各个目标实例的像素级区域。与语义分割不同，实例分割不仅需要区分语义，还要区分不同的目标实例。例如，如果图像中有两条狗，则实例分割需要区分像素属于的两条狗中的哪一条。 9.2 Pascal VOC2012 语义分割数据集 最重要的语义分割数据集之一是 Pascal VOC2012，下面我们深入了解一下这个数据集： 进入路径 ../data/VOCdevkit/VOC2012 之后，我们可以看到数据集的不同组件。ImageSets/Segmentation 路径包含用于训练和测试样本的文本文件，而 JPEGImages 和 SegmentationClass 路径分别存储着每个示例的输入图像和标签。此处的标签也采用图像格式，其尺寸和它所标注的输入图像的尺寸相同。此外，标签中颜色相同的像素属于同一个语义类别。下面将 read_voc_images 函数定义为将所有输入的图像和标签读入内存： 下面我们绘制前5个输入图像及其标签。在标签图像中，白色和黑色分别表示边框和背景，而其他颜色则对应不同的类别： 接下来，我们列举 RGB 颜色值和类名： 通过上面定义的两个常量，我们可以方便地查找标签中每个像素的类索引。我们定义了 voc_colormap2label 函数来构建从上述 RGB 颜色值到类别索引的映射，而 voc_label_indices 函数将 RGB 值映射到在 Pascal VOC2012 数据集中的类别索引： 例如，在第一张样本图像中，飞机头部区域的类别索引为1，而背景索引为0： 之前的实验我们通过再缩放图像使其符合模型的输入形状。然而在语义分割中，这样做需要将预测的像素类别重新映射回原始尺寸的输入图像。这样的映射可能不够精确，尤其在不同语义的分割区域。为了避免这个问题，我们将图像裁剪为固定尺寸，而不是再缩放。具体来说，我们使用图像增广中的随机裁剪，裁剪输入图像和标签的相同区域： 我们通过继承高级 API 提供的 Dataset 类，自定义了一个语义分割数据集类 VOCSegDataset。通过实现 __getitem__ 函数，我们可以任意访问数据集中索引为 idx 的输入图像及其每个像素的类别索引。由于数据集中有些图像的尺寸可能小于随机裁剪所指定的输出尺寸，这些样本可以通过自定义的 filter 函数移除掉。此外，我们还定义了 normalize_image 函数，从而对输入图像的 RGB 三个通道的值分别做标准化： 最后，我们定义以下 load_data_voc 函数来下载并读取 Pascal VOC2012 语义分割数据集。它返回训练集和测试集的数据迭代器： 10. 转置卷积"},{"title":"D2L学习笔记-现代卷积神经网络","date":"2023-03-03T01:57:00.000Z","url":"/posts/21165.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 李沐动手学深度学习（PyTorch）课程学习笔记第六章：现代卷积神经网络。 1. 深度卷积神经网络（AlexNet） AlexNet 和 LeNet 的设计理念非常相似，但也存在显著差异： AlexNet 比相对较小的 LeNet5 要深得多。AlexNet 由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。 AlexNet 使用 ReLU 而不是 Sigmoid 作为其激活函数。 此外，AlexNet 将 Sigmoid 激活函数改为更简单的 ReLU 激活函数。一方面，ReLU 激活函数的计算更简单，它不需要如 Sigmoid 激活函数那般复杂的求幂运算。另一方面，当使用不同的参数初始化方法时，ReLU 激活函数使训练模型更加容易。当 Sigmoid 激活函数的输出非常接近于0或1时，这些区域的梯度几乎为0，因此反向传播无法继续更新一些模型参数。相反，ReLU 激活函数在正区间的梯度总是1。因此，如果模型参数没有正确初始化，Sigmoid 函数可能在正区间内得到几乎为0的梯度，从而使模型无法得到有效的训练。 尽管原文中 AlexNet 是在 ImageNet 上进行训练的，但本文在这里使用的是 Fashion-MNIST 数据集。因为即使在现代 GPU 上，训练 ImageNet 模型，同时使其收敛可能需要数小时或数天的时间。将 AlexNet 直接应用于 Fashion-MNIST 的一个问题是 Fashion-MNIST 图像的分辨率（28×28像素）低于 ImageNet 图像。为了解决这个问题，我们将它们增加到224×224像素（通常来讲这不是一个明智的做法，但在这里这样做是为了有效使用 AlexNet 架构）。 2. 使用块的网络（VGG） 虽然 AlexNet 证明深层神经网络卓有成效，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络。 经典卷积神经网络的基本组成部分是下面的这个序列： 带填充以保持分辨率的卷积层。 非线性激活函数，如 ReLU。 汇聚层，如最大汇聚层。 而一个 VGG 块与之类似，由一系列卷积层组成，后面再加上用于空间下采样的最大汇聚层。 VGG 使用可重复使用的卷积块来构建深度卷积神经网络，不同的卷积块个数和超参数可以得到不同复杂度的变种。 下面的代码中，我们定义了一个名为 vgg_block 的函数来实现一个 VGG 块，该函数有三个参数，分别对应于卷积层的数量 num_convs、输入通道的数量 in_channels 和输出通道的数量 out_channels： 原始 VGG 网络有5个卷积块，其中前两个块各有一个卷积层，后三个块各包含两个卷积层。第一个模块有64个输出通道，每个后续模块将输出通道数量翻倍，直到达到512。由于该网络使用8个卷积层和3个全连接层，因此它通常被称为 VGG-11。 由于 VGG-11 比 AlexNet 计算量更大，因此我们构建了一个通道数较少的网络，足够用于训练 Fashion-MNIST 数据集： 最后我们读取数据集并进行训练，超参数设置：lr, num_epochs = 0.02, 15，训练过程与第一节内容一样，因此不再放出代码。 PS：如果显存不够可以减小 batch_size，从128改为64或32。 3. 网络中的网络（NiN） 回想一下，卷积层的输入和输出由四维张量组成，张量的每个轴分别对应样本、通道、高度和宽度。另外，全连接层的输入和输出通常是分别对应于样本和特征的二维张量。NiN 的想法是在每个像素位置（针对每个高度和宽度）应用一个全连接层。如果我们将权重连接到每个空间位置，我们可以将其视为1×1卷积层（如第五章第四节 PS 中所述），或作为在每个像素位置上独立作用的全连接层。从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征（feature）。 NiN 块以一个普通卷积层开始，后面是两个1×1的卷积层。这两个1×1卷积层充当带有 ReLU 激活函数的逐像素全连接层，对每个像素增加了非线性特性： NiN 和 AlexNet 之间的一个显著区别是 NiN 完全取消了全连接层。相反，NiN 使用一个 NiN 块，其输出通道数等于标签类别的数量。最后放一个全局平均汇聚层（global average pooling layer），生成一个对数几率（logits）。NiN 设计的一个优点是，它显著减少了模型所需参数的数量。然而，在实践中，这种设计有时会增加训练模型的时间。 最后我们读取数据集并进行训练，超参数设置：lr, num_epochs = 0.1, 15，训练过程与第一节内容一样，因此不再放出代码。 4. 含并行连结的网络（GoogLeNet） 在 GoogLeNet 中，基本的卷积块被称为 Inception 块（Inception block）。Inception 块由四条并行路径组成。前三条路径使用窗口大小为1×1、3×3和5×5的卷积层，从不同空间大小中提取信息。中间的两条路径在输入上执行1×1卷积，以减少通道数，从而降低模型的复杂性。第四条路径使用3×3最大汇聚层，然后使用1×1卷积层来改变通道数。这四条路径都使用合适的填充来使输入与输出的高和宽一致，最后我们将每条线路的输出在通道维度上连结，并构成 Inception 块的输出。在 Inception 块中，通常调整的超参数是每层输出通道数。 GoogLeNet 一共使用9个 Inception 块和全局平均汇聚层的堆叠来生成其估计值。Inception 块之间的最大汇聚层可降低维度。第一个模块类似于 AlexNet 和 LeNet，Inception 块的组合从 VGG 继承，全局平均汇聚层避免了在最后使用全连接层。 现在，我们逐一实现 GoogLeNet 的每个模块。第一个模块使用64个通道、7×7卷积层： 第二个模块使用两个卷积层：第一个卷积层是64个通道、1×1卷积层；第二个卷积层使用将通道数量增加三倍的3×3卷积层。这对应于 Inception 块中的第二条路径： 第三个模块串联两个完整的 Inception 块。第一个 Inception 块的输出通道数为 64 + 128 + 32 + 32 = 256，四个路径之间的输出通道数量比为 2 : 4 : 1 : 1，第二个和第三个路径首先将输入通道的数量分别减少到96和16，然后连接第二个卷积层。第二个 Inception 块的输出通道数增加到 128 + 192 + 96 + 64 = 480，四个路径之间的输出通道数量比为 4 : 6 : 3 : 2，第二条和第三条路径首先将输入通道的数量分别减少到128和32： 第四模块更加复杂，它串联了5个 Inception 块，其输出通道数分别是 192 + 208 + 48 + 64 = 512、160 + 224 + 64 + 64 = 512、128 + 256 + 64 + 64 = 512、112 + 288 + 64 + 64 = 528 和 256 + 320 + 128 + 128 = 832： 第五模块包含输出通道数为 256 + 320 + 128 + 128 = 832 和 384 + 384 + 128 + 128 = 1024 的两个 Inception 块。需要注意的是，第五模块的后面紧跟输出层，该模块同 NiN 一样使用全局平均汇聚层，将每个通道的高和宽变成1。最后我们将输出变成二维数组，再接上一个输出个数为标签类别数的全连接层： 最后我们读取数据集并进行训练，超参数设置：lr, num_epochs = 0.1, 15，训练过程与第一节内容一样，因此不再放出代码。 5. 批量规范化（BN） 批量规范化（Batch Normalization）是一种流行且有效的技术，可持续加速深层网络的收敛速度。 使用真实数据时，我们的第一步是标准化输入特征，使其平均值为0，方差为1。直观地说，这种标准化可以很好地与我们的优化器配合使用，因为它可以将参数的量级进行统一。 第二，对于典型的多层感知机或卷积神经网络。当我们训练时，中间层中的变量（例如，多层感知机中的仿射变换输出）可能具有更广的变化范围：不论是沿着从输入到输出的层，跨同一层中的单元，或是随着时间的推移，模型参数随着训练更新的变幻莫测。批量规范化的发明者非正式地假设，这些变量分布中的这种偏移可能会阻碍网络的收敛。直观地说，我们可能会猜想，如果一个层的可变值是另一层的100倍，这可能需要对学习率进行补偿调整。 第三，更深层的网络很复杂，容易过拟合。这意味着正则化变得更加重要。 批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。接下来，我们应用比例系数和比例偏移。正是由于这个基于批量统计的标准化，才有了批量规范化的名称。 请注意，如果我们尝试使用大小为1的小批量应用批量规范化，我们将无法学到任何东西。这是因为在减去均值之后，每个隐藏单元将为0。所以，只有使用足够大的小批量，批量规范化这种方法才是有效且稳定的。请注意，在应用批量规范化时，批量大小的选择可能比没有批量规范化时更重要。 通常，我们将批量规范化层置于全连接层中的仿射变换和激活函数之间。同样，对于卷积层，我们可以在卷积层之后和非线性激活函数之前应用批量规范化。 下面，我们从头开始实现一个具有张量的批量规范化层： 我们现在可以创建一个正确的 BatchNorm 层。这个层将保持适当的参数：拉伸 gamma 和偏移 beta，这两个参数将在训练过程中更新。此外，我们的层将保存均值和方差的移动平均值，以便在模型预测期间随后使用。 为了更好理解如何应用 BatchNorm，下面我们将其应用于 LeNet 模型： 最后我们读取数据集并进行训练，超参数设置：lr, num_epochs = 1, 15，由于网络模型类似 LeNet，因此无需对输入图像进行 Resize 操作，训练过程与第一节内容一样，因此不再放出代码。 6. 残差网络（ResNet） 只有当较复杂的函数类包含较小的函数类时，我们才能确保提高它们的性能。对于深度神经网络，如果我们能将新添加的层训练成恒等映射（identity function）：f(x) = x，新模型和原模型将同样有效。同时，由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。 残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。 ResNet 沿用了 VGG 完整的卷积层设计。残差块里首先有2个有相同输出通道数的卷积层。每个卷积层后接一个批量规范化层和 ReLU 激活函数。然后我们通过跨层数据通路，跳过这2个卷积运算，将输入直接加在最后的 ReLU 激活函数前。这样的设计要求2个卷积层的输出与输入形状一样，从而使它们可以相加。如果想改变通道数，就需要引入一个额外的1×1卷积层来将输入变换成需要的形状后再做相加运算。残差块的实现如下： 此代码生成两种类型的网络：一种是当 use_1x1conv = False 时，应用 ReLU 非线性函数之前，将输入添加到输出。另一种是当 use_1x1conv = True 时，添加通过1×1卷积调整通道和分辨率。 ResNet 的前两层跟之前介绍的 GoogLeNet 中的一样：在输出通道数为64、步幅为2的7×7卷积层后，接步幅为2的3×3最大汇聚层。不同之处在于 ResNet 每个卷积层后增加了批量规范化层。 GoogLeNet 在后面接了4个由 Inception 块组成的模块。ResNet 则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。第一个模块的通道数同输入通道数一致。由于之前已经使用了步幅为2的最大汇聚层，所以无须减小高和宽。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。 下面我们来实现这个模块。注意，我们对第一个模块做了特别处理： 接着在 ResNet 加入所有残差块，这里每个模块使用2个残差块： 最后，与 GoogLeNet 一样，在 ResNet 中加入全局平均汇聚层，以及全连接层输出： 每个模块有4个卷积层（不包括恒等映射的卷积层）。加上第一个7×7卷积层和最后一个全连接层，共有18层。因此，这种模型通常被称为 ResNet-18。通过配置不同的通道数和模块里的残差块数可以得到不同的 ResNet 模型，例如更深的含152层的 ResNet-152。 最后我们读取数据集并进行训练，超参数设置：lr, num_epochs = 0.02, 15，由于 ResNet 性能很强，对于 FashionMNIST 数据集很容易就过拟合了，因此可以将输入图像 Resize 为 (96, 96)，训练过程与第一节内容一样，因此不再放出代码。 ResNet 其它版本的模型结构可以参考： ResNet 及其变种的结构梳理、有效性分析与代码解读。 ResNet18、50网络结构以及 PyTorch 实现代码。 ResNet50网络结构搭建（PyTorch）。 7. 稠密连接网络（DenseNet） ResNet 和 DenseNet 的关键区别在于，DenseNet 输出是连接（用 [.] 表示）而不是如 ResNet 的简单相加。 稠密网络主要由2部分构成：稠密块（dense block）和过渡层（transition layer）。前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。 DenseNet 使用了 ResNet 改良版的“批量规范化、激活和卷积”架构。我们首先实现一下这个架构： 一个稠密块由多个卷积块组成，每个卷积块使用相同数量的输出通道。然而，在前向传播中，我们将每个卷积块的输入和输出在通道维上连结： 例如我们构建一个 DenseBlock(2, 3, 10)，那么两层卷积层分别为 conv_block(3, 10)、conv_block(13, 10)。第一层卷积输出的通道维是10，与输入 X 在通道维上连结后通道维是13，因此第二层卷积输入的通道维是13，第二层卷积输出的通道维是10，与输入 X 在通道维上连结后通道维是23： 由于每个稠密块都会带来通道数的增加，使用过多则会过于复杂化模型。而过渡层可以用来控制模型复杂度。它通过1×1卷积层来减小通道数，并使用步幅为2的平均汇聚层减半高和宽，从而进一步降低模型复杂度： 对上一个例子中稠密块的输出使用通道数为10的过渡层。此时输出的通道数减为10，高和宽均减半： 我们来构造 DenseNet 模型。DenseNet 首先使用同 ResNet 一样的单卷积层和最大汇聚层： 接下来，类似于 ResNet 使用的4个残差块，DenseNet 使用的是4个稠密块。与 ResNet 类似，我们可以设置每个稠密块使用多少个卷积层。这里我们设成4，从而与之前的 ResNet-18 保持一致。稠密块里的卷积层通道数（即增长率）设为32，所以每个稠密块将增加128个通道。 在每个模块之间，ResNet 通过步幅为2的残差块减小高和宽，DenseNet 则使用过渡层来减半高和宽，并减半通道数： 与 ResNet 类似，最后接上全局汇聚层和全连接层来输出结果： 最后我们读取数据集并进行训练，超参数设置：lr, num_epochs = 0.1, 15，将输入图像 Resize 为 (96, 96)，训练过程与第一节内容一样，因此不再放出代码。 下一章：计算机视觉。"},{"title":"D2L学习笔记-卷积神经网络","date":"2023-03-01T01:20:00.000Z","url":"/posts/25122.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 李沐动手学深度学习（PyTorch）课程学习笔记第五章：卷积神经网络。 1. 从全连接层到卷积 假设我们有一个足够充分的照片数据集，数据集中是拥有标注的照片，每张照片具有百万级像素，这意味着网络的每次输入都有一百万个维度。即使将隐藏层维度降低到1000，这个全连接层也将有十亿个参数。 假设我们想从一张图片中找到某个物体。合理的假设是：无论哪种方法找到这个物体，都应该和物体的位置无关。卷积神经网络正是将空间不变性（spatial invariance）的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示： 平移不变性（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。 局部性（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。 2. 图像卷积 严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是互相关运算（cross-correlation），而不是卷积运算。在卷积层中，输入张量和核张量通过互相关运算产生输出张量。 在二维互相关运算中，卷积窗口从输入张量的左上角开始，从左到右、从上到下滑动。当卷积窗口滑动到新一个位置时，包含在该窗口中的部分张量与卷积核张量进行按元素相乘，得到的张量再求和得到一个单一的标量值，由此我们得出了这一位置的输出张量值。 我们可以自己实现如上过程： 卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。 我们可以基于上面定义的 corr2d 函数实现二维卷积层： 现在来看一下卷积层的一个简单应用：通过找到像素变化的位置，来检测图像中不同颜色的边缘。我们假设0为黑色像素，1为白色像素： X 的内容如下： 接下来，我们构造一个高度为1、宽度为2的卷积核K。当进行互相关运算时，如果水平相邻的两元素相同，则输出为零，否则输出为非零： Y 的内容如下： 现在我们使用 PyTorch 的卷积层尝试通过正确结果 Y 是否能学习出我们之前自己构造出的卷积核参数： 3. 填充和步幅 在应用多层卷积时，我们常常丢失边缘像素。由于我们通常使用小卷积核，因此对于任何单个卷积，我们可能只会丢失几个像素。但随着我们应用许多连续卷积层，累积丢失的像素数就多了。解决这个问题的简单方法即为填充（padding）：在输入图像的边界填充元素（通常填充元素是0）。 在计算互相关时，卷积窗口从输入张量的左上角开始，向下、向右滑动。在前面的例子中，我们默认每次滑动一个元素。但是，有时候为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素。我们将每次滑动元素的数量称为步幅（stride）。 4. 多输入多输出通道 当输入包含多个通道时，需要构造一个与输入数据具有相同输入通道数的卷积核，以便与输入数据进行互相关运算。 为了加深理解，我们实现一下多输入通道互相关运算。简而言之，我们所做的就是对每个通道执行互相关操作，然后将结果相加： 到目前为止，不论有多少输入通道，我们还只有一个输出通道。在最流行的神经网络架构中，随着神经网络层数的加深，我们常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。直观地说，我们可以将每个通道看作对不同特征的响应。而现实可能更为复杂一些，因为每个通道不是独立学习的，而是为了共同使用而优化的。因此，多输出通道并不仅是学习多个单通道的检测器。 PS：1*1卷积层通常用于调整网络层的通道数量和控制模型复杂性，其失去了卷积层的特有能力：在高度和宽度维度上，识别相邻元素间相互作用的能力。 5. 汇聚层（池化层） 汇聚层（pooling layer）具有双重目的：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。 与卷积层类似，汇聚层运算符由一个固定形状的窗口组成，该窗口根据其步幅大小在输入的所有区域上滑动（从左至右、从上至下），为固定形状窗口（有时称为汇聚窗口）遍历的每个位置计算一个输出。然而，不同于卷积层中的输入与卷积核之间的互相关计算，汇聚层不包含参数。相反，池运算是确定性的，我们通常计算汇聚窗口中所有元素的最大值或平均值。这些操作分别称为最大汇聚层（maximum pooling）和平均汇聚层（average pooling）。 与卷积层一样，汇聚层也可以改变输出形状。和以前一样，我们可以通过填充和步幅以获得所需的输出形状。下面，我们用深度学习框架中内置的二维最大汇聚层，来演示汇聚层中填充和步幅的使用。 默认情况下，深度学习框架中的步幅与汇聚窗口的大小相同。因此，如果我们使用形状为 (3, 3) 的汇聚窗口，那么默认情况下，我们得到的步幅形状为 (3, 3)。 在处理多通道输入数据时，汇聚层在每个输入通道上单独运算，而不是像卷积层一样在通道上对输入进行汇总。这意味着汇聚层的输出通道数与输入通道数相同。下面，我们将在通道维度上连结张量 X 和 X + 1，以构建具有2个通道的输入： 6. LeNet 本节将介绍 LeNet，它是最早发布的卷积神经网络之一。总体来看，LeNet（LeNet-5）由两个部分组成： 卷积编码器：由两个卷积层组成。 全连接层密集块：由三个全连接层组成。 每个卷积块中的基本单元是一个卷积层、一个 Sigmoid 激活函数和平均汇聚层。请注意，虽然 ReLU 和最大汇聚层更有效，但它们在20世纪90年代还没有出现。每个卷积层使用5×5卷积核和一个 Sigmoid 激活函数。这些层将输入映射到多个二维特征输出，通常同时增加通道的数量。第一卷积层有6个输出通道，而第二个卷积层有16个输出通道。每个2×2池化操作（步幅2）通过空间下采样将维数减少4倍。卷积的输出形状由批量大小、通道数、高度、宽度决定。 虽然卷积神经网络的参数较少，但与深度的多层感知机相比，它们的计算成本仍然很高，因为每个参数都参与更多的乘法。通过使用 GPU，可以用它加快训练。 下一章：现代卷积神经网络。"},{"title":"D2L学习笔记-PyTorch神经网络基础","date":"2023-02-28T02:34:00.000Z","url":"/posts/23526.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 李沐动手学深度学习（PyTorch）课程学习笔记第四章：PyTorch 神经网络基础。 1. 层和块 在构造自定义块之前，我们先回顾一下多层感知机（第三章第二节）的代码，下面的代码生成一个网络，其中包含一个具有256个单元和 ReLU 激活函数的全连接隐藏层，然后是一个具有10个隐藏单元且不带激活函数的全连接输出层： 在这个例子中，我们通过实例化 nn.Sequential 来构建我们的模型，层的执行顺序是作为参数传递的。简而言之，nn.Sequential 定义了一种特殊的 Module，即在 PyTorch 中表示一个块的类，它维护了一个由 Module 组成的有序列表。注意，两个全连接层都是 Linear 类的实例，Linear 类本身就是 Module 的子类。另外，到目前为止，我们一直在通过 net(X) 调用我们的模型来获得模型的输出。这实际上是 net.__call__(X) 的简写。这个前向传播函数非常简单：它将列表中的每个块连接在一起，将每个块的输出作为下一个块的输入。 在下面的代码片段中，我们从零开始编写一个块。它包含一个多层感知机，其具有256个隐藏单元的隐藏层和一个10维输出层。注意，下面的 MLP 类继承了表示块的类。我们的实现只需要提供我们自己的构造函数（Python 中的 __init__ 函数）和前向传播函数： 接着我们实例化多层感知机的层，然后在每次调用前向传播函数时调用这些层： 现在我们可以更仔细地看看 Sequential 类是如何工作的，回想一下 Sequential 的设计是为了把其他模块串起来。为了构建我们自己的简化的 MySequential，我们只需要定义两个关键函数： 一种将块逐个追加到列表中的函数； 一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。 下面的 MySequential 类提供了与默认 Sequential 类相同的功能： Sequential 类使模型构造变得简单，允许我们组合新的架构，而不必定义自己的类。然而，并不是所有的架构都是简单的顺序架构。当需要更强的灵活性时，我们需要定义自己的块。例如，我们可能希望在前向传播函数中执行 Python 的控制流。此外，我们可能希望执行任意的数学运算，而不是简单地依赖预定义的神经网络层： 我们可以混合搭配各种组合块的方法。在下面的例子中，我们以一些想到的方法嵌套块： 2. 参数管理 我们首先看一下具有单隐藏层的多层感知机： 我们从已有模型中访问参数。当通过 Sequential 类定义模型时，我们可以通过索引来访问模型的任意层。这就像模型是一个列表一样，每层的参数都在其属性中。如下所示，我们可以检查第二个全连接层的参数： 这个全连接层包含两个参数，分别是该层的权重和偏置，每个参数都表示为参数类的一个实例。要对参数执行任何操作，首先我们需要访问底层的数值： 参数是复合的对象，包含值、梯度和额外信息。这就是我们需要显式参数值的原因。除了值之外，我们还可以访问每个参数的梯度。在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态： 当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。下面，我们将通过演示来比较访问第一个全连接层的参数和访问所有层： 这为我们提供了另一种访问网络参数的方式，如下所示： 现在让我们看看如果我们将多个块相互嵌套，参数命名约定是如何工作的： 因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们： 知道了如何访问参数后，现在我们看看如何正确地初始化参数。深度学习框架提供默认随机初始化，也允许我们创建自定义初始化方法。 让我们首先调用内置的初始化器。下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量，且将偏置参数设置为0： 我们还可以将所有参数初始化为给定的常数，比如初始化为1： 我们还可以对某些块应用不同的初始化方法。例如，下面我们使用 Xavier 初始化方法初始化第一个神经网络层，然后将第三个神经网络层初始化为常量值42： 有时，深度学习框架没有提供我们需要的初始化方法。在下面的例子中，我们先初始化为-10~10的均匀分布，然后将绝对值小于5的参数置零： 有时我们希望在多个层间共享参数：我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数： 这个例子表明第三个和第五个神经网络层的参数是绑定的。它们不仅值相等，而且由相同的张量表示。因此，如果我们改变其中一个参数，另一个参数也会改变。这里有一个问题：当参数绑定时，梯度会发生什么情况？答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层（即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。 3. 自定义层 我们构造一个没有任何参数的自定义层，下面的 CenteredLayer 类要从其输入中减去均值。要构建它，我们只需继承基础层类并实现前向传播功能： 下面我们继续定义具有参数的层，这些参数可以通过训练进行调整。让我们实现自定义版本的全连接层。回想一下，该层需要两个参数，一个用于表示权重，另一个用于表示偏置项。在此实现中，我们使用 ReLU 作为激活函数。该层需要输入参数：in_units 和 units，分别表示输入数和输出数： 4. 读写文件 加载和保存张量： 将存储在文件中的数据读回内存： 存储一个张量列表，然后把它们读回内存： 我们可以写入或读取从字符串映射到张量的字典。当我们要读取或写入模型中的所有权重时，这很方便： 深度学习框架提供了内置函数来保存和加载整个网络。需要注意的一个重要细节是，这将保存模型的参数而不是保存整个模型： 5. GPU CUDA 的安装配置教程：Anaconda与PyTorch安装教程 在 PyTorch 中，CPU 和 GPU 可以用 torch.device('cpu') 和 torch.device('cuda') 表示： 我们可以查询可用 GPU 的数量： 我们可以查询张量所在的设备。默认情况下，张量是在 CPU 上创建的： 需要注意的是，无论何时我们要对多个项进行操作，它们都必须在同一个设备上。例如，如果我们对两个张量求和，我们需要确保两个张量都位于同一个设备上，否则框架将不知道在哪里存储结果，甚至不知道在哪里执行计算。 有几种方法可以在 GPU 上存储张量。例如，我们可以在创建张量时指定存储设备： 数据在同一个 GPU 上我们才可以将它们相加： 类似地，神经网络模型可以指定设备。下面的代码将模型参数放在 GPU 上： 总之，只要所有的数据和参数都在同一个设备上，我们就可以有效地学习模型。 下一章：卷积神经网络。"},{"title":"D2L学习笔记-多层感知机","date":"2023-02-18T08:36:00.000Z","url":"/posts/46068.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 李沐动手学深度学习（PyTorch）课程学习笔记第三章：多层感知机。 1. 多层感知机的从零实现 FashionMNIST 数据集的读取与第二章第四节一样，此处不再放上代码。 初始化模型参数，我们将实现一个具有单隐藏层的多层感知机，它包含256个隐藏单元。注意，我们可以将这两个变量都视为超参数。通常，我们选择2的若干次幂作为层的宽度。因为内存在硬件中的分配和寻址方式，这么做往往可以在计算上更高效。 为了确保我们对模型的细节了如指掌，我们将实现 ReLU 激活函数，而不是直接调用内置的 relu 函数： 接下来定义模型： 训练函数的代码也与2.4节基本一样，只需将 net.to(device) 与 net.train() 等与 nn.Module 相关的代码去掉即可，因此不放出完整代码： 2. 多层感知机的简洁实现 与 Softmax 回归的简洁实现（第二章第四节）相比，唯一的区别是我们添加了2个全连接层（之前我们只添加了1个全连接层）。第一层是隐藏层，它包含256个隐藏单元，并使用了 ReLU 激活函数。第二层是输出层，因此我们只需要重点看一下模型即可： 3. 模型选择、欠拟合和过拟合 首先我们需要了解训练误差和泛化误差： 训练误差（training error）：模型在训练数据集上计算得到的误差。 泛化误差（generalization error）：模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。 问题是，我们永远不能准确地计算出泛化误差。这是因为无限多的数据样本是一个虚构的对象。在实际中，我们只能通过将模型应用于一个独立的测试集来估计泛化误差，该测试集由随机选取的、未曾在训练集中出现的数据样本构成。 在机器学习中，我们通常在评估几个候选模型后选择最终的模型。这个过程叫做模型选择。有时，需要进行比较的模型在本质上是完全不同的（比如，决策树与线性模型）。又有时，我们需要比较不同的超参数设置下的同一类模型。 例如，训练多层感知机模型时，我们可能希望比较具有不同数量的隐藏层、不同数量的隐藏单元以及不同的激活函数组合的模型。为了确定候选模型中的最佳模型，我们通常会使用验证集。 验证数据集：一个用来评估模型好坏的数据集，训练数据集用来训练模型参数，验证数据集用来选择模型超参数。 例如在数据集中拿出50%的训练数据作为验证数据集。 不要跟训练数据混在一起（常犯错误）。 测试数据集：只用一次的数据集。 例如未来的考试。 例如我出价的房子的实际成交价。 接下来我们需要了解一下模型容量的概念： 模型容量表示模型拟合各种函数的能力。 低容量的模型难以拟合复杂的训练数据。 高容量的模型可以记住所有的训练数据。 是否过拟合或欠拟合主要取决于模型复杂性和可用训练数据集的大小。当我们比较训练和验证误差时，我们要注意两种常见的情况： 训练误差和验证误差都很差，但它们之间仅有一点差距。如果模型不能降低训练误差，这可能意味着模型过于简单（即表达能力不足），无法捕获试图学习的模式。此外，由于我们的训练和验证误差之间的泛化误差很小，我们有理由相信可以用一个更复杂的模型降低训练误差。这种现象被称为欠拟合（underfitting）。 当我们的训练误差明显低于验证误差时要小心，这表明严重的过拟合（overfitting）。注意，过拟合并不总是一件坏事。特别是在深度学习领域，众所周知，最好的预测模型在训练数据上的表现往往比在保留（验证）数据上好得多。最终，我们通常更关心验证误差，而不是训练误差和验证误差之间的差距。 4. 权重衰减 在训练参数化机器学习模型时，权重衰减（weight decay）是最广泛使用的正则化的技术之一，它通常也被称为L2正则化。 通过限制参数值的选择范围来控制模型容量，通常不限制偏移 b。 首先生成一些数据： 定义网络模型与损失函数： 定义优化算法，我们在实例化优化器时直接通过 weight_decay 指定 weight decay 超参数。默认情况下，PyTorch 同时衰减权重和偏移。这里我们只为权重设置了 weight_decay，所以偏置参数不会衰减： 然后进行训练： 通过结果可以看到模型很快就过拟合了，可以通过修改超参数 wd 的值应用权重衰减的方式来缓解过拟合的现象。 5. 暂退法（Dropout） 一个好的模型需要对输入数据的扰动鲁棒。在训练过程中，建议在计算后续层之前向网络的每一层注入噪声，因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性。 这个想法被称为暂退法（dropout）。暂退法在前向传播过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。这种方法之所以被称为暂退法，因为我们从表面上看是在训练过程中丢弃（drop out）一些神经元。在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。 在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。换言之，每个中间活性值 h 以暂退概率 p 由随机变量替换。 Dropout 说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率 p 停止工作（将一些输出项随机置0），这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征。 我们可以将暂退法应用于每个隐藏层的输出（在激活函数之后），并且可以为每一层分别设置暂退概率：常见的技巧是在靠近输入层的地方设置较低的暂退概率。下面的模型将第一个和第二个隐藏层的暂退概率分别设置为0.2和0.5，并且暂退法只在训练期间有效。 Dropout 的从零实现核心代码如下： Dropout 的简洁实现及完整训练代码如下： 6. 数值稳定性和模型初始化 我们可能面临一些问题，要么是梯度爆炸（gradient exploding）问题：参数更新过大，破坏了模型的稳定收敛；要么是梯度消失（gradient vanishing）问题：参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。 例如当 Sigmoid 函数的输入很大或是很小时，它的梯度都会消失。此外，当反向传播通过许多层时，除非我们在刚刚好的地方，这些地方 Sigmoid 函数的输入接近于零，否则整个乘积的梯度可能会消失。 Xavier 初始化：从均值为零，方差为 2 / (n_in + n_out) 的高斯分布中采样权重（n_in 为输入的数量，n_out 为输出的数量）。 下一章：PyTorch 神经网络基础。"},{"title":"D2L学习笔记-线性神经网络","date":"2023-02-02T11:25:00.000Z","url":"/posts/19931.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 李沐动手学深度学习（PyTorch）课程学习笔记第二章：线性神经网络。 1. 线性回归的从零实现 为了简单起见，我们将根据带有噪声的线性模型构造一个人造数据集。我们的任务是使用这个有限样本的数据集来恢复这个模型的参数。 首先我们生成数据集： 训练模型时要对数据集进行遍历，每次抽取一小批量样本，并使用它们来更新我们的模型。由于这个过程是训练机器学习算法的基础，所以有必要定义一个函数，该函数能打乱数据集中的样本并以小批量方式获取数据。 在下面的代码中，我们定义一个 data_iter 函数，该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为 batch_size 的小批量。每个小批量包含一组特征和标签： 在我们开始用小批量随机梯度下降优化我们的模型参数之前，我们需要先有一些参数。在下面的代码中，我们通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重，并将偏置初始化为0： 定义线性回归模型： 定义损失函数： 定义优化算法： 现在我们已经准备好了模型训练所有需要的要素，可以实现主要的训练过程部分了。理解这段代码至关重要，因为从事深度学习后，相同的训练过程几乎一遍又一遍地出现。在每次迭代中，我们读取一小批量训练样本，并通过我们的模型来获得一组预测。计算完损失后，我们开始反向传播，存储每个参数的梯度。最后，我们调用优化算法（随机梯度下降法SGD）来更新模型参数： 2. 线性回归的简洁实现 首先生成数据集： 读取数据集： 接下来我们定义模型，在 PyTorch 中，全连接层在 Linear 类中定义。值得注意的是，我们将两个参数传递到 nn.Linear 中。第一个指定输入特征形状，即2，第二个指定输出特征形状，输出特征形状为单个标量，因此为1： 定义损失函数，计算均方误差使用的是 MSELoss 类，也称平方 L2 范数，默认情况下，它返回所有样本损失的平均值： 定义优化算法： 训练过程代码与我们从零开始实现时所做的非常相似： 3. Softmax回归的从零实现 首先读入 Fashion-MNIST 数据集，原始数据集中的每个样本都是28*28的图像。本节将展平每个图像，把它们看作长度为784的向量。在后面的章节中，我们将讨论能够利用图像空间结构的特征，但现在我们暂时只把每个像素位置看作一个特征。 初始化模型参数，在 Softmax 回归中，我们的输出与类别一样多。因为我们的数据集有10个类别，所以网络输出维度为10。因此，权重将构成一个784*10的矩阵，偏置将构成一个1*10的行向量： 定义 Softmax 操作，注意，虽然这在数学上看起来是正确的，但我们在代码实现中有点草率。矩阵中的非常大或非常小的元素可能造成数值上溢或下溢，但我们没有采取措施来防止这点： 定义 Softmax 操作后，我们可以实现 Softmax 回归模型。下面的代码定义了输入如何通过网络映射到输出。注意，将数据传递到模型之前，我们使用 reshape 函数将每张原始图像展平为向量： 接下来我们定义损失函数，交叉熵采用真实标签的预测概率的负对数似然。这里我们不使用 Python 的 for 循环迭代预测（这往往是低效的），而是通过一个运算符选择所有元素。下面我们创建一个数据样本 y_hat，其中包含2个样本在3个类别的预测概率，以及它们对应的标签 y。有了 y，我们知道在第一个样本中，第一类是正确的预测；而在第二个样本中，第三类是正确的预测。然后使用 y 作为 y_hat 中概率的索引，我们选择第一个样本中第一个类的概率和第二个样本中第三个类的概率： 为了计算精度，我们执行以下操作。首先，如果 y_hat 是矩阵，那么假定第二个维度存储每个类的预测分数。我们使用 argmax 获得每行中最大元素的索引来获得预测类别。然后我们将预测类别与真实 y 元素进行比较。由于等式运算符 == 对数据类型很敏感，因此我们将 y_hat 的数据类型转换为与 y 的数据类型一致。结果是一个包含0（错）和1（对）的张量。最后，我们求和会得到正确预测的数量： 同样，对于任意数据迭代器 data_iter 可访问的数据集，我们可以评估在任意模型 net 的精度，这里定义一个实用程序类 Accumulator，用于对多个变量进行累加。在 evaluate_accuracy 函数中，我们在 Accumulator 实例中创建了2个变量，分别用于存储正确预测的数量和预测的总数量。当我们遍历数据集时，两者都将随着时间的推移而累加： 接下来可以开始进行训练： 可以在项目路径下打开 Anaconda 的 PyTorch 环境，然后使用 TensorBoard 查看训练曲线： 4. Softmax回归的简洁实现 首先读取数据集： 定义模型，我们只需在 Sequential 中添加一个带有10个输出的全连接层，这10个输出分别表示对10个类别的预测概率： 定义训练函数，由于之后很多模型的训练过程也是相似的，因此该函数可以复用： 最后设定超参数训练模型： 下一章：多层感知机。"},{"title":"D2L学习笔记-预备知识","date":"2023-01-18T08:37:00.000Z","url":"/posts/15604.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 李沐动手学深度学习（PyTorch）课程学习笔记第一章：预备知识。 1. 环境安装 首先安装好 PyTorch 环境：Anaconda 与 PyTorch 安装教程。 安装需要的包： 2. 数据操作与数据预处理 张量表示一个数值组成的数组，这个数组可能有多个维度： 我们可以通过张量的 shape 属性来访问张量的形状和张量中元素的总数： 要改变一个张量的形状而不改变元素数量和元素值，我们可以调用 reshape 函数： 使用全0、全1、其他常量或者从特定分布中随机采样的数字： 通过提供包含数值的 Python 列表（或嵌套列表）来为所需张量中的每个元素赋予确定值： 常见的标准算术运算符（+、-、*、/ 和 **）都可以被升级为按元素运算： 我们也可以把多个张量拼接在一起： 通过逻辑运算符构建二元张量： 对张量中的所有元素进行求和会产生一个只有一个元素的张量，或者指定在某一维度上求和： 即使形状不同，我们仍然可以通过调用广播机制（broadcasting mechanism）来执行按元素操作： 与 NumPy 张量相互转化： 将大小为1的张量转换为 Python 标量： 创建一个人工数据集，并存储在 CSV（逗号分隔值）文件中： 从创建的 CSV 文件中加载原始数据集： 为了处理缺失的数据，典型的方法包括插值和删除，这里，我们将考虑插值： 对于 inputs 中的类别值或离散值，我们可以将 NaN 视为一个类别： 现在 inputs 和 outputs 中的所有条目都是数值类型，它们可以转换为张量格式： 3. 线性代数 标量由只有一个元素的张量表示，你可以将向量视为标量值组成的列表： 访问张量的长度和形状： 矩阵和矩阵的转置： 给定具有相同形状的任何两个张量，任何按元素二元运算的结果都将是相同形状的张量： 求和与求平均值： 点积是相同位置的按元素乘积的和，torch.dot 只能对一维向量做点积。注意 NumPy 中的 np.dot 函数计算的是两个矩阵的矩阵乘法，而非对应元素相乘求和： 矩阵向量积： 矩阵乘法，torch.mm 与 np.dot 类似： L2 范数是向量所有元素的平方和的平方根： L1 范数为向量所有元素的绝对值之和： F 范数（弗罗贝尼乌斯范数）是矩阵所有元素的平方和的平方根： 4. 自动微分 先举一个简单的例子： 当 y 不是标量时，向量 y 关于向量 x 的导数的最自然解释是一个矩阵。对于高阶和高维的 y 和 x，求导的结果可以是一个高阶张量。 然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括深度学习中），但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。这里，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和： 有时，我们希望将某些计算移动到记录的计算图之外。例如，假设 y 是作为 x 的函数计算的，而 z 则是作为 y 和 x 的函数计算的。想象一下，我们想计算 z 关于 x 的梯度，但由于某种原因，希望将 y 视为一个常数，并且只考虑到 x 在 y 被计算后发挥的作用。 这里可以分离 y 来返回一个新变量 u，该变量与 y 具有相同的值，但丢弃计算图中如何计算 y 的任何信息。换句话说，梯度不会向后流经 u 到 x。因此，下面的反向传播函数计算 z = u * x 关于 x 的偏导数，同时将 u 作为常数处理，而不是计算 z = x * x * x 关于 x 的偏导数。 使用自动微分的一个好处是：即使构建函数的计算图需要通过 Python 控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到变量的梯度。在下面的代码中，while 循环的迭代次数和 if 语句的结果都取决于输入 a 的值。对于任何 a，存在某个常量标量 k，使得 d = f(a) = k * a，其中 k 的值取决于输入 a，因此可以用 d / a 验证梯度是否正确。 下一章：线性神经网络。"},{"title":"实用机器学习课程笔记（Stanford）","date":"2022-12-22T02:28:00.000Z","url":"/posts/33687.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" Stanford 2021 秋季实用机器学习课程学习笔记。 1. 概论 1.1 课程介绍 Challenges Formulate problem：关注最具影响力的行业问题（如自助超市、自动驾驶汽车等）。 Data：高质量数据的稀缺、隐私问题。 Train models：模型越来越复杂，需要大量数据，成本也越来越高。 Deploy models：计算量大，不适合实时推理。 Monitor：数据分布的变化、公平性问题。 Roles（不同类型的人在 ML 中的作用） Domain experts（领域专家）：具有商业洞察力，知道什么数据是重要的以及在哪可以找到它，确定一个 ML 模型真正的影响。 Data scientists：在 Data mining、Model training and deployment 方面做全栈的工作。 ML experts：定制最先进（state of the art，SOTA）的 ML models。 SDE（软件开发工程师）：开发/维护数据管道、模型训练和服务管道。 Corse topics 本课程的内容为数据科学家需要的技术，但是没有大学传统的 ML/统计/编程方面的教学。 Data 收集、处理数据。 部署的时候场景发生变化导致数据不一样，如 covariate（协变量）、concepts、label 的改变。 独立同分布之外的数据。 Train 模型验证、融合、调优。 迁移学习（Transfer learning）。 多模态（Multi-modality）：如何把不同的数据源融合起来做一个比较大的模型。 Deploy 模型怎样部署。 蒸馏（Distillation）：将比较大的模型提取出精华做的小一点。 Monitor 公平性，之后会讲模型的公平是什么含义。 可解释性，怎样理解模型在干什么。 1.2 数据获取 Flow Chart for data acquisition Discover what data is available 找已经有的数据集。 找基准数据集来检验我们的想法： 例如要做一个新的调超参数的算法，可能需要找一些数据集，且数据集不能太大，要小一点的或者中等大小的，为了客观地检验算法还要考虑找不同方面的数据集。 如果训练比较深的神经网络就需要非常大的数据集。 收集新数据（做一个应用或产品很多时候没有现成的数据集） Source of popular ML datasets MNIST：手写数字。 ImageNet：百万级别的来自搜索引擎的图片，可训练较为深度的模型。 AudioSet：YouTube 上的一些声音的切片，用于做声音的分类。 Kinetics：YouTube 上的一些视频的切片，用于人的行为分类。 KITTI：摄像头或激光雷达等各种 sensor 记录下的交通场景。 Amazon Review：Amazon 产品的用户评论。 SQuAD：维基中的一些 Q-A 对。 Librispeech：1000小时的有声读物。 数据集两大来源：各个网站上爬取，采集数据。 Where to find datasets Paperwithcodes Datasets：学术数据集与排行榜。 Kaggle Datasets：数据科学家提交的 ML datasets。 Google Dataset search：搜索网页上的数据集。 Various toolkits datasets（开源工具包）：TensorFlow、HuggingFace。 会议/公司的 ML 竞赛。 自己组织的 Data lakes（数据湖）。 Datasets comparison 数据集 好处 坏处 学术数据集 数据干净、难度适中 选择面较小、太精简、规模小 竞赛数据集 更接近真实的 ML 应用 仍较精简，且只专注在较热门的应用 原生数据 很灵活 需要很多精力去预处理 Data integration 产品数据通常存放在不同的表中，因此要涉及到表的连接。 Table 1 ID Val 1 Val 2 1 1_val1 1_val2 2 2_val1 2_val2 Table 2 ID Val 3 Val 4 1 1_val3 1_val4 3 3_val3 3_val4 Inner Join T1 & T2 ID Val 1 Val 2 Val 3 Val 4 1 1_val1 1_val2 1_val3 1_val4 Left Join T1 & T2 ID Val 1 Val 2 Val 3 Val 4 1 1_val1 1_val2 1_val3 1_val4 2 2_val1 2_val2 / / 1.3 网页数据抓取 一般不能用 curl，因为网站所有者能使用各种方法阻止。 使用无头浏览器（一个没有 GUI 的网络浏览器）： 你需要大量的新 IP，可以通过云获得很多 IP。 1.4 数据标注 半监督学习Semi-Supervised Learning（SSL） 重点关注有少量标记数据和大量未标记数据的场景。 对没有标注的数据和有标注的数据的数据分布做一些假设： 连续性假设（Continuity assumption）：如果一个样本的特征和另外一个样本相似，那么这两个样本很可能具有相同的标号。 聚类假设（Cluster assumption）：数据具有内在的聚类结构，那么假设一个类里面具有相同的标号。 流形假设（Manifold assumption）：很有可能数据在本质上是在低维的一个流形上分布的。 自学习Self-training 主动学习Active Learning 关注的场景与 SSL 相同，但有人工干预，即选择最有趣的数据（最重要的没有标号的数据）给标注工标注。 不确定性抽样（Uncertainty sampling）：选择一个最不确信的预测让人来判断。 Active Learning + Self-training 弱监督学习Weak Supervision 半自动地生成标号，通常比手动标注的准确率差，但是也是好到可以训练一个还不错的模型。 数据编程（Data programming）：用启发式的方法赋予标号： 关键字搜索、模式匹配、第三方模型。 假设判断一个 YouTube 的评论是垃圾（spam）还是有用的东西（ham）： 2. 机器学习模型 2.1 机器学习模型概览 ML 算法的种类 监督学习（Supervised）：训练有标签的数据来预测标签。 自监督学习（Self-supervised）：标签的生成来自于数据本身。 半监督学习（Semi-supervised）：在有标签和无标签的数据上进行训练，使用模型来预测无标签数据的标签。 无监督学习（Unsupervised）：在未标记的数据上进行训练。 强化学习（Reinforcement）：利用观察与环境互动的结果来采取行动以最大化收益。 本课程最多讨论的内容为监督学习。 监督学习的组成部分 模型（Model）：将输入映射到标签的参数化函数。 损失（Loss）：衡量模型在预测结果方面有多好，即衡量模型预测出来的值和真实值之间的差距，需要指导模型尽量向真实值靠近。 目标函数（Objective）：优化模型参数的目标，例如需要优化模型在训练集合上的所有预测结果的损失之和最小。 优化（Optimization）：解决 Objective 的算法，即把模型中没有指定的参数（可学习的参数）优化为合适的值，使得能够解决目标函数，也就是最小化损失。 监督学习的模型 决策树（Decision trees）：用树来做决定。 线性模型（Linear methods）：决策是根据输入特征的线性组合做出的。 核方法（Kernel machines）：使用核函数衡量两个样本的特征相似度，达到非线性的效果。 神经网络（Neural Networks）：使用神经网络学习特征表示。 2.2 决策树 优点： 可以用来解释，即训练后的模型可以看到叶子结点是什么内容，决策是怎么一步步做下来的。 能够处理数值和类别的特征。 缺点： 非常不稳定，可能数据内产生了一点噪音后整棵树构建出来的样子就不一样了。 如果数据特别复杂，会生成一个特别复杂的树，可以把整个数据里面的各种情况列出来，生成大量的节点，最后会导致过拟合。 不容易并行计算。 随机森林 训练多个决策树以提高稳定性。 树是并行地独立训练的。 对于分类问题可以用多数投票法（例如超过一半的树觉得类别是1，那么它就是1），对于回归问题可以在多棵树上取平均。 为什么叫随机呢？ Bagging：随机抽取训练样本并进行替换。例如样本本来是 [1, 2, 3, 4, 5]，做 Bagging 的时候在里面随机采样5个出来，但是采样可能是有重复的，采样到的结果为 [1, 2, 2, 3, 4]，然后拿到这个 Bagging 出来的数据集后我们就在上面训练一棵树，然后一直重复训练 N 棵树为止。 随机选择一个特征子集，即把 Bagging 出的数据拿出来之后，再从里面的特征中随机采样一些特征列出来（假设树是一个表，那么就是先随机采样出一些行，再随机采样出一些列） 2.3 线性模型 线性回归 一个简单的房价预测模型： 假设有3个特征：卧室数量 x1、浴室数量 x2、居住面积 x3； 预测价格为：y_hat = w1 * x1 + w2 * x2 + w3 * x3 + b； 权重 w1, w2, w3 和偏置 b 将从训练数据中学习。 一般来说，给定数据 x = [x1, x2, ..., xp]，线性回归的预测为：y_hat = w1 * x1 + w2 * x2 + ... + wp * xp + b = &lt;w, x&gt; + b（其中 w 和 x 为长度为 p 的向量，&lt;&gt; 表示内积运算，w 和 b 都是可学习参数）。 线性回归目标函数 假设我们收集了 n 个训练样本 X = [x1, x2, ..., xn]，其中每个 xi 均为长为 p 的向量，将其转置后即为一个 n 行 p 列的矩阵，其对应的标号为 y = [y1, ..., yn]，是一个长为 n 的向量。 目标函数是最小化均方误差（MSE），即优化 w, b 的值使得 sum((yi - &lt;xi, w&gt; - b)**2) / n 最小。 线性回归在分类问题中的应用 回归的输出是一个连续的实数，而对于分类问题，我们要输出对某个样本的类别的预测。 多类别分类： 假设标签为独热编码，即 y = [y1, y2, ..., ym]，如果该样本为第 i 类则 yi = 1，否则 yi = 0。 预测结果 y_hat = [o1, o2, ..., om]，其中 oi 表示预测该样本为第 i 类的概率。 为每个类学习一个线性模型：oi = &lt;x, wi&gt; + bi。 最小化 MSE 损失函数：(y_hat - y)**2 / m。 预测结果所表示的类为 m 个概率中最大的那个，即 argmax(y_hat)。 Mini-batch 随机梯度下降 2.4 神经网络 神经网络就是将手工特征提取的部分换成了一个神经网络。 神经网络通常需要更多的数据和更多的计算，一般都是大数个数量级。 可以选择不同的神经网络架构来更有效地抽取我们的特征： 多层感知机。 卷积神经网络。 循环神经网络。 Transformers。 设计神经网络以结合数据的先验知识。 线性模型到多层感知机（Multilayer Perceptron，MLP） 引入一种全连接层（稠密层，dense），假设输入样本数量为 n，每个样本的特征长度为 m，那么全连接层具有两个可学习参数 w, b，其中 w 是一个 n 行 m 列的实数矩阵，b 是一个长为 n 的向量。则全连接层的计算结果为：y = np.dot(w, x) + b。 线性回归可以认为是一个只有一个输出的全连接层。 Softmax 回归可以认为是一个有 C 个输出的全连接层，C 表示类别的数量。 多层感知机的目的是实现一个非线性的模型，但是如果只是简单使用多个全连接层是没用的，多个线性操作的叠加还是一个线性操作，因此还需要加入非线性函数（激活函数）。 激活函数是一个基于元素的非线性函数： sigmoid(x) = 1 / (1 + np.exp(-x))。 relu(x) = max(x, 0)。 非线性的激活函数能让我们得到非线性模型。 可以堆叠多个隐藏层（例如多个 dense 层和 activation 层堆叠），得到更深层次的模型。 超参数：隐藏层数量 hidden layers，每个隐藏层的输出大小 outputs of each hidden layer（最后一层的输出无法改变）。 代码实现： 2.5 卷积神经网络 全连接层到卷积神经网络 以一个图像识别任务为例，使用 MLP 模型学习 ImageNet（每张图像大小为300*300像素，有1000个类别），我们假设其中一个隐藏层具有10000个输出： 它会产生10亿个可学习参数，这太大了！ 因为全连接的输出是所有输入元素的加权和，而且每个输出的权重是不一样的。 识别图像中的物体： 平移不变性：无论对象在哪里，输出都是相似的。 局部性：像素与其周围像素的相关性比较高，因为图像中的物体都是连续性的。 将先验知识构建到模型结构中： 用更少的参数（#params）实现相同的模型容量。 卷积层（Convolution layer） 局部性：从 k * k 大小的输入窗口计算输出，即做局部的计算。 平移不变性：输出使用相同的 k * k 权重（核）。 卷积层的模型参数不依赖于输入/输出的大小。 一个卷积核可以被学习成去识别一个图像里面的模式，比如识别绿色通道中的某个块状物体，识别某个方向上的纹理 代码： 池化层（Pooling layer） 卷积层对输入的位置很敏感，即输入中模式的转换/旋转会导致输出中模式类似地变化，因此我们需要一定的对未知移动的鲁棒性。 池化层在大小为 k * k 的窗口中计算平均值/最大值/最小值。 代码： 卷积神经网络（Convolutional Neural Networks，CNN） 卷积神经网络的原理为叠加卷积层来提取特征。 激活函数应用于每个卷积层之后。 使用池化操作来降低位置敏感性。 现代 CNN 是具有各种超参数和层连接的深度神经网络（AlexNet, VGG, inception, ResNet, MobileNet）。 2.6 循环神经网络 全连接层到循环神经网络 语言模型：给出一个句子前面的一些词，预测下一个词是什么。例如：hello -&gt; world、hello world -&gt; !。 单纯使用 MLP 不能很好地处理序列信息，例如长度的变化和时序的变化。 循环神经网络的原理为将上一个全连接层输出的状态复制一份作为隐藏状态 H，与下一个输入状态进行拼接后再进行预测。即：h_t = RNN(W_hh * h' + W_hx * x_t + b_h)，其中 h' 为隐藏状态，x_t 为当前输入。 代码： 3. 模型评估 3.1 评估指标 损失（Loss）衡量模型在预测监督学习结果的方面有多好。 评估模型性能的其他指标： 模型相关的指标：例如分类的精度，物体检测的 mAP。 商业相关的指标：例如收益，推理延迟（如模型能在100毫秒之内返回结果）。 我们一般通过考虑多种指标来选择模型。 二分类的评估指标 Accuracy：正确的预测数量/样本总数 Precision：预测结果为类 i 且实际结果也为类 i 的数量/预测结果为类 i 的数量 Recall：预测结果为类 i 且实际结果也为类 i 的数量/实际结果为类 i 的数量 F1：平衡 Precision 和 Recall 的指标，为 Precision 和 Recall 的调和平均值：2pr / (p + r) 二分类中的 AUC 和 ROC AUC 为 ROC 曲线下的面积，大小范围为 [0.5, 1]。 衡量模型分离这两个类的能力。 选择决策阈值 x，如果输出 y_hat &gt;= x 则预测为正类，否则为负类。 展示广告的商业指标 最优化收入和客户体验。 Latency：广告应该与其他内容同时显示给用户。 ASN：平均每页显示的广告数量。 CTR：用户实际点击率。 ACP：广告商每次点击支付的平均价格。 收益 = 页面浏览量 * ASN * CTR * ACP。 "},{"title":"MySQL面试题总结","date":"2022-12-04T09:13:00.000Z","url":"/posts/48028.html","tags":[["Interview","/tags/Interview/"]],"categories":[["Interview","/categories/Interview/"]],"content":" MySQL 常见面试题总结，文章将不断更新。 1. SQL 基础 1.1 数据库的三大范式是什么？ 第一范式（1NF）：强调的是列的原子性，即数据库表的每一列都是不可分割的原子数据项。 第二范式（2NF）：要求实体的属性完全依赖于主关键字。所谓完全依赖是指不能存在仅依赖主关键字一部分的属性，即在 1NF 基础上消除非主属性对主码的部分函数依赖。第二范式需要确保数据库表中的每一列都和主键相关，而不能只与主键的某一部分相关（主要针对联合主键而言）。 第三范式（3NF）：任何非主属性不依赖于其它非主属性（在 2NF 基础上消除传递依赖）。第三范式需要确保数据表中的每一列数据都和主键直接相关，而不能间接相关。 1.2 NOSQL 和 SQL 的区别？ SQL 指关系型数据库，主要代表：SQL Server、Oracle、MySQL（开源）、PostgreSQL（开源）。关系型数据库存储结构化数据，这些数据逻辑上以行列二维表的形式存在，每一列代表数据的一种属性，每一行代表一个数据实体。 NoSQL 指非关系型数据库，主要代表：MongoDB、Redis。NoSQL 数据库逻辑上提供了不同于二维表的存储方式，存储方式可以是 JSON 文档、哈希表或者其他方式。 选择 SQL 还是 NoSQL，考虑以下因素： ACID vs. BASE：关系型数据库支持 ACID 即原子性、一致性、隔离性和持续性，NoSQL 采用更宽松的模型 BASE，即基本可用，软状态和最终一致性。我们需要考虑对于面对的应用场景，ACID 是否是必须的，比如银行应用就必须保证 ACID，否则一笔钱可能被使用两次；又比如社交软件不必保证 ACID，因为一条状态的更新对于所有用户读取先后时间有数秒不同并不影响使用。 扩展性对比：NoSQL 数据之间无关系，这样就非常容易扩展，也无形之间在架构的层面上带来了可扩展的能力。比如 Redis 自带主从复制模式、哨兵模式、切片集群模式。相反关系型数据库的数据之间存在关联性，水平扩展较难，需要解决跨服务器 JOIN，分布式事务等问题。 1.3 MySQL 支持哪些存储引擎？ MySQL 支持多种存储引擎，比如 InnoDB、MyISAM、Memory、Archive 等等。在大多数的情况下，直接选择使用 InnoDB 引擎都是最合适的，InnoDB 也是 MySQL 的默认存储引擎。 MyISAM 和 InnoDB 的区别有哪些： InnoDB 支持事务，MyISAM 不支持。 InnoDB 支持外键，MyISAM 不支持。 InnoDB 是聚集索引，数据文件是和索引绑在一起的，必须要有主键，通过主键索引效率很高；MyISAM 是非聚集索引，数据文件是分离的，索引保存的是数据文件的指针，主键索引和辅助索引是独立的。 InnoDB 不支持全文索引，MyISAM 支持全文索引，查询效率上 MyISAM 更高。 InnoDB 不保存表的具体行数，MyISAM 用一个变量保存了整个表的行数。 MyISAM 采用表级锁（table-level locking）；InnoDB 支持行级锁（row-level locking）和表级锁，默认为行级锁。 1.4 超键、候选键、主键、外键分别是什么？ 超键：在关系中能唯一标识元组的属性集称为关系模式的超键。一个属性可以为作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。 候选键：是最小超键，即没有冗余元素的超键。 主键：数据库表中对储存数据对象予以唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（NULL）。 外键：在一个表中存在的另一个表的主键称此表的外键。 1.5 SQL 约束有哪几种？ NOT NULL：用于控制字段的内容一定不能为空（NULL）。 UNIQUE：控制字段内容不能重复，一个表允许有多个 UNIQUE 约束。 PRIMARY KEY：也是用于控制字段内容不能重复，但它在一个表只允许出现一个。 FOREIGN KEY：用于预防破坏表之间连接的动作，也能防止非法数据插入外键列，因为它必须是它指向的那个表中的值之一。 CHECK：用于控制字段的值范围。 1.6 MySQL 怎么连表查询？ 数据库有四种联表查询类型：内连接（INNER JOIN）、左外连接（LEFT JOIN）、右外连接（RIGHT JOIN）、全外连接（FULL JOIN）。 内连接：返回两个表中连接条件完全匹配的所有行（即两个表的交集）。 左外连接：返回左表的所有行，即使它们在右表中没有匹配的行，对于左表中存在而右表中没有匹配的行，右表的结果列将填充为 NULL。 右外连接：返回右表的所有行，即使它们在左表中没有匹配的行，对于右表中存在而左表中没有匹配的行，左表的结果列将填充为 NULL，RIGHT JOIN 本质上是 LEFT JOIN 的反向操作。 全外连接：返回左表和右表的所有行，当某一行在另一个表中没有匹配时，另一个表的结果列将填充为 NULL，它结合了 LEFT JOIN 和 RIGHT JOIN 的结果。 连表查询写法如下： 需要注意的是 MySQL 本身并不直接支持 FULL JOIN 语法，需要使用 UNION 组合 LEFT JOIN 和 RIGHT JOIN 的结果： 1.7 MySQL 如何避免重复插入数据？ （1）使用 UNIQUE 约束 在表的相关列上添加 UNIQUE 约束，确保每个值在该列中唯一。例如： 如果尝试插入重复的 email，MySQL 会返回错误。 （2）使用 INSERT ... ON DUPLICATE KEY UPDATE 这种语句允许在插入记录时处理重复键的情况，如果插入的记录与现有记录冲突，可以选择更新现有记录： （3）使用 INSERT IGNORE 该语句会在插入记录时忽略那些因重复键而导致的插入错误。例如： 如果 email 已经存在，这条插入语句将被忽略而不会返回错误。 选择哪种方法取决于具体的需求： 如果需要保证全局唯一性，使用 UNIQUE 约束是最佳做法。 如果需要插入和更新结合可以使用 ON DUPLICATE KEY UPDATE。 对于快速忽略重复插入，INSERT IGNORE 是合适的选择。 1.8 MySQL 中的 VARCHAR 和 CHAR 有什么区别？ CHAR 是一个定长字段，假如申请了 CHAR(10) 的空间，那么无论实际存储多少内容，该字段都占用 10 个字符；而 VARCHAR 是变长的，也就是说申请的只是最大长度，占用的空间为实际字符长度再加一，最后一个字符存储使用了多长的空间。 在检索效率上来讲，CHAR 优于 VARCHAR，因此在使用中，如果确定某个字段的值的长度，可以使用 CHAR，否则应该尽量使用 VARCHAR 节约存储空间，例如存储用户 MD5 加密后的密码，则可以使用 CHAR。 1.9 VARCHAR 后面的数字代表字节还是字符？ VARCHAR 后面括号里的数字代表的是字符数，而不是字节数。比如 VARCHAR(10)，这里的 10 表示该字段最多可以存储 10 个字符，字符的字节长度取决于所使用的字符集： 如果字符集是 ASCII 字符集：ASCII 字符集每个字符占用 1 个字节，那么 VARCHAR(10) 最多可以存储 10 个 ASCII 字符，同时占用的存储空间最多为 10 个字节（不考虑额外的长度记录开销）。 如果字符集是 UTF-8 字符集，它的每个字符可能占用 1 到 4 个字节，对于 VARCHAR(10) 的字段，它最多可以存储 10 个字符，但占用的字节数会根据字符的不同而变化。 1.10 INT(1) 和 INT(10) 在 MySQL 有什么不同？ INT(1) 和 INT(10) 的区别主要在于显示宽度，而不是存储范围或数据类型本身的大小： 本质是显示宽度，不改变存储方式：INT 的存储固定为 4 字节，所有 INT（无论写成 INT(1) 还是 INT(10)）占用的存储空间均为 4 字节。括号内的数值是显示宽度，用于在特定场景下控制数值的展示格式。 唯一作用场景：ZEROFILL 补零显示，当字段设置 ZEROFILL 时，数字显示时会用前导零填充至指定宽度。比如，字段类型为 INT(4) ZEROFILL，实际存入 5 则显示为 0005，实际存入 12345 显示仍为 12345（宽度超限时不截断）。 1.11 MySQL 中 IN 和 EXISTS 区别？ MySQL 中的 IN 语句是把外表和内表作 Hash 连接，而 EXISTS 语句是对外表作 Loop 循环，每次 Loop 循环再对内表进行查询。一直大家都认为 EXISTS 比 IN 语句的效率要高，这种说法其实是不准确的。这个是要区分环境的： 如果查询的两个表大小相当，那么用 IN 和 EXISTS 差别不大。 如果两个表中一个较小，一个是大表，则子查询表大的用 EXISTS，子查询表小的用 IN。 NOT IN 和 NOT EXISTS：如果查询语句使用了 NOT IN，那么内外表都进行全表扫描，没有用到索引；而 NOT EXISTS 的子查询依然能用到表上的索引。所以无论哪个表大，用 NOT EXISTS 都比 NOT IN 要快。 1.12 DROP、DELETE 与 TRUNCATE 的区别？ 三者都表示删除，但是三者有一些差别： DELETE TRUNCATE DROP 类型 属于 DML 属于 DDL 属于 DDL 回滚 可回滚 不可回滚 不可回滚 删除内容 表结构还在，删除表的全部或者一部分数据 表结构还在，删除表中的所有数据 从数据库中删除表，所有的数据行、索引和权限也会被删除 删除速度 删除速度慢，需要逐行删除 删除速度快 删除速度最快 1.13 什么是存储过程？有哪些优缺点？ 存储过程是一些预编译的 SQL 语句。 更加直白的理解：存储过程可以说是一个记录集，它是由一些 T-SQL 语句组成的代码块，这些 T-SQL 语句代码像一个方法一样实现一些功能（对单表或多表的增删改查），然后再给这个代码块取一个名字，在用到这个功能的时候调用它就行了。 存储过程是一个预编译的代码块，执行效率比较高，一个存储过程替代大量 T-SQL 语句，可以降低网络通信量，提高通信速率，可以一定程度上确保数据安全。 但是，在互联网项目中，其实是不太推荐存储过程的，比较出名的就是阿里的《Java 开发手册》中禁止使用存储过程，我个人的理解是，在互联网项目中，迭代太快，项目的生命周期也比较短，人员流动相比于传统的项目也更加频繁，在这样的情况下，存储过程的管理确实是没有那么方便，同时，复用性也没有写在服务层那么好。 1.14 MySQL 执行查询的过程？ 客户端通过 TCP 连接发送连接请求到 MySQL 连接器，连接器会对该请求进行权限验证及连接资源分配。 查缓存（当判断缓存是否命中时，MySQL 不会进行解析查询语句，而是直接使用 SQL 语句和客户端发送过来的其他原始信息。所以，任何字符上的不同，例如空格、注解等都会导致缓存的不命中）。 语法分析（SQL 语法是否写错了）：如何把语句给到预处理器，检查数据表和数据列是否存在，解析别名看是否存在歧义。 优化：是否使用索引，生成执行计划。 交给执行器，将数据保存到结果集中，同时会逐步将数据缓存到查询缓存中，最终将结果集返回给客户端。 更新语句执行会复杂一点，需要检查表是否有排它锁，写 binlog、刷盘、是否执行 commit。 2. 事务 2.1 什么是数据库事务？ 事务是一个不可分割的数据库操作序列，也是数据库并发控制的基本单位，其执行的结果必须使数据库从一种一致性状态变到另一种一致性状态。事务是逻辑上的一组操作，要么都执行，要么都不执行。 事务最经典也经常被拿出来说例子就是转账了。 假如小明要给小红转账1000元，这个转账会涉及到两个关键操作就是：将小明的余额减少1000元，将小红的余额增加1000元。万一在这两个操作之间突然出现错误比如银行系统崩溃，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。 2.2 事务具有的四个特征？ 事务就是一组原子性的操作，这些操作要么全部发生，要么全部不发生。事务把数据库从一种一致性状态转换成另一种一致性状态。 原子性（Atomicity）：事务是数据库的逻辑工作单位，事务中包含的各操作要么都做，要么都不做。 一致性（Consistency）：事务执行的结果必须是使数据库从一个一致性状态变到另一个一致性状态。因此当数据库只包含成功事务提交的结果时，就说数据库处于一致性状态。如果数据库系统运行中发生故障，有些事务尚未完成就被迫中断，这些未完成事务对数据库所做的修改有一部分已写入物理数据库，这时数据库就处于一种不正确的状态，或者说是不一致的状态。 隔离性（Isolation）：一个事务的执行不能被其它事务干扰。即一个事务内部的操作及使用的数据对其它并发事务是隔离的，并发执行的各个事务之间不能互相干扰。 持续性（Durability）：也称永久性，指一个事务一旦提交，它对数据库中的数据的改变就应该是永久性的。接下来的其它操作或故障不应该对其执行结果有任何影响。 2.3 MySQL 的四种隔离级别？ Read Uncommitted（读取未提交内容） 在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为脏读（Dirty Read）。 Read Committed（读取提交内容） 这是大多数数据库系统的默认隔离级别（但不是 MySQL 默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交的事务所做的改变。这种隔离级别也支持所谓的不可重复读（Nonrepeatable Read），因为同一事务的其他实例在该实例处理其间可能会有新的 commit，所以同一 select 可能返回不同结果。 Repeatable Read（可重读） 这是 MySQL 的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，这会导致另一个棘手的问题：幻读（Phantom Read）。 Serializable（可串行化） 通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。 隔离级别 脏读 不可重复读 幻影读 Read Uncommitted 有 有 有 Read Committed 无 有 有 Repeatable Read 无 无 有 Serializable 无 无 无 MySQL 默认采用的是 REPEATABLE-READ 隔离级别，Oracle 默认采用的是 READ-COMMITTED 隔离级别。 事务隔离机制的实现基于锁机制和并发调度。其中并发调度使用的是 MVVC（多版本并发控制），通过保存修改的旧版本信息来支持并发一致性读和回滚等特性。 因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是 READ-COMMITTED（读取提交内容），但是你要知道的是 InnoDB 存储引擎默认使用 REPEATABLE-READ（可重读）并不会有任何性能损失。 InnoDB 存储引擎在分布式事务的情况下一般会用到 SERIALIZABLE（可串行化）隔离级别。 2.4 什么是脏读、不可重复读与幻读？ 脏读：事务 A 读取了事务 B 更新的数据，然后 B 回滚操作，那么 A 读取到的数据是脏数据。 不可重复读：事务 A 多次读取同一数据，事务 B 在事务 A 多次读取的过程中，对数据作了更新并提交，导致事务 A 多次读取同一数据时，结果不一致。 幻读：系统管理员 A 将数据库中所有学生的成绩从具体分数改为 ABCDE 等级，但是系统管理员 B 就在这个时候插入了一条具体分数的记录，当系统管理员 A 改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。 总结：不可重复读侧重于修改，幻读侧重于新增或删除（多了或少了行），脏读是一个事务回滚影响另外一个事务。 2.5 事务的实现原理？ 事务是基于重做日志文件（redo log）和回滚日志（undo log）实现的。 每提交一个事务必须先将该事务的所有日志写入到重做日志文件进行持久化，数据库就可以通过重做日志来保证事务的原子性和持久性。 每当有修改事务时，还会产生 undo log，如果需要回滚，则根据 undo log 的反向语句进行逻辑操作，比如 insert 一条记录就 delete 一条记录。undo log 主要实现数据库的一致性。 2.6 介绍一下 MySQL 事务日志？ InnoDB 事务日志包括 redo log 和 undo log。 undo log 指事务开始之前，在操作任何数据之前，首先将需操作的数据备份到一个地方。redo log 指事务中操作的任何数据，将最新的数据备份到一个地方。 事务日志的目的：实例或者介质失败，事务日志文件就能派上用场。 redu log redo log 不是随着事务的提交才写入的，而是在事务的执行过程中，便开始写入 redo log 中。具体的落盘策略可以进行配置。防止在发生故障的时间点，尚有脏页未写入磁盘，在重启 MySQL 服务的时候，根据 redo log 进行重做，从而达到事务的未入磁盘数据进行持久化这一特性。redo log 是为了实现事务的持久性而出现的产物。 undo log undo log 用来回滚行记录到某个版本。事务未提交之前，undo log 保存了未提交之前的版本数据，undo log 中的数据可作为数据旧版本快照供其他并发事务进行快照读。是为了实现事务的原子性而出现的产物，在 MySQL InnoDB 存储引擎中用来实现多版本并发控制。 2.7 什么是 MySQL 的 binlog？ MySQL 的 binlog 是记录所有数据库表结构变更（例如 CREATE、ALTER TABLE）以及表数据修改（例如 INSERT、UPDATE、DELETE）的二进制日志。binlog 不会记录 SELECT 和 SHOW 这类操作，因为这类操作对数据本身并没有修改，但你可以通过查询通用日志来查看 MySQL 执行过的所有语句。 MySQL binlog 以事件形式记录，还包含语句执行所消耗的时间，MySQL 的二进制日志是事务安全型的。binlog 的主要目的是复制和恢复。 binlog 有三种格式，各有优缺点： statement：基于 SQL 语句的模式，某些语句和函数如 UUID、LOAD DATA INFILE 等在复制过程中可能导致数据不一致甚至出错。 row：基于行的模式，记录的是行的变化，很安全。但是 binlog 会比其他两种模式大很多，在一些大表中清除大量数据时在 binlog 中会生成很多条语句，可能导致从库延迟变大。 mixed：混合模式，根据语句来选用是 statement 还是 row 模式。 2.8 在事务中可以混合使用存储引擎吗？ 尽量不要在同一个事务中使用多种存储引擎，MySQL 服务器层不管理事务，事务是由下层的存储引擎实现的。 如果在事务中混合使用了事务型和非事务型的表（例如 InnoDB 和 MyISAM 表），在正常提交的情况下不会有什么问题。 但如果该事务需要回滚，非事务型的表上的变更就无法撤销，这会导致数据库处于不一致的状态，这种情况很难修复，事务的最终结果将无法确定。所以，为每张表选择合适的存储引擎非常重要。 2.9 什么是 MVCC？ MVCC，即多版本并发控制。MVCC 的实现，是通过保存数据在某个时间点的快照来实现的。根据事务开始的时间不同，每个事务对同一张表，同一时刻看到的数据可能是不一样的。 2.10 MVCC 的实现 对于 InnoDB，聚簇索引记录中包含 3 个隐藏的列： ROW ID：隐藏的自增 ID，如果表没有主键，InnoDB 会自动按 ROW ID 产生一个聚集索引树。 事务 ID：记录最后一次修改该记录的事务 ID。 回滚指针：指向这条记录的上一个版本。 我们举个例子，假如现在有两个事务： 事务1：insert into t1(a, b) values (1, 1); 事务2：update t1 set b = 666 where a = 1; 如图，首先 insert 语句向表 t1 中插入了一条数据，a 字段为1，b 字段为1，ROW ID 也为1，事务 ID 假设为1，回滚指针假设为 null。当执行 update t1 set b = 666 where a = 1 时，大致步骤如下： 数据库会先对满足 a = 1 的行加排他锁； 然后将原记录复制到 undo 表空间中； 修改 b 字段的值为666，修改事务 ID 为2； 并通过隐藏的回滚指针指向 undo log 中的历史记录； 事务提交，释放前面对满足 a = 1 的行所加的排他锁。 因此可以总结出 MVCC 实现的原理大致是： InnoDB 每一行数据都有一个隐藏的回滚指针，用于指向该行修改前的最后一个历史版本，这个历史版本存放在 undo log 中。如果要执行更新操作，会将原记录放入 undo log 中，并通过隐藏的回滚指针指向 undo log 中的原记录。其它事务此时需要查询时，就是查询 undo log 中这行数据的最后一个历史版本。 MVCC 最大的好处是读不加锁，读写不冲突，极大地增加了 MySQL 的并发性。通过 MVCC，保证了事务 ACID 中的隔离性。 3. 锁 3.1 为什么要加锁？ 当多个用户并发地存取数据时，在数据库中就会产生多个事务同时存取同一数据的情况。若对并发操作不加控制就可能读取和存储不正确的数据，破坏数据库的一致性。因此需要加锁使得在多用户环境下保证数据库的完整性和一致性。 3.2 按照锁的粒度分数据库锁有哪些？ 在关系型数据库中，可以按照锁的粒度把数据库锁分为行级锁（InnoDB 引擎）、表级锁(MyISAM 引擎）和页级锁（BDB引擎）。 行级锁 行级锁是 MySQL 中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁和排他锁。 开销大，加锁慢，会出现死锁，锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 表级锁 表级锁是 MySQL 中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分 MySQL 引擎支持。最常使用的 MyISAM 与 InnoDB 都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。 开销小，加锁快，不会出现死锁，锁定粒度大，发生锁冲突的概率最高，并发度最低。 页级锁 页级锁是 MySQL 中锁定粒度介于行级锁和表级锁之间的一种锁。表级锁速度快，但冲突多，行级锁冲突少，但速度慢。所以取了折衷的页级锁，一次锁定相邻的一组记录。BDB 支持页级锁。 开销和加锁时间界于表锁和行锁之间，会出现死锁，锁定粒度界于表锁和行锁之间，并发度一般。 MyISAM 和 InnoDB 存储引擎使用的锁： MyISAM 采用表级锁（table-level locking）。 InnoDB 支持行级锁（row-level locking）和表级锁，默认为行级锁。 3.3 从锁的类别上分 MySQL 都有哪些锁呢？ 从锁的类别上来讲，有共享锁和排他锁。 共享锁：又叫做读锁，当用户要进行数据的读取时，对数据加上共享锁。共享锁可以同时加上多个。 排他锁：又叫做写锁，当用户要进行数据的写入时，对数据加上排他锁。排他锁只可以加一个，它和其它的排他锁，共享锁都相斥。 用上面的例子来说就是用户的行为有两种，一种是来看房，多个用户一起看房是可以接受的。一种是真正的入住一晚，在这期间，无论是想入住的还是想看房的都不可以。 锁的粒度取决于具体的存储引擎，InnoDB 实现了行级锁，页级锁，表级锁。他们的加锁开销从大到小，并发能力也是从大到小。 3.4 数据库的乐观锁和悲观锁是什么？怎么实现的？ 数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。乐观并发控制（乐观锁）和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。 悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。在查询完数据的时候就把事务锁起来，直到提交事务。实现方式：使用数据库中的锁机制。 乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。在修改数据的时候把事务锁起来，通过 version 的方式来进行锁定。实现方式：乐观锁一般会使用版本号机制或 CAS 算法实现。 两种锁的使用场景： 从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。 但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行 retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。 3.5 InnoDB 引擎的行锁是怎么实现的？ InnoDB 是基于索引来完成行锁的。 例如：select * from tab_with_index where id = 1 for update; for update 可以根据条件来完成行锁锁定，并且 id 是有索引键的列，如果 id 不是索引键那么 InnoDB 将完成表锁，并发将无从谈起。 3.6 什么是死锁？怎么解决？ 死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方的资源，从而导致恶性循环的现象。常见的解决死锁的方法有： 如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。 在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率。 对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率。 如果业务处理不好可以用分布式事务锁或者使用乐观锁。 3.7 隔离级别与锁的关系？ 在 Read Uncommitted 级别下，读取数据不需要加共享锁，这样就不会跟被修改的数据上的排他锁冲突。 在 Read Committed 级别下，读操作需要加共享锁，但是在语句执行完以后释放共享锁。 在 Repeatable Read 级别下，读操作需要加共享锁，但是在事务提交之前并不释放共享锁，也就是必须等待事务执行完毕以后才释放共享锁。 SERIALIZABLE 是限制性最强的隔离级别，因为该级别锁定整个范围的键，并一直持有锁，直到事务完成。 3.8 优化锁方面的意见？ 使用较低的隔离级别。 设计索引，尽量使用索引去访问数据，加锁更加精确，从而减少锁冲突。 选择合理的事务大小，给记录显示加锁时，最好一次性请求足够级别的锁。例如，修改数据的话最好申请排他锁，而不是先申请共享锁，修改时再申请排他锁，这样会导致死锁。 不同的程序访问一组表的时候，应尽量约定一个相同的顺序访问各表，对于一个表而言，尽可能固定顺序地获取表中的行，这样将大大减少死锁的机会。 尽量使用相等条件访问数据，这样可以避免间隙锁对并发插入的影响。 不要申请超过实际需要的锁级别。 数据查询的时候不是必要，不要使用加锁。MySQL 的 MVCC 可以实现事务中的查询不用加锁，优化事务性能：MVCC 只在 Read Committed（读提交）和 Repeatable Read（可重复读）两种隔离级别。 对于特定的事务，可以使用表锁来提高处理速度或者减少死锁的可能。 4. 索引 4.1 索引是什么？ 索引是一种特殊的文件（InnoDB 数据表上的索引是表空间的一个组成部分），它们包含着对数据表里所有记录的引用指针。 索引是一种数据结构。数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据。索引的实现通常使用 B 树及其变种 B+ 树。更通俗地说，索引就相当于目录。为了方便查找书中的内容，通过对内容建立索引形成目录。而且索引是一个文件，它是要占据物理空间的。 MySQL 索引的建立对于 MySQL 的高效运行是很重要的，索引可以大大提高 MySQL 的检索速度。比如我们在查字典的时候，前面都有检索的拼音和偏旁、笔画等，然后找到对应字典页码，打开字典的页数就可以知道我们要搜索的某一个 key 的全部值的信息了。 4.2 索引有哪些优缺点？ 索引的优点： 可以大大加快数据的检索速度，这也是创建索引的最主要的原因。 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。 索引的缺点： 时间方面：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，会降低增删改的执行效率。 空间方面：索引需要占用物理空间。 4.3 MySQL 有哪几种索引类型？ 从存储结构上来划分：BTree 索引（B-Tree 或 B+Tree 索引）、Hash 索引、full-index 全文索引、R-Tree 索引。这里所描述的是索引存储时保存的形式。 从应用层次来分：普通索引、唯一索引、复合索引。 普通索引：即一个索引只包含单个列，一个表可以有多个单列索引。 唯一索引：索引列的值必须唯一，但允许有空值。 复合索引：多列值组成一个索引，专门用于组合搜索，其效率大于索引合并。 聚簇索引（聚集索引）：并不是一种单独的索引类型，而是一种数据存储方式。具体细节取决于不同的实现，InnoDB 的聚簇索引其实就是在同一个结构中保存了 B-Tree 索引（技术上来说是 B+Tree）和数据行。 非聚簇索引：不是聚簇索引，就是非聚簇索引。 根据表中数据的物理顺序与键值的逻辑（索引）顺序关系：聚集索引，非聚集索引。 4.4 说一说索引的底层实现？ Hash 索引：基于哈希表实现，只有精确匹配索引所有列的查询才有效，对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码（hash code），并且 Hash 索引将所有的哈希码存储在索引中，同时在索引表中保存指向每个数据行的指针。 B-Tree 索引（MySQL 使用 B+Tree）：B-Tree 能加快数据的访问速度，因为存储引擎不再需要进行全表扫描来获取数据，数据分布在各个节点之中。 B+Tree 索引：B-Tree 的改进版本，同时也是数据库索引所采用的存储结构。数据都在叶子节点上，并且增加了顺序访问指针，每个叶子节点都指向相邻的叶子节点的地址。相比 B-Tree 来说，进行范围查找时只需要查找两个节点，进行遍历即可。而 B-Tree 需要获取所有节点，相比之下 B+Tree 效率更高。B+Tree 性质如下： n 棵子树的节点包含 n 个关键字，不用来保存数据而是保存数据的索引。 所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身根据关键字的大小自小而大顺序链接。 所有的非终端结点可以看成是索引部分，结点中仅含其子树中的最大（或最小）关键字。 B+ 树中，数据对象的插入和删除仅在叶节点上进行。 B+ 树有 2 个头指针，一个是树的根节点，一个是最小关键码的叶节点。 4.5 为什么索引结构默认使用 B+Tree，而不是 B-Tree，Hash，二叉树，红黑树？ B-tree：从两个方面来回答： B+ 树的磁盘读写代价更低：B+ 树的内部节点并没有指向关键字具体信息的指针，因此其内部节点相对 B 树更小，如果把所有同一内部节点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多，一次性读入内存的需要查找的关键字也就越多，相对 IO 读写次数就降低了。 由于 B+ 树的数据都存储在叶子结点中，分支结点均为索引，方便扫库，只需要扫一遍叶子结点即可，但是 B 树因为其分支结点同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序来扫，所以 B+ 树更加适合区间查询的情况，所以通常 B+ 树用于数据库索引。 Hash： 虽然可以快速定位，但是没有顺序，IO 复杂度高； 基于 Hash 表实现，只有 Memory 存储引擎显式支持哈希索引； 适合等值查询，如 =、in()、&lt;=&gt;，不支持范围查询； 因为不是按照索引值顺序存储的，就不能像 B+Tree 索引一样利用索引完成排序； Hash 索引在查询等值时非常快； 因为 Hash 索引始终索引所有列的全部内容，所以不支持部分索引列的匹配查找； 如果有大量重复键值的情况下，哈希索引的效率会很低，因为存在哈希碰撞问题。 二叉树：树的高度不均匀，不能自平衡，查找效率跟数据有关（树的高度），并且 IO 代价高。 红黑树：树的高度随着数据量增加而增加，IO 代价高。 4.6 讲一讲聚簇索引与非聚簇索引？ 在 InnoDB 里，索引 B+Tree 的叶子节点存储了整行数据为主键索引，也被称之为聚簇索引，即将数据存储与索引放到了一块，找到索引也就找到了数据。 而索引 B+Tree 的叶子节点存储了主键的值为非主键索引，也被称之为非聚簇索引、二级索引。 聚簇索引与非聚簇索引的区别： 非聚簇索引与聚簇索引的区别在于非聚簇索引的叶子节点不存储表中的数据，而是存储该列对应的主键（行号）。 对于 InnoDB 来说，想要查找数据我们还需要根据主键再去聚簇索引中进行查找，这个再根据聚簇索引查找数据的过程，我们称为回表。第一次索引一般是顺序 IO，回表的操作属于随机 IO。需要回表的次数越多，即随机 IO 次数越多，我们就越倾向于使用全表扫描。 通常情况下，主键索引（聚簇索引）查询只会查一次，而非主键索引（非聚簇索引）需要回表查询多次。当然，如果是覆盖索引的话，查一次即可。 注意：MyISAM 无论主键索引还是二级索引都是非聚簇索引，而 InnoDB 的主键索引是聚簇索引，二级索引是非聚簇索引。我们自己建立的索引基本都是非聚簇索引。 4.7 非聚簇索引一定会回表查询吗？ 不一定，这涉及到查询语句所要求的字段是否全部命中了索引，如果全部命中了索引，那么就不必再进行回表查询。一个索引包含（覆盖）所有需要查询字段的值，被称之为“覆盖索引”。举个简单的例子，假设我们在员工表的年龄上建立了索引，那么当进行 select score from stuaent where score &gt; 90 的查询时，在索引的叶子节点上，已经包含了 score 信息，不会再次进行回表查询。 4.8 联合索引是什么？为什么需要注意联合索引中的顺序？ MySQL 可以使用多个字段同时建立一个索引，叫做联合索引。在联合索引中，如果想要命中索引，需要按照建立索引时的字段顺序挨个使用，否则无法命中索引。具体原因为： MySQL 使用索引时需要索引有序，假设现在建立了 name, age, school 的联合索引，那么索引的排序为：先按照 name 排序，如果 name 相同，则按照 age 排序，如果 age 的值也相等，则按照 school 进行排序。 当进行查询时，此时索引仅仅按照 name 严格有序，因此必须首先使用 name 字段进行等值查询，之后对于匹配到的列而言，其按照 age 字段严格有序，此时可以使用 age 字段用做索引查找，以此类推。因此在建立联合索引的时候应该注意索引列的顺序，一般情况下，将查询需求频繁或者字段选择性高的列放在前面。此外可以根据特例的查询或者表结构进行单独的调整。 4.9 MySQL 的最左前缀原则是什么？ 最左前缀原则就是最左优先，在创建多列索引时，要根据业务需求，where 子句中使用最频繁的一列放在最左边。MySQL 会一直向右匹配直到遇到范围查询（&gt;、&lt;、between、like）就停止匹配，比如：对于 a = 1 and b = 2 and c &gt; 3 and d = 4，如果建立 (a, b, c) 顺序的索引，d 是用不到索引的，如果建立 (a, b, d, c) 的索引则都可以用到，a, b, d 的顺序可以任意调整。= 和 in 可以乱序，比如 a = 1 and b = 2 and c = 3 建立 (a, b, c) 索引可以任意顺序，MySQL 的查询优化器会帮你优化成索引可以识别的形式。 4.10 前缀索引是什么？ 因为可能我们索引的字段非常长，这既占内存空间，也不利于维护。所以我们就想，如果只把很长字段的前面的公共部分作为一个索引，就会产生超级加倍的效果。但是，我们需要注意，order by 不支持前缀索引。 创建前缀索引的流程如下： 先计算完整列的选择性：select count(distinct col_1)/count(1) from table_1； 再计算不同前缀长度的选择性：select count(distinct left(col_1, 4))/count(1) from table_1； 找到最优长度之后，创建前缀索引：create index idx_front on table_1 (col_1(4))。 4.11 如何创建索引？ 创建索引有以下三种方式： （1）在执行 CREATE TABLE 时创建索引： （2）使用 ALTER TABLE 命令添加索引： ALTER TABLE 用来创建普通索引、UNIQUE 索引或 PRIMARY KEY 索引。 其中 table_name 是要增加索引的表名，column_list 指出对哪些列进行索引，如果索引多列则各列之间用逗号分隔。 索引名 index_name 可自己命名，缺省时，MySQL 将根据第一个索引列赋一个名称。另外，ALTER TABLE 允许在单个语句中更改多个表，因此可以同时创建多个索引。 （3）使用 CREATE INDEX 命令创建索引： 4.12 创建索引时需要注意什么？ 非空字段：应该指定列为 NOT NULL，除非你想存储 NULL。在 MySQL 中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。你应该用 0 或者一个特殊的值或者一个空串代替空值； 取值离散（变量各个取值之间的差异程度）大的字段的列放到联合索引的前面，可以通过 count() 函数查看字段的差异值，返回值越大说明字段的唯一值越多，字段的离散程度高； 索引字段越小越好：数据库的数据存储以页为单位，一页存储的数据越多则一次 I/O 操作获取的数据越多，效率越高。 "},{"title":"英语日常学习记录","date":"2022-12-02T02:58:00.000Z","url":"/posts/4115.html","tags":[["Others","/tags/Others/"]],"categories":[["Others","/categories/Others/"]],"content":" 记录日常积累的一些英语口语句子，日积月累不断进步！ 1. 短句 我不知道 I don’t know.（中性，根据语气判断凶不凶） Sorry, I have no idea. / I haven’t got a clue.（很地道） I’m afraid I don’t know.（更礼貌） How should/would I know? / Do I look like a walking encyclopedia?（不客气） 你会说英语吗 Can you …（你会吗 / 你介不介意 / 你是不是被允许） Can you cook?（你会做饭吗 / 可以你做饭吗 / 你能做饭吗） Can you do sth.? 可能太直接、不礼貌，比如 Can you be quiet? Do you do sth.? 像是问你平时做不做某事，如果平时做，那肯定说明是会的 Do you speak English? / Can you speak English? 都表示你会说英语吗，都可以用 我在外面 Outside 指的是在房间、建筑或某个地方的外面或附近，不能指出门办事、出门玩 I’m coming, I’m just outside.（我到门口了） I’ll wait for you outside.（我在门口附近等你） Let’s eat outside.（在室外吃饭） Eat out.（下馆子） She is out with her friends.（她和朋友出去玩） They are out working.（他们外出办事） 喝咖啡 喝一般得说 have 或 want Let’s have coffee. I want a coke. Do you want a drink? 如果说 grad a drink/food 一般是比较随意的、快的、看情况的这种感觉，意思就是如果你有时间，我们可以一起喝杯咖啡，如果你没时间那也没关系 Let’s grab some coffee. Holding my coffee.（拿着咖啡） 很喜欢 英式英语中不能说 Quite like，quite 一般表示有点儿，例如我有点累：I’m quite tired. 所以在英式英语中顺序是 Don’t like、Quite like、Like、Really like 而在美式英语中顺序是 Don’t like、Like、Quite like、Really like I love shopping.（我很喜欢购物） I’m a huge fan of British food.（我非常喜欢英国料理） I’m obsessed with American accents.（我非常喜欢美式口音） 我知道了 I see. / I get it. / I got it. / I understand. 表示以前不知道，现在才知道 Got it? / Get it? / Got it. 可以不加 ‘I’，但是 I got it. 一定要加 I I know. / Of course. / Yep.（嗯哼）/ That goes without saying. 表示已经知道了，不需要你告诉我 No shit, Sherlock. 阴阳怪气表达，这么明显还用你说？ 美女/帅哥 Beautiful girl. / Pretty girl. 指真正漂亮的女生 Excuse me. / Sorry. / Hi. + (miss, mate, bro) 表示称呼陌生人（如服务员）美女或者帅哥 Sorry mate, just to check…（不好意思帅哥，想问下…） 上车/下车 In the car. 在车上 Get in the car. 上车 Get out (of) the car/taxi. 下车/下出租（从坐着的姿势直接下车） Get off the bus/train/plane. 下公交/动车/飞机（先站起来再下车） Get off my car. 表示别碰我的车 Get down from the car. 表示别站在车顶/引擎盖上 纠结/很值 Which should I buy? I’m in two minds. 我应该买哪个？我很纠结 They’re both worth the money. 他们都很值（worthy 表示值得/配得上的人或事物，不能用于价格值不值） 近视度数 What’s your prescription? / How strong are your glasses? 你眼睛多少度？ This eye is minus two, and this eye is minus two point five. 这只眼睛200度，这只250度 I’m short-sighted not long-sighted. 我是近视眼，不是远视眼 I’m wearing contact lenses. 我戴隐形眼镜 吃什么容易长胖 This steak is very fatty. 这个牛排太肥了 It’s very fattening. 太容易长胖了 Fruit is good for me. 吃水果对身体好 称呼老师 日常一般不会在老师面前直接称呼 Teacher，只会在介绍自己的职业或是间接称呼别人会用 小学、初高中比较严格，一般说：Mr/Mrs/Miss/Ms + 姓氏，也可以单独说 Sir/Miss 在大学，教书的老师可以称呼为 Professor/Doctor + 姓氏 手机没电借充电宝 My phone is dead. 我的手机没电了 Can I use your power bank? 我能用一下你的充电宝吗？ Can I also borrow that charging cable? 我能再借一下那个充电线吗？ 2. 情景对话 Day1 Hey bro!（Hello 较正式，bro 更不正式，因此 Hello bro 很奇怪） I said hello to you, but you didn’t say hello back to me. Did your parents not teach you manners?（没有家教，该句特别 savage，即太过直接、犀利、甚至无理） What shall we eat?（别说 We eat what?） Anything is OK. / Anything will do. / Whatever you want. / I don’t mind. / You choose. / I’m easy.（表示都行，不能说 Both are OK，因为没有给选择） Hot pot.（火锅，别说成 Fire pot 了） Drink some beer? / Hot pot and beer? Definitely. / Great idea. / That sounds great to me.（必须的 / 好主意 / 我看行） Day2 Why are you late?（你怎么来的这么晚） Oh, my taxi was late. / There was a lot of traffic, so I was late.（出租车来晚了 / 堵车了） It’s dark now.（天黑了） What shall we eat? What do you wanna eat? / It’s your call. / You decide.（你想吃什么 / 你决定吧）"},{"title":"PyTorch深度学习入门（CIFAR10分类）","date":"2022-12-01T10:22:00.000Z","url":"/posts/48394.html","tags":[["AI","/tags/AI/"]],"categories":[["AI","/categories/AI/"]],"content":" 通过 CIFAR10 数据集的分类问题初入门 Deep Learning，也是开坑 AI 系列的第一篇文章。 相关环境的搭建可以转至：Anaconda 与 PyTorch 安装教程。 1. 常用函数 （1）路径函数 在 os 模块中常用的路径相关函数有： os.listdir(path)：将 path 目录下的内容列成一个 list。 os.path.join(path1, path2)：拼接路径：path1\\path2。 例如： （2）辅助函数 dir()：不带参数时，返回当前范围内的变量、方法和定义的类型列表；带参数时，返回参数的属性、方法列表。 help(func)：查看函数 func 的使用说明。 例如： 2. 数据加载 2.1 Dataset 数据读取和预处理是进行机器学习的首要操作，PyTorch 提供了很多方法来完成数据的读取和预处理。 其中 Dataset 表示数据集，torch.utils.data.Dataset 是代表这一数据的抽象类。你可以自己定义你的数据类，继承和重写这个抽象类，非常简单，只需要定义 __len__ 和 __getitem__ 这个两个函数即可，例如： 通过上面的方式，可以定义我们需要的数据类，可以通过迭代的方式来获取每一个数据，但这样很难实现取 batch、shuffle 或者是多线程去读取数据。 2.2 DataLoader torch.utils.data.DataLoader 构建可迭代的数据装载器，我们在训练的时候，每一个 for 循环，每一次 iteration，就是从 DataLoader 中获取一个 batch_size 大小的数据的。打个比方如果 Dataset 是一副完整的扑克牌，那么 DataLoader 就是抽取几张组成的一部分扑克牌。 DataLoader 的参数很多，但我们常用的主要有以下几个： dataset：Dataset 类，决定从哪个数据集读取数据。 batch_size：批大小。 num_works：是否多进程读取机制。 shuffle：每个 Epoch 是否乱序。 drop_last：当样本数不能被 batch_size 整除时，是否舍弃最后一批数据。 要理解这个 drop_last，首先，得先理解 Epoch、Iteration 和 Batch_size 的概念： Epoch：所有训练样本都已输入到模型中，称为一个 Epoch。 Iteration：一批样本输入到模型中，称为一个 Iteration。 Batch_size：一批样本的大小，决定一个 Epoch 有多少个 Iteration。 DataLoader 的作用就是构建一个数据装载器，根据我们提供的 batch_size 的大小，将数据样本分成一个个的 Batch 去训练模型，而这个分的过程中需要把数据取到，这个就是借助 Dataset 的 __getitem__ 方法。 例如： 接下来使用 CIFAR10 数据集再展示一次 DataLoader 的用法： PS：部分看不懂的代码可以先去学后面的 transform 以及 tensorboard。 3. TensorBoard 3.1 add_scalar TensorBoard 原本是 TensorFlow 的可视化工具，PyTorch 从1.2.0开始支持 TensorBoard。之前的版本也可以使用 TensorBoardX 代替。 先进入 Anaconda 的 PyTorch 环境，安装 TensorBoard： 在项目根目录下新建一个文件夹 logs，TensorBoard 的工作流程简单来说是将代码运行过程中的，某些你关心的数据保存在这个文件夹中（由代码中的 writer 完成），再读取这个文件夹中的数据，用浏览器显示出来（在命令行运行 TensorBoard 完成）。 我们先绘制一个 y = x 的图像，运行以下代码： add_scalar 函数主要有三个参数： tag：数据标识符，可以理解为数据图像的标题。 scalar_value：保存的值，即纵轴上的值。 global_step：记录的步长，即横轴的值，一般会设置一个不断增加的 step。 运行后会看到 logs 文件夹下生成了一个文件，然后我们在 PyCharm 终端的 PyTorch 环境中打开 TensorBoard（要在当前项目中进入 PyTorch 环境，否则 --logdir 的路径就不能用相对路径了）： 打开  即可看到绘制的图像。 如果因为某些原因导致端口冲突可以指定端口： 3.2 add_image add_image 函数主要有三个参数： tag：同 add_scalar。 img_tensor：图像数据，类型必须是 torch.Tensor、numpy.ndarry 或 string/blobname。 global_step：同 add_scalar。 可以看到传入的图片数据有类型限制，目前还没学到 torch.Tensor 类型，以 numpy.ndarry 为例，因此我们需要先安装一下 NumPy，还是在 PyTorch 环境中安装： 使用 PIL 打开一个图像，将其转换成 NumPy 数组： 可以看到图片的形状是三维的数据，前两个数据分别表示高度和宽度，第三个数据表示通道数，可以记为 (H, W, C)，简写为 HWC。 add_image 函数传入图片时格式默认为 CHW，如果格式不匹配需要设定函数中的 dataformats 参数，例如： 运行后打开 TensorBoard 即可在 IMAGES 页面下看到图片。 4. Transform 4.1 Transform的概念与基本用法 transforms 在计算机视觉工具包 torchvision 下，包含了很多种对图像数据进行变换的类，这些都是在我们进行图像数据读入步骤中必不可少的，通过图像变换可以将图片变成不同的类型，或者可以通过旋转、裁切等手段对图像数据集的图像进行变换，起到扩充数据集与数据增强的作用。 transforms 主要使用的类为：transforms.ToTensor，该类能够将 PIL.Image 或者 ndarray 类型的数据转换为 tensor，并且归一化至 [0, 1]。注意归一化至 [0, 1] 是直接除以255，若自己的 ndarray 数据尺度有变化，则需要自行修改。 为什么需要 tensor 数据类型？因为它包装了反向传播神经网络所需要的一些基础的参数，因此在神经网络中需要将图片类型转换为 tensor 类型进行训练。 例如： 4.2 Transform的常用类 transforms.Compose：Compose 能够将多种变换组合在一起。例如下面的代码可以先将 PIL.Image 中心裁切，然后再转换成 tensor： transforms.CenterCrop：需要传入参数 size，表示以 (size, size) 的大小从中心裁剪，参数也可以为 (height, width)。例如： transforms.RandomCrop：需要传入参数 size，表示以 (size, size) 的大小随机裁剪，参数也可以为 (height, width)。 transforms.Normalize(mean, std)：对数据按通道进行标准化，即先减均值 mean，再除以标准差 std，注意是 HWC 格式，处理公式为：output[channel] = (input[channel] - mean[channel]) / std[channel]，例如： transforms.Resize：需要传入参数 (height, width) 和 interpolation，表示重置图像的分辨率为 (h, w)，也可以传入一个整数 size，这样会将较短的那条边缩放至 size，另一条边按原图大小等比例缩放。interpolation 为插值方法选择，默认为 PIL.Image.BILINEAR，例如： transforms.ToPILImage：：将 tensor 或者 ndarray 的数据转换为 PIL.Image 类型数据，参数 mode 默认为 None，表示1通道， mode=3 表示3通道，默认转换为 RGB，4通道默认转换为 RGBA。 5. Torchvision数据集使用方法 Torchvision 官方文档 Torchvision 中的 torchvision.datasets 就是 Torchvision 提供的标准数据集，其中有很多已经构建和训练好的网络模型，在不同的领域下各自有着很优秀的性能。 我们以 CIFAR10 为例，该数据集包括了60000张32*32像素的图像，总共有10个类别，每个类别有6000张图像，其中有50000张图像为训练图像，10000张为测试图像。其使用说明如下图所示： root：数据集存放的路径。 train：如果为 True，创建的数据集就为训练集，否则创建的数据集就为测试集。 transform：使用 transforms 中的变换操作对数据集进行变换。 target_transform：对 target 进行 transform。 download：如果为 True，就会自动从网上下载这个数据集，否则就不会下载。 例如： 刚开始运行时可以看到正在从网上下载数据集，如果下载速度非常慢可以复制链接去迅雷之类的地方下载，下载好后自己创建设定的路径，将数据集放过来即可。 然后设置断点，用 Debug 模式运行一下代码，我们可以查看一下数据集的内容，数据集 train_data 中的 classes 表示图像的种类，classes_to_idx 表示将种类映射为整数，targets 表示每张图像对应的种类编号，试着输出一下第一张图的信息： 现在展示如何使用 transform 参数，假设我们需要将数据集的图像都转换成 tensor 类型： 6. 神经网络Torch.NN基本骨架的使用 torch.nn 能够帮助我们更优雅地训练神经网络，使神经网络代码更加简洁和灵活。官方文档：Torch.NN。 在文档中可以看到第一块内容叫做 Container（容器），这就相当于神经网络的骨架，Container 之后的东西就用于往骨架里面填充，如 Convolution Layers（卷积层）、Pooling Layers（池化层），有卷积神经网络基础的小伙伴对这些词应该都很熟悉了。 Container 中有六个模块：Module、Sequential、ModuleList、ModuleDict、ParameterList、ParameterDict，其中最常用的为 Module，这是所有神经网络的最基本的类，其基本的构造方式如下： 现在我们尝试自己创建一个简单的神经网络，并输出前向传播的结果： 我们以 Conv2d 函数为例，该函数的官方文档：TORCH.NN.FUNCTIONAL.CONV2D。 该函数有以下几个参数： input：输入的图像，size 为 (mini_batch, in_channels, height, width)。 weight：卷积核的大小，size 为 (out_channels, in_channels/groups, height, width)。 bias：偏置，默认为 None。 stride：步长，用来控制卷积核移动间隔，如果为 x 则水平和竖直方向的步长都为 x，如果为 (x, y) 则竖直方向步长为 x，水平方向步长为 y。 padding：在输入图像的边沿进行扩边操作，以保证图像输入输出前后的尺寸大小不变，在 PyTorch 的卷积层定义中，默认的 padding 为零填充，即在边缘填充0。 padding_mode：扩边的方式。 dilation：设定了取数之间的间隔。 例如： 7. Convolution Layers与Pooling Layers 由于图像是二维的，因此基本上最常用到的就是二维的卷积层和池化层：torch.nn.Conv2d、torch.nn.MaxPool2d，官方文档：torch.nn.Conv2d、Pooling Layers。 7.1 Convolution Layers 卷积运算能够提取输入图像的不同特征，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。 torch.nn.Conv2d 的主要参数有以下几个： in_channels：输入图像的通道数，彩色图像一般都是三通道。 out_channels：通过卷积后产生的输出图像的通道数。 kernel_size：可以是一个数或一个元组，表示卷积核的大小，卷积核的参数是从数据的分布中采样得到的，这些数是多少无所谓，因为在神经网络训练的过程中就是对这些参数进行不断地调整。 stride：步长。 padding：填充。 padding_mode：填充模式，有 zeros、reflect、replicate、circular，默认为 zeros。 dilation：可以是一个数或一个元组，表示卷积核各个元素间的距离，也称空洞卷积。 group：一般设置为1，基本用不到。 bias：偏置，一般设置为 True。 例如以下代码构建了一个只有一层卷积层的神经网络，该卷积层的输入和输出通道数都为三通道，卷积核大小为3*3，步长为1，无填充，然后用 CIFAR10 测试数据集进行测试： 运行后可以打开 TensorBoard 查看一下效果。 7.2 Pooling Layers Pooling Layers 中的 MaxPool 表示最大池化，也称上采样；MaxUnpool 表示最小池化，也称下采样；AvgPool 表示平均池化。其中最常用的为 MaxPool2d，官方文档：torch.nn.MaxPool2d。 最大池化的目的是保留输入数据的特征，同时减小特征的数据量。 torch.nn.MaxPool2d 的主要参数有以下几个： kernel_size：用来取最大值的窗口（池化核）大小，和之前的卷积核类似。 stride：步长，注意默认值为 kernel_size。 padding：填充，和 Conv2d 一样。 dilation：池化核中各个元素间的距离，和 Conv2d 一样。 return_indices：如果为 True，表示返回值中包含最大值位置的索引。注意这个最大值指的是在所有窗口中产生的最大值，如果窗口产生的最大值总共有5个，就会有5个返回值。 ceil_mode：如果为 True，表示在计算输出结果形状的时候，使用向上取整，否则默认向下取整。 输出图像的形状的计算公式可以在官方文档中查看。 接下来我们用代码实现这个池化层： 我们用图像来试试效果： 运行后可以打开 TensorBoard 查看一下效果。 8. Non-linear Activations与Linear Layers 8.1 Non-linear Activations 非线性激活的目的是为了在网络中引入一些非线性特征，因为非线性特征越多才能训练出符合各种曲线（特征）的模型。 非线性激活函数官方文档：Non-linear Activations。 有深度学习基础的同学应该知道最常用的非线性激活函数就是 ReLU 和 Sigmoid 函数，多分类问题会在输出层使用 Softmax 函数（如果损失函数使用的是交叉熵误差函数 CrossEntropyLoss 则会自动计算 Softmax，无需创建 Softmax 层）。这三个函数在 PyTorch 中分别为 nn.ReLU、nn.Sigmoid 和 nn.Softmax。 这两个函数的输入都是只需指明 batch_size 即可，在 PyTorch1.0 之后的版本任何形状的数据都能被计算，无需指定 batch_size。 nn.ReLU 只有一个需要设置的参数 inplace，如果为 True 表示计算结果直接替换到输入数据上，例如： 构建 ReLU 层代码如下： 由于 ReLU 对图像处理的直观效果不明显，我们使用 Sigmoid 对图像进行处理： 8.2 Linear Layers 线性层官方文档：Linear Layers。 PyTorch 的 nn.Linear 是用于设置网络中的全连接层的，需要注意的是全连接层的输入与输出都是二维张量，一般形状为：[batch_size, size]，不同于卷积层要求输入输出是四维张量，因此在将图像传入全连接层之前一般都会展开成一维的。 nn.Linear 有三个参数分别如下： in_features：指的是输入的二维张量的大小，即输入的 [batch_size, size] 中的 size。 out_features：指的是输出的二维张量的大小，即输出的二维张量的形状为 [batch_size, output_size]，当然，它也代表了该全连接层的神经元个数。从输入输出的张量的 shape 角度来理解，相当于一个输入为 [batch_size, in_features] 的张量变换成了 [batch_size, out_features] 的输出张量。 bias：偏置，相当于 y = ax + b 中的 b。 代码示例如下： 9. 神经网络模型搭建小实战 9.1 Sequential torch.nn.Sequential 是一个 Sequential 容器，能够在容器中嵌套各种实现神经网络中具体功能相关的类，来完成对神经网络模型的搭建。模块的加入一般有两种方式，一种是直接嵌套，另一种是以 OrderedDict 有序字典的方式进行传入，这两种方式的唯一区别是： 使用 OrderedDict 搭建的模型的每个模块都有我们自定义的名字。 直接嵌套默认使用从零开始的数字序列作为每个模块的名字。 （1）直接嵌套方法的代码如下： （2）使用 OrderedDict 的代码如下： 9.2 小实战 由于代码很简单，都是学过的内容进行组装，因此直接看代码： 使用 add_graph 函数可以在 TensorBoard 中生成神经网络的计算图，通过计算图可以很清晰地看到每一层计算时数据流入流出的结果，双击相应的标签可以进一步深入查看更详细的信息。 10. 损失函数与反向传播 10.1 Loss Functions 具有深度学习理论基础的同学对损失函数和反向传播一定不陌生，在此不详细展开理论介绍。损失函数是指用于计算标签值和预测值之间差异的函数，在机器学习过程中，有多种损失函数可供选择，典型的有距离向量，绝对值向量等。使用损失函数的流程概括如下： 计算实际输出和目标之间的差距。 为我们更新输出提供一定的依据（反向传播）。 损失函数的官方文档：Loss Functions。 （1）nn.L1Loss：平均绝对误差（MAE，Mean Absolute Error），计算方法很简单，取预测值和真实值的绝对误差的平均数即可。 PyTorch1.13中 nn.L1Loss 数据形状规定如下： Input：(*)，means any number of dimensions. Target：(*)，same shape as the input. Output：scalar. If reduction is none, then (*), same shape as the input. 早先的版本需要指定 batch_size 大小，现在不需要了。可以设置参数 reduction，默认为 mean，即取平均值，也可以设置为 sum，顾名思义就是取和。 测试代码如下： （2）nn.MSELoss：均方误差（MSE，Mean Squared Error），即预测值和真实值之差的平方和的平均数。 该损失函数的用法与 nn.L1Loss 相似，代码如下： （3）nn.CrossEntropyLoss：交叉熵误差，训练分类 C 个类别的模型的时候较常用这个损失函数，一般用在 Softmax 层后面，计算公式较为复杂，可以在官网中查看。 测试代码如下： 10.2 Backward 接下来以 CIFAR10 数据集为例，用上一节搭建的神经网络先设置 batch_size 为1，看一下输出结果： 现在我们来尝试解决第二个问题，即损失函数如何为我们更新输出提供一定的依据（反向传播）。 例如对于卷积层来说，其中卷积核中的每个参数就是我们需要调整的，每个参数具有一个属性 grad 表示梯度，反向传播时每一个要更新的参数都会求出对应的梯度，在优化的过程中就可以根据这个梯度对参数进行优化，最终达到降低损失函数值的目的。 PyTorch 中对损失函数计算出的结果使用 backward 函数即可计算出梯度： 我们在计算反向传播之前设置断点，然后可以在 PyCharm 下方的变量区域通过目录 network/model/Protected Attributes/_modules/'0'/weight/grad 查看到某一层参数的梯度，在反向传播之前为 None，执行反向传播的代码后可以看到 grad 处有数值了。 我们有了各个节点参数的梯度，接下来就可以选用一个合适的优化器，来对这些参数进行优化。 10.3 Optimizer 优化器 torch.optim 的官方文档：TORCH.OPTIM。 优化器主要是在模型训练阶段对模型的可学习参数进行更新，常用优化器有：SGD、RMSprop、Adam等。优化器初始化时传入传入模型的可学习参数，以及其他超参数如 lr、momentum 等，例如： 在训练过程中先调用 optimizer.zero_grad() 清空梯度，再调用 loss.backward() 反向传播，最后调用 optimizer.step() 更新模型参数，例如： 接下来我们来训练20轮神经网络，看看损失函数值的变化： 可以看到每一轮所有 batch 的损失函数值的总和确实在不断降低了。 11. 现有网络模型的使用及修改 11.1 VGG16模型的使用 我们以 VGG16 为例，该网络模型是用于大规模图像识别的超深度卷积神经网络，官方文档：VGG16。 该网络模型主要有以下参数： weights：可以设置成 torchvision.models.VGG16_Weights.DEFAULT，DEFAULT 表示自动使用最新的数据。老版本为 pretrained，如果为 True，表示使用预先训练好的权重，在官网可以看到这个权重是在 ImageNet-1K 数据集训练的，默认为不使用预先训练好的权重。 progress：如果为 True，则显示下载的进度条，默认为 True。 注意，下载网络时默认的下载路径是 C:\\Users\\&lt;username&gt;\\.cache，因此在下载模型前，我们需要修改路径：打开 D:\\Anaconda3_Environments\\envs\\PyTorch\\Lib\\site-packages\\torch 中的 hub.py 文件，搜索 load_state_dict_from_url，然后修改 model_dir 即可： 然后我们输出一下这个网络模型： 可以看到这个模型的分类结果为1000类，那么假如我们需要分类 CIFAR10 该如何应用这个网络模型呢？一种方法就是直接将最后一层 Linear 的 out_features 改为10，还有一种方法就是再添加一层 in_features=1000, out_features=10 的 Linear： 可以看到效果是比之前自己构建的网络模型好很多的。 11.2 模型的保存与读取 我们在对某些模型进行修改后可能想将其保存下来，方便以后用到时无需再构建一遍网络，可以按以下的方式将整个模型保存到路径 models/CIFAR10_VGG16.pth： 其对应的加载模型的方式为： 还有一种保存方式是将模型中的参数保存成字典的形式，官方建议使用该方式： 其对应的加载模型的方式为： 注意如果是保存自己构建的网络模型，需要在模型的类的源代码中将该类导入进来，例如在 test_save.py 中用以下代码保存自己的网络： 在 test_load.py 中导入时需要这样写： 12. 完整训练模型的方法 12.1 训练模型时的注意事项 （1）通常我们会将超参数的设置放在一起，使代码更加直观且方便修改： （2）我们在每一轮 epoch 中会先对训练集进行训练，然后使用测试集进行正确率的测试，因此一般我们会记录总共训练的次数 total_train_step 以及总共测试的次数 total_test_step，方便后续绘图使用。 （3）在开始训练之前一般需要将模型设置成训练状态，在测试之前需要设置成评估状态，这两种状态会影响少部分的层例如 Dropout 和 BatchNorm： （4）在分类问题中计算准确率一般用以下方法： （5）测试时不能对模型进行任何干扰，即在测试的时候神经网络不能产生梯度，因此在每次测试前需要加上以下代码： 12.2 使用GPU进行训练 前提：电脑有 NVIDIA 显卡，配置好了 CUDA，可以使用 torch.cuda.is_available() 来检查 CUDA 是否可用。 使用 GPU 训练的时候，需要将 Module 对象和 Tensor 类型的数据转移到 GPU 上进行计算，一般来说即为将网络模型、数据、损失函数放到 GPU 上计算。 使用 GPU 训练的方式有两种，第一种是使用 cuda() 函数，例如： 另一种是使用 to(device)，device 就是我们选择用来训练模型的设备，该方式与 cuda() 有一点细微的差别如下： 对于 Tensor 类型的数据（图像、标签等），使用 to(device) 之后，需要接收返回值，返回值才是正确设置了 device 的 Tensor。 对于 Module 对象（网络模型、损失函数），只用调用 to(device) 就可以将模型设置为指定的 device，不必接收返回值，当然接收返回值也是可以的。 例如： 注意如果加载在 GPU 上训练好的模型，然后想在 CPU 上使用，需要映射回 CPU： 12.3 CIFAR10_Net_Simple_v3 最后放上经过自己调参达到88%左右的正确率的模型和训练代码吧： 至此已经成功入门 PyTorch 啦！可以正式进入 Deep Learning 的学习啦！"},{"title":"Kratos-Rebirth主题修改部分细节教程","date":"2022-12-01T04:30:00.000Z","url":"/posts/9012.html","tags":[["Hexo","/tags/Hexo/"]],"categories":[["Hexo","/categories/Hexo/"]],"content":" 记录一下自己在使用 Kratos-Rebirth 过程中的一些样式微调。 1. 部分文本的修改 1.1 主页标题 在主题目录中的 source/css/kratosr.min.css 文件（之后也是在这个文件）中找到 .kratos-cover .desc h2，改成以下内容： 1.2 主页副标题 找到 .kratos-cover .desc p,.kratos-cover .desc span，改成以下内容： 1.3 主页文章标题 找到 .kratos-entry-header a,.kratos-entry-header span，改成以下内容： 1.4 进入文章页面时的标题 找到 .kratos-entry-title，改成以下内容： 2. 内容宽度 2.1 主体宽度 主体宽度是指文章区域与右侧目录标签区域整体的宽度，找到 @media (min-width:1200px)，其后的 .container&#123;width:xx&#125; 即为主体部分宽度，改成以下内容： 找到 @media (min-width:992px)，改成以下内容： 找到 @media (min-width:992px) and (max-width:1199px)，改成以下内容： 找到 @media (min-width:768px) and (max-width:991px)，改成以下内容： 2.2 文章与目录区域宽度占比 可以看到之前在 @media (min-width:992px) 中有 col-md-x 的类，其中 col-md-4 是右侧目录区域宽度占比，col-md-8 是文章区域宽度占比，在页面宽度超过 1320px 后我们将文章区域的宽度占比从66%改为70%： 3. 导航栏 3.1 导航栏选项宽度固定 找到 #kratos-menu-wrap .sf-menu a，将导航栏中的选项卡宽度固定，并让文字居中： 3.2 下拉子菜单偏移位置修改 找到 #kratos-desktop-topnav .sf-menu&gt;li&gt;ul.sub-menu 将下拉菜单的偏移量 left:-18px; 删去： 3.3 下拉子菜单文字宽度 找到 #kratos-desktop-topnav .sf-menu&gt;li&gt;ul.sub-menu a，改成以下内容： 4. 代码块 在主题目录中的 source/css/highlight/light.min.css 文件中找到 figure.highlight&#123;，将代码块顶部横条部分背景改成以下内容： 找到 figure.highlight .code&#123;，将代码块内容部分背景改成以下内容： 找到 figure.highlight .gutter pre，将代码块左侧代码行部分背景改成以下内容： "},{"title":"Anaconda与PyTorch安装教程","date":"2022-11-25T13:56:00.000Z","url":"/posts/15428.html","tags":[["Others","/tags/Others/"]],"categories":[["Others","/categories/Others/"]],"content":" 搭建 PyTorch 环境属实不容易，折腾了一整天，记录一下踩雷后的总结吧。 1. Anaconda的安装与命令介绍 1.1 安装Anaconda 首先前往 Anaconda 官网：Anaconda，下载安装文件。本文下载的为 Windows Python3.9 版本。 安装时没有需要特别注意的，设置好相应的安装路径即可，本文安装路径为：D:\\Anaconda3。 安装好后打开开始菜单能看到启动项：Anaconda Prompt，打开后如果看到命令行最左侧有 (base) 标识说明安装成功，可以查看版本号： 在 Anaconda 中我们会创建很多环境，那么我们需要先设置环境创建的路径，默认是在 C:\\Users\\XXX\\.conda 下的。 先在想要存放的地方创建文件夹 Anaconda3_Environments，本文创建在 D 盘，然后在该文件夹中再创两个文件夹：D:\\Anaconda3_Environments\\envs 和 D:\\Anaconda3_Environments\\pkgs。 在开始菜单打开 Anaconda Navigator，点击左上角的 File-Preferences-Configure Conda，修改为以下信息，然后保存即可： 1.2 Anaconda常用命令 创建名为 PyTorch 的环境，Python 版本为3.9： 删除名为 PyTorch 的环境（注意删除环境时要在 base 环境下，别在要删除的环境下）： 查看当前的所有环境： 激活 PyTorch 环境： 退出环境： 更新 conda 及 Anaconda： 如果下载速度很慢，例如下载 PyTorch 时，可以先按以下命令的方式修改镜像源： 查看相关镜像源： 将镜像源恢复成默认设置： 2. PyTorch的安装与配置 2.1 安装PyTorch 首先查看本机的 CUDA 版本，CUDA Version 即为版本号，本文的版本号为11.6： 前往 PyTorch 官网：PyTorch，在 Get Started 中选择好相应的选项：Stable、Windows、Conda、Python、CUDA 11.6，然后会生成一条安装命令（注意如果想在自己电脑上跑通代码，就选 CUDA，如果不需要在自己电脑上跑，而是在服务器上跑，或者没有独立显卡，就选 CPU。独立显卡需要 NVIDIA 显卡。这里我们一定要选择和自己版本相同或更低的 CUDA）。 本文使用离线与在线相结合的方式进行安装，也可以直接使用官网的命令安装但是速度很慢，或者修改镜像源后再安装。 前往清华大学镜像源：清华大学 PyTorch 镜像源，手动下载 pytorch、torchvision 以及 torchaudio。注意版本号要对应，本文下载的为：pytorch-1.13.0-py3.9_cuda11.6_cudnn8_0.tar.bz2、torchvision-0.14.0-py39_cu116.tar.bz2、torchaudio-0.13.0-py39_cu116.tar.bz2。py 和 cu 后面的数字分别表示 Python 和 CUDA 的版本号，找到对应版本进行下载即可。 下载好后进入 Anaconda 环境进行离线安装，本文下载路径为 D 盘根目录，在 PyTorch 环境中安装： 前往 CUDA Toolkit Archive 下载对应版本的 CUDA 套件（注意如果电脑是 Win10，Version 需要选10）。 下载好后打开程序安装 NVIDIA GPU Computing Toolkit，安装时路径需要使用默认的，即 C:\\Program Files\\NVIDIA GPU Computing Toolkit 以及 C:\\Program Files\\NVIDIA Corporation。 安装好后打开命令行检查版本： 进入 PyTorch 环境，安装剩余的包，此处还需要等待一段时间，但是最大的包已经离线安装了所以会快很多： 打开 Python，输入以下内容进行测试，没有报错即安装成功： 由于安装好 Anaconda 后顺带装了 Jupyter，但是他默认是装在 base 环境中的，因此我们还需要进入 PyTorch 环境中安装相应的包： 安装好后输入以下命令打开 Jupyter： 如果在 D 盘启动需要加上路径： 2.2 PyCharm配置PyTorch 在 PyCharm 中设置 Python 解释器，在 Conda 环境中选择现有环境，解释器选择：D:\\Anaconda3_Environments\\envs\\PyTorch\\python.exe，Conda 可执行文件选择：D:\\Anaconda3\\Scripts\\conda.exe。 设置好后即可在 Python 解释器选择菜单中找到 Python 3.9 (PyTorch) 选项，应用该环境后可以在底部导航栏打开 Python 控制台，用之前测试过的代码进行测试： 接下来需要修改 PyCharm 的终端，使其打开不是 Windows 默认的终端而是 Anaconda 的终端。先找到开始菜单中 Anaconda Prompt 的文件路径，然后查看属性，目标中有一段内容为：%windir%\\System32\\cmd.exe &quot;/K&quot; D:\\Anaconda3\\Scripts\\activate.bat D:\\Anaconda3。 将目标中的路径从 cmd.exe 开始之后的内容复制下来，进入 PyCharm，在文件-设置-工具-终端中修改 Shell 路径： 然后打开终端即可找到熟悉的感觉，即出现了 (base)。"},{"title":"计算机网络面试题总结","date":"2022-11-24T03:14:00.000Z","url":"/posts/54431.html","tags":[["Interview","/tags/Interview/"]],"categories":[["Interview","/categories/Interview/"]],"content":" 计算机网络常见面试题总结，文章将不断更新。 1. 概述 1.1 计算机网络的各层协议及作用？ 计算机网络体系可以大致分为三种：OSI 七层模型、TCP/IP 四层模型和五层模型。 OSI 七层模型：大而全，但是比较复杂、而且是先有了理论模型，没有实际应用。 TCP/IP 四层模型：是由实际应用发展总结出来的，从实质上讲，TCP/IP 只有最上面两层，最下面一层没有什么具体内容，TCP/IP 参考模型没有真正描述这一层的实现。 TCP/IP 五层模型：五层模型只出现在计算机网络教学过程中，这是对七层模型和四层模型的一个折中，既简洁又能将概念阐述清楚。 七层网络体系结构各层的主要功能： 应用层：为应用程序提供交互服务。在互联网中的应用层协议有很多，如域名系统 DNS，支持万维网应用的 HTTP 协议，支持电子邮件的 SMTP 协议等。 表示层：主要负责数据格式的转换，如加密解密、转换翻译、压缩解压缩等。 会话层：负责在网络中的两节点之间建立、维持和终止通信，如服务器验证用户登录便是由会话层完成的。 运输层：有时也译为传输层，向主机进程提供通用的数据传输服务。该层主要有以下两种协议： TCP：提供面向连接的、可靠的数据传输服务。 UDP：提供无连接的、尽最大努力的数据传输服务，但不保证数据传输的可靠性。 网络层：选择合适的路由和交换结点，确保数据及时传送。主要包括 IP 协议。 数据链路层：数据链路层通常简称为链路层。将网络层传下来的 IP 数据包组装成帧，并在相邻节点的链路上传送帧。 物理层：实现相邻节点间比特流的透明传输，尽可能屏蔽传输介质和通信手段的差异。 2. TCP/IP 2.1 TCP和UDP的区别？ TCP UDP 是否连接 面向连接 无连接 是否可靠 可靠传输，使用流量控制和拥塞控制 不可靠传输，不使用流量控制和拥塞控制 是否有序 有序，消息在传输过程中可能会乱序，TCP 会重新排序 无序 传输速度 慢 快 连接对象个数 只能一对一通信 支持一对一、一对多、多对一和多对多交互通信 传输方式 面向字节流 面向报文 首部开销 首部开销大，最小20字节，最大60字节 首部开销小，仅8字节 适用场景 适用于要求可靠传输的应用，例如文件传输 适用于实时应用例如 IP 电话、视频会议、直播等 总结：TCP 用于在传输层有必要实现可靠传输的情况，UDP 用于对高速传输和实时性有较高要求的通信。TCP 和 UDP 应该根据应用目的按需使用。 2.2 TCP和UDP对应的应用场景是什么？ TCP 是面向连接的，能保证数据的可靠性交付，因此经常用于： FTP 文件传输。 HTTP/HTTPS。 SMTP 简单邮件传输。 UDP 是无连接的，它可以随时发送数据，再加上 UDP 本身的处理既简单又高效，因此经常用于： 包总量较少的通信，如 DNS、SNMP 等。 视频、音频等多媒体通信。 广播通信。 2.3 TCP的三次握手机制？ 第一次握手：客户端请求建立连接，向服务端发送一个同步报文（SYN = 1），同时选择一个随机数 seq = x 作为初始序列号，并进入 SYN_SENT（同步已发送）状态，等待服务器确认。 第二次握手：服务端收到连接请求报文后，如果同意建立连接，则向客户端发送同步确认报文（SYN = 1, ACK = 1），确认号为 ack = x + 1，同时选择一个随机数 seq = y 作为初始序列号，此时服务器进入 SYN_RECV（同步收到）状态。 第三次握手：客户端收到服务端的确认后，向服务端发送一个确认报文（ACK = 1），确认号为 ack = y + 1，序列号为 seq = x + 1，客户端和服务器进入 ESTABLISHED（已建立连接）状态，完成三次握手。 理想状态下，TCP 连接一旦建立，在通信双方中的任何一方主动关闭连接之前，TCP 连接都将被一直保持下去。 2.4 为什么需要三次握手，而不是两次？ 主要有三个原因： 防止已过期的连接请求报文突然又传送到服务器，因而产生错误和资源浪费。 在双方两次握手即可建立连接的情况下，假设客户端发送报文段A请求建立连接，由于网络原因造成A暂时无法到达服务器，服务器接收不到请求报文段就不会返回确认报文段。 客户端在长时间得不到应答的情况下重新发送请求报文段B，这次B顺利到达服务器，服务器随即返回确认报文并进入 ESTABLISHED 状态，客户端在收到确认报文后也进入 ESTABLISHED 状态，双方建立连接并传输数据，之后正常断开连接。 此时姗姗来迟的报文段A才到达服务器，服务器随即返回确认报文并进入 ESTABLISHED 状态，但是已经进入 CLOSED 状态的客户端无法再接受确认报文段，更无法进入 ESTABLISHED 状态，这将导致服务器长时间单方面等待，造成资源浪费。 三次握手才能让双方均确认自己和对方的发送和接收能力都正常。 第一次握手：客户端只是发送处请求报文段，什么都无法确认，而服务器可以确认自己的接收能力和对方的发送能力正常。 第二次握手：客户端可以确认自己发送能力和接收能力正常，对方发送能力和接收能力正常。 第三次握手：服务器可以确认自己发送能力和接收能力正常，对方发送能力和接收能力正常。 可见三次握手才能让双方都确认自己和对方的发送和接收能力全部正常，这样就可以愉快地进行通信了。 告知对方自己的初始序号值，并确认收到对方的初始序号值。 TCP 实现了可靠的数据传输，原因之一就是 TCP 报文段中维护了序号字段和确认序号字段，通过这两个字段双方都可以知道在自己发出的数据中，哪些是已经被对方确认接收的。这两个字段的值会在初始序号值的基础上递增，如果是两次握手，只有发起方的初始序号可以得到确认，而另一方的初始序号则得不到确认。 2.5 为什么需要三次握手，而不是四次？ 因为三次握手已经可以确认双方的发送和接收能力正常，双方都知道彼此已经准备好，而且也可以完成对双方初始序号值的确认，也就无需第四次握手了。 第一次握手：服务端确认自己收、对方发报文功能正常。 第二次握手：客户端确认自己发、自己收、对方收、对方发报文功能正常，客户端认为连接己建立。 第三次握手：服务端确认自己发、对方收报文功能正常，此时双方均建立连接，可以正常通信。 2.6 什么是SYN洪泛攻击？如何防范？ SYN 洪泛攻击属于 DOS 攻击的一种，它利用 TCP 协议缺陷，通过发送大量的半连接请求，耗费 CPU 和内存资源。 原理： 在三次握手过程中，服务器发送 [SYN/ACK] 包（即第二个包）之后、收到客户端的 [ACK] 包（即第三个包）之前的 TCP 连接称为半连接（half-open connect），此时服务器处于 SYN_RECV（等待客户端响应）状态。如果接收到客户端的 [ACK]，则 TCP 连接成功，如果未接收到，则会不断重发请求直至成功。 SYN 攻击的攻击者在短时间内伪造大量不存在的 IP 地址，向服务器不断地发送 [SYN] 包，服务器回复 [SYN/ACK] 包，并等待客户的确认。由于源地址是不存在的，服务器需要不断的重发直至超时。 这些伪造的 [SYN] 包将长时间占用未连接队列，影响了正常的 SYN，导致目标系统运行缓慢、网络堵塞甚至系统瘫痪。 检测：当在服务器上看到大量的半连接状态时，特别是源 IP 地址是随机的，基本上可以断定这是一次 SYN 攻击。 防范： 通过防火墙、路由器等过滤网关防护。 通过加固 TCP/IP 协议栈防范，如增加最大半连接数，缩短超时时间。 SYN Cookies 技术。SYN Cookies 是对 TCP 服务器端的三次握手做一些修改，专门用来防范 SYN 洪泛攻击的一种手段。 2.7 三次握手连接阶段，如果最后一次ACK包丢失，会发生什么？ 服务端： 第三次的 ACK 包在网络中丢失，那么服务端该 TCP 连接的状态为 SYN_RECV,并且会根据 TCP 的超时重传机制，会等待3秒、6秒、12秒后重新发送 SYN + ACK 包，以便客户端重新发送 ACK 包。 如果重发指定次数之后，仍然未收到客户端的 ACK 应答，那么一段时间后，服务端自动关闭这个连接。 客户端： 客户端认为这个连接已经建立，如果客户端向服务端发送数据，服务端将以 RST 包（Reset，表示复位，用于异常的关闭连接）响应。此时，客户端便知道第三次握手失败。 2.8 TCP的四次挥手过程？ 第一次挥手：客户端向服务端发送连接释放报文（FIN = 1, ACK = 1），主动关闭连接，同时等待服务端的确认，客户端进入 FIN_WAIT_1（终止等待1）状态。序列号 seq = u，为客户端上次发送的报文的最后一个字节的序号 + 1。 第二次挥手：服务端收到连接释放报文后，立即发出确认报文（ACK = 1），序列号 seq = v，为服务端上次发送的报文的最后一个字节的序号 + 1，确认号 ack = u + 1，服务端进入 CLOSE_WAIT（关闭等待）状态。 此时 TCP 连接处于半关闭状态，即客户端到服务端的连接已经释放了，但是服务端到客户端的连接还未释放。这表示客户端已经没有数据发送了，但是服务端可能还要给客户端发送数据。 第三次挥手：客户端收到服务端的确认后进入 FIN_WAIT_2（终止等待2）状态，等待服务端发出连接释放报文段。服务端向客户端发送连接释放报文（FIN = 1, ACK = 1），主动关闭连接，同时等待A的确认，服务端进入 LAST_ACK（最后确认）状态。 序列号 seq = w，即服务端上次发送的报文的最后一个字节的序号 + 1，可能在半关闭状态服务端又发送了一些数据。 确认号 ack = u + 1，与第二次挥手相同，因为这段时间客户端没有发送数据。 第四次挥手：客户端收到服务端的连接释放报文后，立即发出确认报文（ACK = 1），序列号 seq = u + 1，确认号为 ack = w + 1。 此时，客户端就进入了 TIME_WAIT（时间等待）状态。注意此时客户端到 TCP 连接还没有释放，必须经过2 * MSL（最长报文段寿命）的时间后，才进入 CLOSED 状态。而服务端只要收到客户端发出的确认，就立即进入 CLOSED 状态。可以看到，服务端结束 TCP 连接的时间要比客户端早一些。 2.9 为什么连接的时候是三次握手，关闭的时候却是四次握手？ 服务器在收到客户端的 FIN 报文段后，可能还有一些数据要传输，所以不能马上关闭连接，但是会做出应答，返回 ACK 报文段. 接下来可能会继续发送数据，在数据发送完后，服务器会向客户单发送 FIN 报文，表示数据已经发送完毕，请求关团连接。服务器的 ACK 和 FIN 一般都会分开发送，从而导致多了一次，因此一共需要四次挥手。 2.10 为什么客户端的TIME_WAIT状态必须等待2MSL？ 主要有两个原因: 确保最后一个 ACK 报文段能够到达服务端，从而使服务端正常关闭连接。 第四次挥手时，客户端第四次挥手的 ACK 报文段不一定会到达服务端。服务端会超时重传 FIN/ACK 报文段，此时如果客户端已经断开了连接，那么就无法响应服务端的二次请求，这样服务端迟迟收不到 FIN/ACK 报文段的确认，就无法正常断开连接。 MSL 是报文段在网络上存活的最长时间。客户端等待 2MSL 时间，即：客户端 ACK 报文段 1MSL 超时 + 服务端 FIN 报文段 1MSL 传输，就能够收到服务端重传的 FIN/ACK 报文段，然后客户端重传一次 ACK 报文段，并重新启动 2MSL 计时器。如此保证服务端能够正常关闭。 如果服务端重发的 FIN 报文段没有成功地在 2MSL 时间里传给客户端，服务端则会继续超时重试直到断开连接。 防止已失效的连接请求报文段出现在之后的连接中。 TCP 要求在 2MSL 内不使用相同的序列号。客户端在发送完最后一个 ACK 报文段后，再经过时间 2MSL，就可以保证本连接持续的时间内产生的所有报文段都从网络中消失。这样就可以使下一个连接中不会出现这种旧的连接请求报文段。或者即使收到这些过时的报文，也可以不处理它。 2.11 如果已经建立了连接，但是客户端出现故障了怎么办？ 通过定时器 + 超时重试机制，尝试获取确认，直到最后会自动断开连接。 具体而言，TCP 设有一个保活计时器。服务器每收到一次客户端的数据，都会重新设置这个计时器，时间通常是设置为2小时。若2小时还没有收到客户端的任何数据，服务器就发送一个探测报文段，之后则每隔75秒钟发送一次，若一连发送10个探测报文段后客户端依然没有响应，那么服务器就认为客户端出现故障，接着就关闭这个连接。 2.12 TIME_WAIT是服务器端还是客户端的状态? TIME_WAIT 是主动断开连接的一方会进入的状态，一般情况下，都是客户端所处的状态，服务器端一般设置不主动关闭连接。 TIME_WAIT 需要等待 2MSL，在大量短连接的情况下，TIME_WAIT 会太多，这也会消耗很多系统资源。对于服务器来说，在 HTTP 协议里指定 KeepAlive（浏览器重用一个 TCP 连接来处理多个 HTTP 请求），由浏览器来主动断开连接，可以一定程度上减少服务器的这个问题。 2.13 TCP协议如何保证可靠性，即如何实现可靠传输？ TCP 主要提供了检验和、序列号/确认应答、超时重传、滑动窗口、拥塞控制和流量控制等方法实现了可靠性传输。 检验和：通过检验和的方式，接收端可以检测出来数据是否有差错和异常，假如有差错就会直接丢弃 TCP 报文段，重新发送。 序列号/确认应答：序列号的作用不仅仅是应答的作用，有了序列号能够将接收到的数据根据序列号排序，并且去掉重复序列号的数据。 TCP 传输的过程中，每次接收方收到数据后，都会对传输方进行确认应答。也就是发送 ACK 报文段，这个 ACK 报文段当中带有对应的确认序列号，告诉发送方，接收到了哪些数据，下一次的数据从哪里发。 滑动窗口：滑动窗口既提高了报文传输的效率，也避免了发送方发送过多的数据而导致接收方无法正常处理的异常。 超时重传：超时重传的时间是指发送出去的数据包到接收到确认包之间的时间，如果超过了这个时间会被认为是丢包了，需要重传。最大超时时间是动态计算的。 拥塞控制：在数据传输过程中，可能由于网络状态的问题，造成网络拥堵，此时引入拥塞控制机制，在保证 TCP 可靠性的同时，提高性能。 流量控制：如果主机A一直向主机B发送数据，不考虑主机B的接收能力，则可能导致主机B的接收缓冲区满了而无法再接收数据，从而会导致大量的数据丢包，引发重传机制。而在重传的过程中，若主机B的接收缓冲区情况仍未好转，则会将大量的时间浪费在重传数据上，降低传送数据的效率。所以引入流量控制机制，主机B通过告诉主机A自己接收缓冲区的大小，来使主机A控制发送的数据量。流量控制与 TCP 协议报头中的窗口大小有关。 2.14 详细讲一下TCP的滑动窗口？ 在进行数据传输时，如果传输的数据比较大，就需要拆分为多个数据包进行发送。TCP 协议需要对数据进行确认后，才可以发送下一个数据包。这样一来，就会在等待确认应答包环节浪费时间。 为了避免这种情况，TCP 引入了窗口概念。窗口大小指的是不需要等待确认应答包而可以继续发送数据包的最大值。 滑动窗口里面也分为有三种类型的数据，第一种是已经发送且收到确认但是未按序到达，即没有在窗口尾部形成一段连续的序列；第二种是已经发送但是未被确认的数据；第三种是等待发送的数据。随着已发送的数据不断被确认，窗口内等待发送的数据也会不断被发送。整个窗口就会不断往前移动，让还没轮到的数据进入窗口内。 可以看到滑动窗口起到了一个限流的作用，也就是说当前滑动窗口的大小决定了当前 TCP 发送包的速率，而滑动窗口的大小取决于拥塞控制窗口和流量控制窗口的两者间的最小值。 2.15 详细讲一下拥塞控制？ TCP 一共使用了四种算法来实现拥塞控制： 慢开始（slow-start） 拥塞避免（congestion avoidance） 快重传（fast retransmit） 快恢复（fast recovery） 发送方维持一个叫做拥塞窗口 cwnd（congestion window）的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。 慢开始：不要一开始就发送大量的数据，由小到大逐渐增加拥塞窗口的大小。 例如一开始发送方先设置 cwnd = 1，发送第一个报文段，等发送方接收到对方的确认后把 cwnd 从1增大到2。此后每经过一个传输轮次，拥塞窗口 cwnd 就加倍。 为了防止拥塞窗口 cwnd 增长过大引起网络拥塞，还需要设置一个慢开始门限 ssthresh 状态变量。 当 cwnd &lt; ssthresh 时，使用慢开始算法。 当 cwnd &gt; ssthresh 时，停止使用慢开始算法改用拥塞避免算法。 当 cwnd = ssthresh 时，即可使用慢开始算法，也可使用拥塞避免算法。 拥塞避免：拥塞避免算法让拥塞窗口缓慢增长，即每经过一个往返时间 RTT 就把发送方的拥塞窗口 cwnd 加一而不是加倍。这样拥塞窗口按线性规律缓慢增长。 快重传：我们可以剔除一些不必要的拥塞报文，提高网络吞吐量。比如接收方在收到一个失序的报文段后就立即发出重复确认，而不要等到自己发送数据时捎带确认。快重传规定：发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传计时器时间到期。 快恢复：主要是配合快重传，当发送方连续收到三个重复确认时，就执行乘法减小算法，把 ssthresh 门限减半（为了预防网络发生拥塞），但接下来并不执行慢开始算法，因为如果网络出现拥塞的话就不会收到好几个重复的确认，收到三个重复确认说明网络状况还可以。 3. HTTP/HTTPS 3.1 HTTP常见的状态码有哪些？ 常见状态码： 200：服务器已成功处理了请求。通常，这表示服务器提供了请求的网页。 301：请求的网页已永久移动到新位置。服务器返回此响应（对 GET 或 HEAD 请求的响应）时，会自动将请求者转到新位置（永久移动）。 302：服务器目前从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求（临时移动）。 400：客户端请求有语法错误，不能被服务器所理解。 403：服务器收到请求，但是拒绝提供服务。 404：服务器找不到请求的网页。 500：服务器遇到错误，无法完成请求。 状态码开头代表类型： 类别 原因短语 1XX Informational（信息性状态码） 接收的请求正在处理 2XX Success（成功状态码） 请求正常处理完毕 3XX Redirection（重定向状态码） 需要进行附加操作以完成请求 4XX Client Error（客户端错误状态码） 服务器无法处理请求 5XX Server Error（服务器错误状态码） 服务器处理请求出错 3.2 状态码301和302的区别是什么？ 共同点：301和302状态码都表示重定向，就是说浏览器在拿到服务器返回的这个状态码后会自动跳转到一个新的 URL 地址，这个地址可以从响应的 Location 首部中获取（用户看到的效果就是他输入的地址A瞬间变成了另一个地址B）。 不同点：301表示旧地址A的资源已经被永久地移除了（这个资源不可访问了），搜索引擎在抓取新内容的同时也将旧的网址交换为重定向之后的网址。302表示旧地址A的资源还在（仍然可以访问），这个重定向只是临时地从旧地址A跳转到地址B，搜索引擎会抓取新的内容而保存旧的网址。SEO 中302好于301。 重定向原因： 网站调整（如改变网页目录结构）。 网页被移到一个新地址。 网页扩展名改变（如应用需要把 .php 改成 .html 或 .shtml）。 3.3 HTTP常用的请求方式有哪些？ 方法 作用 GET 获取资源 POST 传输实体主体 PUT 上传文件 DELETE 删除文件 HEAD 和 GET 方法类似，但是只返回报文首部，不返回报文实体主体部分 PATCH 对资源进行部分修改 OPTIONS 查询指定的 URL 支持的方法 CONNECT 要求用隧道协议连接代理 TRACE 服务器会将通信路径返回给客户端 为了方便记忆，可以将 PUT、DELETE、POST、GET 理解为客户端对服务端的增删改查： PUT：上传文件，向服务器添加数据。 DELETE：删除文件。 POST：传输数据，向服务器提交数据，对服务器数据进行更新。 GET：获取资源，查询服务器资源。 3.4 GET请求和POST请求的区别？ 使用上的区别： 作用：GET 用于获取资源，而 POST 用于传输实体。 参数：GET 使用 URL 或 Cookie 传参，而 POST 将数据放在 Request Body 中，这个是因为 HTTP 协议用法的约定。 缓存：GET 请求会被浏览器主动缓存，而 POST 不会，除非手动设置。 请求长度：GET 方式提交的数据有长度限制，基本为2kb，而 POST 的数据则可以非常大，这个是因为它们使用的操作系统和浏览器设置的不同引起的区别。 安全性：POST 比 GET 安全，因为数据在地址栏上不可见，而 GET 的参数直接暴露在 URL 上。这个说法没毛病，但依然不是 GET 和 POST 本身的区别。 本质区别：GET 和 POST 最大的区别主要是 GET 请求是幂等性的，POST 请求不是。这个是它们本质区别。（幂等性是指一次和多次请求某一个资源应该具有同样的副作用。简单来说意味着对同一 URL 的多个请求应该返回同样的结果） 3.5 解释一下HTTP长连接和短连接？ 在 HTTP/1.0 中，默认使用的是短连接。也就是说，浏览器和服务器每进行一次 HTTP 操作，就建立一次连接，但任务结束就中断连接。如果客户端浏览器访问的某个 HTML 或其他类型的 Web 页中包含有其他的 Web 资源，如 JavaScript 文件、图像文件、CSS 文件等，当浏览器每遇到这样一个 Web 资源，就会建立一个 HTTP 会话。 但从 HTTP/1.1 起，默认使用长连接，用以保持连接特性。使用长连接的 HTTP 协议，会在响应头加入这行代码：Connection:keep-alive。 在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输 HTTP 数据的 TCP 连接不会关闭，如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的连接。Keep-Alive 不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（例如 Apache）中设定这个时间。实现长连接要客户端和服务端都支持长连接。 HTTP 协议的长连接和短连接，实质上是 TCP 协议的长连接和短连接。 3.6 HTTP1.0和HTTP1.1的区别？ 长连接：HTTP1.1 支持长连接（Persistent Connection）和请求的流水线（Pipelining）处理，在一个 TCP 连接上可以传送多个 HTTP 请求和响应，减少了建立和关闭连接的消耗和延迟，在 HTTP1.1 中默认开启 Connection: keep-alive，一定程度上弥补了 HTTP1.0 每次请求都要创建连接的缺点。 缓存处理：在 HTTP1.0 中主要使用 header 里的 If-Modified-Since, Expires 来做为缓存判断的标准，HTTP1.1 则引入了更多的缓存控制策略，可供选择的缓存头来控制缓存策略。 带宽优化及网络连接的使用：HTTP1.0 中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP1.1 则在请求头引入了 range 头域，它允许只请求资源的某个部分，即返回码是206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。 错误通知的管理：在 HTTP1.1 中新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突，410（Gone）表示服务器上的某个资源被永久性的删除。 Host 头处理：在 HTTP1.0 中认为每台服务器都绑定一个唯一的 IP 地址，因此，请求消息中的 URL 并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个 IP 地址。HTTP1.1 的请求消息和响应消息都应支持 Host 头域，且请求消息中如果没有 Host 头域会报告一个错误（400 Bad Request）。 3.7 HTTP1.1和HTTP2.0的区别？ HTTP2.0 相比 HTTP1.1 支持的特性： 新的二进制格式：HTTP1.1 的解析是基于文本的。基于文本协议的格式解析存在天然缺陷，文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，二进制则不同，只认0和1的组合。基于这种考虑 HTTP2.0 的协议解析决定采用二进制格式，实现方便且健壮。 多路复用：即连接共享，每一个 request 都是用作连接共享机制的。一个 request 对应一个 id，这样一个连接上可以有多个 request，每个连接的 request 可以随机的混杂在一起，接收方可以根据 request 的 id 将 request 再归属到各自不同的服务端请求里面。 头部压缩：HTTP1.1 的头部（header）带有大量信息，而且每次都要重复发送。HTTP2.0 使用 encoder 来减少需要传输的 header 大小，通讯双方各自 cache 一份 header fields 表，既避免了重复 header 的传输，又减小了需要传输的大小。 服务端推送：服务器除了对最初请求的响应外，服务器还可以额外地向客户端推送资源，而无需客户端明确的请求。 3.8 HTTP和HTTPS的区别？ HTTP HTTPS 端口 80 443 安全性 无加密，安全性较差 有加密机制，安全性较高 资源消耗 较少 由于加密处理，资源消耗更多 是否需要证书 不需要 需要 协议 运行在 TCP 协议之上 运行在 SSL 协议之上，SSL 运行在 TCP 协议之上 3.9 HTTPS的优缺点？ 优点： 安全性： 使用 HTTPS 协议可认证用户和服务器，确保数据发送到正确的客户机和服务器。 HTTPS 协议是由 SSL + HTTP 协议构建的可进行加密传输、身份认证的网络协议，要比 HTTP 协议安全，可防止数据在传输过程中不被窃取、改变，确保数据的完整性。 HTTPS 是现行架构下最安全的解决方案，虽然不是绝对安全，但它大幅增加了中间人攻击的成本。 SEO 方面：谷歌曾在2014年8月份调整搜索引擎算法，并称比起同等 HTTP 网站，采用 HTTPS 加密的网站在搜索结果中的排名将会更高。 缺点： 在相同网络环境中，HTTPS 相比 HTTP 无论是响应时间还是耗电量都有大幅度上升。 HTTPS 的安全是有范围的，在黑客攻击、服务器劫持等情况下几乎起不到作用。 在现有的证书机制下，中间人攻击依然有可能发生。 HTTPS 需要更多的服务器资源，也会导致成本的升高。 3.10 HTTPS的原理？ 客户端请求 HTTPS 网址，例如：，然后连接到 Server 的443端口（HTTPS 默认端口，类似于 HTTP 的80端口）。 采用 HTTPS 协议的服务器必须要有一套数字 CA（Certification Authority）证书。颁发证书的同时会产生一个私钥和公钥。私钥由服务端自己保存，不可泄漏。公钥则是附带在证书的信息中，可以公开的。证书本身也附带一个证书电子签名，这个签名用来验证证书的完整性和真实性，可以防止证书被篡改。 服务器响应客户端请求，将证书传递给客户端，证书包含公钥和大量其他信息，比如证书颁发机构信息，公司信息和证书有效期等。 客户端解析证书并对其进行验证。如果证书不是可信机构颁布，或者证书中的域名与实际域名不一致，或者证书已经过期，就会向访问者显示一个警告，由其选择是否还要继续通信。 如果证书没有问题，客户端就会从服务器证书中取出服务器的公钥A。然后客户端还会生成一个随机码 KEY，并使用公钥A将其加密。 客户端把加密后的随机码 KEY 发送给服务器，作为后面对称加密的密钥。 服务器在收到随机码 KEY 之后会使用私钥B将其解密。经过以上这些步骤，客户端和服务器终于建立了安全连接，完美解决了对称加密的密钥泄露问题，接下来就可以用对称加密愉快地进行通信了。 服务器使用密钥（随机码 KEY）对数据进行对称加密并发送给客户端，客户端使用相同的密钥（随机码 KEY）解密数据。 双方使用对称加密愉快地传输所有数据。 3.11 什么是Cookie和Session？ HTTP Cookie（也叫 Web Cookie 或浏览器 Cookie）是服务器发送到用户浏览器并保存在本地的一小块数据，它会在浏览器下次向同一服务器再发起请求时被携带并发送到服务器上。通常，它用于告知服务端两个请求是否来自同一浏览器，如保持用户的登录状态。Cookie 使基于无状态的 HTTP 协议记录稳定的状态信息成为了可能。 Cookie 主要用于以下三个方面： 会话状态管理（如用户登录状态、购物车、游戏分数或其它需要记录的信息）。 个性化设置（如用户自定义设置、主题等）。 浏览器行为跟踪（如跟踪分析用户行为等）。 Session 代表着服务器和客户端一次会话的过程。Session 对象存储特定用户会话所需的属性及配置信息。这样，当用户在应用程序的 Web 页之间跳转时，存储在 Session 对象中的变量将不会丢失，而是在整个用户会话中一直存在下去。当客户端关闭会话，或者 Session 超时失效时会话结束。 3.12 Cookie和Session是如何配合的呢？ 用户第一次请求服务器的时候，服务器根据用户提交的相关信息，创建对应的 Session，请求返回时将此 Session 的唯一标识信息 SessionID 返回给浏览器，浏览器接收到服务器返回的 SessionID 信息后，会将此信息存入到 Cookie 中，同时 Cookie 记录此 SessionID 属于哪个域名。 当用户第二次访问服务器的时候，请求会自动判断此域名下是否存在 Cookie 信息，如果存在则自动将 Cookie 信息也发送给服务端，服务端会从 Cookie 中获取 SessionID，再根据 SessionID 查找对应的 Session 信息，如果没有找到说明用户没有登录或者登录失效，如果找到 Session 证明用户已经登录可执行后面操作。 根据以上流程可知，SessionID 是连接 Cookie 和 Session 的一道桥梁，大部分系统也是根据此原理来验证用户登录状态。 3.13 Cookie和Session的区别？ 作用范围不同：Cookie 保存在客户端（浏览器），Session 保存在服务器端。 存取方式的不同：Cookie 只能保存 ASCII，Session 可以存任意数据类型，一般情况下我们可以在 Session 中保持一些常用变量信息，比如说 UserID 等。 有效期不同：Cookie 可设置为长时间保持，比如我们经常使用的默认登录功能，Session 一般失效时间较短，客户端关闭或者 Session 超时都会失效。 隐私策略不同：Cookie 存储在客户端，比较容易遭到不法获取，早期有人将用户的登录名和密码存储在 Cookie 中导致信息被窃取；Session 存储在服务端，安全性相对 Cookie 要好一些。 存储大小不同：单个 Cookie 保存的数据不能超过4K，Session 可存储数据远高于 Cookie。 "},{"title":"Hexo使用Abbrlink生成文章固定链接","date":"2022-11-23T03:11:00.000Z","url":"/posts/18884.html","tags":[["Hexo","/tags/Hexo/"]],"categories":[["Hexo","/categories/Hexo/"]],"content":" 本文介绍如何使用 Abbrlink 插件生成形如  的 Hexo 文章固定编号链接。 Hexo 默认的静态 url 格式是：:year/:month/:day/:title，也就是按照年、月、日、文章标题来生成固定链接的。如：。 使用 Abbrlink 插件可以使每篇文章都有一个唯一的编号，并将文章的链接用这个编号唯一区别，这样链接中不会出现中文，也不会因为修改文章的日期而导致链接的改变。 首先我们先安装插件，在博客根目录中打开命令行，输入以下命令： 修改根目录下的 _config.yml 文件，修改文件中的 permalink: 配置项，且添加一个配置项 abbrlink:，修改后的结果如下： 其中，alg 属性表示算法，目前支持 crc16 和 crc32 算法，默认值是 crc16。rep 表示形式，即生成的链接可以是十六进制格式也可以是十进制格式，默认值是十进制格式，示例如下： 注意：在生成之前就要改好算法和形式，不然后面再改的话会导致链接不统一。"},{"title":"本科阶段学习生活回顾","date":"2022-11-20T03:33:00.000Z","url":"/posts/62882.html","tags":[["Essay","/tags/Essay/"]],"categories":[["Essay","/categories/Essay/"]],"content":" 大学本科生涯即将结束，浅浅地在此总结回忆一下这段生活吧~ 2019-2020 2019.09 录取的专业是兰州理工大学土木工程学院测绘工程专业，高中同班同学ycy去了同校的电子信息科学与技术专业（我的一志愿，他比我高了一分被他抢了hhh） 第一次出远门挺不适应的，和爸妈分别也是很难受QAQ。 刚去学校没有经验，和ycy坐动车先到武汉再去兰州，从早上一直坐到晚上十一点多，还花了一千多块，简直俩冤大头。到站后去旅馆住了一晚，第二天早上去学校。 我拖了一个29寸的大行李箱和一个20寸的登机箱，带了被子还有几件厚外套 刚到学校时认不清路，学长学姐来问我时我也没搭理hhh，自己摸索着办完了注册手续然后找到了宿舍楼。 到学校的第一顿午餐就和ycy干了一碗牛肉面（兰州拉面），感觉不算很好吃，但是确实便宜，5r一碗。 我的舍友都挺不错的，有一个是福建的老乡lcq，一个辽宁的lty，然后剩下三个是甘肃本地的yhy、bm、zx。 学校的图书馆确实很大很新，夜景也很漂亮，让人有种想进去学习的欲望~ 进入了为期两周的军训生活 军训期间有点累，突然更想家了，加上刚来学校，那段时间就挺难受的。 军训结束后和舍友出去吃了一顿，顺便逛了一圈（怀念爆发 COVID-19 前的生活） 然后到了大一正式的学习生活 上了大学感觉和高中最明显的差别就是班集体的概念淡化了很多，基本上平时只会和舍友走在一起。 大一每天早上有跑操，晚上有晚自习，有时候感觉自己像个高中生。 刚上高数课时感觉大家都很积极，每天前排都老早就被抢了，我和我舍友基本也是抢前两排。 入学英语考试考进了A班，可以在12月就报名英语四级考试，因此开始每天碎片化背单词。 课余时间基本都会跑去图书馆，第一次体验在图书馆学习的感觉，和家里氛围确实差太多了，在图书馆基本不会有摸鱼的想法hhh。 2019.10 国庆节和舍友一起去了成都，在火车上对面做了几个本校的学姐，聊了几句 在成都去了杜甫草堂，感觉不是很有意思，有个舍友特别喜欢文学，在那里面待了一整天。然后去了动物园，第一次看到大熊猫ovo。春熙路、太古里也是必去的地方，还有九眼桥下的整条 Bar 街，属实是体会到了什么叫做灯红酒绿。 顺便去逛了一圈四川大学，985名校确实大多了，建筑也豪华不少，还在纪念品店买了一些川大纪念品。 国庆结束后回学校的晚上感觉天气就变凉了不少，感觉已经快和福建的冬天差不多了。 2019.11-2020.01 国庆假期结束后就该好好学习了，什么时候努力都不晚~ 大一上册基本就是学那几门必修课，时间比较充裕，因此课余时间基本都在学英语。 而且当时想着是跨专业考研计算机，考研的话英语也是非常重要的，可以提前积累。 考四级的时候我的英语词汇量从高中的2k提升到了8k左右，语法还是零基础，买了一套四级练习基本全刷完了，第一次四级也挺顺利的，554分通过！ 四级考完就该准备期末考了。因为对计算机感兴趣，就提前学完了C语言，很多宿舍的人都跑来这问我题目hhh。 最后大一上的期末考也很顺利，高数和C语言这类高学分的课都是接近满分，总成绩自然也被拉上去了。 2020.02-2020.08 年前去逛了两场漫展，然后差不多就爆发了 COVID-19 过年的这段时间基本天天在家里，然后看新闻，不过我家这边的小县城倒是很安全。 大一下的开学也延期了，正式开始了网课生活。网课期间我在家也不像在学校那样有动力了，基本天天挂着网课打游戏。 那时候在肝 Destiny 2 和 GTA V，感觉又找回了高中打游戏的状态hhh。 到了六月左右学校又通知返校，那时候我的其他高中同学就没听谁说还要返校的。 回学校抓紧速成了一下，顺利通过期末考，再次回家。 2020-2021 2020.09 这时候通知可以报名转专业，当时感觉转专业可能会很难，但还是报了一下 转专业期间认识了班上的一位女生cl，发现她也报的和我一个专业，然后慢慢地就熟了。 最后我竟然总分第一成功转来了计算机科学与技术！当时自己也想不到。cl调剂去了同院的大数据。 那么这时候目标就改成了考研本专业！这样就不用跨专业了。 大概在同一时间段，大一学年的评奖评优也出来了，评上了校一等奖学金和校三好学生。 转专业后分配了一个转专业宿舍，一共5个人，其中有一个是通信工程的zaj 我转专业后没多久也搬宿舍了，这时候新宿舍只有我和zaj，剩下几位还待原宿舍不搬。 zaj简直是带动整个宿舍的优良作息规律，必须早睡早起。 九月份考了因为 COVID-19 影响而推迟的英语六级，最后六级也是一次过，虽然是428分低分飘过hhh。 2020.10-2021.01 继续进入学习状态，学计算机后才知道自学远比上课听讲更有效 转专业后得补修没修过的大一课程，最难搞的就是大物了。 我也是提前自学完了 C++，然后十月份报名了团体程序设计天梯赛的校赛，排在八十多名，打了个酱油hhh。 此后开始正式进入算法学习阶段，每天一整天在图书馆都在学算法和写题，后来颈椎都快出问题了。 在做 C++ 课设的时候认识了lj老师，是个很不错的老师，她知道课设都是我自己写的，答辩的时候就没问我问题了，然后开始和我闲聊，问我是不是想打比赛，然后说之后培训我可以去听。 到了十二月，先是考了第二次六级，这次511分通过！ 这时候苦练了大概两个月的算法，继续参加了天梯赛校赛，拿了第十名！lj和zh老师说我有潜力。 同月的 ACM-ICPC 校队选拔赛也是第一名！ 到了学期末的时候另一个舍友hxc也搬过来了，感觉宿舍热闹了一点，还建了一个三个人的小群。 比赛结束后又是进入紧张的期末备考阶段，然后顺利考完，回家过年。 2021.02-2021.08 大二下学期就进入了比赛的主场了，基本所有比赛的省赛国赛都在下学期 返校后也是复健了一下算法，寒假在家基本又颓废了。 寒假组队 ICPC 的时候认识了dyy，大学真正的导师来了hhh。 三四月开始继续训练算法，四月有 ICPC 昆明站、蓝桥杯省赛和天梯赛国赛。 第一次打 ICPC 的时候属实被打蒙了，感觉自己是个 Newbie，成功打铁。 蓝桥杯比较水，拿下省一，天梯赛状态也不错，wjt第一名220+，我211分，拿下个人国三。 打完算法比赛后dyy拉我开发项目参加计算机设计大赛。当时我对开发技术一无所知。 dyy教我学了 Qt，然后用 C++ 开发了一个线上考试系统，但是当时时间紧张加上我太 Newbie 所以没实现什么功能，但是最后拿下西北赛区一等奖，震惊。 五月份还和dyy参加了数学建模校赛，教我 Word 的排版方法，感觉要被嫌弃死hhh。之后去了宁夏理工学院参加 ICPC 银川站，成功打铁，但是和dyy还有sy玩的还挺开心的（旅游组实锤）。 六月打完了蓝桥杯国赛，拿下国二。去了西北工业大学参加 ICPC 全国邀请赛，成功打铁，这次没怎么旅游，还是考完数据库连夜赶过去的，都没休息好。 大二学年结束后发现成绩排到了专业第一，这时候又改变目标了，要不冲保研吧ovo。 2021-2022 2021.09-2022.01 大二结束后暑假没有回家，而是留学校培训数学建模，然后继续训练算法 暑假期间第四个舍友zyj也搬过来了，因为原学院要搬去本部了，被迫驱逐。 我、hxc、twl、zyj都留校培训数学建模，兰州夏天也是挺难熬的，早上热晚上蚊子多，天天晚上打蚊子打到深夜。 暑假结束后直接建模国赛，感觉做的很好但是最后没获奖，不会被一眼假了吧，震惊。 到了大三评奖评优的时候了 大二一整年感觉应该算是状态最巅峰的时期，因此打算直接冲一波国家奖学金，这国奖答辩阵仗属实挺大的（吓）。 答完辩之后本来好像是没戏了，但是突然发现隔壁班的一个同学加分项有点浑水摸鱼，就直接去找导员重新核对，最后算下来险胜拿下了国奖（8K真是美滋滋~）。 然后就是继续训练算法，十月份的天梯赛校赛拿下第一名。这次校赛鲲鹏有赞助，还拿了个华为耳机ovo。 到了期末还是冲刺考试，想保研成绩肯定得稳住，学期结束还是稳住了专业第一。 2022.02-2022.08 保研前最后一个学期的冲刺了 这个学期的天梯赛不是cy姥姥出题了，换了一个，难度比之前大了不少，直接翻车车，国赛169分没拿到个人国三，最后是团队国三。 蓝桥杯国赛 LCA 公式写错，DJ 模板题建成单向边，继续翻车车，只有国三。 被dyy拉着搞计算机设计大赛，开发一个线上编辑器，我和舍友hxc基本都是划水的。最后拿下西北赛区一等奖，合理，没拿到国奖，震惊。 到了七月份，突然爆发 COVID-19，7月9日中午还在学校做课设，突然就通知能回家的赶紧回家，提前放假，当晚就和hxc还有dyy到了机场，第二天直接润回家。 我先飞到了南昌，然后路上还认识了一个江西的也是我们学校的一个能动院学弟，最后没有留联系方式，短暂的缘分hhh。 2022-2023 2022.09-2023.01 九月突然爆发 COVID-19，没办法返校，保研的事情线上办完了，最后也是以第一名的总分保研了。 9月28日，填了推免系统信息，最后录取了西安电子科技大学的人工智能学院，也算是给过去三年的努力画上了句号。 十月除学校组织返校，我去福州度假了hhh，就申请了不返校，结果这波是我预判了兰州的预判。 这波返校后兰州 COVID-19 大爆发，一直到现在十一月都快结束了，那边还很严重，突然感觉当时没返校待家里是多幸福的一件事。 2023.02-2023.06 一转眼就到了本科的最后一个学期了呀，不得不感慨时间过的真的太快了 这是本科生涯的最后一次返校了，有大半年没见到舍友了。 通信的舍友zaj考研南理工353分，本来我们寝室几个舍友还都挺开心的，结果出来复试线是350分后大家都惊了，去年复试线是325，今年由于 COVID-19 原因很多考研er都很不容易，我们宿舍本以为复试线大概会在320左右顶天了，350也是真的从来没想过。 擦线进复试，他情绪瞬间低落到谷底，开始纠结要不要去线下复试，我们其他几个安慰了好久最后还是决定冲一把，由于hxc之前大创挂过他的名字，因此就给他讲了很多项目内容帮助他充实简历和自我介绍，实验部分找了对门的zsy来辅导，就这么持续了一两周后他就去南京复试了，但是很遗憾最后还是被刷了。 复试完他先回家休息了几天才来学校，在这期间我和hxc商量着等他回来咱宿舍一起去吃顿饭，然后我俩一起请他，他回来后我们去吃了一顿海底捞，还叫上了隔壁班的好兄弟nzx，吃完后去兰州老街逛了一圈，在回学校的路上下起了雨夹雪，105路车上的显示屏放的是去年祝福兰理工毕业生的海报，感觉在车上心情复杂，即将要毕业了。 zaj过了一段时间恢复心态后决定二战，我们也很支持他，hxc是今年一战，因为去年秋招就业不太好找工作，所以他们俩就互相鼓励，最后一定会上岸的！！！ 2023.04.17这也是在学校过的最后一个生日了 生日的时候cl给我买了个蛋糕，当天把蛋糕带到了东三和学弟们一起吃，wfy和cxq分别还带了小红包和零食，学弟们真的都很好。 四五月份的主要工作就是毕业设计了，我的题目是《基于机器学习的北半球积雪覆盖数据分析系统》，指导老师是lj，对于这个毕设我还是比较划水的，因为主要心思还是放在学习未来研究方向的基础，然后师兄偶尔也会给我派点小活，这个学期基本都在创客和学弟们吹水水，偶尔还会出去玩玩。 这段时间真的过的很快很快，从开题报告到中期答辩很快就到了终期答辩，老师让我和dyy去6.6的系级答辩，然后我俩都进了院级答辩，时间就在第二天6.7，最后我俩在院级答辩时被刷了，这下是真正毕业啦！ 答辩完后由于其他同学都是在6.12才小组答辩，我就先和对门的zsy出去拍了几张照片（偷穿他们电信院的学士服hhh）。 其实那段时间还挺欢乐的，并没有多想什么，很快就到了6.16，这是我们院集体拍毕业照的时候，早上我们班拍完后下午我们宿舍就约上zsy一起出去拍，还叫上了学弟们一起来拍，拍的过程还是很欢乐的，傍晚和nzx回红柳创客梦工厂把照片导出来，并且挑了一部分比较好的稍微修了一下（其实很多照片都很nice！），这时候看到这些照片中大家的笑容后心情就变得有些复杂了起来，可能这就是定格我们本科时候的最后一组照片了。 过了两天cl约上我和hxc出去用手机拍了几张，还叫上了dyy，到了6.20就是我们院拿双证的时候了，拿证时遇到了曾经一起搞数学建模的女同学qmh，她也拉着我还有身边的zaj一起在求是楼5楼很简单地拍了一张。 6.21一大早七点zaj就要走了，刚好我醒了，伸出头看了一眼，他说了句再见了兄弟们，然后就出门了，简简单单的告别，瞬间让我感觉到了太多的不舍，随后我和nzx去本部给成绩单盖章，我拉上了原专业的舍友也是简单地拍了几张合影，可惜福建老乡老早就润回家了，他考上了福大，剩下几个考上的学校都是在西安，还有一个找工作也是找在了西安那边，大家未来还有很多机会团建hhh。 江西舍友zyj在中午的时候也走了，我那时候还在本部，下午回来后，简单在宿舍和hxc随便聊了会，到了差不多六点，他也准备走了，我和他还有nzx一起去东门吃了顿“叫了只炸鸡”，吃完后我和nzx就送他上车了，还是很简单的告别，走了兄弟们，以后再见。 这晚回宿舍后就剩我一个人了，作为最后一个走的确实感觉到有很多的不舍，一地凌乱的垃圾，被搬空的床位，还有空荡荡的桌子… 6.21晚上几个学弟们把我叫出去通宵，先是去玩了剧本杀，然后去了一家酒吧喝到了早上六点，这一晚过的也很快乐，这段时间nlh动不动就说“易哥你延毕吧我舍不得你”，虽然说的时候感觉都是在说说笑笑，但是内心确实多少会有波澜，只是真的没办法阻挡时间的流逝。 早上回学校后头有点晕就去小睡了一会，起来后也差不多要到我要走的时候了，这时候对门zsy也还没走，突然过来和我说了一句“昨天几句很简单的话语不经意间就送走了好多兄弟”，我说没事以后还有机会再见的，他说“话都是这么说，实际上很难再完整地聚在一块了”。 这时候内心的心情真的很难受，看着宿舍，一幕幕往事瞬间浮现出来，确实，未来真的有机会再见吗？要几年后，五年还是十年？还是等到有人结婚？ 临走之前拍了一张宿舍的照片，还有对门zsy的宿舍，zsy刚好不在宿舍，就给他发了条简单的消息，真是造化弄人唉。 nzx送我去南门坐105路，上车后也是简单的告别，祝我路上注意安全，一路顺风。 在车上越想越难受，流了眼泪但是没滴下来，果然人多想就会陷入其中，不过最后还是得离开这边，再见了兰州。 写在最后 2019~2023，四年大学时光到此告一段落，并不后悔不远千里来到了兰州，当然并不是因为学校也不是因为兰州，是因为在这里遇到的朋友，勤奋上进还有趣的所有舍友们（hxc、zaj、zyj、twl）、严厉但是热心而且实力超强的dyy、外向热心但是说话攻击性极强的cl、说话亲切幽默还有实力的nzx、帅气的苏州富豪学霸zsy、班级唯一一个主动找我聊天的女生lr、幽默风趣动不动闯到我们宿舍来唠嗑的wh、帅气的全栈大佬lsl、在各个协会风生水起的铁兄弟nlh、经常送礼十分热情的cxq、物联网算法卷王且乐观努力的大佬wfy、气场强大的 OI 爷syh、整天修仙看妹妹的ycl、创客第一大帅哥ylq、… 虽然不善于交际，认识的人不算多，但是完全足够了，每一个朋友都值得作为一份永久的回忆，天下没有不散的筵席，大家最后都会各奔东西，为了各自的生活继续努力着，每个人都只是去到祖国最需要他们的地方了，但是希望未来真的还能有机会再见面，不管在多少年后。 如舍友hxc所说的，不怀念兰州，怀念兄弟们，怀念朋友们。"},{"title":"Hexo搭建Github博客教程","date":"2022-11-19T16:11:00.000Z","url":"/posts/47192.html","tags":[["Hexo","/tags/Hexo/"]],"categories":[["Hexo","/categories/Hexo/"]],"content":" 使用 Hexo 搭建我的 Github 个人网站：My Github Blog。 1. 环境配置 （1）安装 Git Bash：Windows 安装配置 Git 教程。 （2）安装 NodeJS：NodeJS 的安装及配置。 （3）修改 npm 镜像源： （4）安装 Hexo： （5）安装部署插件： 此处如果出现 npm ERR! Error: EPERM: operation not permitted, mkdir 'E:\\NodeJS\\node_modules\\.corepack-xTCBGLKh 之类的错误需要以管理员身份打开 cmd，然后在 cmd 中安装。 2. 本地博客搭建 首先创建文件夹 Hexo，然后进入该文件夹，创建文件夹 blog，使用管理员身份打开 cmd，进入 blog 文件夹，初始化 Hexo 博客： 然后在本地启动一下看看效果： 然后打开链接： 查看一下页面内容，之后我们进行页面调试都是在这个本地链接进行的。 使用 VS Code 打开 blog 文件夹，其中，source/_posts 文件夹下存放我们写的文章，themes 文件夹存放博客的主题，_config.yml 是博客的全局配置文件，_config.landscape.yml 是博客的主题配置文件。 3. 部署至Github 在 Github 创建一个名为 用户名.github.io 的仓库，例如：AsanoSaki.github.io 在 VS Code 中打开 blog 文件夹，找到 _config.yml 文件，找到 deploy，按照以下格式进行修改： 最后执行以下命令： 在部署的时候如果出现警告：LF will be replaced by CRLF the next time Git touches it，可以用以下指令禁用将 LF 自动转换为 CRLF： 然后访问域名：https://用户名.github.io/ 即可进入自己的博客啦。 4. 博客主题设置 Hexo 主题官网：Hexo Themes。 将下载好的主题放到 blog/themes 文件夹中，然后将根目录下的 _config.yml 中的主题修改为下载的主题即可，例如： 然后进入主题的文件夹，该文件夹下也有一个 _config.yml 文件，修改这个文件的内容即可修改当前博客主页的样式。 5. 博客备份 在 Github 新建一个名为 blog 的私有仓库，然后在 Hexo/blog 目录下打开 Git Bash，执行以下命令： 如果在执行 git add . 时出现警告：You've added another git repository inside your current repository.，可以将 blog 目录中的 .deploy_git 文件夹删除。"},{"title":"Web学习笔记-React（配置环境、ES6语法补充、Components）","date":"2022-11-18T10:00:00.000Z","url":"/posts/60453.html","tags":[["Web","/tags/Web/"]],"categories":[["Web","/categories/Web/"]],"content":" 本文记录 React 的学习过程，内容为配置环境、ES6 语法补充、Components。 React 是一个用于构建用户界面的库。React 不是一个框架，它的应用甚至不局限于 Web 开发，它可以与其他库一起使用以渲染到特定环境。 React 官网：React。 1. React配置环境 React 是一个声明式，高效且灵活的用于构建用户界面的 JavaScript 库。使用 React 可以将一些简短、独立的代码片段组合成复杂的 UI 界面，这些代码片段被称作 components。React 能够构建那些数据会随时间改变的大型应用。 React 特性： React 为了能够方便地去维护我们的页面，它在内存里面创建了一个虚拟的 DOM 树：Virtual DOM，这是一个轻量级的虚拟的 DOM，就是 React 抽象出来的一个对象，描述 DOM 应该什么样子的，应该如何呈现。通过这个 Virtual DOM 去更新真实的 DOM，由这个 Virtual DOM 管理真实 DOM 的更新。 数据驱动：当某一个元素里的数据发生变化后，React 会重新将有可能修改的元素都修改一遍，然后与真实的 DOM 树对比是否有区别，React 分析完后最终只会修改真实改变的结点。由于在内存里修改对象的速度很快，因此 React 效率很高。 React 一般不直接手写 JS，而是通过编写 JSX 文件，JSX 比 JS 更好写一点，React 会先将 JSX 编译成 JS。 （1）安装 Git Bash：Windows 安装配置 Git 教程。 （2）安装 NodeJS：NodeJS 的安装及配置。 （3）安装 create-react-app： 打开 Git bash，执行以下命令： 如果速度很慢，可以先修改镜像源再尝试安装： 如果安装完成后出现警告：npm WARN deprecated tar@2.2.2: This version of tar is no longer supported, and will not receive security updates. Please upgrade asap.，可以先更新 tar 试试： 如果还是有警告，且创建项目时（例如执行 create-react-app react-app）报错：bash: create-react-app: command not found，使用 npx 创建项目： 或者用 npm 创建项目： 创建好后进入项目文件夹启动项目： 启动后访问 localhost:3000 即可访问页面，ctrl+c 可停止服务。 （4）配置 VS Code 插件：Simple React Snippets、Prettier - Code formatter Simple React Snippets 为 React 智能化自动补全插件。 例如输入 imrc 即可补全出以下内容： 输入 cc 即可补全出以下内容： Prettier - Code formatter 为代码格式化插件，安装好后在 JSX 代码中通过 VS Code 一键格式化快捷键：ctrl + k + f 即可配置格式化插件。 （5）创建 React App： 在目标目录下右键打开 Git Bash，在终端中执行： 启动成功后会在本地开一个3000端口，页面效果已在上文展示。此时使用 VS Code 打开 react-app 文件夹可以看到项目的目录结构。 其中，node_modules 用来维护各种 JS 库，未来安装的所有依赖项都会放在该文件夹下；public 中的 index.html 就是我们未来渲染出的页面，该文件中只有一个 &lt;div id=&quot;root&quot;&gt;&lt;/div&gt;；src 中的 index.js 代码如下： 其中 App 的定义在 App.js 中： 该 App 组件就定义了页面的具体内容，且我们能够发现该 JS 文件中有 HTML 代码，因此该文件即为 JSX 文件，能够在 JavaScript 的基础上支持 XML（可扩展标记语言），HTML 也是一种特殊的 XML。 JSX 是 React 中的一种语言，会被 Babel 编译成标准的 JavaScript。 2. ES6语法补充 ES6，全称 ECMAScript 6.0，是 JavaScript 的版本标准。此处添加一些 React 中常用的语法糖。 （1）使用 bind() 函数绑定 this 取值 在 JavaScript 中，函数里的 this 指向的是执行时的调用者，而非定义时所在的对象。例如： 运行结果为： 使用 bind() 函数绑定 this 的取值为 person。例如： 运行结果为： （2）箭头函数的简写方式 当函数参数只有一个时可以将括号去掉，当函数体只有一个 return 语句时可以把 return 和 &#123;&#125; 一起去掉，例如： 等价于： （3）箭头函数不重新绑定 this 的取值 （4）对象的解构 （5）数组和对象的展开 （6）Named exports 与 Default exports Named Export：可以 export 多个，import 的时候需要加大括号，名称需要匹配，即之前使用的方式。 Default Export：最多 export 一个，import 的时候不需要加大括号，可以直接定义别名。 3. Components React 应用程序是由组件（Component）组成的。组件是一段可重用代码，一个组件是 UI（用户界面）的一部分，它拥有自己的逻辑和外观，用于渲染、管理和更新应用中的 UI 元素。组件可以小到一个按钮，也可以大到整个页面。 （1）创建项目 首先创建一个新项目 box-app： 安装 bootstrap 库： bootstrap 的引入方式： （2）创建 Component 在 src 文件夹中创建一个文件夹 components 存放组件，然后在 components 文件夹中创建一个 JSX 文件 box.jsx（使用 .js 后缀也一样，只是用 .jsx 后区分起来跟明显一点）其框架如下： 然后我们需要在 index.js 中将组件渲染出来： （3）创建按钮 由于 Component 中的 render() 函数只能 return 一个元素，因此当子节点数量大于1时，可以用 &lt;div&gt; 或 &lt;React.Fragment&gt; 将其括起来。 （4）内嵌表达式 JSX 中使用 &#123;&#125; 在 HTML 标签中嵌入表达式。 （5）设置属性 由于 class 是 JS 中的关键字，因此 HTML 标签中的 class 需要改为 className。CSS 属性也需要修改，例如：background-color 修改为 backgroundColor，其它属性也是类似的。 以上的综合示例： （6）数据驱动改变 Style 例如： （7）渲染列表 可以使用 map 函数渲染一个列表，每个元素需要具有唯一的 key 属性，用来帮助 React 快速找到被修改的 DOM 元素，例如： （8）Conditional Rendering 利用逻辑表达式的短路原则： 与表达式中 expr1 &amp;&amp; expr2，当 expr1 为假时返回 expr1 的值，否则返回 expr2 的值。 或表达式中 expr1 || expr2，当 expr1 为真时返回 expr1 的值，否则返回 expr2 的值。 （9）绑定事件 例如可以使用 onClick 绑定按钮的点击事件，注意需要妥善处理好绑定事件函数的 this，示例如下： （10）修改 state 需要使用 this.setState() 函数，每次调用 this.setState() 函数后，会自动重新调用 this.render() 函数，用来修改虚拟 DOM 树。React 只会修改不同步的实际 DOM 树节点。例如： （11）给事件函数添加参数 可以定义一个临时函数绑定事件，然后在该函数中调用原函数并传入参数，或者直接在绑定事件的时候用一个临时的箭头函数返回传入参数的原函数。例如： 上一章：Web学习笔记-JavaScript。 下一章：Web学习笔记-React（组合Components）。"},{"title":"Windows安装配置Git教程","date":"2022-11-18T03:45:00.000Z","url":"/posts/31252.html","tags":[["Others","/tags/Others/"]],"categories":[["Others","/categories/Others/"]],"content":" Windows 下安装 Git 以及配置与 Github 的远程连接。 图文博客前往：Windows 安装配置 Git 教程（2023.10.06 Git2.42.0）。 （1）首先前往 Git 官网，下载安装文件（64-bit Git for Windows Setup）。 （2）打开安装程序，设置好安装路径，然后点击下一步进入 Select Components 界面，可以选用默认设置，也可以勾上 Additional icons（桌面图标） 和 Add a Git Bash Profile to Windows Terminal。 （3）Select Start Menu Folder 页面直接点击下一步。 （4）Choosing default editor 页面用于选择编辑器，一般直接使用 Vim 即可。 （5）Adjusting the name of the ... 页面是设置 Git 初始化分支的名称，默认为 master（选择 Let Git decide），也可以选择自定义。 （6）Adjusting your PATH environment 页面选择使用 Git 的方式，通常选 Git from the command line and also from 3rd-party software，即既可以通过命令行也可以通过第三方软件使用 Git。 （7）Choosing the SSH executable 页面选择 SSH，第一个 Use bundled OpenSSH即可。 （8）Choosing HTTPS transport backend 页面选择 HTTPS 传输后端，第一个选项 Use the OpenSSL library 使用 OpenSSL 库，第二个选项使用本机 Windows 安全通道库，选第一个即可。 （9）Configuring the line ending conversions 页面配置结束行转换方式，也就是 Git 处理文本结束行的方式，Windows 选择第一个 Checkout Windows-style ... 即可。 （10）Configuring the terminal emulator ... 页面配置终端使用 Git Bash，第一个选项 Use MinTTY 是使用 MinTTY 作为终端模拟器，第二个选项是使用 Windows 的默认控制台，一般选择第一个。 （11）Choose the default behavior of git pull 页面选择 git pull 的默认行为，第一个选项 Default 是 git pull 的标准行为，尽可能快进当前分支到一个被捕获的分支，否则创建合并提交；第二个选项是将当前分支改为获取的分支。如果没有要重基的本地提交，这相当于快进；第三个选项是仅仅快进，快进到获取的分支，如果不可能，就失败。此处选择第一个选项即可。 （12）Choose a credential helper 页面选择 Git 凭证助手，选第一个 Git Credential Manager 即可。 （13）Configuring extra options 页面配置额外特性，第一个选项 Enable file system caching 是启用文件系统缓存，第二个选项是支持符号链接，勾上第一个即可。 （14）Configuring experimental options 页面设置实验特性，一般不用选，直接安装即可。 （15）安装好后在任意目录下点击鼠标右键应该能看到 Open Git Bash here 选项，我们可以通过该选项在某一目录中打开 Git Bash，然后我们打开后输入 git -v，即可看到 Git 的版本号。 在窗口顶部右键即可打开设置页面（Options），可以调整字体大小与窗口的默认大小。 （16）输入以下命令配置 Git： 例如： 生成公钥： 将 ~/.ssh/id_rsa.pub 中的内容复制到 GitHub 的 SSH Keys 中。 最后测试是否能连接上 GitHub： 结果如下说明配置成功： 如果出现报错提示：ssh: connect to host github.com port 22: Connection timed out，说明22端口可能被防火墙屏蔽了，可以尝试连接 GitHub 的443端口，我们将 ~/.ssh/config 文件修改成以下内容，这样 SSH 连接 GitHub 的时候就会使用443端口： "},{"title":"Web学习笔记-JavaScript","date":"2022-11-14T03:14:00.000Z","url":"/posts/40580.html","tags":[["Web","/tags/Web/"]],"categories":[["Web","/categories/Web/"]],"content":" 本文记录 JavaScript 的学习过程。 JavaScript 是一种具有函数优先的轻量级，解释型或即时编译型的编程语言。目前 JavaScript 的标准是 ECMAScript6，简称 ES6。 1. JS的调用方式与执行顺序 JS 常见使用方式有以下几种： 直接在 HTML 的 &lt;script type=&quot;module&quot;&gt;&lt;/script&gt; 标签内写 JS 代码。 直接引入 .js 文件：&lt;script type=&quot;module&quot; src=&quot;/static/js/index.js&quot;&gt;&lt;/script&gt;。 将所需的代码通过 import 关键字引入到当前作用域。 例如 /static/js/index.js 文件中的内容为： &lt;script type=&quot;module&quot;&gt;&lt;/script&gt; 中的内容为： 执行顺序： 类似于 HTML 与 CSS，按从上到下的顺序执行。 事件驱动执行。 HTML、CSS、JavaScript 三者之间的关系： CSS 控制 HTML； JavaScript 控制 HTML 与 CSS； 为了方便开发与维护，尽量按照上述顺序写代码。例如：不要在 HTML 中调用 JavaScript 中的函数。 2. 变量与运算符 （1）let 与 const：用来声明变量，作用范围为当前作用域。 let 用来定义变量。 const 用来定义常量。 例如： （2）变量类型： number：数值变量，例如：1, 2.5。 string：字符串，例如：&quot;acwing&quot;, 'AsanoSaki'，单引号与双引号均可。字符串中的每个字符为只读类型。 boolean：布尔值，例如：true, false。 object：对象，类似于 C++ 中的指针，例如：[1, 2, 3]，&#123; name: &quot;AsanoSaki&quot;, age: 18 &#125;，null。 undefined：未定义的变量。 类似于 Python，JavaScript 中的变量类型可以动态变化。 （3）运算符： 与 C++、Python、Java 类似，不同点： ** 表示乘方。 等于与不等于用 === 和 !==。 3. 输入与输出 （1）输入 从 HTML 与用户的交互中输入信息，例如通过 input、textarea 等标签获取用户的键盘输入，通过 click、hover 等事件获取用户的鼠标输入。 通过 Ajax 与 WebSocket 从服务器端获取输入。 标准输入。 （2）输出 调试用 console.log()，会将信息输出到浏览器控制台。 改变当前页面的 HTML 与 CSS。 通过 Ajax 与 WebSocket 将结果返回到服务器。 通过 HTML 输入输出示例： 通过标准输入输出示例： （3）格式化字符串 字符串中填入数值： 定义多行字符串： 保留两位小数： 4. 判断语句 JavaScript 中的 if-else 语句与 C++、Python、Java 中类似。例如： JavaScript 中的逻辑运算符也与 C++、Java 中类似：&amp;&amp; 表示与、|| 表示或、! 表示非。 5. 循环语句 JavaScript 中的循环语句与 C++ 中类似，也包含 for、while、do while 循环。 （1）for 循环 枚举对象或数组时可以使用： for-in 循环：可以枚举数组中的下标，以及对象中的 key。 for-of 循环：可以枚举数组中的值，以及对象中的 value。 例如： （2）while 循环 （3）do while 循环 do while 语句与 while 语句非常相似。唯一的区别是，do while 语句限制先循环体后检查条件。不管条件的值如何，我们都要至少执行一次循环体。 6. 对象 英文名称：Object。 类似于 C++ 中的 map，由 key:value 对构成。 value 可以是变量、数组、对象、函数等。 函数定义中的 this 用来引用该函数的“拥有者”。 例如： 对象属性与函数的调用方式： （1）用 . 调用：person.name、person.add_money()。 （2）用 [] 调用：person[&quot;name&quot;]、person[&quot;add_money&quot;]()。 7. 数组 数组是一种特殊的对象，类似于 C++ 中的数组，但是 JavaScript 数组中的元素类型可以不同（数组中的元素可以是变量、数组、对象、函数）。例如： 可以通过下标访问数组元素，例如： 数组的常用属性和函数： 属性 length：返回数组长度。注意 length 是属性，不是函数，因此调用的时候不要加 ()。 函数 push()：向数组末尾添加元素。 函数 pop()：删除数组末尾的元素。 函数 splice(a, b)：删除从下标 a 开始的 b 个元素。 函数 sort()：将整个数组从小到大排序。 自定义比较函数：array.sort(cmp)，函数 cmp 输入两个需要比较的元素，返回一个实数，负数表示第一个参数排在第二个参数前面，零表示相等，正数表示第一个参数排在第二个参数后面。因此如果要实现从大到小排序只需要令函数为：function(a, b) &#123; return b - a; &#125;。 8. 函数 JavaScript 中的函数是用对象来实现的，定义完函数后是允许再对这个对象进行修改的。函数的定义方式如下： 函数返回值：如果未定义返回值，则返回 undefined。 9. 类 与 C++ 中的 Class 类似，但是不存在私有成员，this 指向类的实例。 （1）定义 （2）继承 注意： super 这个关键字，既可以当作函数使用，也可以当作对象使用。 作为函数调用时，代表父类的构造函数，且只能用在子类的构造函数之中。 作为对象时，指向父类的原型对象。 在子类的构造函数中，只有调用 super 之后，才可以使用 this 关键字。 成员重名时，子类的成员会覆盖父类的成员，类似于 C++ 中的多态。 （3）静态方法 在成员函数前添加 static 关键字即可。静态方法可以被子类继承，但是不会被类的实例继承，只能通过类名来调用。例如： （4）静态变量 在 ES6 中，只能通过 class.propname 定义和访问，子类可以继承父类的静态变量，即可以通过子类名访问静态变量。例如： 10. 事件 JavaScript 的代码一般通过事件触发。可以通过 addEventListener 函数为元素绑定事件的触发函数。 常见的触发函数如下： （1）鼠标 click：鼠标左键点击。 dblclick：鼠标左键双击。 contextmenu：鼠标右键点击。 mousedown：鼠标按下，包括左键、滚轮、右键。 event.button：0表示左键，1表示中键，2表示右键。 mouseup：鼠标弹起，包括左键、滚轮、右键。 event.button：0表示左键，1表示中键，2表示右键。 例如： （2）键盘 keydown：某个键是否被按住，事件会连续触发。 event.code：返回按的是哪个键。 event.altKey、event.ctrlKey、event.shiftKey 分别表示是否同时按下了 alt、ctrl、shift 键。 keyup：某个按键是否被释放。 event 常用属性同上。 keypress：紧跟在 keydown 事件后触发，只有按下字符键时触发，适用于判定用户输入的字符。 event 常用属性同上。 keydown、keyup、keypress 的关系类似于鼠标的 mousedown、mouseup、click。 （3）表单 focus：聚焦某个元素。 blur：取消聚焦某个元素。 change：某个元素的内容发生了改变。 （4）窗口 需要作用到 window 元素上。 resize：当窗口大小放生变化。 scroll：滚动指定的元素。 load：当元素全部被加载完成。 11. 常用库 11.1 jQuery jQuery 能够让我们更加方便地去获取前端的某一个标签、绑定某个事件、改变前端的某个标签的 CSS 属性。 （1）下载地址：jQuery 官网。 （2）使用方式：在 &lt;head&gt; 元素中添加：&lt;script src=&quot;/Web Application Lesson/static/js/jquery-3.6.1.min.js&quot;&gt;&lt;/script&gt;。 （3）选择器 $(selector)，selector 类似于 CSS 的选择器。例如： （4）事件 $(selector).on(event, func) 绑定事件，例如： $(selector).off(event, func) 删除事件，例如： 当存在多个相同类型的事件触发函数时，可以通过 click.name 来区分，例如： 在事件触发的函数中的 return false 等价于同时执行： e.stopPropagation()：阻止事件向上传递。例如 a 是 div 的子标签，当点击 a 时同样会触发 div 的 click 事件，当在 a 的事件触发函数中加上该语句时点击 a 就不会触发 div 的 click 事件。 e.preventDefault()：阻止事件的默认行为。例如点击 a 时不打开链接，并向上传递触发 div 的 click 事件。 （5）元素的隐藏、展现 $A.hide()：隐藏，可以添加参数，表示消失时间（毫秒）。 $A.show()：展现，可以添加参数，表示出现时间。 $A.fadeOut()：颜色淡退至消失，可以添加参数，表示消失时间。 $A.fadeIn()：颜色淡增至出现，可以添加参数，表示出现时间。 （6）元素的添加、删除 $('&lt;div class=&quot;mydiv&quot;&gt;&lt;span&gt;Hello World&lt;/span&gt;&lt;/div&gt;')：构造一个 jQuery 对象。 $A.append($B)：将 $B 添加到 $A 的末尾。 $A.prepend($B)：将 $B 添加到 $A 的开头。 $A.remove()：删除元素 $A。 $A.empty()：清空元素 $A 的所有儿子。 （7）对类的操作（此处 class_name 无需加 .） $A.addClass(class_name)：添加某个类。 $A.removeClass(class_name)：删除某个类。 $A.hasClass(class_name)：判断某个类是否存在。 （8）对 CSS 的操作 $(&quot;div&quot;).css(&quot;background-color&quot;)：获取某个 CSS 的属性。 $(&quot;div&quot;).css(&quot;background-color&quot;, &quot;yellow&quot;)：设置某个 CSS 的属性。 同时设置多个 CSS 的属性（注意 JS 中带 - 的标签必须加引号，如果不加会被当做减号）： （9）对标签属性的操作（除 class、id 外可以随意创造新的属性，例如：&lt;div abc=&quot;abc&quot;&gt;&lt;/div&gt;） $('div').attr('id')：获取属性。 $('div').attr('id', 'ID')：设置属性。 （10）对 HTML 内容、文本的操作 不需要背每个标签该用哪种，用到的时候 Google 或者 Bing 即可。 $A.html()：获取、修改 HTML 内容（加参数即可修改），例如：&lt;div&gt;&lt;span&gt;span content&lt;/span&gt;&lt;/div&gt; 输出为 &lt;span&gt;span content&lt;/span&gt;。 $A.text()：获取、修改文本信息，例如：&lt;div&gt;&lt;span&gt;span content&lt;/span&gt;&lt;/div&gt; 输出为 span content。 $A.val()：获取、修改文本的值，一般用在 input、textarea 中。 （11）查找 $(selector).parent(filter)：查找父元素。 $(selector).parents(filter)：查找所有祖先元素。 $(selector).children(filter)：在所有子元素中查找。 $(selector).find(filter)：在所有后代元素中查找。 （12）ajax ajax 可以让我们在不刷新页面的情况下只从服务器端获取某些数据，一般是获取一个 json 数据。 GET 方法（从服务器端获取内容）： POST 方法（把表单内容提交给服务器）： 11.2 setTimeout与setInterval （1）setTimeout(func, delay) 经过 delay 毫秒后，执行函数 func()，可以使用 clearTimeout() 关闭定时器。例如： （2）setInterval(func, delay) 每隔 delay 毫秒，执行一次函数 func()，第一次在第 delay 毫秒后执行，可以使用 clearInterval() 关闭周期执行的函数。例如： 11.3 requestAnimationFrame requestAnimationFrame(func) 函数会在下次浏览器刷新页面之前执行一次，一般浏览器每秒刷新60次，因此通常会用递归写法使其每秒执行60次 func() 函数。调用时会向 func() 传入一个参数，表示函数执行的时间戳，单位为毫秒。例如： 使用 setTimeout 和 setInterval 实现以上效果的代码如下： 与 setTimeout 和 setInterval 的区别： requestAnimationFrame 渲染动画的效果更好，性能更佳。该函数可以保证每两次调用之间的时间间隔相同，但 setTimeout 与 setInterval 不能保证这点。setTmeout 两次调用之间的间隔包含回调函数的执行时间；setInterval 只能保证按固定时间间隔将回调函数压入栈中，但具体的执行时间间隔仍然受回调函数的执行时间影响。 当页面在后台时，因为页面不再渲染，因此 requestAnimationFrame 不再执行。但 setTimeout 与 setInterval 函数会继续执行。 11.4 Map与Set （1）Map Map 对象保存键值对。 用 for...of 或者 forEach 可以按插入顺序遍历。 键值可以为任意值，包括函数、对象或任意基本类型。 常用 API： set(key, value)：插入键值对，如果 key 已存在，则会覆盖原有的 value。 get(key)：查找关键字，如果不存在，返回 undefined。 size：返回键值对数量。 has(key)：返回是否包含关键字 key。 delete(key)：删除关键字 key。 clear()：删除所有元素。 例如： （2）Set Set 对象允许你存储任何类型的唯一值，无论是原始值或者是对象引用。 用 for...of 或者 forEach 可以按插入顺序遍历。 常用 API： add()：添加元素。 has()：返回是否包含某个元素。 size：返回元素数量。 delete()：删除某个元素。 clear()：删除所有元素 例如： 11.5 localStorage localStorage 可以在用户的浏览器上存储键值对。常用 API： setItem(key, value)：插入。 getItem(key)：查找。 removeItem(key)：删除。 clear()：清空。 例如： 11.6 JSON JSON 对象用于序列化对象、数组、数值、字符串、布尔值和 null。常用 API： JSON.parse()：将字符串解析成对象。 JSON.stringify()：将对象转化为字符串。 例如： 11.7 日期 （1）返回值为整数的 API，数值为 1970-1-1 00:00:00 UTC（世界标准时间）到某个时刻所经过的毫秒数： Date.now()：返回现在时刻。 Date.parse(&quot;2022-04-15T15:30:00.000+08:00&quot;)：返回北京时间2022年4月15日15:30:00的时刻。 （2）与 Date 对象的实例相关的 API： new Date()：返回现在时刻。 new Date(&quot;2022-04-15T15:30:00.000+08:00&quot;)：返回北京时间2022年4月15日15:30:00的时刻。 两个 Date 对象实例的差值为毫秒数。 getDay()：返回星期，0表示星期日，1-6表示星期一至星期六。 getDate()：返回日，数值为1-31。 getMonth()：返回月，数值为0-11。 getFullYear()：返回年份。 getHours()：返回小时。 getMinutes()：返回分钟。 getSeconds()：返回秒。 getMilliseconds()：返回毫秒。 例如： 11.8 WebSocket 与服务器建立全双工连接。常用 API： new WebSocket('ws://localhost:8080');：建立 WS 连接。 send()：向服务器端发送一个字符串。一般用 JSON 将传入的对象序列化为字符串。 onopen：类似于 onclick，当连接建立时触发。 onmessage：当从服务器端接收到消息时触发。 close()：关闭连接。 onclose：当连接关闭后触发。 11.9 window window.open(&quot;;)：在新标签栏中打开页面。 location.reload()：刷新页面。 location.href = &quot;;：在当前标签栏中打开页面。 11.10 Canvas Canvas 教程参考：Canvas Tutorial (English Version)、Canvas 教程（中文）。 上一章：Web学习笔记-CSS。 下一章：Web学习笔记-React（配置环境、ES6语法补充、Components）。"},{"title":"Web学习笔记-CSS","date":"2022-11-09T07:49:00.000Z","url":"/posts/11050.html","tags":[["Web","/tags/Web/"]],"categories":[["Web","/categories/Web/"]],"content":" 本文记录 CSS 的学习过程。 CSS（层叠样式表）是一种用来为结构化文档（如 HTML 文档或 XML 应用）添加样式（字体、间距和颜色等）的计算机语言，CSS 文件扩展名为：.css。 1. 样式定义方式 （1）行内样式表（inline style sheet） 直接定义在标签的 style 属性中，仅对当前标签产生影响。 （2）内部样式表（internal style sheet） 定义在 style 标签中，通过选择器影响对应的标签，可以对同一个页面中的多个标签产生影响。 （3）外部样式表（external style sheet） 定义在 .css 样式文件中，通过选择器影响对应的标签。可以用 link 标签引入某些页面，可以对多个页面产生影响。 首先在 /static/css 文件夹下创建 style.css 文件，将之前定义的样式代码移到该文件下： 然后在 .html 文件中用 link 链接该样式表即可： 2. 选择器 （1）标签选择器 例如选择所有 div 标签： （2）ID 选择器 例如选择 ID 为 rect_1 的标签： （3）类选择器 例如选择所有 rectangle 类的标签（注意：习惯上一个页面的 id 是唯一的，而 class 不是唯一的；且一个标签可以同时有多个 class，用空格隔开即可，多个 class 的效果根据 .css 文件中的定义顺序进行覆盖，后定义的覆盖先定义的样式）： （4）伪类选择器 伪类用于定义元素的特殊状态。 链接伪类选择器： :link：链接访问前的样式 :visited：链接访问后的样式 :hover：鼠标悬停时的样式 :active：鼠标点击后长按时的样式 :focus：聚焦后的样式 位置伪类选择器：:nth-child(n)：选择是其父标签第 n 个子元素的所有元素。 目标伪类选择器：:target：当 url 指向该元素时生效。 以上就是较为常用的选择器，现在来看一个综合示例，首先是 index.html 代码： style.css代码： （5）复合选择器 由两个及以上基础选择器组合而成的选择器。 element1, element2：同时选择元素 element1 和元素 element2。 element.class：选则包含某类的 element 元素。 element1 + element2：选择紧跟 element1 的 element2 元素。 element1 element2：选择 element1 内的所有 element2 元素。 element1 &gt; element2：选择父标签是 element1 的所有 element2 元素。 （6）通配符选择器 *：选择所有标签。 [attribute]：选择具有某个属性的所有标签。 [attribute=value]：选择 attribute 值为 value 的所有标签。 （7）伪元素选择器 将特定内容当做一个元素，选择这些元素的选择器被称为伪元素选择器。 ::first-letter：选择第一个字母。 ::first-line：选择第一行。 ::selection：选择已被选中的内容。 ::after：可以在元素后插入内容。 ::before：可以在元素前插入内容。 （8）样式渲染优先级 权重大小，越具体的选择器权重越大：!important &gt; 行内样式 &gt; ID 选择器 &gt; 类与伪类选择器 &gt; 标签选择器 &gt; 通用选择器。 权重相同时，后面的样式会覆盖前面的样式。 继承自父元素的权重最低。 3. 颜色 （1）预定义的颜色值 black、white、red、green、blue、lightblue 等。 （2）16进制表示法 使用6位16进制数表示颜色，例如：#ADD8E6。 其中第1-2位表示红色，第3-4位表示绿色，第5-6位表示蓝色。 简写方式：#ABC，等价于 #AABBCC。 （3）RGB 表示法 rgb(173, 216, 230)，其中第一个数表示红色，第二个数表示绿色，第三个数表示蓝色。 （4）RGBA 表示法 rgba(173, 216, 230, 0.5)，前三个数同上，第四个数表示透明度。 （5）取色方式 网页里的颜色，可以在 Chrome 浏览器的调试模式下获取 其他颜色可以使用 QQ 的截图软件：直接按 c 键，可以复制 RGB 颜色值；按住 shift 再按 c 键，可以复制16进制颜色值。 4. 文本 长度单位： px：设备上的像素点 %：相对于父元素的百分比 em：相对于当前元素的字体大小（倍） rem：相对于根元素的字体大小（倍） vw：相对于视窗宽度的百分比 vh：相对于视窗高度的百分比 （1）text-align text-align 属性定义行内内容（例如文字）如何相对它的块父元素对齐。text-align 并不控制块元素自己的对齐，只控制它的行内内容的对齐。 （2）line-height line-height 属性用于设置多行元素的空间量，如多行文本的间距。对于块级元素，它指定元素行盒（line boxes）的最小高度。对于非替代的 inline 元素，它用于计算行盒（line box）的高度。当 line-height 与 height 相等时可以让字体竖直居中。 （3）letter-spacing letter-spacing 属性用于设置文本字符的间距。 （4）text-indent text-indent 属性能定义一个块元素首行文本内容之前的缩进量。 （5）text-decoration text-decoration 属性是用于设置文本的修饰线外观的（下划线、上划线、贯穿线/删除线或闪烁）它是 text-decoration-line，text-decoration-color，text-decoration-style，和新出现的text-decoration-thickness 属性的缩写。 （6）text-shadow text-shadow 为文字添加阴影。可以为文字与 text-decorations 添加多个阴影，阴影值之间用逗号隔开。每个阴影值由(X方向的偏移量 Y方向的偏移量 模糊半径 颜色值)组成。 综合示例： 5. 字体 （1）font-size font-size 属性指定字体的大小。因为该属性的值会被用于计算 em 和 ex 长度单位，定义该值可能改变其他元素的大小。 （2）font-style font-style 属性允许你选择 font-family 字体下的 italic 或 oblique 样式。 （3）font-weight font-weight 属性指定了字体的粗细程度。一些字体只提供 normal 和 bold 两种值。 （4）font-family font-family 属性允许您通过给定一个有先后顺序的，由字体名或者字体族名组成的列表来为选定的元素设置字体。属性值用逗号隔开。浏览器会选择列表中第一个该计算机上有安装的字体，或者是通过 @font-face 指定的可以直接下载的字体。 综合示例： 6. 背景 （1）background-color background-color 属性会设置元素的背景色，属性的值为颜色值或关键字 transparent（透明）二者选其一。 （2）background-image background-image 属性用于为一个元素设置一个或者多个背景图像。渐变色：linear-gradient(rgba(0, 0, 255, 0.5), rgba(255, 255, 0, 0.5))。 （3）background-size background-size 属性设置背景图片大小。图片可以保有其原有的尺寸，或者拉伸到新的尺寸，或者在保持其原有比例的同时缩放到元素的可用空间的尺寸。 （4）background-repeat background-repeat 属性定义背景图像的重复方式。背景图像可以沿着水平轴，垂直轴，两个轴重复，或者根本不重复。 （5）background-position background-position 属性为背景图片设置初始位置。 （6）background-attachment background-attachment 属性决定背景图像的位置是在视口内固定，或者随着包含它的区块滚动。 综合示例： 7. 边框 （1）border-style border-style 属性用来设定元素所有边框的样式。其内容为：(border-top-style border-right-style border-bottom-style border-left-style)，之后的所有属性设置内容格式也如此。 （2）border-width border-width 属性用于设置元素的边框宽度。 （3）border-color border-color 属性用于设置元素四个边框的颜色。 （4）border-radius border-radius 属性允许你设置元素外边框的圆角。当使用一个半径时确定一个圆形，当使用两个半径时确定一个椭圆。这个（椭）圆与边框的交集形成圆角效果。 （5）border-collapse border-collapse 属性是用来决定表格的边框是分开的还是合并的。在分隔模式下，相邻的单元格都拥有独立的边框。在合并模式下，相邻单元格共享边框。 综合示例： 8. 元素展示格式 （1）display block：独占一行，width、height、margin、padding 均可控制，width 默认100%。例如 &lt;div&gt;。 inline：可以共占一行，width 与 height 无效，水平方向的 margin 与 padding 有效，竖直方向的 margin 与 padding 无效，width 默认为本身内容宽度。例如 &lt;span&gt;。 inline-block：可以共占一行，width、height、margin、padding 均可控制，width 默认为本身内容宽度。例如 &lt;img&gt;。 （2）white-space white-space 属性是用来设置如何处理元素中的空白。 （3）text-overflow text-overflow 属性确定如何向用户发出未显示的溢出内容信号。它可以被剪切，显示一个省略号或显示一个自定义字符串。 （4）overflow overflow 属性定义当一个元素的内容太大而无法适应块级格式化上下文的时候该做什么。它是 overflow-x 和 overflow-y 的简写属性。 9. 内边距与外边距 （1）margin margin 属性为给定元素设置所有四个（上下左右）方向的外边距属性。 可以接受1~4个值（上、右、下、左的顺序）。 可以分别指明四个方向：margin-top、margin-right、margin-bottom、margin-left。 可取值： length：固定值，例如：20px。 percentage：相对于包含块的宽度，以百分比值为外边距，例如：20%。 auto：让浏览器自己选择一个合适的外边距。有时，在一些特殊情况下，该值可以使元素居中。 外边距重叠： 块的上外边距 margin-top 和下外边距 margin-bottom 有时合并（折叠）为单个边距，其大小为单个边距的最大值（或如果它们相等，则仅为其中一个），这种行为称为边距折叠。 父元素与后代元素：父元素没有上边框和 padding 时，后代元素的 margin-top 会溢出，溢出后父元素的 margin-top 会与后代元素取最大值。 （2）padding padding 属性控制元素所有四条边的内边距区域。 可以接受1~4个值（上、右、下、左的顺序）。 可以分别指明四个方向：padding-top、padding-right、padding-bottom、padding-left。 可取值： length：固定值。 percentage：相对于包含块的宽度，以百分比值为内边距。 10. 盒子模型 box-sizing：定义了 user agent 应该如何计算一个元素的总宽度和总高度。 content-box：是默认值，设置 border 和 padding 均会增加元素的宽高。 border-box：设置 border 和 padding 不会改变元素的宽高，而是挤占内容区域。 11. 位置 position：用于指定一个元素在文档中的定位方式。 定位类型： 定位元素（positioned element）是 position 值为 relative、absolute、fixed 或 sticky 的元素。（换句话说，它是除 static 以外的任何东西）。 相对定位元素（relatively positioned element）是 position 值为 relative 的元素。top 和 bottom 属性指定相对其正常位置的垂直偏移量，left 和 right 属性指定水平偏移量。 绝对定位元素（absolutely positioned element）是 position 值为 absolute 或 fixed 的元素。 粘性定位元素（stickily positioned element）是 position 值为 sticky 的元素。 取值： static：该关键字指定元素使用正常的布局行为，即元素在文档常规流中当前的布局位置。此时 top、right、bottom、left 和 z-index 属性无效，其中 z-index 属性指定元素在Z轴上在第几层，也就是垂直于屏幕朝外的方向。 relative：该关键字下，元素先放置在未添加定位时的位置，然后在不改变页面布局的前提下调整元素位置（因此会在此元素未添加定位时所在位置即初始位置留下空白）。top、right、bottom、left 等调整元素相对于初始位置的偏移量。 absolute：元素会被移出正常文档流，并不为元素预留空间，通过指定元素相对于最近的非 static 定位祖先元素的偏移，来确定元素位置。绝对定位的元素可以设置外边距（margins），且不会与其他边距合并。 fixed：元素会被移出正常文档流，并不为元素预留空间，而是通过指定元素相对于屏幕视口（viewport）的位置来指定元素位置。元素的位置在屏幕滚动时不会改变。 sticky：元素根据正常文档流进行定位，然后相对它的最近滚动祖先（nearest scrolling ancestor）和 containing block（最近块级祖先，nearest block-level ancestor），包括 table-related 元素，基于 top、right、bottom 和 left 的值进行偏移。偏移值不会影响任何其他元素的位置。 综合示例： 12. 浮动 （1）float float 属性指定一个元素应沿其容器的左侧或右侧放置，允许文本和内联元素环绕它。该元素从网页的正常流动（文档流）中移除，尽管仍然保持部分的流动性（与绝对定位相反）。 由于 float 意味着使用块布局，它在某些情况下修改 display 值的计算值：display 为 inline 或 inline-block 时，使用 float 后会统一变成 block。 取值： left：表明元素必须浮动在其所在的块容器左侧的关键字。 right：表明元素必须浮动在其所在的块容器右侧的关键字。 （2）clear 有时，你可能想要强制元素移至任何浮动元素下方。比如说，你可能希望某个段落与浮动元素保持相邻的位置，但又希望这个段落从头开始强制独占一行。此时可以使用 clear。 取值： left：清除左侧浮动。 right：清除右侧浮动。 both：清除左右两侧浮动。 13. 中期实战 现在可以实现一个用户个人信息卡以及B站个人资料卡，限于篇幅就不放上代码了。 14. flex布局 flex 属性设置了 flex 项目如何增大或缩小以适应其 flex 容器中可用的空间。 （1）flex-direction flex-direction 属性指定了内部元素是如何在 flex 容器中布局的，定义了主轴的方向（正方向或反方向）。 取值： row：flex 容器的主轴被定义为与文本方向相同。主轴起点和主轴终点与内容方向相同。 row-reverse：表现和 row 相同，但是置换了主轴起点和主轴终点。 column：flex 容器的主轴和交叉轴相同。主轴起点与主轴终点和书写模式的前后点相同。 column-reverse：表现和 column 相同，但是置换了主轴起点和主轴终点。 （2）flex-wrap flex-wrap 属性指定 flex 元素单行显示还是多行显示。如果允许换行，这个属性允许你控制行的堆叠方向。 取值： nowrap：默认值，不换行。 wrap：换行，第一行在上方。 wrap-reverse：换行，第一行在下方。 （3）flex-flow flex-flow 属性是 flex-direction 和 flex-wrap 的简写。默认值为：row nowrap。 （4）justify-content justify-content 属性定义了浏览器如何沿 flex 容器的主轴和 grid 容器的内联轴分配内容项之间和周围的空间。 取值： flex-start：默认值，沿主轴起点方向对齐。 flex-end：沿主轴终点方向对齐。 start：如果主轴是 row 或 row-reverse，则左对齐，如果主轴是 column 或 column-reverse，则上对齐。 end：如果主轴是 row 或 row-reverse，则右对齐，如果主轴是 column 或 column-reverse，则下对齐。 space-between：沿主轴的两端对齐。 space-around：在主轴上均匀分配弹性元素。相邻元素间距离相同。第一个元素到主轴起始位置的距离和最后一个元素到主轴结束位置的距离将会是相邻元素之间距离的一半。 space-evenly：沿着主轴均匀分布在指定的对齐容器中。相邻元素之间的间距，主轴起始位置到第一个元素的间距，主轴结束位置到最后一个元素的间距，都完全一样。 center：容器中的元素居中，且元素之间紧贴没有空隙。 （5）align-items align-items 属性将所有直接子节点上的 align-self 值设置为一个组。align-self 属性设置项目在其包含块中在交叉轴方向上的对齐方式。 取值： flex-start：元素向交叉轴起点对齐。 flex-end：元素向交叉轴终点对齐。 center：元素在交叉轴居中。 stretch：元素在交叉轴方向被拉伸到与 flex 容器相同的高度或宽度（元素未被设定高度或宽度的前提下）。 （6）align-content align-content 属性设置了浏览器如何沿着 flex 布局的交叉轴和 grid 布局的主轴在内容项之间和周围分配空间。 取值： flex-start：所有行从交叉轴起点开始填充，第一行的交叉轴起点边和容器的交叉轴起点边对齐，接下来的每一行紧跟前一行中间没有空隙。 flex-end：所有行从交叉轴末尾开始填充，最后一行的交叉轴终点边和容器的交叉轴终点边对齐，同时所有后续行与前一行紧贴。 center：所有行朝向容器的交叉轴中心填充，每行互相紧挨，相对于容器居中对齐，容器的交叉轴起点边和第一行的距离相等于容器的交叉轴终点边和最后一行的距离。 stretch：拉伸所有行来填满剩余空间。剩余空间平均地分配给每一行。 （7）order order 属性定义 flex 项目的顺序，值越小越靠前。 （8）flex-grow flex-grow 属性设置 flex 容器主尺寸的 flex 增长系数，也就是 flex 容器中的元素随着容器尺寸的增大而增大（nowrap 前提下）。负值无效，默认为0。 （9）flex-shrink flex-shrink 属性指定了 flex 元素的收缩规则。flex 元素仅在默认宽度之和大于 flex 容器的时候才会发生收缩，其收缩的大小是依据 flex-shrink 的值。负值无效，默认为1。 （10）flex-basis flex-basis 属性指定了 flex 元素在主轴方向上的初始大小。取值可以是长度例如：100px，也可以是一个相对于其父 flex 容器主轴尺寸的百分比。不允许为负值。默认为 auto。 （11）flex flex-grow、flex-shrink、flex-basis 的缩写。 常用取值： auto：flex: 1 1 auto none：flex: 0 0 auto 综合样例： 15. 响应式布局 （1）media 查询：可以查询屏幕的各种信息，比如查询宽度，当屏幕宽度满足特定条件时应用某种 CSS。例如： （2）栅格系统：预先将屏幕宽度分为12份，然后设定各元素在不同的屏幕宽度下应该占几份，例如： （3）Bootstrap 根据上述例子可以发现如果自己手动实现 CSS 代码会很长，因此可以直接使用 Bootstrap，首先前往官网下载：Bootstrap 官网。然后在 static 中创建一个新文件夹 third_party（第三方），将下载好的 Bootstrap 放进来。在代码中引入 bootstrap.min.css 以及 bootstrap.min.js 后即可直接使用，例如： 此外，Bootstrap 还提供了很多设计好的组件，直接在官网中根据示例代码即可学习与使用，例如实现一个简单好看的表单： 上一章：Web学习笔记-HTML。 下一章：Web学习笔记-JavaScript。"},{"title":"Web学习笔记-HTML","date":"2022-11-08T04:05:00.000Z","url":"/posts/28433.html","tags":[["Web","/tags/Web/"]],"categories":[["Web","/categories/Web/"]],"content":" 本文记录 HTML 的学习过程。 MDN 官方文档：MDN Web Docs。 1. VS Code环境配置 （1）Live Server 由于一般写网站时都是部署在 Linux 上，该插件可以模拟一个终端，相当于模拟了一个真正的开发环境（后端）安装后在 VS Code 右下角即可看到 Go Live 按钮，在 HTML 文件中点击该按钮即可在本地打开一个页面。 （2）Auto Rename Tag 当修改 HTML 标签时，自动修改对应的标签对。 （3）自动格式化 在 Setting-Text Editor-Formatting 中勾选 Format On Save，这样保存代码的时候会自动格式化代码。 2. HTML基础标签 2.1 HTML文件结构 HTML 的所有标签为树形结构，一般都有一个开始标签和一个结束标签，开始标签和结束标签之间的标签就是子节点，同级的标签就是兄弟节点，例如： （1）&lt;html&gt; 标签 表示一个HTML文档的根（顶级元素），所以它也被称为根元素。所有其他元素必须是此元素的后代。 （2）&lt;head&gt; 标签 规定文档相关的配置信息（元数据），包括文档的标题，引用的文档样式和脚本等。 （3）&lt;body&gt; 标签 表示文档的内容。document.body 属性提供了可以轻松访问文档的 body 元素的脚本。 （4）&lt;title&gt; 标签 定义文档的标题，显示在浏览器的标题栏或标签页上。它只应该包含文本，若是包含有标签，则它包含的任何标签都将被忽略。 （5）&lt;meta&gt; 标签 表示那些不能由其它 HTML 元相关（meta-related）元素（&lt;base&gt;、&lt;link&gt;、&lt;script&gt;、&lt;style&gt; 或 &lt;title&gt;）之一表示的任何元数据信息。 常见属性： charset：这个属性声明了文档的字符编码。如果使用了这个属性，其值必须是与 ASCII 大小写无关（ASCII case-insensitive）的 utf-8。 name：name 和 content 属性可以一起使用，以 名 - 值 对的方式给文档提供元数据，其中 name 作为元数据的名称，content 作为元数据的值。 （6）link 标签 规定了当前文档与外部资源的关系。该元素最常用于链接样式表，此外也可以被用来创建站点图标 &lt;icon&gt;，例如： （7）&lt;!-- 多行注释 --&gt; HTML 中只有多行注释，没有单行注释。 综合示例如下： 2.2 文本标签 文本标签虽然很多，但大部分可看成是预定好样式的 &lt;div&gt; 和 &lt;span&gt;。 （1）&lt;div&gt; 标签 &lt;div&gt; 元素（或 HTML 文档分区元素）是一个通用型的流内容容器，在不使用 CSS 的情况下，其对内容或布局没有任何影响。其他块级标签例如：&lt;h1&gt;, &lt;p&gt;, &lt;pre&gt;, &lt;ul&gt;, &lt;ol&gt;, &lt;table&gt;。方便后续为某一块内容设置样式，且在 JS 中可以对各个 div 进行操作。即在逻辑上将某一块代码归为一类。 （2）&lt;span&gt; 标签 &lt;span&gt; 元素是短语内容的通用行内容器，并没有任何特殊语义。可以使用它来编组元素以达到某种样式意图（通过使用类 class 或者 id 属性），或者这些元素有着共同的属性，比如 lang。应该在没有其他合适的语义元素时才使用它。&lt;span&gt; 与 &lt;div&gt; 元素很相似，但 &lt;div&gt; 是一个块元素而 &lt;span&gt; 则是行内元素。其他内联标签例如：&lt;i&gt;, &lt;b&gt;, &lt;del&gt;, &lt;ins&gt;, &lt;td&gt;, &lt;a&gt;。 （3）&lt;h1&gt; - &lt;h6&gt; 标签 HTML 标题（Heading）元素呈现了六个不同的级别的标题，&lt;h1&gt; 级别最高，而 &lt;h6&gt; 级别最低。 （4）&lt;p&gt; 标签 &lt;p&gt; 元素（或者说 HTML 段落元素）表示文本的一个段落。该元素通常表现为一整块与相邻文本分离的文本，或以垂直的空白隔离或以首行缩进。另外，&lt;p&gt; 是块级元素。 （5）&lt;pre&gt; 标签 &lt;pre&gt; 元素表示预定义格式文本。在该元素中的文本通常按照原文件中的编排，以等宽字体的形式展现出来，文本中的空白符（比如空格和换行符）都会显示出来。（紧跟在 &lt;pre&gt; 开始标签后的换行符也会被省略） （6）&lt;br&gt; 标签 &lt;br&gt; 元素在文本中生成一个换行（回车）符号。此元素在写诗和地址时很有用，这些地方的换行都非常重要。 （7）&lt;hr&gt; 标签 &lt;hr&gt; 元素表示段落级元素之间的主题转换（例如，一个故事中的场景的改变，或一个章节的主题的改变）。 在 HTML 的早期版本中，它是一个水平线。现在它仍能在可视化浏览器中表现为水平线，但目前被定义为语义上的，而不是表现层面上。所以如果想画一条横线，请使用适当的 CSS 样式来修饰。 （8）&lt;i&gt; 标签 &lt;i&gt; 元素用于表现因某些原因需要区分普通文本的一系列文本。例如技术术语、外文短语或是小说中人物的思想活动等，它的内容通常以斜体显示。 （9）&lt;b&gt; 标签 HTML 提醒注意（Bring Attention To）元素 &lt;b&gt; 用于吸引读者的注意到该元素的内容上（如果没有另加特别强调）。这个元素过去被认为是粗体（Boldface）元素，并且大多数浏览器仍然将文字显示为粗体。尽管如此，你不应将 &lt;b&gt; 元素用于显示粗体文字；替代方案是使用 CSS 中的 font-weight 属性来创建粗体文字。 （10）&lt;del&gt; 标签 &lt;del&gt; 元素表示一些被从文档中删除的文字内容，显示效果为在文字内容中间划一道横线。比如可以在需要显示修改记录或者源代码差异的情况使用这个标签。&lt;ins&gt; 标签的作用恰恰于此相反：表示文档中添加的内容。 （11）&lt;ins&gt; 标签 &lt;ins&gt; 元素定义已经被插入文档中的文本，显示效果为在文字内容底部划一道横线。 2.3 图片 &lt;img&gt; 元素将一份图像嵌入文档。默认为行内元素，即 display: inline。 （1）src 属性 该属性是必须的，它包含了你想嵌入的图片的文件路径。 （2）alt 属性 该属性包含一条对图像的文本描述，这不是强制性的，但对可访问性而言，它十分有用。屏幕阅读器会将这些描述读给需要使用阅读器的使用者听，让他们知道图像的含义。或者如果由于某种原因无法加载图像，普通浏览器也会在页面上显示 alt 属性中的备用文本：例如，网络错误、内容被屏蔽或链接过期时。 （3）height 属性 图像的高度，在 HTML5 中的单位是 CSS 像素，在 HTML4 中既可以是像素也可以是百分比。可以只指定 width 和 height 中的一个值，浏览器会根据原始图像比例进行缩放。 （4）width 属性 图像的宽度，在 HTML5 中的单位是 CSS 像素，在 HTML4 中既可以是像素也可以是百分比。 2.4 音频与视频 （1）&lt;audio&gt; 标签 &lt;audio&gt; 元素用于在文档中嵌入音频内容。&lt;audio&gt; 元素可以包含一个或多个音频资源，这些音频资源可以使用 src 属性或者 &lt;source&gt; 元素来进行描述：浏览器将会选择最合适的一个来使用。也可以使用 MediaStream 将这个元素用于流式媒体。 使用 src 属性播放： &lt;audio&gt; 与多个 &lt;source&gt; 元素： 这个例子包含了多个 &lt;source&gt; 元素。如果能够播放的话，浏览器就会试图去加载第一个 source 元素；如果不行，那就退而求其次去加载第二个。 （2）&lt;video&gt; 标签 &lt;video&gt; 元素用于在 HTML 或者 XHTML 文档中嵌入媒体播放器，用于支持文档内的视频播放。你也可以将 &lt;video&gt; 标签用于音频内容，但是 &lt;audio&gt; 元素可能在用户体验上更合适。该标签的使用方法与 &lt;audio&gt; 相同。 2.5 超链接 &lt;a&gt; 元素（或称锚元素）可以通过它的 href 属性创建通向其他网页、文件、同一页面内的位置、电子邮件地址或任何其他 URL 的超链接。&lt;a&gt; 中的内容应该指明链接的意图。如果存在 href 属性，当 &lt;a&gt; 元素聚焦时按下回车键就会激活它。如果点击链接打开新标签页面需要加入属性：target=&quot;_blank&quot;。 2.6 表单 （1）&lt;form&gt; 标签 &lt;form&gt; 元素表示文档中的一个区域，此区域包含交互控件，用于向 Web 服务器提交信息。其常用的属性介绍如下： action：处理表单提交的 URL，就是想让表单提交到的地址。 method：浏览器提交表单使用的 HTTP 请求方式，若为 &quot;get&quot; 表单数据会附加在 action 属性的 URL 中，并以 '?' 作为分隔符；若为 &quot;post&quot; 表单数据会包含在表单体内然后发送给服务器。 （2）&lt;input&gt; 标签 &lt;input&gt; 元素表示一个用来填写内容的输入框，常见类型有： &lt;input type=&quot;text&quot;&gt;：创建基础的单行文本框。 &lt;input type=&quot;number&quot;&gt;：用于让用户输入一个数字。其包括内置验证以拒绝非数字输入。浏览器可能会选择提供步进箭头，让用户可以使用鼠标增加和减少输入的值，或者只需用指尖敲击即可。 &lt;input type=&quot;email&quot;&gt;：带有 email（电子邮箱）类型标记的输入框元素（&lt;input&gt;）能够让用户输入或编辑一个电子邮箱地址，此外，如果指定了 multiple 属性，用户还可以输入多个电子邮箱地址。在表单提交前，输入框会自动验证输入值是否是一个或多个合法的电子邮箱地址（非空值且符合电子邮箱地址格式）CSS 伪标签 :valid 和 :invalid 能够在校验后自动应用。 &lt;input type=&quot;password&quot;&gt;：&lt;input&gt; 元素里有一种叫做 password 的值，给我们一个方法让用户更加安全的输入密码。这个元素是作为一行纯文本编辑器控件呈现的，其中文本被遮蔽以致于无法读取，通常通过用诸如星号（*）或点（•）等符号替换每个字符来实现。这个符号会根据用户的浏览器和操作系统来具体显示哪个。 &lt;input type=&quot;radio&quot;&gt;：&lt;input&gt; 的 radio 类型元素默认渲染为小型圆圈图表，填充即为激活，类似于复选框（checkbox）类型。单选按钮允许你选择单一的值（value 属性）来提交表单。name 属性相同的为一组 radio，提交表单时的数据为 name=value 形式。 &lt;input type=&quot;checkbox&quot;&gt;：复选框，如果添加 checked 属性表示该复选框是否被默认选中（当页面加载时），当表单被提交时，只有当前被选中的复选框的 value 属性的值会被提交给服务器。 常用属性有： name：名称，提交表单时以 name=value 的形式提交 id：唯一ID maxlength：最大长度 minlength：最小长度 required：是否必填 placeholder：当表单控件为空时，控件中显示的内容 综合示例如下： （3）&lt;textarea&gt; 标签 &lt;textarea&gt; 元素表示一个多行纯文本编辑控件，当你希望用户输入一段相当长的、不限格式的文本，例如评论或反馈表单中的一段意见时，这很有用。参数 rows 指定初始的行数，cols 指定初始的列数。 （4）&lt;select&gt; 与 &lt;option&gt; 标签： &lt;select&gt; 元素表示一个提供选项菜单的控件。 （5）&lt;button&gt; 标签 &lt;button&gt; 元素表示一个可点击的按钮，可以用在表单或文档其它需要使用简单标准按钮的地方。默认情况下，HTML 按钮的显示样式接近于 user agent 所在的宿主系统平台（用户操作系统）的按钮，但你可以使用 CSS 来改变按钮的样貌。 2.7 列表 （1）&lt;ul&gt; 与 &lt;li&gt; 标签 &lt;ul&gt; 元素（或称 HTML 无序列表元素）表示一个内可含多个元素的无序列表或项目符号列表，通常渲染为一个用小点或者小圆圈表示的列表。 （2）&lt;ol&gt; 与 &lt;li&gt; 标签： &lt;ol&gt; 元素表示有序列表，通常渲染为一个带编号的列表。 （3）&lt;dl&gt;、&lt;dt&gt; 与 &lt;dd&gt; 标签 &lt;dl&gt; 元素（或 HTML 描述列表元素）是一个包含术语定义以及描述的列表，通常用于展示词汇表或者元数据（键-值对列表）。 2.8 表格 （1）&lt;table&gt; 标签 table 元素表示表格数据，即通过二维数据表表示的信息。 （2）&lt;thead&gt; 标签 &lt;thead&gt; 元素定义了一组定义表格的列头的行。 （3）&lt;tbody&gt; 标签 &lt;tbody&gt; 元素定义一组数据行。 （4）&lt;tr&gt; 标签 &lt;tr&gt; 元素定义表格中的行。同一行可同时出现 &lt;td&gt; 和 &lt;th&gt; 元素。 （5）&lt;th&gt; 标签 &lt;th&gt; 元素定义表格内的表头单元格。 （6）&lt;td&gt; 标签 &lt;td&gt; 元素定义了一个包含数据的表格单元格。 （7）&lt;caption&gt; 标签 &lt;caption&gt; 元素（or HTML 表格标题元素）展示一个表格的标题，它常常作为 &lt;table&gt; 的第一个子元素出现，同时显示在表格内容的最前面，但是，它同样可以被 CSS 样式化，所以，它同样可以出现在相对于表格的任意位置。 综合示例： 2.9 语义标签 （1）&lt;header&gt; &lt;header&gt; 元素用于展示介绍性内容，通常包含一组介绍性的或是辅助导航的实用元素。它可能包含一些标题元素，但也可能包含其他元素，比如 Logo、搜索框、作者名称，等等。 （2）&lt;nav&gt; &lt;nav&gt; 元素表示页面的一部分，其目的是在当前文档或其他文档中提供导航链接。导航部分的常见示例是菜单，目录和索引。 （3）&lt;section&gt; &lt;section&gt; 元素表示一个包含在 HTML 文档中的独立部分，它没有更具体的语义元素来表示，一般来说会有包含一个标题。 （4）&lt;figure&gt; &lt;figure&gt; 元素代表一段独立的内容，经常与说明（caption）&lt;figcaption&gt; 配合使用，并且作为一个独立的引用单元。当它属于主内容流（main flow）时，它的位置独立于主体。这个标签经常是在主文中引用的图片，插图，表格，代码段等等，当这部分转移到附录中或者其他页面时不会影响到主体。 （5）&lt;figcaption&gt; &lt;figcaption&gt; 元素是与其相关联的图片的说明/标题，用于描述其父节点 &lt;figure&gt; 元素里的其他数据。这意味着 &lt;figcaption&gt; 在 &lt;figure&gt; 块里是第一个或最后一个。同时 HTML Figcaption 元素是可选的；如果没有该元素，这个父节点的图片只是会没有说明/标题。 （6）&lt;article&gt; &lt;article&gt; 元素表示文档、页面、应用或网站中的独立结构，其意在成为可独立分配的或可复用的结构，例如它可能是论坛帖子、杂志或新闻文章、博客、用户提交的评论、交互式组件，或者其他独立的内容项目。 （7）&lt;aside&gt; &lt;aside&gt; 元素表示一个和其余页面内容几乎无关的部分，被认为是独立于该内容的一部分并且可以被单独的拆分出来而不会使整体受影响。其通常表现为侧边栏或者标注框（call-out boxes）。 （8）&lt;footer&gt; &lt;footer&gt; 元素表示最近一个章节内容或者根节点（sectioning root）元素的页脚。一个页脚通常包含该章节作者、版权数据或者与文档相关的链接等信息。 综合示例： 2.10 特殊符号 &amp;lt;：&lt;，小于号或显示标记； &amp;gt;：&gt;，大于号或显示标记； &amp;amp;：&amp;，可用于显示其它特殊字符； &amp;quot;：&quot;，引号； &amp;reg;：®，已注册； &amp;copy;：©，版权； &amp;trade;：™，商标； &amp;nbsp;：空格。 上一章：无。 下一章：Web学习笔记-CSS。"},{"title":"Linux学习笔记-管道、环境变量与Docker","date":"2022-09-28T07:14:00.000Z","url":"/posts/63179.html","tags":[["Linux","/tags/Linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 本文记录 Linux 的学习过程，内容为管道、环境变量与 Docker。 Docker 官网：Docker Hub。 1. 管道 （1）概念 管道类似于文件重定向，可以将前一个命令的 stdout 重定向到下一个命令的 stdin。 （2）要点 管道命令仅处理 stdout，会忽略 stderr。 管道右边的命令必须能接受 stdin。 多个管道命令可以串联。 （3）与文件重定向的区别 文件重定向左边为命令，右边为文件。 管道左右两边均为命令，左边有 stdout，右边有 stdin。 （4）举例 统计当前目录下所有 Python 文件的总行数，其中 find、xargs、wc 等命令可以参考：Linux学习笔记-命令、Tmux与Vim。 2. 环境变量 （1）概念 Linux 系统中会用很多环境变量来记录配置信息。 环境变量类似于全局变量，可以被各个进程访问到。我们可以通过修改环境变量来方便地修改系统配置。 （2）查看 列出当前环境下的所有环境变量： 输出某个环境变量的值： （3）修改 环境变量的定义、修改、删除操作可以参考Linux学习笔记-Shell这一节的内容。 为了将对环境变量的修改应用到未来所有环境下，可以将修改命令放到 ~/.bashrc 文件中。修改完 ~/.bashrc 文件后，需要执行 source ~/.bashrc，来将修改应用到当前的 bash 环境下。 为何将修改命令放到 ~/.bashrc，就可以确保修改会影响未来所有的环境呢？ 每次启动 bash，都会先执行 ~/.bashrc。 每次 ssh 登陆远程服务器，都会启动一个 bash 命令行给我们。 每次 tmux 新开一个 pane，都会启动一个 bash 命令行给我们。 所以未来所有新开的环境都会加载我们修改的内容。 （4）常见环境变量 HOME：用户的家目录。 PATH：可执行文件（命令）的存储路径。路径与路径之间用 : 分隔。当某个可执行文件同时出现在多个路径中时，会选择从左到右数第一个路径中的执行。下列所有存储路径的环境变量，均采用从左到右的优先顺序。 LD_LIBRARY_PATH：用于指定动态链接库（.so 文件）的路径，其内容是以冒号分隔的路径列表。 C_INCLUDE_PATH：C 语言的头文件路径，内容是以冒号分隔的路径列表。 CPLUS_INCLUDE_PATH：CPP 的头文件路径，内容是以冒号分隔的路径列表。 PYTHONPATH：Python 导入包的路径，内容是以冒号分隔的路径列表。 JAVA_HOME：JDK 的安装目录。 CLASSPATH：存放 Java 导入类的路径，内容是以冒号分隔的路径列表。 3. Docker 3.1 Docker安装 Ubuntu 系统 Docker 官网安装教程：Docker Install Docs。 本文安装 Docker 所使用的 OS 版本为：Ubuntu 22.04 (LTS)。依次执行以下命令安装 Docker： （1）更新 apt： （2）允许 apt 通过 HTTPS 使用存储库： （3）添加 Docker 的官方 GPG 密钥： （4）设置 repository： （5）安装 Docker Engine，首先更新 apt： （6）安装 Docker Engine、containerd、Docker Compose： （7）检查版本： 3.2 Docker教程 （1）将当前用户添加到 docker 用户组 为了避免每次使用 docker 命令都需要加上 sudo 权限，可以将当前用户加入安装中自动创建的 docker 用户组（可以参考官方文档）： 执行完此操作后，需要退出服务器（即关闭系统），再重新登录回来，才可以省去 sudo 权限。 重启 Docker 服务命令如下： 查看 Docker 运行状态： （2）镜像（images） 一个 Docker 中可以有很多镜像，镜像就相当于模板，每个镜像中又可以有很多容器。 用相同镜像生成的容器环境都一样，如果 Docker 安装在云服务器上，那么每个容器也就相当于是一个独立的云服务器。 迁移项目的时候即将容器先生成一个镜像，然后把镜像传到远程服务器上。 docker pull ubuntu:20.04 或 docker pull ubuntu:latest：拉取一个镜像。 docker images：列出本地所有镜像。 docker save -o ubuntu_latest.tar ubuntu:latest：将镜像 ubuntu:latest 导出到本地文件 ubuntu_latest.tar 中，导出后记得给文件加上可读权限：chmod +r ubuntu_latest.tar。 docker image rm ubuntu:latest 或 docker rmi ubuntu:latest：删除镜像 ubuntu:latest。 docker [container] commit CONTAINER IMAGE_NAME:TAG：创建某个 container 的镜像，[] 表示 container 为可选字段。 docker load -i ubuntu_latest.tar：将镜像 ubuntu:latest 从本地文件 ubuntu_latest.tar 中加载出来。 （3）容器（container） docker [container] create -it ubuntu:latest：利用镜像 ubuntu:latest 创建一个容器。 docker ps -a：查看本地的所有容器，docker ps 为查看运行中的容器。 docker [container] start CONTAINER：启动容器，CONTAINER 可以是 ID 或 NAMES。 docker [container] stop CONTAINER：停止容器。 docker [container] restart CONTAINER：重启容器。 docker [contaienr] run -itd ubuntu:latest：创建并启动一个容器，可以加上参数 -p 20000:22 表示将容器的22端口映射到本地的20000端口，因为本地的22端口已经被占用了，且如果是在云服务器安装 Docker 还需要修改云服务器安全组配置，把20000端口放行。 docker [container] attach CONTAINER：进入容器。 先按 Ctrl+p，再按 Ctrl+q 可以挂起容器，即退出但不关闭容器。 按 Ctrl+d 可以退出并关闭容器。 docker [container] exec CONTAINER COMMAND：在容器中执行 COMMAND 命令。 docker [container] rm CONTAINER：删除容器。 docker container prune：删除所有已停止的容器。 docker export -o xxx.tar CONTAINER：将容器导出到本地文件 xxx.tar 中。 docker import xxx.tar image_name:tag：将本地文件 xxx.tar 导入成镜像，并将镜像命名为 image_name:tag。 docker export/import 与 docker save/load 的区别： export/import 会丢弃历史记录和元数据信息，仅保存容器当时的快照状态。 save/load 会保存完整记录，体积更大。 docker top CONTAINER：查看某个容器内的所有进程。 docker stats：查看所有容器的统计信息，包括 CPU、内存、存储、网络等信息。 docker cp xxx CONTAINER:xxx 或 docker cp CONTAINER:xxx xxx：在本地和容器间复制文件。 docker rename NAMES1 NAMES2：将 NAMES1 容器重命名为 NAMES2。 docker update CONTAINER --memory 500MB：修改容器限制，更多修改内容可以在官网查找。 进入容器后输入 passwd 可以设置 root 密码。 （4）云服务器配置示例 首先在 AC Terminal 中操作： 然后去云平台控制台中修改安全组配置，放行端口：20000。 返回 AC Terminal，即可通过 SSH 登录自己的 Docker 容器： 创建 acs 用户： 最后可以配置 Docker 容器的别名和免密登录。 Tips：如果 apt-get 下载软件速度较慢，可以参考清华大学开源软件镜像站中的内容，修改软件源。 （5）数据迁移 如果想保留 Docker 数据库里的数据，需要用 save 和 laod 迁移。 首先将容器打包成镜像： 导出镜像： 将导出的镜像文件传到目标主机上，然后导入镜像： 查看镜像： 生成一个新的容器： 3.3 Docker的代理地址是什么？ 在 Docker 中，代理地址是用于帮助 Docker 服务和容器访问网络的服务器地址。如果你的网络环境需要通过代理服务器来访问互联网，那么你需要为 Docker 配置代理服务器的地址。这个地址通常由你的网络管理员或者互联网服务提供商提供。 Docker 的代理配置主要有以下几种情况： 为 Docker Daemon 配置代理：这主要是为了支持 docker pull 等需要访问公网的 Docker 命令。你可以在 Docker 的守护进程的环境变量中设置代理。 为 Docker 容器配置代理：这主要是为了让运行在 Docker 容器中的应用能够通过代理访问网络。你可以在启动容器时，通过 docker run 命令的 -e 参数来设置代理。 为 Docker Build 配置代理：这主要是为了在构建 Docker 镜像时，让 docker build 命令能够通过代理下载依赖的文件。你可以在执行 docker build 命令时，通过 --build-arg 参数来设置代理。 具体的代理地址和如何获取，需要咨询你的网络管理员或者互联网服务提供商。如果你是在公司或者学校的网络环境下，可能需要通过内部的代理服务器来访问互联网。如果你是在家庭网络环境下，一般来说不需要配置代理，除非你的互联网服务提供商要求你这么做。如果你在中国大陆地区，由于网络封锁的原因，你可能需要配置代理服务器才能访问一些国外的网站。 上一章：Linux学习笔记-Thrift。 下一章：无。"},{"title":"Linux学习笔记-Thrift","date":"2022-09-26T06:24:00.000Z","url":"/posts/20905.html","tags":[["Linux","/tags/Linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 本文记录 Linux 的学习过程，内容为 RPC 软件框架：Thrift。 Thrift 官网：Apache Thrift。 1. Thrift概述 1.1 基本概念 Thrift 是一个 RPC（远程过程调用协议 Remote Procedure Call Protocol）软件框架，用来进行可扩展且跨语言的服务的开发。它结合了功能强大的软件堆栈和代码生成引擎，以构建在 C++、Java、Go、Python、PHP、Ruby、Erlang、Perl、Haskell、C#、Cocoa、JavaScript、Node.js、Smalltalk、OCaml 这些编程语言间无缝结合的、高效的服务。Thrift 允许定义一个简单的定义文件中的数据类型和服务接口，以作为输入文件，编译器生成代码用来方便地生成 RPC 客户端和服务器通信的无缝跨编程语言。 1.2 Thrift IDL Thrift 采用接口定义语言 IDL（Interface Definition Language）来定义通用的服务接口，然后通过 Thrift 提供的编译器，可以将服务接口编译成不同语言编写的代码，通过这个方式来实现跨语言的功能。 通过命令调用 Thrift 提供的编译器将服务接口编译成不同语言编写的代码。 这些代码又分为服务端和客户端，将所在不同进程（或服务器）的功能连接起来。 1.3 如何创建Thrift服务？ 定义服务接口（存放接口的文件夹就是 Thrift 文件）。 作为服务端的服务，需要生成 server。 作为请求端的服务，需要生成 client。 1.4 实例讲解 假设我们要实现一个游戏的匹配系统，这个游戏的功能可能运行在一个或多个服务器（进程）上，而 Thrift 就是将不同服务器不同语言的功能连接起来。 游戏本体（假设用 Python 实现）、匹配系统（假设用 C++ 实现）、数据存储服务器这三个节点（功能）是完全独立的，既可以在同一个服务器上，也可以在不同服务器上。每一个节点就是一个进程，每个进程可以使用不同的语言来实现。 游戏节点到匹配节点需要实现一条有向边（可以包含多个函数），表示向匹配系统添加和移除玩家 add_user、remove_user，因此游戏节点需要实现 match_client 端，表示可以调用匹配服务器的函数；匹配系统需要实现 match_server 端，表示可以让游戏节点的 Client 端调用自身的函数。同时匹配系统还需实现 save_client 端，因为需要将数据传给服务器存储 save_data（假设数据存储服务器已实现 save_server 端）。 2. Thrift教程 首先创建一个游戏系统文件夹 game、匹配系统文件夹 match_system、保存各种接口的文件夹 thrift。 2.1 match_server框架 在 thrift 文件夹中创建一个文件：match.thrift，内容如下： 前往 Thrift 官网，点击 Tutorial，再点击 C++，即可看到如何通过这个接口生成一个 C++ 版本的服务器。命令如下： 在 match_system 文件夹中创建一个文件夹 src，表示源文件。在 src 文件夹中输入以下命令： 执行后会发现该目录下生成了一个 gen-cpp 的文件夹，为了后续方便操作，将文件夹改个名： 将自动实现好的文件移出来： 由于该文件里的函数还没有进行逻辑实现，因此先在每个函数中加上 return 0; 后编译一遍，文件内容如下（可以使用 gg=G 进行格式化）： 接下来进行编译链接，链接的时候需要用到 Thrift 的动态链接库，需要加上 -lthrift： 这时输入 ./main 即可运行程序，但是此时什么内容都没有。Thrift 只是将接口实现好了，具体的业务逻辑没有实现。我们可以先将文件上传至 Git，上传的时候注意一般不将 .o 文件和可执行文件上传： 2.2 match_client框架与实现 首先同样在 game 文件夹中创建 src 文件夹，进入 src 文件夹后我们需要生成 Python 代码： 生成后该目录下有个文件夹 gen-py，也就是生成了 Python 的服务器端，同样将其改个名： 创建文件 client.py，将官网中 Python 客户端的代码（前四行是为了将当前路径加入到 Python 的环境变量中，可以删掉）复制过来，并进行简单的修改： 然后我们将 match_system/src 中的 main 执行后，再执行 game/src 中的 client.py： 可以看到 main 程序那边输出：add_user。说明我们的 match_client 端和 match_server 端已经初步实现了，此时更新一下 Git，注意 .pyc 文件也最好不要上传： 接着我们进行优化，从控制台输入用户信息，并指定是添加还是删除用户，修改后的 client.py 代码如下： 这样我们的 match_client 端就算是完成了。 2.3 match_server_v2.0实现 由于 server 端一方面需要读入或者移出用户，另一方面还要不断地去匹配，因此需要有一个线程去不断添加用户进来，一个线程去进行匹配，匹配完后再将信息传给一个服务器，且这两个操作是完全独立的，有可能长时间没有用户添加进来，但是匹配系统能够匹配两个已经匹配了很久的人。因此在这里需要用到并行技术，C++ 多线程需要使用到 &lt;thread&gt; 头文件。 多线程相关知识点： IP 和端口：如果把 IP 地址比作一间房子，端口就是出入这间房子的门。真正的房子只有几个门，但是一个 IP 地址的端口可以有65536个之多！端口是通过端口号来标记的，端口号只有整数，范围是从0到65535。同一个端口只能由一个进程来监听。所以我们一旦启动了一个服务，那么这个服务就不能在被另一个进程启动了。服务器的端口号要与客户端的端口号相同。 &lt;thread&gt; 库：C++ 中有一个 thread 的库，可以用来开线程。通过定义一个变量将函数名作为参数，就能开一个线程了，具体使用可以看后文代码。 首先定义线程的操作：并行中经典的生产者和消费者模型。生产者、消费者是两个线程。本样例中的生产者：add_user()、remove_user()；消费者：匹配用户的功能。 生产者和消费者之间需要一个媒介。这个媒介可以有很多种方法。比如：消费队列。很多语言都有自己实现的消费队列，也可以自己实现消费队列。实现消费队列，就需要用到一些锁（Mutex）。锁是并行编程的基本概念。 互斥锁：在编程中，引入了对象互斥锁的概念，来保证共享数据操作的完整性。每个对象都对应于一个可称为“互斥锁”的标记，这个标记用来保证在任一时刻，只能有一个线程访问该对象。 锁有两个操作：一个 P 操作（上锁），一个 V 操作（解锁）。 定义互斥锁：mutex m。锁一般使用信号量来实现的，mutex 其实就是一个信号量（它特殊也叫互斥量）。互斥量就是同一时间能够分给一个人，即 S = 1。信号量 S = 10 表示可以将信号量分给10个人来用。 P 操作的主要动作是： （1）S - 1； （2）若 S - 1 后仍大于或等于0，则进程继续执行； （3）若 S - 1 后小于0，则该进程被阻塞后放入等待该信号量的等待队列中，然后转进程调度。 V 操作的主要动作是： （1）S + 1； （2）若 S + 1 后结果大于0，则进程继续执行； （3）若 S + 1 后结果小于或等于0，则从该信号的等待队列中释放一个等待进程，然后再返回原进程继续执行或转进程调度。 对于 P 和 V 都是原子操作，就是在执行 P 和 V 操作时，不会被插队，从而实现对共享变量操作的原子性。 特殊：S = 1 表示互斥量，表示同一时间，信号量只能分配给一个线程。 多线程为啥要用锁？因为多线程可能共享一个内存空间，导致出现重复读取并修改的现象。 我们将程序功能修改为傻瓜式匹配，只要匹配池中的玩家数大于等于2，那么就将前两名玩家进行匹配，修改后的 match_server 端 main.cpp 代码如下： 由于使用了线程库，因此编译的时候需要加上参数 -pthread： 此时可以打开 match_server 端和 match_client 端，然后在 client 端添加玩家看看 server 端的匹配结果。 2.4 save_client实现 假设 save_server 端已经实现，在 thrift 文件夹中创建文件 save.thrift，内容如下： 可以看到直接调用 save_server 端的接口函数 save_data 即可（该函数的实现我们不关心）。 在 match_system/src 文件夹中输入以下指令生成接口的 C++ 实现，并重命名，然后需要将自动生成的服务端代码删去： 接下来我们将 Thrift 官网 C++ 教程中的 Client 端代码抄下来并相应进行修改，修改后的 main.cpp 内容如下： 2.5 match_server_v3.0实现 通过修改匹配函数 match() 实现将分差小于等于50的玩家进行匹配，修改后的代码如下： 2.6 match_server_v4.0实现 通过 Thrift 官网 C++ 教程下的 Server 端代码可以将 match_server 改为多线程，修改后的 main.cpp 代码如下： 2.7 match_server_v5.0实现 通过对匹配机制的修改，实现玩家每等待一秒钟，匹配的分数区间扩大50分，修改后的 main.cpp 代码如下： 上一章：Linux学习笔记-SSH与Git。 下一章：Linux学习笔记-管道、环境变量与Docker。"},{"title":"Linux学习笔记-SSH与Git","date":"2022-04-30T05:56:00.000Z","url":"/posts/20491.html","tags":[["Linux","/tags/Linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 本文记录 Linux 的学习过程，内容为 SSH 与 Git。 1. SSH 1.1 SSH登录 （1）基本用法： 如果是 Ubuntu 的裸环境需要先安装 SSH： 安装完成后，SSH 服务默认自动启动，你可以通过以下命令校验服务运行状态： 重启 SSH 服务： 远程登录服务器： 第一次登录时会提示： 输入 yes，然后回车即可。这样会将该服务器的信息记录在 ~/.ssh/known_hosts 文件中。然后输入密码即可登录到远程服务器中。 默认登录端口号为 22。如果想登录某一特定端口可以加参数 -p： （2）配置文件： 创建文件 ~/.ssh/config。 然后在文件中输入： 例如： 之后再使用服务器时，可以直接使用别名 myserver1、myserver2。 （3）配置公钥免密登录： 创建密钥： 然后一直回车即可，执行结束后，在 ~/.ssh/ 目录下会多两个文件： 之后想免密码登录哪个服务器，就将公钥传给哪个服务器即可。 例如，想免密登录 myserver 服务器。则将公钥中的内容，复制到 myserver 中的 ~/.ssh/authorized_keys 文件里即可。 也可以使用如下命令一键添加公钥： （4）执行命令： 命令格式： 例如： 或者： 或者： 1.2 SCP远程拷贝文件 命令格式： 功能：将 source 路径下的文件复制到 destination 中。 一次复制多个文件： 复制文件夹（将本地家目录中的 tmp 文件夹复制到 myserver 服务器中的 /home/acs/ 目录下）： 将本地家目录中的 tmp 文件夹复制到 myserver 服务器中的 ~/homework/ 目录下： 将 myserver 服务器中的 ~/homework/ 文件夹复制到本地的当前路径下： 指定服务器的端口号： 注意：scp 的 -r -P 等参数尽量加在 source 和 destination 之前。 使用 scp 配置其他服务器的 vim 和 tmux： 2. Git 2.1 Git基本概念 工作区：仓库的目录。工作区是独立于各个分支的。 暂存区：数据暂时存放的区域，类似于工作区写入版本库前的缓存区。暂存区是独立于各个分支的。 版本库：存放所有已经提交到本地仓库的代码版本 版本结构：树结构，树中每个节点代表一个代码版本。 2.2 Git常用命令 git config --global user.name xxx：设置全局用户名，信息记录在 ~/.gitconfig 文件中。 git config --global user.email xxx@xxx.com：设置全局邮箱地址，信息记录在 ~/.gitconfig 文件中。 git init：将当前目录配置成 Git 仓库，信息记录在隐藏的 .git 文件夹中。 git add XX：将 XX 文件添加到暂存区。 git add .：将所有待加入暂存区的文件加入暂存区。 git rm --cached XX：将文件从仓库索引目录中删掉。 git commit -m &quot;给自己看的备注信息&quot;：将暂存区的内容提交到当前分支。 git status：查看仓库状态。 git diff XX：查看 XX 文件相对于暂存区修改了哪些内容。 git log：查看当前分支的所有版本。 git log --pretty=oneline：每个版本用一行显示。 git reflog：查看HEAD指针的移动历史（包括被回滚的版本）。 git reset --hard HEAD^ 或 git reset --hard HEAD~：将代码库回滚到上一个版本。 git reset --hard HEAD^^：往上回滚两次，以此类推。 git reset --hard HEAD~100：往上回滚100个版本。 git reset --hard 版本号：回滚到某一特定版本。 git checkout -- XX 或 git restore XX：将 XX 文件尚未加入暂存区的修改全部撤销。 git restore --staged XX：将 XX 文件从暂存区撤出，不会更改文件的内容。 git remote add origin git@git.acwing.com:xxx/XXX.git：将本地仓库关联到远程仓库。 git remote -v：查看当前 Git 仓库有没有关联远程仓库，如果已经有关联则会显示具体远程仓库路径，如果没有返回，说明没有关联任何远程仓库。 git remote rm XXX：解除与远程仓库的关联，例如：git remote rm origin。 git push -u (第一次需要-u以后不需要)：将当前分支推送到远程仓库。 git push origin branch_name：将本地的某个分支推送到远程仓库。 git clone git@git.acwing.com:xxx/XXX.git：将远程仓库 XXX 下载到当前目录下。 git checkout -b branch_name：创建并切换到 branch_name 这个分支。 git branch：查看所有分支和当前所处分支。 git checkout branch_name：切换到 branch_name 这个分支。 git merge branch_name：将分支 branch_name 合并到当前分支上。 git branch -d branch_name：删除本地仓库的 branch_name 分支。 git branch branch_name：创建新分支 branch_name。 git push --set-upstream origin branch_name：设置本地的 branch_name 分支对应远程仓库的 branch_name 分支。 git push -d origin branch_name：删除远程仓库的 branch_name 分支。 git pull：将远程仓库的当前分支与本地仓库的当前分支合并。 git pull origin branch_name：将远程仓库的 branch_name 分支与本地仓库的当前分支合并。 git branch --set-upstream-to=origin/branch_name1 branch_name2：将远程的 branch_name1 分支与本地的 branch_name2 分支对应。 git checkout -t origin/branch_name：将远程的 branch_name 分支拉取到本地。 git stash：将工作区和暂存区中尚未提交的修改存入栈中。 git stash apply：将栈顶存储的修改恢复到当前分支，但不删除栈顶元素。 git stash drop：删除栈顶存储的修改。 git stash pop：将栈顶存储的修改恢复到当前分支，同时删除栈顶元素。 git stash list：查看栈中所有元素。 上一章：Linux学习笔记-Shell。 下一章：Linux学习笔记-Thrift。"},{"title":"Linux学习笔记-Shell","date":"2022-03-17T01:39:00.000Z","url":"/posts/31281.html","tags":[["Linux","/tags/Linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 本文记录 Linux 的学习过程，内容为 Shell 命令语言。 1. Shell 概论 Shell 是我们通过命令行与操作系统沟通的语言。 Shell 脚本可以直接在命令行中执行，也可以将一套逻辑组织成一个文件，方便复用。 Linux 中常见的 Shell 脚本有很多种，常见的有： Bourne Shell（/usr/bin/sh 或 /bin/sh） Bourne Again Shell（/bin/bash） C Shell（/usr/bin/csh） K Shell（/usr/bin/ksh） zsh … Linux 系统中一般默认使用 bash，所以接下来讲解 bash 中的语法。 文件开头需要写 #! /bin/bash，指明 bash 为脚本解释器。 新建一个 test.sh 文件，内容如下： 运行方式： （1）作为可执行文件 （2）用解释器执行 2. 注释 （1）单行注释 每行中 # 之后的内容均是注释： （2）多行注释 其中 EOF 可以换成其它任意字符串，例如： 3. 变量 3.1 定义变量 定义变量不需要加 $ 符号，例如： 3.2 使用变量 使用变量，需要加上 $ 符号，或者 $&#123;&#125; 符号。花括号是可选的，主要为了帮助解释器识别变量边界。 3.3 只读变量 使用 readonly 或者 declare 可以将变量变为只读。 3.4 删除变量 unset 可以删除变量。 3.5 变量类型 自定义变量（局部变量）：子进程不能访问的变量。 环境变量（全局变量）：子进程可以访问的变量。 自定义变量改成环境变量： 环境变量改为自定义变量： 3.6 字符串 字符串可以用单引号，也可以用双引号，也可以不用引号，不用引号与双引号是一样的。 单引号与双引号的区别： 单引号中的内容会原样输出，不会执行、不会取变量。 双引号中的内容可以执行、可以取变量。 获取字符串长度： 提取子串： 4. 默认变量 4.1 文件参数变量 在执行 Shell 脚本时，可以向脚本传递参数。$1 是第一个参数，$2 是第二个参数，以此类推。特殊的，$0 是文件名（包含路径）。例如： 创建文件 test.sh： 然后执行该脚本： 4.2 其它参数相关变量 $#：代表文件传入的参数个数，如上例中值为4。 $*：由所有参数构成的用空格隔开的字符串，如上例中值为 &quot;$1 $2 $3 $4&quot;。 $@：每个参数分别用双引号括起来的字符串，如上例中值为 &quot;$1&quot; &quot;$2&quot; &quot;$3&quot; &quot;$4&quot;。 $$：脚本当前运行的进程 ID。 $?：上一条命令的退出状态（注意不是 stdout，而是 exit code）。0表示正常退出，其他值表示错误。 $(command)：返回 command 这条命令的 stdout（可嵌套） `command`：返回 command 这条命令的 stdout（不可嵌套）。注意是 ~ 下面的那个点号。 5. 数组 数组中可以存放多个不同类型的值，只支持一维数组，初始化时不需要指明数组大小，数组下标从0开始。 5.1 数组定义 数组用小括号表示，元素之间用空格隔开。例如： 也可以直接定义数组中某个元素的值： 5.2 读取数组中元素的值 格式： 例如： 5.3 读取整个数组 格式： 例如： 5.4 数组长度 类似于字符串： 例如： 6. expr命令 expr 命令用于求表达式的值，格式为： 表达式说明： 用空格隔开每一项。 用反斜杠放在 Shell 特定的字符前面（发现表达式运行错误时，可以试试转义）。 对包含空格和其他特殊字符的字符串要用引号括起来。 expr 会在 stdout 中输出结果。如果为逻辑关系表达式，则若结果为真，stdout 为1，否则为0。 expr 的 exit code：如果为逻辑关系表达式，则若结果为真，exit code 为0，否则为1。 6.1 字符串表达式 length STRING：返回 STRING 的长度。 index STRING CHARSET：CHARSET 中任意单个字符在 STRING 中最前面的字符位置，下标从1开始。如果在 STRING 中完全不存在 CHARSET 中的字符，则返回0。 substr STRING POSITION LENGTH：返回 STRING 字符串中从 POSITION 开始，长度最大为 LENGTH 的子串。如果 POSITION 或 LENGTH 为负数、0或非数值，则返回空字符串。 示例： 6.2 整数表达式 expr 支持普通的算术操作，算术表达式优先级低于字符串表达式，高于逻辑关系表达式。 + -：加减运算。两端参数会转换为整数，如果转换失败则报错。 * / %：乘、除与取模运算。两端参数会转换为整数，如果转换失败则报错。注意 * 需要转义。 ()：可以改变优先级，但需要用反斜杠转义。 示例： 6.3 逻辑关系表达式 |：如果第一个参数非空且非0，则返回第一个参数的值，否则返回第二个参数的值，但要求第二个参数的值也是非空或非0，否则返回0。如果第一个参数是非空或非0时，不会计算第二个参数。 &amp;：如果两个参数都非空且非0，则返回第一个参数，否则返回0。如果第一个参为0或为空，则不会计算第二个参数。 &lt; &lt;= = == != &gt;= &gt;：比较两端的参数，如果为 true，则返回1，否则返回0。== 是 = 的同义词。expr 首先尝试将两端参数转换为整数，并做算术比较，如果转换失败，则按字符集排序规则做字符比较。 ()：可以改变优先级，但需要用反斜杠转义。 示例： 7. read命令 read 命令用于从标准输入中读取单行数据。当读到文件结束符时，exit code 为1，否则为0。 参数说明： -p：后面可以接提示信息。 -t：后面跟秒数，定义输入字符的等待时间，超过等待时间后会自动忽略此命令。 实例： 8. echo命令 echo 用于输出字符串。命令格式： （1）显示普通字符串 （2）显示转义字符 （3）显示变量 （4）显示换行 输出结果： （5）显示不换行 输出结果： （6）显示结果定向至文件 （7）原样输出字符串，不进行转义或取变量（用单引号） 输出结果： （8）显示命令的执行结果 输出结果： 9. printf命令 printf 命令用于格式化输出，类似于 C/C++ 中的 printf 函数。默认不会在字符串末尾添加换行符。 命令格式： 脚本内容： 输出结果： 10. test命令与判断符号[] 10.1 逻辑运算符 &amp;&amp; 表示与，|| 表示或。 二者具有短路原则： expr1 &amp;&amp; expr2：当 expr1 为假时，直接忽略 expr2。 expr1 || expr2：当 expr1 为真时，直接忽略 expr2。 表达式的 exit code 为0，表示真；为非零，表示假（与 C/C++ 中的定义相反）。 10.2 test命令 在命令行中输入 man test，可以查看 test 命令的用法。 test 命令用于判断文件类型，以及对变量做比较。 test 命令用 exit code 返回结果，而不是使用 stdout。0表示真，非0表示假。 例如： （1）文件类型判断 其它参数如下： -e：文件是否存在。 -f：是否为文件。 -d：是否为目录。 （2）文件权限判断 其它参数如下： -r：文件是否可读。 -w：文件是否可写。 -x：文件是否可执行。 -s：是否为非空文件。 （3）整数间比较 其它参数如下： -eq：a 是否等于 b。 -ne：a 是否不等于 b。 -gt：a 是否大于 b。 -lt：a 是否小于 b。 -ge：a 是否大于等于 b。 -le：a 是否小于等于 b。 （4）字符串比较 test -z STRING：判断 STRING 是否为空，如果为空，则返回 true。 test -n STRING：判断 STRING 是否非空，如果非空，则返回 true（-n 可以省略）。 test str1 == str2：判断 str1 是否等于 str2。 test str1 != str2：判断 str1 是否不等于 str2。 （5）多重条件判定 其它参数如下： -a：两条件是否同时成立。 -o：两条件是否至少一个成立。 !：取反，如 test ! -x file，当 file 不可执行时，返回 true。 10.3 判断符号[] [] 与 test 用法几乎一模一样，更常用于 if 语句中。另外 [[]] 是 [] 的加强版，支持的特性更多。 例如： 注意： [] 内的每一项都要用空格隔开。 [] 内的变量，最好用双引号括起来。 [] 内的常数，最好用单或双引号括起来。 例如： 11. 判断语句 11.1 if…then形式 类似于 C/C++ 中的 if-else 语句。 （1）单层 if： 示例： 输出结果： （2）单层 if-else： 示例： 输出结果： （3）多层 if-elif-elif-else： 示例： 输出结果： 11.2 case…esac形式 类似于 C/C++ 中的 switch 语句。 命令格式： 示例： 输出结果： 12. 循环语句 12.1 for…in…do…done 命令格式： 示例一，输出 a 2 cc，每个元素一行： 示例二，输出当前路径下的所有文件名，每个文件名一行： 示例三，输出1到10： 示例四，使用 &#123;1..10&#125; 或者 &#123;a..z&#125;： 12.2 for ((…;…;…)) do…done 命令格式： 示例，输出1到10，每个数占一行： 12.3 while…do…done while...do...done：当条件为假时结束。 命令格式： 示例，文件结束符为 Ctrl+d，输入文件结束符后 read 指令返回 false： 12.4 until…do…done until...do...done：当条件为真时结束。 命令格式： 示例，当用户输入 yes 或者 YES 时结束，否则一直等待读入： 12.5 break break：跳出当前一层循环，注意与 C/C++ 不同的是：break 不能跳出 case 语句。 示例，每读入非 EOF 的字符串，会输出一遍1到7。该程序可以输入 Ctrl+d 文件结束符来结束，也可以直接用 Ctrl+c 杀掉该进程： 12.6 continue continue：跳出当前循环。 示例，输出1到10中的所有奇数： 直接关闭进程的方式： 使用 top 命令找到进程的 PID。 输入 kill -9 PID 即可关掉此进程。 13. 函数 bash 中的函数类似于 C/C++ 中的函数，但 return 的返回值与 C/C++ 不同，返回的是 exit code，取值为 [0, 255]，0表示正常结束。 如果想获取函数的输出结果，可以通过 echo 输出到 stdout 中，然后通过 $(function_name) 来获取 stdout 中的结果。 函数的 return 值可以通过 $? 来获取。 命令格式： （1）不获取 return 值和 stdout 值 输出结果： （2）获取 return 值和 stdout 值（不写 return 时，默认 return 0） 输出结果： （3）函数的输入参数 在函数内，$1 表示第一个输入参数，$2 表示第二个输入参数，依此类推。 注意：函数内的 $0 仍然是文件名，而不是函数名。 输出结果： （4）函数内的局部变量 可以在函数内定义局部变量，作用范围仅在当前函数内。可以在递归函数中定义局部变量。 命令格式： 例如： 输出结果： 第一行为函数内的 name 变量，第二行为函数外调用 name 变量，会发现此时该变量不存在。 14. exit命令 exit 命令用来退出当前 Shell 进程，并返回一个退出状态；使用 $? 可以接收这个退出状态。 exit 命令可以接受一个整数值作为参数，代表退出状态。如果不指定，默认状态值是0。 exit 退出状态只能是一个介于0到255之间的整数，其中只有0表示成功，其它值都表示失败。 示例，创建脚本 test.sh，内容如下： 执行该脚本： 15. 文件重定向 每个进程默认打开3个文件描述符： stdin：标准输入，从命令行读取数据，文件描述符为0。 stdout：标准输出，向命令行输出数据，文件描述符为1。 stderr：标准错误输出，向命令行输出数据，文件描述符为2。 可以用文件重定向将这三个文件重定向到其他文件中。 （1）重定向命令列表 command &gt; file：将 stdout 重定向到 file 中。 command &lt; file：将 stdin 重定向到 file 中。 command &gt;&gt; file：将 stdout 以追加方式重定向到 file 中。 command n&gt; file：将文件描述符 n 重定向到 file 中。 command n&gt;&gt; file：将文件描述符 n 以追加方式重定向到 file 中。 （2）输入和输出重定向 （3）同时重定向 stdin 和 stdout 创建 bash 脚本： 创建 input.txt，里面的内容为： 执行命令： 16. 引入外部脚本 类似于 C/C++ 中的 include 操作，bash 也可以引入其他文件中的代码。 语法格式： 示例，创建 test1.sh，内容为： 然后创建 test2.sh，内容为： 执行命令： 上一章：Linux学习笔记-命令、Tmux与Vim。 下一章：Linux学习笔记-SSH与Git。"},{"title":"Linux学习笔记-命令、Tmux与Vim","date":"2022-03-15T10:06:00.000Z","url":"/posts/53725.html","tags":[["Linux","/tags/Linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 本文记录 Linux 的学习过程，内容为 Linux 常用文件管理命令、Tmux、Vim。 1. 常用命令 Linux 中描述路径有两种方式（假设当前用户的目录为 AsanoSaki）： 绝对路径：从根目录（即 /）开始描述，例如：/home/AsanoSaki/main.cpp。 相对路径：从当前的路径开始描述，例如：AsanoSaki/main.cpp（当前在 home 中）。 绝对路径的开头一定是 /，相对路径开头一定不是 /。 . 表示当前目录，.. 表示上一目录，假如当前在 AsanoSaki 目录下，则路径 ../AsanoSaki/./../AsanoSaki 表示同一路径。 ~/ 表示家目录，等价于 /home/AsanoSaki。 1.1 常用文件管理命令 ctrl+c：取消命令，并且换行。如当前有一个程序正在运行且一直无法停止，则可以使用该操作将当前正在运行的程序中止。另一个作用是中断当前正在输入的这一行，直接跳到下一行重新输入。 ctrl+u：清空本行命令。 tab：可以补全命令和文件名，如果补全不了快速按两下 tab 键，可以显示备选选项。 ls：列出当前目录下所有文件，蓝色的是文件夹，白色的是普通文件，绿色的是可执行文件。 pwd：显示当前路径。 cd XXX：进入 XXX 目录下，cd .. 表示返回上层目录，cd - 表示返回上一个待过的目录。cd 后面既可以用相对路径也可以用绝对路径，不加目录则默认返回家目录。 cp XXX YYY：将 XXX 文件复制成 YYY，XXX 和 YYY 可以是一个路径，比如将目录 a 中的文件 tmp.txt 复制到目录 b 中：cp a/tmp.txt b。如果想顺带将复制后的文件重命名则可以写成：cp a/tmp.txt b/tmp2.txt。如果想把目录 a 整个复制到目录 b 下则可以写：cp a b -r。 mkdir XXX：创建目录 XXX。如在当前目录下创建文件夹 a：mkdir a。使用绝对路径在目录 a 下创建文件夹 b：mkdir /home/AsanoSaki/a/b。直接创建 a 里有 b，b 里有 c 的目录：mkdir a/b/c -p。 rm XXX：删除普通文件。rm XXX -r：删除文件夹。删除多个文件：rm tmp1.txt tmp2.txt。删除当前目录下的所有 txt 文件：rm *.txt。删除 a 中的所有文件：rm a/*。 mv XXX YYY：将 XXX 文件移动（剪切）到 YYY，和 cp 命令一样，XXX 和 YYY 可以是一个路径，重命名也是用这个命令。 touch XXX：创建一个文件。 cat XXX：展示文件 XXX 中的内容。 1.2 其它常用命令 （1）系统状况 top：查看所有进程的信息（Linux 的任务管理器）。 打开后，输入 M：按使用内存排序。 打开后，输入 P：按使用 CPU 排序。 打开后，输入 q：退出。 df -h：查看硬盘使用情况。 free -h：查看内存使用情况。 du -sh：查看当前目录占用的硬盘空间。 ps aux：查看所有进程。 kill -9 pid：杀死编号为 pid 的进程。 传递某个具体的信号：kill -s SIGTERM pid。 netstat -nt：查看所有网络连接。 w：列出当前登陆的用户。 ping www.baidu.com：检查是否连网。 （2）文件权限 chmod：修改文件权限 chmod +x xxx：给 xxx 添加可执行权限。 chmod -x xxx：去掉 xxx 的可执行权限。 chmod 777 xxx：将 xxx 的权限改成 777（三个数字按顺序分别表示 Owner、Group、Other Users，每个数字的二进制例如7的二进制为111，表示具有 rwx 权限，某一位为0表示没有该权限）。 chmod 777 xxx -R：递归修改整个文件夹的权限。 （3）文件检索 find /path/to/directory/ -name '*.py'：搜索某个文件路径下的所有 *.py 文件。 grep xxx：从 stdin 中读入若干行数据，如果某行中包含 xxx，则输出该行；否则忽略该行。 wc：统计行数、单词数、字节数。 既可以从 stdin 中直接读入内容；也可以在命令行参数中传入文件名列表。 wc -l：统计行数。 wc -w：统计单词数。 wc -c：统计字节数。 tree：展示当前目录的文件结构。 tree /path/to/directory/：展示某个目录的文件结构。 tree -a：展示隐藏文件。 ag xxx：搜索当前目录下的所有文件，检索 xxx 字符串。 cut：分割一行内容。 从 stdin 中读入多行数据。 echo $PATH | cut -d ':' -f 3,5：输出 PATH 用 : 分割后第3、5列数据。 echo $PATH | cut -d ':' -f 3-5：输出 PATH 用 : 分割后第3-5列数据。 echo $PATH | cut -c 3,5：输出 PATH 的第3、5个字符。 echo $PATH | cut -c 3-5：输出 PATH 的第3-5个字符。 sort：将每行内容按字典序排序。 可以从 stdin 中读取多行数据。 可以从命令行参数中读取文件名列表。 xargs：将 stdin 中的数据用空格或回车分割成命令行参数。 find . -name '*.py' | xargs cat | wc -l：统计当前目录下所有 Python 文件的总行数 （4）查看文件内容 more：浏览文件内容。 回车：下一行。 空格：下一页。 b：上一页。 q：退出。 less：与 more 类似，功能更全。 回车：下一行。 y：上一行。 Page Down：下一页。 Page Up：上一页。 q：退出。 head -3 xxx：展示 xxx 的前3行内容。 同时支持从 stdin 读入内容。 tail -3 xxx：展示 xxx 末尾3行内容。 同时支持从 stdin 读入内容。 （5）用户相关 history：展示当前用户的历史操作。内容存放在 ~/.bash_history 中。 （6）工具 md5sum：计算 md5 哈希值。 可以从 stdin 读入内容。 也可以在命令行参数中传入文件名列表。 time command：统计 command 命令的执行时间。 ipython3：交互式 Python3 环境。可以当做计算器，或者批量管理文件。 ! echo &quot;Hello World&quot;：! 表示执行 shell 脚本。 watch -n 0.1 command：每0.1秒执行一次 command 命令。 tar：压缩文件。 tar -zcvf xxx.tar.gz /path/to/file/*：压缩。 tar -zxvf xxx.tar.gz：解压缩。 diff xxx yyy：查找文件 xxx 与 yyy 的不同点。 （7）安装软件 sudo command：以 root 身份执行 command 命令。 apt-get install xxx：安装软件。 pip install xxx --user --upgrade：安装 Python 包。 2. Tmux与Vim 2.1 Tmux （1）功能 分屏。 允许断开 Terminal 连接后，继续运行进程。 （2）结构 一个 Tmux 可以包含多个 session，一个 session 可以包含多个 window，一个 window 可以包含多个 pane。 （3）常用操作 tmux [-u]：新建一个 session，其中包含一个 window，window 中包含一个 pane，pane 里打开了一个 shell 对话框，-u 参数可以在 Tmux 中显示中文内容。 按下 Ctrl + a 后手指松开，然后按 %：将当前 pane 左右平分成两个 pane。 按下 Ctrl + a 后手指松开，然后按 &quot;（注意是双引号）：将当前 pane 上下平分成两个 pane。 Ctrl + d：关闭当前 pane；如果当前 window 的所有 pane 均已关闭，则自动关闭 window；如果当前 session 的所有 window 均已关闭，则自动关闭 session。 鼠标点击可以选则 pane。 按下 Ctrl + a 后手指松开，然后按方向键：选择相邻的 pane。 鼠标拖动 pane 之间的分割线，可以调整分割线的位置。 按住 Ctrl + a 的同时按方向键，可以调整 pane 之间分割线的位置。 按下 Ctrl + a 后手指松开，然后按 z：将当前 pane 全屏/取消全屏。 按下 Ctrl + a 后手指松开，然后按 d：挂起当前 session。 tmux a：打开之前挂起的 session。 按下 Ctrl + a 后手指松开，然后按 s：选择其它 session： 方向键上：选择上一项； 方向键下：选择下一项； 方向键右：展开当前项； 方向键左：闭合当前项。 按下 Ctrl + a 后手指松开，然后按 c：在当前 session 中创建一个新的 window。 按下 Ctrl + a 后手指松开，然后按 w：选择其他 window，操作方法与选择 session 完全相同。 按下 Ctrl + a 后手指松开，然后按 PageUp/PageDown：翻阅当前 pane 内的内容。注意第一次唤醒该操作时只能按 PageUp。 鼠标滚轮：翻阅当前 pane 内的内容。 在 tmux 中选中文本时，需要按住 shift 键。（仅支持 Windows 和 Linux，不支持 Mac，不过该操作并不是必须的，因此影响不大） tmux 中复制/粘贴文本的通用方式： 按下 Ctrl + a 后松开手指，然后按 [。 用鼠标选中文本，被选中的文本会被自动复制到 tmux 的剪贴板。 按下 Ctrl + a 后松开手指，然后按 ]，会将剪贴板中的内容粘贴到光标处。 注意：Tmux 的配置文件为 ~/.tmux.conf，默认 Tmux 前缀快捷键是 Ctrl + b！本文是已经修改过了配置文件后的操作说明，不过基本上操作逻辑都是一样的。 2.2 Vim （1）功能 命令行模式下的文本编辑器。 根据文件扩展名自动判别编程语言。支持代码缩进、代码高亮等功能。 使用方式：vim &lt;filename&gt;。 如果已有该文件，则打开它。 如果没有该文件，则打开个一个新的文件，并命名为 filename。 （2）模式 一般命令模式 默认模式。命令输入方式：类似于打游戏放技能，按不同字符，即可进行不同操作。可以复制、粘贴、删除文本等。 编辑模式 在一般命令模式里按下 i，会进入编辑模式。 按下 ESC 会退出编辑模式，返回到一般命令模式。 命令行模式 在一般命令模式里按下 :/? 三个字符中的任意一个，会进入命令行模式。命令行在最下面。 可以查找、替换、保存、退出、配置编辑器等。 （3）操作 i：进入编辑模式。 ESC：进入一般命令模式。 h 或左方向键：光标向左移动一个字符。 j 或下方向键：光标向下移动一个字符。 k 或上方向键：光标向上移动一个字符。 l 或右方向键：光标向右移动一个字符。 n&lt;Space&gt;：n 表示数字，按下数字后再按空格，光标会向右移动 n 个字符。 0 或 Home：光标移动到本行开头。 $ 或 End：光标移动到本行末尾。 G：光标移动到最后一行。 :n 或 nG：n 为数字，光标移动到第 n 行。 gg：光标移动到第一行，相当于 1G。 n&lt;Enter&gt;：n 为数字，光标向下移动 n 行。 /word：向光标之下寻找第一个值为 word 的字符串。 ?word：向光标之上寻找第一个值为 word 的字符串。 n：重复前一个查找操作。 N：反向重复前一个查找操作。 :n1,n2s/word1/word2/g：n1 与 n2 为数字，在第 n1 行与 n2 行之间寻找 word1 这个字符串，并将该字符串替换为 word2。 :1,$s/word1/word2/g：将全文的 word1 替换为 word2。 :1,$s/word1/word2/gc：将全文的 word1 替换为 word2，且在替换前要求用户确认。 v：选中文本，连续按两次 ESC 取消选中。 d：删除选中的文本。 dd：删除当前行（其实是剪切）。 ggdG：删除全部内容。 y：复制选中的文本。 yy：复制当前行。 ggyG：复制全部内容。 p：将复制的数据在光标的下一行/下一个位置粘贴。 u：撤销。 Ctrl + r：取消撤销。 &gt;：将选中的文本整体向右缩进一次。 &lt;：将选中的文本整体向左缩进一次。 :w：保存。 :w!：强制保存。 :q：退出。 :q!：强制退出。 :wq：保存并退出。 :set paste：设置成粘贴模式，取消代码自动缩进。 :set nopaste：取消粘贴模式，开启代码自动缩进。 :set nu：显示行号。 :set nonu：隐藏行号。 gg=G：将全文代码格式化。 :noh：关闭查找关键词高亮。 Ctrl + q：当 Vim 卡死时，可以取消当前正在执行的命令。 异常处理： 每次用 Vim 编辑文件时，会自动创建一个 &lt;filename&gt;.swp 的临时文件。 如果打开某个文件时，该文件的 swp 文件已存在，则会报错。此时解决办法有两种： 找到正在打开该文件的程序，并退出。 直接删掉该 swp 文件即可。 Vim 的配置文件在 ~/.vimrc 中。 上一章：无。 下一章：Linux学习笔记-Shell。"},{"title":"VS Code实用插件推荐与使用教程","date":"2022-01-25T09:57:00.000Z","url":"/posts/45632.html","tags":[["Others","/tags/Others/"]],"categories":[["Others","/categories/Others/"]],"content":" 推荐一些 VS Code 基础插件，例如：简体中文、C++、背景、主题等。 1. Chinese (Simplified) 简体中文插件，不用多说了，上来第一个先装这个。 2. C/C++ 需要编写调试运行 C/C++ 文件所需的插件，安装完成后在工作空间的顶层文件夹中新建一个 .vscode 文件夹，新建两个文件名字分别为 tasks.json 和 launch.json。 其中，tasks.json 文件内容固定如下： launch.json 文件内容如下，注意 &quot;miDebuggerPath&quot; 的路径需要选择自己电脑上安装的 MinGW 的 gdb.exe 路径： 如果使用 TDM-GCC 那么 launch.json 文件的内容如下： 配置完成后编写 C++ 代码即可成功编译运行啦！（注意源文件的路径中不能有任何中文的文件夹） 3. Markdown All in One 安装了该插件后即可编写 Markdown 文件（文件后缀名为 .md）并且支持实时预览。 4. MASM/TASM 编写汇编语言代码必备插件。 5. background 更换背景的插件，安装完成后点击&quot;文件&quot;-“首选项”-“设置”-“扩展”，打开 Plugin background config. background 插件配置，点击&quot;在 settings.json 中编辑&quot;，添加一段配置代码（注意得添加在大括号内）： 6. Atom One Dark/Light Theme 最经典的一款深色/浅色皮肤，强烈推荐！ 7. Python Python 这个插件必装！别问为啥！因为它是微软 VS Code 开发团队自己开发的，亲儿子的级别。虽然 VS Code 不安装任何插件也能高亮 Python 代码，但该件提供的功能远不止如此，还有很多强大的功能。 注意：如果装有 Code Runner 插件，运行 Python 代码时可能会出现中文乱码问题，其实 Python 运行不需要 Code Runner 插件。 8. Python Extension Pack 这是一个 Python 扩展包，它依赖于以下扩展包： Python：高亮、调试（多线程、远程）、智能提示、代码格式化、重构、单元测试、代码片段、数据科学（使用 Jupyter）、PySpark 等等。 Jinja：对 Visual Studio 代码的 Jinja 模板语言支持。 Django：为有期限的完美主义者提供了漂亮的语法和限定范围的片段。 Visual Studio IntelliCode：在 Visual Studio 代码中为 Python 开发人员提供人工智能辅助的生产力功能，并基于对代码的理解和机器学习提供见解。 Python Environment Manager：提供从一个地方查看和管理所有 Python 环境和包的能力。 Python Docstring Generator：基于多个可选模板模式，为类和方法快速插入带有上下文推断参数的 Python 注释块。 Python Indent：在 Visual Studio 代码中更正 Python 缩进。 Jupyter：为 Python 语言提供 Jupyter Notebook 支持，用于数据科学、科学计算和机器学习。 "},{"title":"Linux与Windows下Vim配置方案推荐","date":"2021-11-21T02:15:00.000Z","url":"/posts/11764.html","tags":[["Others","/tags/Others/"]],"categories":[["Others","/categories/Others/"]],"content":" Vim 真香！！！不会吧不会还有人在用 IDE 吧（bushi） 1. 前言 可能很多萌新程序员会问为什么很多大佬写代码时用的都是 Vim 而不是自己熟悉的 Visual Studio、VS Code、IDEA、PyCharm、CLion 等这些 IDE 呢？用 Vim 编辑有什么优点呢？ Vim 对硬件需求小：如果只是编写一个相对来说不是那么庞大的程序的话，用 IDE 有种“大材小用”的感觉，启动、编译运行相比 Vim 来说都满了很多，占用内存资源也很大，因此平常自己写写代码不是搞大型开发的话用 Vim 绝对会感觉又快又方便； Vim 自由度高：无论什么 IDE，那终究还是别人搭建好的编辑平台，你永远驯服不了，而对于 Vim，从头到尾的元素都是可以由自己 DIY 的，就像是自己专属的开发环境，里面的元素都是自己最喜欢且最熟悉的； Vim 光标移动效率高：对于新手而言，肯定会觉得 Vim 很难用，效率很低，那是因为还不熟悉 Vim 的整套光标移动快捷键，一旦熟练了快捷键后，你会发现用鼠标移动光标是一件特别慢还特别麻烦的事情，用 Vim 后完全可以脱离鼠标，让光标移动的比鼠标操作灵活很多（毫不夸张）； Vim 编辑效率高：同样的，Vim 有着一整套编辑操作的快捷键组合，且可以扩展各种插件，编辑代码的功能方面绝不逊于各大 IDE。 那么如何配置一个属于自己的 Vim 环境呢？本文就来介绍一下 .vimrc 文件的配置（Windows 操作系统环境下）。 2. Windows下.vimrc文件的配置 在 Windows 操作系统环境下，用户可以进入自己的用户根目录（C:\\Users\\你的用户id）下，新建一个文本文件，在其中输入以下内容： 然后保存，将文件名改成 .vimrc ，这时候再打开 Vim 看一下效果。 配置文件中的每一行代码的功能在边上都有相应的注释（&quot; 之后的内容即为注释），要打开/关闭某一项功能只需在代码的最前面加上/删去 &quot; 即可。 对于主题，个人喜好的是 elflord 和 desert，字体的话 Consolas yyds！剩下的元素大家可以根据自己的喜好进行调整。 对于 F5 自动编译功能，如果使用的是 Linux 环境，那么需要将以下红框部分的代码修改为 exec &quot;! ./%&lt;&quot; 3. Linux下.vimrc文件的配置 3.1 Onedark主题配置 本人较为喜欢的主题为 Onedark，因此在本文中介绍一下 Linux 环境下怎么配置 Onedark 主题。 首先在 ~/.vim 文件夹中创建两个文件夹：colors 和 autoload，如果已经有了那么跳过此步。 然后将 Onedark 主题中的东西下载至 .vim 文件夹中，可以直接使用命令行： 下载完成后在 .vim 文件夹中有个 onedark.vim 文件夹，将 onedark.vim/colors 文件夹中的 onedark.vim 文件复制到 ~/.vim/colors 中，将 onedark.vim/autoload 文件夹中的 onedark.vim 文件复制到 ~/.vim/autoload 中： 然后修改 .vimrc 文件，将原本的主题设置注释掉，添加一行 colorscheme onedark，至此配置就完成了。 3.2 Linux环境下.vimrc文件推荐 以下为本人目前正在使用的一套 Vim 配置，配置代码如下： "},{"title":"NodeJS的安装及配置","date":"2021-10-13T03:38:00.000Z","url":"/posts/11062.html","tags":[["Others","/tags/Others/"]],"categories":[["Others","/categories/Others/"]],"content":" Node.js 是一个基于 Chrome V8 引擎的 JavaScript 运行环境。本文介绍如何安装与配置Node.js。 使用教程见官方文档：Node.js Docs。 一、官网下载及安装NodeJS 官网下载地址：NodeJS Download。 本文的安装路径为：D:\\NodeJS。 安装完成后打开命令行窗口校验版本： 测试 npm 是否安装成功，由于新版的 NodeJS 已经集成了 npm，所以之前 npm 也一并安装好了，同样可以使用命令行窗口校验： 二、环境变量配置 在 D:\\NodeJS 目录下新建两个文件夹：node_global 和 node_cache。在该文件夹下然后长按 Shift + 鼠标右键，选择打开 PowerShell 窗口或者打开 Git Bash，输入以下命令： 接着配置环境变量，右键&quot;我的电脑&quot;-“属性”-“高级系统设置”，点击&quot;高级&quot;选项卡，选择&quot;环境变量&quot;进行配置。在&quot;系统变量&quot;下点击&quot;新建&quot;，变量名和路径如下（node_modules 目录直接手打即可）： 在&quot;用户变量&quot;的 Path 变量上点击&quot;编辑&quot;，更改 npm 安装的默认路径（默认是 C:\\Users\\&lt;用户名&gt;\\AppData\\Roaming\\npm）： 修改 npm 的镜像源： 配置完成后，尝试在命令行窗口中安装一些环境（若安装出错可用管理员身份打开命令行窗口）： "},{"title":"算法竞赛C++ STL详解","date":"2021-10-05T09:40:00.000Z","url":"/posts/46520.html","tags":[["C++","/tags/C/"]],"categories":[["C++","/categories/C/"]],"content":" 本文介绍了什么是 STL 以及如何使用 STL 更高效偷懒地解题。本篇文章将会长期更新，欢迎大家一起监督学习。 1. STL概念 STL（Standard Template Library，标准模板库），是惠普实验室开发的一系列软件的统称。现主要出现在 C++ 中，STL 从广义上分为：容器（Container）、算法（Algorithm）和迭代器（Iterator）。STL 几乎所有的代码都采用了模板类或者模板函数，这相比传统的由函数和类组成的库来说提供了更好的代码重用机会。 2. STL六大组件 STL 提供了六大组件，彼此之间可以组合套用，这六大组件分别是容器、算法、迭代器、仿函数、适配器、空间配置器。其中，在算法竞赛中用到最多的为容器、算法与迭代器。 容器（Container）：STL 容器为各种数据结构，如 vector、stack、queue、map、set 等，用来存放数据，从实现角度来看，STL 容器是一种 class template。 算法（Algorithm）：STL 的算法多数定义在 &lt;algorithm&gt; 头文件中，其中包括了各种常用的算法，如 sort、find、copy、reverse 等，从实现角度来看，STL 算法是一种 function template。 迭代器（Iterator）：STL 迭代器扮演了容器与算法之间的胶合剂，共有五种类型，从实现角度来看，迭代器是一种将 opetator*、opetator-&gt;、operator++ 等指针相关操作予以重载的 class template。所有 STL 容器都附带有自己专属的迭代器，只有容器的设计者才知道如何遍历自己的元素。 仿函数（Functor）：行为类似函数，可作为算法的某种策略，从实现角度来看，仿函数是一种重载了 operator() 的 class 或者 class template。 适配器（Adaptor）：一种用来修饰容器或仿函数或迭代器接口的东西。 空间配置器（Allocator）：负责空间的配置与管理。从实现角度来看，配置器是一个实现了动态空间配置、空间管理、空间释放的 class template。 3. STL容器 相信很多人学习 STL 就是为了在比赛中能够更好地装B运用各种数据结构和算法，提高解题速度。确实，使用 STL 中的容器能够不需要自己手写定义各种数据结构，使用 STL 中的算法能够不需要自己手写实现各种基本算法，因此本部分对于算法巨巨们是最为重要的一部分，那么 STL 容器究竟有哪些呢？在做题中该如何使用呢？ 3.1 vector vector 又称变长数组，定义在 &lt;vector&gt; 头文件中，vector 容器是动态空间，随着元素的加入，它的内部机制会自动扩充空间以容纳新的元素。因此 vector 的运用对于内存的合理利用与运用的灵活性有很大的帮助。 vector 的定义方式： vector 的常用内置函数： 3.2 stack stack 又称栈，是一种后进先出（Last In First Out，LIFO）的数据结构，定义在 &lt;stack&gt; 头文件中，stack 容器允许新增元素、移除元素、取得栈顶元素，但是除了最顶端以外，没有任何方法可以存取 stack 的其它元素，换言之，stack 不允许有遍历行为。 stack 的定义方式： stack 的常用内置函数： 3.3 string string 又称字符串，定义在 &lt;string&gt; 头文件中。C 风格的字符串（以空字符结尾的字符数组）太过复杂难于掌握，因此 C++ 标准库定义了一种 string 类。string 和 vector&lt;char&gt; 在数据结构、内存管理等方面都是相同的。但是，vector&lt;char&gt; 只是单纯的一个“char 元素的容器”，而 string 不仅是一个“char 元素的容器”，它还扩展了一些针对字符串的操作，例如 string 可以使用 c_str() 函数转换为 C 风格的字符串，vector 中并未对输入输出流操作符进行重载，因此无法直接对 vector&lt;char&gt; 进行 cin 或者 cout 这样的操作，但是 string 可以，且 vector&lt;char&gt; 并不能直接实现字符串的拼接，但是 string 可以，string 中重载了 +, += 运算符。 string 的定义方式： string 的常用内置函数： string 的 erase() 与 remove() 函数的用法： 3.4 queue/priority_queue queue 又称队列，是一种先进先出（First In First Out，FIFO）的数据结构，定义在 &lt;queue&gt; 头文件中，queue 容器允许从一端（称为队尾）新增元素（入队），从另一端（称为队头）移除元素（出队）。 priority_queue 又称优先队列，同样定义在 &lt;queue&gt; 头文件中，与 queue 不同的地方在于我们可以自定义其中数据的优先级，优先级高的排在队列前面，优先出队。priority_queue 具有 queue 的所有特性，包括基本操作，只是在这基础上添加了内部的一个排序，它的本质是用堆实现的，因此可分为小根堆与大根堆，小根堆中较小的元素排在前面，大根堆中较大的元素排在前面。（创建 priority_queue 时默认是大根堆！） queue/priority_queue 的定义方式： queue/priority_queue 的常用内置函数： 3.5 deque deque 又称双端队列，定义在 &lt;deque&gt; 头文件中，vector 容器是单向开口的连续内存空间，deque 则是一种双向开口的连续线性空间。所谓的双向开口，意思是可以在头尾两端分别做元素的插入和删除操作，当然，vector也可以在头尾两端插入元素，但是在其头部进行插入操作效率很低。deque 和 vector 最大的差异一是在于 deque 允许使用常数项时间在头部进行元素的插入和删除操作，二是在于 deque 没有容量的概念，因为它是动态的以分段连续空间组合而成，随时可以增加一段新的空间并链接起来。 deque 的定义方式： deque 的常用内置函数： 3.6 map/multimap map/multimap 又称映射，定义在 &lt;map&gt; 头文件中，map 和 multimap 的底层实现机制都是红黑树。map 的功能是能够将任意类型的元素映射到另一个任意类型的元素上，并且所有的元素都会根据元素的键值自动排序。map 所有的元素都是 pair，同时拥有键值和实值（即 (key, value) 对），key 被视为键值，value 被视为实值，map 不允许两个元素有相同的键值。multimap 和 map 的操作类似，唯一区别是 multimap 的键值允许重复。 map/multimap 的定义方式： map/multimap 的常用内置函数： 3.7 set/multiset set/multiset 又称集合，定义在 &lt;set&gt; 头文件中。set 的特性是所有元素都会根据元素的键值自动被排序，set 的元素不像 map 那样可以同时拥有键值和实值，set 的元素既是键值又是实值，set 不允许两个元素有相同的键值，因此总结来说就是 set 中的元素是有序且不重复的。multiset 的特性和用法和 set 完全相同，唯一的区别在于 multiset 允许有重复元素，set 和 multiset 的底层实现都是红黑树。 set/multiset 的定义方式： set/multiset 的常用内置函数： 3.8 unordered_map/unordered_set unordered_map/unordered_set 分别定义在 &lt;unordered_map&gt; 与 &lt;unordered_set&gt; 头文件中，内部采用的是 hash 表结构，拥有快速检索的功能。与 map/set 相比最大的区别在于 unordered_map/unordered_set 中的元素是无序的，增删改查的时间复杂度为 O(1)（map/set 增删改查的时间复杂度为 O(logn)），但是不支持 lower_bound()/upper_bound() 函数。 unordered_map/unordered_set 的定义方式： unordered_map/unordered_set 的常用内置函数： 4. STL算法 C++ 标准库定义了一组泛型算法，之所以称为泛型指的是它们可以操作在多种容器上，不但可以作用于标准库类型，还可以用在内置数组类型甚至其它类型的序列上。泛型算法定义在 &lt;algorithm&gt; 头文件中，标准库还定义了一组泛化的算术算法（Generalized Numeric Algorithm），定义在 &lt;numeric&gt; 头文件中。使用方法如下： "}]