<!DOCTYPE html>
<html lang="zh-CN">
    <head>
  <!-- 元数据 -->
  <meta charset="utf-8">
  <link rel="icon" href="/images/favicon.ico">
  
  <title>D2L学习笔记-现代卷积神经网络 | AsanoSaki</title>
  
  <meta name="author" content="AsanoSaki" />
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="robots" content="index,follow" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <meta name="format-detection" content="telphone=no, email=no" />
  
    <meta name="keywords" content="AI" />
  
  <meta name="description" content="D2L学习笔记-现代卷积神经网络">
<meta property="og:type" content="article">
<meta property="og:title" content="D2L学习笔记-现代卷积神经网络">
<meta property="og:url" content="https://asanosaki.github.io/posts/21165.html">
<meta property="og:site_name" content="AsanoSaki">
<meta property="og:description" content="D2L学习笔记-现代卷积神经网络">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://asanosaki.github.io/images/favicon.ico">
<meta property="article:published_time" content="2023-03-03T01:57:00.000Z">
<meta property="article:modified_time" content="2023-11-26T07:57:39.296Z">
<meta property="article:author" content="AsanoSaki">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://asanosaki.github.io/images/favicon.ico">
  <link rel="alternate" href="atom.xml" type="application/atom+xml">
  <!-- 站点验证相关 -->
  
    
    
    
  
  <!-- 样式表文件 -->
  <link rel="stylesheet" id="kratos-css" href="/css/kratosr.min.css" media="all"></script>
  
    <link rel="stylesheet" id="darkmode-css" href="/css/kr-color-dark.min.css" media="(prefers-color-scheme: dark)"></script>
    <script src="/js/kr-dark.min.js"></script>
  
  
    <link rel="stylesheet" id="highlight-css" href="/css/highlight/light.min.css" media="all"></script>
  
  <link rel="stylesheet" id="fontawe-css" href="/vendors/font-awesome@4.7.0/css/font-awesome.min.css" media="all"></script>
  <link rel="stylesheet" id="nprogress-css" href="/vendors/nprogress@0.2.0/nprogress.css" media="all"></script>
  
  
  
    <link rel="stylesheet" href="/vendors/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"></script>
  
  <!-- 不得不预先加载的一些JS文件 -->
  <script src="/vendors/jquery@3.6.0/dist/jquery.min.js"></script>
  
    <script src="/vendors/qrcode_js@1.0.0/qrcode.min.js"></script>
  
  
  <style>
    
      .kratos-cover.kratos-cover-2 {
        background-image: url('/images/banner2.jpg');
      }
    
    
      @media(min-width:768px) {
        body.custom-background {
          background-image: url('/images/opacity.png');
        }
      }
    
  </style>
  
<meta name="generator" content="Hexo 5.4.2"></head>


    <body class="custom-background">
        <div id="kratos-wrapper">
    <div id="kratos-page">
        <div id="kratos-header">
            <header id="kratos-desktop-topnav" class="kratos-topnav">
                <div class="container">
                    <div class="nav-header">
                        <nav id="kratos-menu-wrap">
                            <ul id="kratos-primary-menu" class="sf-menu">
                                
                                    
                                    
                                        
                                            <li><a href="/"><i class="fa fa-home"></i>Home</a></li>
                                        
                                    
                                        
                                            <li><a href="/archives/"><i class="fa fa-file"></i>Archives</a></li>
                                        
                                    
                                        
                                            <li>
                                                <a><i class="fa fa-paw"></i>Friends</a>
                                                <ul class="sub-menu">
                                                    
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://angels-d.github.io">Angels-D</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="http://syh521.cn">SYH</a></li>
                                                        
                                                    
                                                </ul>
                                            </li>
                                        
                                    
                                        
                                            <li>
                                                <a><i class="fa fa-link"></i>My Links</a>
                                                <ul class="sub-menu">
                                                    
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://github.com/AsanoSaki">GitHub</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://gitee.com/asanosaki">Gitee</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://git.acwing.com/AsanoSaki">AcGit</a></li>
                                                        
                                                    
                                                        
                                                            <li><a href="https://asanosaki.github.io">BLOG</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://asanosaki.blog.csdn.net">CSDN</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://www.acwing.com/user/myspace/index/82581">AcWing</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://www.luogu.com.cn/user/459347">LuoGu</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://codeforces.com/profile/AsanoSaki">CodeForces</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://leetcode.cn/u/asanosaki">LeetCode</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://space.bilibili.com/12300056">Bilibili</a></li>
                                                        
                                                    
                                                </ul>
                                            </li>
                                        
                                    
                                        
                                            <li>
                                                <a><i class="fa fa-heart"></i>Favorite</a>
                                                <ul class="sub-menu">
                                                    
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://www.runoob.com">Runoob</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://wallhaven.cc">Wallhaven</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="http://www.flysheep6.com">Flysheep</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://thrift.apache.org">Thrift</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://developer.mozilla.org/zh-CN">MDN</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://v5.bootcss.com">Bootstrap</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://zh-hans.react.dev">React</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://www.nextjs.cn">Next.js</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://cn.vuejs.org">Vue.js</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://docs.djangoproject.com/zh-hans/4.2">Django</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle">Spring Boot</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://hub.docker.com">Docker Hub</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://fontawesome.com.cn">Font Awesome</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://www.online-convert.com">File Convert</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://pytorch.org">PyTorch</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai">D2L</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://board.xcpcio.com">XCPC Board</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://kr-demo.candinya.com/posts/Kratos-Rebirth-Manual">Kratos MNL</a></li>
                                                        
                                                    
                                                </ul>
                                            </li>
                                        
                                    
                                
                            </ul>
                        </nav>
                    </div>
                </div>
            </header>
            <header id="kratos-mobile-topnav" class="kratos-topnav">
                <div class="container">
                    <div class="color-logo"><a href="/">AsanoSaki</a></div>
                    <div class="nav-toggle">
                        <a class="kratos-nav-toggle js-kratos-nav-toggle">
                            <i></i>
                        </a>
                    </div>
                </div>
            </header>
        </div>
        <div class="kratos-start kratos-hero-2">
            <!-- <div class="kratos-overlay"></div> -->
            <div class="kratos-cover kratos-cover-2 text-center">
                <div class="desc desc2 animate-box">
                    <a href="/">
                        <h2>AsanoSaki</h2> <br />
                        <span>夏天的海边有冰淇淋口味的海风~</span>
                    </a>
                </div>
            </div>
        </div>

        <div id="kratos-blog-post">
            <div class="container">
                <div id="main" class="row">
                    

        

            <section class="col-md-8">

        

            <article itemscope itemtype="https://schema.org/Article">
    
    <link itemprop="mainEntityOfPage" href="https://asanosaki.github.io/posts/21165.html">
    <div class="kratos-hentry kratos-post-inner clearfix">
        <header class="kratos-entry-header">
            
                <h1 class="kratos-entry-title text-center" itemprop="name headline">D2L学习笔记-现代卷积神经网络</h1>
            
            
            <ul class="kratos-post-meta text-center">
                <li><time datetime="2023-03-03T01:57:00.000Z" itemprop="datePublished"><i class="fa fa-calendar"></i> 2023-03-03</time></li>
                <li itemprop="author" itemscope itemtype="https://schema.org/Person">
                    <i class="fa fa-user"></i> Author <span itemprop="name">AsanoSaki</span>
                </li>
                
                    <li>
                        <i class="fa fa-edit"></i> 
                        
                        
                            ~18.81K
                        
                        words
                    </li>
                
                
            </ul>
        </header>
        <div class="kratos-post-content">
            
            <div id="expire-alert" class="alert alert-warning hidden" role="alert">
                <div class="icon"><i class="fa fa-warning"></i></div>
                <div class="text"><p>本文最后编辑于 <time datetime="1700985459296"></time> 前，其中的内容可能需要更新。</p></div>
            </div>
            
            
            
                <div class="kratos-post-inner-toc toc-div-class" >
                    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88AlexNet%EF%BC%89"><span class="toc-text">1. 深度卷积神经网络（AlexNet）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E4%BD%BF%E7%94%A8%E5%9D%97%E7%9A%84%E7%BD%91%E7%BB%9C%EF%BC%88VGG%EF%BC%89"><span class="toc-text">2. 使用块的网络（VGG）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%EF%BC%88NiN%EF%BC%89"><span class="toc-text">3. 网络中的网络（NiN）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%90%AB%E5%B9%B6%E8%A1%8C%E8%BF%9E%E7%BB%93%E7%9A%84%E7%BD%91%E7%BB%9C%EF%BC%88GoogLeNet%EF%BC%89"><span class="toc-text">4. 含并行连结的网络（GoogLeNet）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%89%B9%E9%87%8F%E8%A7%84%E8%8C%83%E5%8C%96%EF%BC%88BN%EF%BC%89"><span class="toc-text">5. 批量规范化（BN）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C%EF%BC%88ResNet%EF%BC%89"><span class="toc-text">6. 残差网络（ResNet）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E7%A8%A0%E5%AF%86%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9C%EF%BC%88DenseNet%EF%BC%89"><span class="toc-text">7. 稠密连接网络（DenseNet）</span></a></li></ol>
                </div>
            
            <hr />
            <div itemprop="articleBody"><blockquote>
<p>李沐动手学深度学习（PyTorch）课程学习笔记第六章：现代卷积神经网络。</p>
</blockquote>
<span id="more"></span>
<h2 id="1-深度卷积神经网络（AlexNet）">1. 深度卷积神经网络（AlexNet）</h2>
<p>AlexNet 和 LeNet 的设计理念非常相似，但也存在显著差异：</p>
<ul>
<li>AlexNet 比相对较小的 LeNet5 要深得多。AlexNet 由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。</li>
<li>AlexNet 使用 ReLU 而不是 Sigmoid 作为其激活函数。</li>
</ul>
<p>此外，AlexNet 将 Sigmoid 激活函数改为更简单的 ReLU 激活函数。一方面，ReLU 激活函数的计算更简单，它不需要如 Sigmoid 激活函数那般复杂的求幂运算。另一方面，当使用不同的参数初始化方法时，ReLU 激活函数使训练模型更加容易。当 Sigmoid 激活函数的输出非常接近于0或1时，这些区域的梯度几乎为0，因此反向传播无法继续更新一些模型参数。相反，ReLU 激活函数在正区间的梯度总是1。因此，如果模型参数没有正确初始化，Sigmoid 函数可能在正区间内得到几乎为0的梯度，从而使模型无法得到有效的训练。</p>
<p>尽管原文中 AlexNet 是在 ImageNet 上进行训练的，但本文在这里使用的是 Fashion-MNIST 数据集。因为即使在现代 GPU 上，训练 ImageNet 模型，同时使其收敛可能需要数小时或数天的时间。将 AlexNet 直接应用于 Fashion-MNIST 的一个问题是 Fashion-MNIST 图像的分辨率（28×28像素）低于 ImageNet 图像。为了解决这个问题，我们将它们增加到224×224像素（通常来讲这不是一个明智的做法，但在这里这样做是为了有效使用 AlexNet 架构）。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> util.functions <span class="keyword">import</span> train_classifier</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    <span class="comment"># 这里使用11*11的更大窗口来捕捉对象，同时步幅为4，以减少输出的高度和宽度，此外输出通道的数目远大于LeNet</span></span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">1</span>), nn.ReLU(),  <span class="comment"># (96, 54, 54)</span></span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),  <span class="comment"># (96, 26, 26)</span></span><br><span class="line">    <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">    nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.ReLU(),  <span class="comment"># (256, 26, 26)</span></span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),  <span class="comment"># (256, 12, 12)</span></span><br><span class="line">    <span class="comment"># 使用三个连续的卷积层和较小的卷积窗口，除了最后的卷积层，输出通道的数量进一步增加</span></span><br><span class="line">    <span class="comment"># 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度</span></span><br><span class="line">    nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),  <span class="comment"># (384, 12, 12)</span></span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),  <span class="comment"># (384, 12, 12)</span></span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),  <span class="comment"># (256, 12, 12)</span></span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),  <span class="comment"># (256, 5, 5)</span></span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    <span class="comment"># 这里全连接层的输出数量是LeNet中的好几倍，因此使用Dropout层来减轻过拟合</span></span><br><span class="line">    nn.Linear(<span class="number">6400</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 最后是输出层，由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">trans = transforms.Compose([transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)), transforms.ToTensor()])</span><br><span class="line">mnist_train = torchvision.datasets.FashionMNIST(root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">mnist_test = torchvision.datasets.FashionMNIST(root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">train_iter = data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line">test_iter = data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">lr, num_epochs = <span class="number">0.01</span>, <span class="number">50</span></span><br><span class="line"></span><br><span class="line">train_classifier(net, train_iter, test_iter, num_epochs, lr, device, <span class="string">&#x27;../logs/AlexNet_train_log&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="2-使用块的网络（VGG）">2. 使用块的网络（VGG）</h2>
<p>虽然 AlexNet 证明深层神经网络卓有成效，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络。</p>
<p>经典卷积神经网络的基本组成部分是下面的这个序列：</p>
<ul>
<li>带填充以保持分辨率的卷积层。</li>
<li>非线性激活函数，如 ReLU。</li>
<li>汇聚层，如最大汇聚层。</li>
</ul>
<p>而一个 VGG 块与之类似，由一系列卷积层组成，后面再加上用于空间下采样的最大汇聚层。</p>
<p>VGG 使用<strong>可重复使用</strong>的卷积块来构建深度卷积神经网络，不同的卷积块个数和超参数可以得到不同复杂度的变种。</p>
<p>下面的代码中，我们定义了一个名为 <code>vgg_block</code> 的函数来实现一个 VGG 块，该函数有三个参数，分别对应于卷积层的数量 <code>num_convs</code>、输入通道的数量 <code>in_channels</code> 和输出通道的数量 <code>out_channels</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">vgg_block</span>(<span class="params">num_convs, in_channels, out_channels</span>):</span><br><span class="line">    layers = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    layers.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure>
<p>原始 VGG 网络有5个卷积块，其中前两个块各有一个卷积层，后三个块各包含两个卷积层。第一个模块有64个输出通道，每个后续模块将输出通道数量翻倍，直到达到512。由于该网络使用8个卷积层和3个全连接层，因此它通常被称为 VGG-11。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">vgg</span>(<span class="params">conv_arch</span>):</span><br><span class="line">    conv_blks = []</span><br><span class="line">    in_channels = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 卷积层部分</span></span><br><span class="line">    <span class="keyword">for</span> (num_convs, out_channels) <span class="keyword">in</span> conv_arch:</span><br><span class="line">        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))</span><br><span class="line">        in_channels = out_channels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        *conv_blks, nn.Flatten(),</span><br><span class="line">        <span class="comment"># 全连接层部分</span></span><br><span class="line">        nn.Linear(out_channels * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">conv_arch = ((<span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>))</span><br><span class="line">net = vgg(conv_arch)</span><br></pre></td></tr></table></figure>
<p>由于 VGG-11 比 AlexNet 计算量更大，因此我们构建了一个通道数较少的网络，足够用于训练 Fashion-MNIST 数据集：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv_arch = ((<span class="number">1</span>, <span class="number">16</span>), (<span class="number">1</span>, <span class="number">32</span>), (<span class="number">2</span>, <span class="number">64</span>), (<span class="number">2</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">128</span>))</span><br><span class="line">net = vgg(conv_arch)</span><br></pre></td></tr></table></figure>
<p>最后我们读取数据集并进行训练，超参数设置：<code>lr, num_epochs = 0.02, 15</code>，训练过程与第一节内容一样，因此不再放出代码。</p>
<p>PS：如果显存不够可以减小 <code>batch_size</code>，从128改为64或32。</p>
<h2 id="3-网络中的网络（NiN）">3. 网络中的网络（NiN）</h2>
<p>回想一下，卷积层的输入和输出由四维张量组成，张量的每个轴分别对应样本、通道、高度和宽度。另外，全连接层的输入和输出通常是分别对应于样本和特征的二维张量。NiN 的想法是<strong>在每个像素位置（针对每个高度和宽度）应用一个全连接层</strong>。如果我们将权重连接到每个空间位置，我们可以将其视为1×1卷积层（如第五章第四节 PS 中所述），或作为在每个像素位置上独立作用的全连接层。从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征（feature）。</p>
<p>NiN 块以一个普通卷积层开始，后面是两个1×1的卷积层。这两个1×1卷积层充当带有 ReLU 激活函数的逐像素全连接层，对每个像素增加了非线性特性：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">nin_block</span>(<span class="params">in_channels, out_channels, kernel_size, strides, padding</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>), nn.ReLU())</span><br></pre></td></tr></table></figure>
<p>NiN 和 AlexNet 之间的一个显著区别是 NiN 完全取消了全连接层。相反，NiN 使用一个 NiN 块，其输出通道数等于标签类别的数量。最后放一个全局平均汇聚层（global average pooling layer），生成一个对数几率（logits）。NiN 设计的一个优点是，它显著减少了模型所需参数的数量。然而，在实践中，这种设计有时会增加训练模型的时间。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>, padding=<span class="number">0</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    nin_block(<span class="number">384</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),  <span class="comment"># 标签类别数是10</span></span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),  <span class="comment"># 将四维的输出转成二维的输出，其形状为(批量大小,10)</span></span><br><span class="line">    nn.Flatten())</span><br></pre></td></tr></table></figure>
<p>最后我们读取数据集并进行训练，超参数设置：<code>lr, num_epochs = 0.1, 15</code>，训练过程与第一节内容一样，因此不再放出代码。</p>
<h2 id="4-含并行连结的网络（GoogLeNet）">4. 含并行连结的网络（GoogLeNet）</h2>
<p>在 GoogLeNet 中，基本的卷积块被称为 Inception 块（Inception block）。Inception 块由四条并行路径组成。前三条路径使用窗口大小为1×1、3×3和5×5的卷积层，从不同空间大小中提取信息。中间的两条路径在输入上执行1×1卷积，以减少通道数，从而降低模型的复杂性。第四条路径使用3×3最大汇聚层，然后使用1×1卷积层来改变通道数。这四条路径都使用合适的填充来使<strong>输入与输出的高和宽一致</strong>，最后我们将每条线路的输出在<strong>通道维度</strong>上连结，并构成 Inception 块的输出。在 Inception 块中，通常调整的超参数是每层输出通道数。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module):</span><br><span class="line">    <span class="comment"># c1-c4是每条路径的输出通道数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 线路1，单1x1卷积层</span></span><br><span class="line">        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路2，1x1卷积层后接3x3卷积层</span></span><br><span class="line">        self.p2_1 = nn.Conv2d(in_channels, c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[<span class="number">0</span>], c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路3，1x1卷积层后接5x5卷积层</span></span><br><span class="line">        self.p3_1 = nn.Conv2d(in_channels, c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[<span class="number">0</span>], c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 线路4，3x3最大汇聚层后接1x1卷积层</span></span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="comment"># 在通道维度（第一维）上连结输出</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>GoogLeNet 一共使用9个 Inception 块和全局平均汇聚层的堆叠来生成其估计值。Inception 块之间的最大汇聚层可降低维度。第一个模块类似于 AlexNet 和 LeNet，Inception 块的组合从 VGG 继承，全局平均汇聚层避免了在最后使用全连接层。</p>
<p>现在，我们逐一实现 GoogLeNet 的每个模块。第一个模块使用64个通道、7×7卷积层：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>第二个模块使用两个卷积层：第一个卷积层是64个通道、1×1卷积层；第二个卷积层使用将通道数量增加三倍的3×3卷积层。这对应于 Inception 块中的第二条路径：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">b2 = nn.Sequential(nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>第三个模块串联两个完整的 Inception 块。第一个 Inception 块的输出通道数为 <code>64 + 128 + 32 + 32 = 256</code>，四个路径之间的输出通道数量比为 <code>2 : 4 : 1 : 1</code>，第二个和第三个路径首先将输入通道的数量分别减少到96和16，然后连接第二个卷积层。第二个 Inception 块的输出通道数增加到 <code>128 + 192 + 96 + 64 = 480</code>，四个路径之间的输出通道数量比为 <code>4 : 6 : 3 : 2</code>，第二条和第三条路径首先将输入通道的数量分别减少到128和32：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b3 = nn.Sequential(Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>),</span><br><span class="line">                   Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>第四模块更加复杂，它串联了5个 Inception 块，其输出通道数分别是 <code>192 + 208 + 48 + 64 = 512</code>、<code>160 + 224 + 64 + 64 = 512</code>、<code>128 + 256 + 64 + 64 = 512</code>、<code>112 + 288 + 64 + 64 = 528</code> 和 <code>256 + 320 + 128 + 128 = 832</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">b4 = nn.Sequential(Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>第五模块包含输出通道数为 <code>256 + 320 + 128 + 128 = 832</code> 和 <code>384 + 384 + 128 + 128 = 1024</code> 的两个 Inception 块。需要注意的是，第五模块的后面紧跟输出层，该模块同 NiN 一样使用全局平均汇聚层，将每个通道的高和宽变成1。最后我们将输出变成二维数组，再接上一个输出个数为标签类别数的全连接层：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">b5 = nn.Sequential(Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">                   nn.Flatten())</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<p>最后我们读取数据集并进行训练，超参数设置：<code>lr, num_epochs = 0.1, 15</code>，训练过程与第一节内容一样，因此不再放出代码。</p>
<h2 id="5-批量规范化（BN）">5. 批量规范化（BN）</h2>
<p>批量规范化（Batch Normalization）是一种流行且有效的技术，可持续加速深层网络的收敛速度。</p>
<p>使用真实数据时，我们的第一步是标准化输入特征，使其平均值为0，方差为1。直观地说，这种标准化可以很好地与我们的优化器配合使用，因为它可以将参数的量级进行统一。</p>
<p>第二，对于典型的多层感知机或卷积神经网络。当我们训练时，中间层中的变量（例如，多层感知机中的仿射变换输出）可能具有更广的变化范围：不论是沿着从输入到输出的层，跨同一层中的单元，或是随着时间的推移，模型参数随着训练更新的变幻莫测。批量规范化的发明者非正式地假设，这些变量分布中的这种偏移可能会阻碍网络的收敛。直观地说，我们可能会猜想，如果一个层的可变值是另一层的100倍，这可能需要对学习率进行补偿调整。</p>
<p>第三，更深层的网络很复杂，容易过拟合。这意味着正则化变得更加重要。</p>
<p>批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，我们首先规范化输入，即通过<strong>减去其均值并除以其标准差</strong>，其中两者均基于当前小批量处理。接下来，我们应用<strong>比例系数和比例偏移</strong>。正是由于这个基于批量统计的标准化，才有了批量规范化的名称。</p>
<p>请注意，如果我们尝试使用大小为1的小批量应用批量规范化，我们将无法学到任何东西。这是因为在减去均值之后，每个隐藏单元将为0。所以，只有使用足够大的小批量，批量规范化这种方法才是有效且稳定的。请注意，在应用批量规范化时，批量大小的选择可能比没有批量规范化时更重要。</p>
<p>通常，我们将批量规范化层置于全连接层中的仿射变换和激活函数之间。同样，对于卷积层，我们可以在卷积层之后和非线性激活函数之前应用批量规范化。</p>
<p>下面，我们从头开始实现一个具有张量的批量规范化层：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">batch_norm</span>(<span class="params">X, gamma, beta, moving_mean, moving_var, eps, momentum</span>):</span><br><span class="line">    <span class="comment"># 通过is_grad_enabled来判断当前模式是训练模式还是预测模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> torch.is_grad_enabled():</span><br><span class="line">        <span class="comment"># 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差</span></span><br><span class="line">        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 假设只考虑全连接和二维卷积</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(X.shape) <span class="keyword">in</span> (<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(X.shape) == <span class="number">2</span>:</span><br><span class="line">            <span class="comment"># 使用全连接层的情况，计算特征维上的均值和方差</span></span><br><span class="line">            mean = X.mean(dim=<span class="number">0</span>)</span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。</span></span><br><span class="line">            <span class="comment"># 这里我们需要保持X的形状以便后面可以做广播运算</span></span><br><span class="line">            mean = X.mean(dim=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(dim=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 训练模式下，用当前的均值和方差做标准化</span></span><br><span class="line">        X_hat = (X - mean) / torch.sqrt(var + eps)</span><br><span class="line">        <span class="comment"># 更新移动平均的均值和方差</span></span><br><span class="line">        moving_mean = momentum * moving_mean + (<span class="number">1.0</span> - momentum) * mean</span><br><span class="line">        moving_var = momentum * moving_var + (<span class="number">1.0</span> - momentum) * var</span><br><span class="line">    Y = gamma * X_hat + beta  <span class="comment"># 缩放和移位</span></span><br><span class="line">    <span class="keyword">return</span> Y, moving_mean.data, moving_var.data</span><br></pre></td></tr></table></figure>
<p>我们现在可以创建一个正确的 BatchNorm 层。这个层将保持适当的参数：拉伸 <code>gamma</code> 和偏移 <code>beta</code>，这两个参数将在训练过程中更新。此外，我们的层将保存均值和方差的移动平均值，以便在模型预测期间随后使用。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BatchNorm</span>(nn.Module):</span><br><span class="line">    <span class="comment"># num_features：全连接层的输出数量或卷积层的输出通道数</span></span><br><span class="line">    <span class="comment"># num_dims：2表示全连接层，4表示卷积层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_features, num_dims</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> num_dims == <span class="number">2</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0</span></span><br><span class="line">        self.gamma = nn.Parameter(torch.ones(shape))</span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(shape))</span><br><span class="line">        <span class="comment"># 非模型参数的变量初始化为0和1</span></span><br><span class="line">        self.moving_mean = torch.zeros(shape)</span><br><span class="line">        self.moving_var = torch.ones(shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># 如果X不在内存上，将moving_mean和moving_var，复制到X所在显存上</span></span><br><span class="line">        <span class="keyword">if</span> self.moving_mean.device != X.device:</span><br><span class="line">            self.moving_mean = self.moving_mean.to(X.device)</span><br><span class="line">            self.moving_var = self.moving_var.to(X.device)</span><br><span class="line">        <span class="comment"># 保存更新过的moving_mean和moving_var</span></span><br><span class="line">        Y, self.moving_mean, self.moving_var = batch_norm(X, self.gamma,</span><br><span class="line">            self.beta, self.moving_mean, self.moving_var, eps=<span class="number">1e-5</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">        <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>
<p>为了更好理解如何应用 BatchNorm，下面我们将其应用于 LeNet 模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), BatchNorm(<span class="number">6</span>, num_dims=<span class="number">4</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), BatchNorm(<span class="number">16</span>, num_dims=<span class="number">4</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), BatchNorm(<span class="number">120</span>, num_dims=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), BatchNorm(<span class="number">84</span>, num_dims=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<p>最后我们读取数据集并进行训练，超参数设置：<code>lr, num_epochs = 1, 15</code>，由于网络模型类似 LeNet，因此无需对输入图像进行 Resize 操作，训练过程与第一节内容一样，因此不再放出代码。</p>
<h2 id="6-残差网络（ResNet）">6. 残差网络（ResNet）</h2>
<p>只有当较复杂的函数类包含较小的函数类时，我们才能确保提高它们的性能。对于深度神经网络，如果我们能将新添加的层训练成恒等映射（identity function）：<code>f(x) = x</code>，新模型和原模型将同样有效。同时，由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。</p>
<p>残差网络的核心思想是：每个附加层都应该更容易地<strong>包含原始函数</strong>作为其元素之一。</p>
<p>ResNet 沿用了 VGG 完整的卷积层设计。残差块里首先有2个有相同输出通道数的卷积层。每个卷积层后接一个批量规范化层和 ReLU 激活函数。然后我们通过跨层数据通路，跳过这2个卷积运算，将输入直接加在最后的 ReLU 激活函数前。这样的设计要求2个卷积层的输出与输入形状一样，从而使它们可以相加。如果想改变通道数，就需要引入一个额外的1×1卷积层来将输入变换成需要的形状后再做相加运算。残差块的实现如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Residual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_channels, num_channels, use_1x1conv=<span class="literal">False</span>, strides=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=strides)</span><br><span class="line">        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">1</span>, stride=strides)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv3 = <span class="literal">None</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(num_channels)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> self.conv3:</span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        Y += X</span><br><span class="line">        <span class="keyword">return</span> F.relu(Y)</span><br></pre></td></tr></table></figure>
<p>此代码生成两种类型的网络：一种是当 <code>use_1x1conv = False</code> 时，应用 ReLU 非线性函数之前，将输入添加到输出。另一种是当 <code>use_1x1conv = True</code> 时，添加通过1×1卷积调整通道和分辨率。</p>
<p>ResNet 的前两层跟之前介绍的 GoogLeNet 中的一样：在输出通道数为64、步幅为2的7×7卷积层后，接步幅为2的3×3最大汇聚层。不同之处在于 ResNet 每个卷积层后增加了批量规范化层。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.BatchNorm2d(<span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>GoogLeNet 在后面接了4个由 Inception 块组成的模块。ResNet 则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。第一个模块的通道数同输入通道数一致。由于之前已经使用了步幅为2的最大汇聚层，所以无须减小高和宽。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。</p>
<p>下面我们来实现这个模块。注意，我们对第一个模块做了特别处理：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_block</span>(<span class="params">input_channels, num_channels, num_residuals, first_block=<span class="literal">False</span></span>):</span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">            blk.append(Residual(input_channels, num_channels, use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(Residual(num_channels, num_channels))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure>
<p>接着在 ResNet 加入所有残差块，这里每个模块使用2个残差块：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">b2 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">b3 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">b4 = nn.Sequential(*resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">b5 = nn.Sequential(*resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p>最后，与 GoogLeNet 一样，在 ResNet 中加入全局平均汇聚层，以及全连接层输出：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5,</span><br><span class="line">                    nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">                    nn.Flatten(), nn.Linear(<span class="number">512</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<p>每个模块有4个卷积层（不包括恒等映射的卷积层）。加上第一个7×7卷积层和最后一个全连接层，共有18层。因此，这种模型通常被称为 ResNet-18。通过配置不同的通道数和模块里的残差块数可以得到不同的 ResNet 模型，例如更深的含152层的 ResNet-152。</p>
<p>最后我们读取数据集并进行训练，超参数设置：<code>lr, num_epochs = 0.02, 15</code>，由于 ResNet 性能很强，对于 FashionMNIST 数据集很容易就过拟合了，因此可以将输入图像 Resize 为 <code>(96, 96)</code>，训练过程与第一节内容一样，因此不再放出代码。</p>
<p>ResNet 其它版本的模型结构可以参考：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/54289848">ResNet 及其变种的结构梳理、有效性分析与代码解读</a>。</li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/085f4c8256f1">ResNet18、50网络结构以及 PyTorch 实现代码</a>。</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/New_WR/article/details/121777644">ResNet50网络结构搭建（PyTorch）</a>。</li>
</ul>
<h2 id="7-稠密连接网络（DenseNet）">7. 稠密连接网络（DenseNet）</h2>
<p>ResNet 和 DenseNet 的关键区别在于，DenseNet 输出是连接（用 <code>[.]</code> 表示）而不是如 ResNet 的简单相加。</p>
<p>稠密网络主要由2部分构成：稠密块（dense block）和过渡层（transition layer）。前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。</p>
<p>DenseNet 使用了 ResNet 改良版的“批量规范化、激活和卷积”架构。我们首先实现一下这个架构：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv_block</span>(<span class="params">input_channels, num_channels</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>一个稠密块由多个卷积块组成，每个卷积块使用相同数量的输出通道。然而，在前向传播中，我们将每个卷积块的输入和输出在通道维上连结：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DenseBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_convs, input_channels, num_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(DenseBlock, self).__init__()</span><br><span class="line">        layer = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">            layer.append(conv_block(num_channels * i + input_channels, num_channels))</span><br><span class="line">        self.net = nn.Sequential(*layer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.net:</span><br><span class="line">            Y = blk(X)</span><br><span class="line">            <span class="comment"># 连接通道维度上每个块的输入和输出</span></span><br><span class="line">            X = torch.cat((X, Y), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>
<p>例如我们构建一个 <code>DenseBlock(2, 3, 10)</code>，那么两层卷积层分别为 <code>conv_block(3, 10)</code>、<code>conv_block(13, 10)</code>。第一层卷积输出的通道维是10，与输入 <code>X</code> 在通道维上连结后通道维是13，因此第二层卷积输入的通道维是13，第二层卷积输出的通道维是10，与输入 <code>X</code> 在通道维上连结后通道维是23：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">blk = DenseBlock(<span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line">X = torch.randn(<span class="number">4</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">Y = blk(X)</span><br><span class="line"><span class="built_in">print</span>(Y.shape)  <span class="comment"># torch.Size([4, 23, 8, 8])</span></span><br></pre></td></tr></table></figure>
<p>由于每个稠密块都会带来通道数的增加，使用过多则会过于复杂化模型。而过渡层可以用来控制模型复杂度。它通过1×1卷积层来<strong>减小通道数</strong>，并使用步幅为2的平均汇聚层减半高和宽，从而进一步降低模型复杂度：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">transition_block</span>(<span class="params">input_channels, num_channels</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p>对上一个例子中稠密块的输出使用通道数为10的过渡层。此时输出的通道数减为10，高和宽均减半：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">blk = transition_block(<span class="number">23</span>, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(blk(Y).shape)  <span class="comment"># torch.Size([4, 10, 4, 4])</span></span><br></pre></td></tr></table></figure>
<p>我们来构造 DenseNet 模型。DenseNet 首先使用同 ResNet 一样的单卷积层和最大汇聚层：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">    nn.BatchNorm2d(<span class="number">64</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>接下来，类似于 ResNet 使用的4个残差块，DenseNet 使用的是4个稠密块。与 ResNet 类似，我们可以设置每个稠密块使用多少个卷积层。这里我们设成4，从而与之前的 ResNet-18 保持一致。稠密块里的卷积层通道数（即增长率）设为32，所以每个稠密块将增加128个通道。</p>
<p>在每个模块之间，ResNet 通过步幅为2的残差块减小高和宽，DenseNet 则使用过渡层来减半高和宽，并减半通道数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># num_channels为当前的通道数</span></span><br><span class="line">num_channels, growth_rate = <span class="number">64</span>, <span class="number">32</span></span><br><span class="line">num_convs_in_dense_blocks = [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]</span><br><span class="line">blks = []</span><br><span class="line"><span class="keyword">for</span> i, num_convs <span class="keyword">in</span> <span class="built_in">enumerate</span>(num_convs_in_dense_blocks):</span><br><span class="line">    blks.append(DenseBlock(num_convs, num_channels, growth_rate))</span><br><span class="line">    <span class="comment"># 上一个稠密块的输出通道数</span></span><br><span class="line">    num_channels += num_convs * growth_rate</span><br><span class="line">    <span class="comment"># 在稠密块之间添加一个转换层，使通道数量减半</span></span><br><span class="line">    <span class="keyword">if</span> i != <span class="built_in">len</span>(num_convs_in_dense_blocks) - <span class="number">1</span>:</span><br><span class="line">        blks.append(transition_block(num_channels, num_channels // <span class="number">2</span>))</span><br><span class="line">        num_channels = num_channels // <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>与 ResNet 类似，最后接上全局汇聚层和全连接层来输出结果：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    b1, *blks,</span><br><span class="line">    nn.BatchNorm2d(num_channels), nn.ReLU(),</span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(num_channels, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<p>最后我们读取数据集并进行训练，超参数设置：<code>lr, num_epochs = 0.1, 15</code>，将输入图像 Resize 为 <code>(96, 96)</code>，训练过程与第一节内容一样，因此不再放出代码。</p>
<p>下一章：<a href="/posts/24840.html">计算机视觉</a>。</p>
</div>
        </div>
        
        <footer class="kratos-entry-footer clearfix">
            
                <div class="post-like-donate text-center clearfix" id="post-like-donate">
                
                
                    <a class="share" href="javascript:;"><i class="fa fa-share-alt"></i> Share</a>
                    <div class="share-wrap" style="display: none;">
    <div class="share-group">
        <a href="javascript:;" class="share-plain qq" onclick="share('qq');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-qq"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain qzone" onclick="share('qzone');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-star"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain weixin pop style-plain" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-weixin"></i>
            </div>
            <div class="share-int">
                <div class="qrcode" id="wechat-qr"></div>
                <p>打开微信“扫一扫”，打开网页后点击屏幕右上角分享按钮</p>
            </div>
        </a>
        <a href="javascript:;" class="share-plain weibo" onclick="share('weibo');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-weibo"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain facebook style-plain" onclick="share('facebook');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-facebook"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain twitter style-plain" onclick="share('twitter');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-twitter"></i>
            </div>
        </a>
    </div>
    <script type="text/javascript">
        $(()=>{
            new QRCode("wechat-qr", {
                text: "https://asanosaki.github.io/posts/21165.html",
                width: 150,
                height: 150,
                correctLevel : QRCode.CorrectLevel.H
            });
        });
        function share(dest) {
            const qqBase        = "https://connect.qq.com/widget/shareqq/index.html?";
            const weiboBase     = "https://service.weibo.com/share/share.php?";
            const qzoneBase     = "https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?";
            const facebookBase  = "https://www.facebook.com/sharer/sharer.php?";
            const twitterBase   = "https://twitter.com/intent/tweet?";
            const hostUrl       = "https://asanosaki.github.io/posts/21165.html";
            const title         = "「D2L学习笔记-现代卷积神经网络」";
            const excerpt       = `李沐动手学深度学习（PyTorch）课程学习笔记第六章：现代卷积神经网络。`;
            let _URL;
            switch (dest) {
                case "qq"       : _URL = qqBase+"url="+hostUrl+"&title="+title+"&desc=&summary="+excerpt+"&site=cxpy";     break;
                case "weibo"    : _URL = weiboBase+"url="+hostUrl+"&title="+title+excerpt;                                 break;
                case "qzone"    : _URL = qzoneBase+"url="+hostUrl+"&title="+title+"&desc=&summary="+excerpt+"&site=cxpy";  break;
                case "facebook" : _URL = facebookBase+"u="+hostUrl;                                                        break;
                case "twitter"  : _URL = twitterBase+"text="+title+excerpt+"&url="+hostUrl;                                break;
            }
            window.open(_URL);
        };
    </script>
</div>
                
                </div>
            
            <div class="footer-tag clearfix">
                <div class="pull-left">
                <i class="fa fa-tags"></i>
                    <a class="tag-none-link" href="/tags/AI/" rel="tag">AI</a>
                </div>
                <div class="pull-date">
                    <time datetime="2023-11-26T07:57:39.296Z" itemprop="dateModified">Last edited: 2023-11-26</time>
                </div>
            </div>
        </footer>
    </div>
    
        <nav class="navigation post-navigation clearfix" role="navigation">
            
            <div class="nav-previous clearfix">
                <a title=" D2L学习笔记-卷积神经网络" href="/posts/25122.html">&lt; Previous</a>
            </div>
            
            
            <div class="nav-next clearfix">
                <a title=" D2L学习笔记-计算机视觉" href="/posts/24840.html">Next &gt;</a>
            </div>
            
        </nav>
    
    
</article>

        

            </section>

        

                
            

<section id="kratos-widget-area" class="col-md-4 hidden-xs hidden-sm">
    <!-- 文章和页面根据splitter来分割，没有的话就从头开始设置为sticky -->
    
    
                <aside id="krw-about" class="widget widget-kratos-about clearfix">
    <div class="photo-background"></div>
    <div class="photo-wrapper clearfix">
        <div class="photo-wrapper-tip text-center">
            <img class="about-photo" src="/images/head.webp" loading="lazy" decoding="auto" />
        </div>
    </div>
    <div class="textwidget">
        <p class="text-center"></p>
    </div>
    <div class="site-meta">
        <a class="meta-item" href="/archives/">
            <span class="title">
                Articles
            </span>
            <span class="count">
                87
            </span>
        </a>
        <a class="meta-item" href="/categories/">
            <span class="title">
                Classifications
            </span>
            <span class="count">
                9
            </span>
        </a>
        <a class="meta-item" href="/tags/">
            <span class="title">
                Tags
            </span>
            <span class="count">
                9
            </span>
        </a>
    </div>
</aside>
            
                    <div class="sticky-area">
                
                    <aside id="krw-toc" class="widget widget-kratos-toc clearfix toc-div-class" >
    <div class="photo-background"></div>
    <h4 class="widget-title no-after">
        <i class="fa fa-compass"></i>
        Contents
        <span class="toc-progress-bar" role="progressbar" aria-label="阅读进度："></span>
    </h4>
    <div class="textwidget">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88AlexNet%EF%BC%89"><span class="toc-text">1. 深度卷积神经网络（AlexNet）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E4%BD%BF%E7%94%A8%E5%9D%97%E7%9A%84%E7%BD%91%E7%BB%9C%EF%BC%88VGG%EF%BC%89"><span class="toc-text">2. 使用块的网络（VGG）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%EF%BC%88NiN%EF%BC%89"><span class="toc-text">3. 网络中的网络（NiN）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%90%AB%E5%B9%B6%E8%A1%8C%E8%BF%9E%E7%BB%93%E7%9A%84%E7%BD%91%E7%BB%9C%EF%BC%88GoogLeNet%EF%BC%89"><span class="toc-text">4. 含并行连结的网络（GoogLeNet）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%89%B9%E9%87%8F%E8%A7%84%E8%8C%83%E5%8C%96%EF%BC%88BN%EF%BC%89"><span class="toc-text">5. 批量规范化（BN）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C%EF%BC%88ResNet%EF%BC%89"><span class="toc-text">6. 残差网络（ResNet）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E7%A8%A0%E5%AF%86%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9C%EF%BC%88DenseNet%EF%BC%89"><span class="toc-text">7. 稠密连接网络（DenseNet）</span></a></li></ol>
    </div>
</aside>
                
                
  <aside id="krw-categories" class="widget widget-kratos-categories clearfix">
    <h4 class="widget-title"><i class="fa fa-folder"></i>Contents</h4>
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AI/">AI</a><span class="category-list-count">18</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Interview/">Interview</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Others/">Others</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">18</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Web/">Web</a><span class="category-list-count">9</span></li></ul>
  </aside>


            
                
  <aside id="krw-tags" class="widget widget-kratos-tags clearfix">
    <h4 class="widget-title"><i class="fa fa-tags"></i>Tags aggregation</h4>
      <div class="tag-clouds">
        <a href="/tags/AI/" style="font-size: 0.77em;">AI</a> <a href="/tags/C/" style="font-size: 0.6em;">C++</a> <a href="/tags/Hexo/" style="font-size: 0.63em;">Hexo</a> <a href="/tags/Interview/" style="font-size: 0.67em;">Interview</a> <a href="/tags/Java/" style="font-size: 0.8em;">Java</a> <a href="/tags/Linux/" style="font-size: 0.67em;">Linux</a> <a href="/tags/Others/" style="font-size: 0.7em;">Others</a> <a href="/tags/Python/" style="font-size: 0.77em;">Python</a> <a href="/tags/Web/" style="font-size: 0.73em;">Web</a>
      </div>
  </aside>

            
                
  <aside id="krw-posts" class="widget widget-kratos-posts">
  <h4 class="widget-title"><i class="fa fa-file"></i>Latest articles</h4>
  <div class="tab-content">
      <ul class="list-group">
        
        
          
          
            <a class="list-group-item" href="/posts/32409.html"><i class="fa  fa-book"></i> Determined AI部署与使用教程</a>
            
          
        
          
          
            <a class="list-group-item" href="/posts/5710.html"><i class="fa  fa-book"></i> SpringBoot学习笔记-实现微服务：匹配系统（中）</a>
            
          
        
          
          
            <a class="list-group-item" href="/posts/2329.html"><i class="fa  fa-book"></i> Spring面试题总结</a>
            
          
        
          
          
            <a class="list-group-item" href="/posts/53737.html"><i class="fa  fa-book"></i> Java进阶面试题总结</a>
            
          
        
          
          
            <a class="list-group-item" href="/posts/36915.html"><i class="fa  fa-book"></i> Java基础面试题总结</a>
            
          
        
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
      </ul>
  </div>
  </aside>

            
    </div>
</section>
        
        </div>
    </div>
</div>
<footer>
    <div id="footer"  >
        <div class="container">
            <div class="row">
                <div class="col-md-6 col-md-offset-3 footer-list text-center">
                    <ul class="kratos-social-icons">
                        <!-- Keep for compatibility -->
                        <li><a target="_blank" rel="nofollow" href="https://weibo.com/u/1952449115"><i class="fa fa-weibo"></i></a></li>
                        <li><a href="mailto:mail@1195595343@qq.com"><i class="fa fa-envelope"></i></a></li>
                        
                        
                        
                        
                        
                        <li><a target="_blank" rel="nofollow" href="https://github.com/AsanoSaki"><i class="fa fa-github"></i></a></li>
                        
                        <!-- New links -->
                        
                    </ul>
                    <ul class="kratos-copyright">
                        <div>
                            <li>&copy; 2024 AsanoSaki Copyright.</li>
                            <li>This site is running<span id="span_dt">Loading...</span></li>
                        </div>
                        <div>
                            <li>Theme <a href="https://github.com/Candinya/Kratos-Rebirth" target="_blank">Kratos:Rebirth</a></li>
                            <li>Site built with&nbsp;<i class="fa fa-heart throb" style="color:#d43f57"></i>&nbsp;by AsanoSaki.</li>
                        </div>
                        <div>
                            <li>Powered by <a href="https://hexo.io" target="_blank" rel="nofollow">Hexo</a></li>
                            <li>Hosted on <a href="https://github.com/" target="_blank">Github Pages</a></li>
                        </div>
                        <div>
                            
                            
                        </div>
                    </ul>
                </div>
            </div>
        </div>
        <div class="kr-tool text-center">
            <div class="tool">
                
                    <div class="box search-box">
                        <a href="/search/">
                            <span class="fa fa-search"></span>
                        </a>
                    </div>
                
                
                    <div class="box theme-box" id="darkmode-switch">
                        <span class="fa fa-adjust"></span>
                    </div>
                
                
                
            </div>
            <div class="box gotop-box">
                <span class="fa fa-chevron-up"></span>
            </div>
        </div>
    </div>
</footer>
</div>
</div>

        <script defer src="/vendors/bootstrap@3.3.4/dist/js/bootstrap.min.js"></script>
<script defer src="/vendors/nprogress@0.2.0/nprogress.js"></script>
<script>
    if (!window.kr) {
        window.kr = {};
    }
    window.kr.notMobile = (!(navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i)));
    window.kr.siteRoot = "/";
</script>





    <script defer src="/vendors/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>

<script defer src="/js/kratosr.min.js"></script>
<script defer src="/js/pjax.min.js"></script>



<!-- Extra support for third-party plguins  -->


    </body>
</html>