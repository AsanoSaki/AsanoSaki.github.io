<!DOCTYPE html>
<html lang="zh-CN">
    <head>
  <!-- 元数据 -->
  <meta charset="utf-8">
  <link rel="icon" href="/images/favicon.ico">
  
  <title>PyTorch深度学习入门(CIFAR10分类) | AsanoSaki</title>
  <meta name="author" content="AsanoSaki" />
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="robots" content="index,follow" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <meta name="format-detection" content="telphone=no, email=no" />
  
    <meta name="keywords" content="AI" />
  
  <meta name="description" content="PyTorch深度学习入门(CIFAR10分类)">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch深度学习入门(CIFAR10分类)">
<meta property="og:url" content="https://asanosaki.github.io/posts/48394.html">
<meta property="og:site_name" content="AsanoSaki">
<meta property="og:description" content="PyTorch深度学习入门(CIFAR10分类)">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://asanosaki.github.io/images/favicon.ico">
<meta property="article:published_time" content="2022-12-01T10:22:00.000Z">
<meta property="article:modified_time" content="2023-05-26T12:38:11.493Z">
<meta property="article:author" content="AsanoSaki">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://asanosaki.github.io/images/favicon.ico">
  
  <!-- 站点验证相关 -->
  
    
    
    
  
  <!-- 样式表文件 -->
  <link rel="stylesheet" id="kratos-css" href="/css/kratosr.min.css" media="all"></script>
  
    <link rel="stylesheet" id="darkmode-css" href="/css/kr-color-dark.min.css" media="(prefers-color-scheme: dark)"></script>
    <script src="/js/kr-dark.min.js"></script>
  
  
    <link rel="stylesheet" id="highlight-css" href="/css/highlight/light.min.css" media="all"></script>
  
  
  <link rel="stylesheet" id="fontawe-css" href="/vendors/font-awesome@4.7.0/css/font-awesome.min.css" media="all"></script>
  <link rel="stylesheet" id="nprogress-css" href="/vendors/nprogress@0.2.0/nprogress.css" media="all"></script>
  
  
    <link rel="stylesheet" href="/vendors/aplayer@1.10.1/dist/APlayer.min.css"></script>
  
  
    <link rel="stylesheet" href="/vendors/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"></script>
  
  <!-- 不得不预先加载的一些JS文件 -->
  <script src="/vendors/jquery@3.6.0/dist/jquery.min.js"></script>
  
    <script src="/vendors/qrcode_js@1.0.0/qrcode.min.js"></script>
  
  
  <style>
    
      .kratos-cover.kratos-cover-2 {
        background-image: url('https://z4a.net/images/2023/02/23/background03.jpg');
      }
    
    
      @media(min-width:768px) {
        body.custom-background {
          background-image: url('https://z4a.net/images/2023/02/23/background02.jpg');
        }
      }
    
  </style>
  
<meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="AsanoSaki" type="application/atom+xml">
</head>


    <body class="custom-background">
        <div id="kratos-wrapper">
    <div id="kratos-page">
        <div id="kratos-header">
            <header id="kratos-desktop-topnav" class="kratos-topnav">
                <div class="container">
                    <div class="nav-header">
                        <nav id="kratos-menu-wrap">
                            <ul id="kratos-primary-menu" class="sf-menu">
                                
                                    
                                        <li>
                                            
                                                <a href="/">
                                            
                                                
                                                    <i class="fa fa-home"></i>
                                                
                                                Home
                                            </a>
                                            
                                        </li>
                                    
                                        <li>
                                            
                                                <a href="/archives/">
                                            
                                                
                                                    <i class="fa fa-file"></i>
                                                
                                                Archives
                                            </a>
                                            
                                        </li>
                                    
                                        <li>
                                            
                                                <a>
                                            
                                                
                                                    <i class="fa fa-paw"></i>
                                                
                                                Friends
                                            </a>
                                            
                                                <ul class="sub-menu">
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://angels-d.github.io">
                                                                
                                                                Angels-D
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="http://syh521.cn">
                                                                
                                                                SYH
                                                            </a>
                                                        </li>
                                                    
                                                </ul>
                                            
                                        </li>
                                    
                                        <li>
                                            
                                                <a>
                                            
                                                
                                                    <i class="fa fa-link"></i>
                                                
                                                Links
                                            </a>
                                            
                                                <ul class="sub-menu">
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://github.com/AsanoSaki">
                                                                
                                                                GitHub
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://gitee.com/asanosaki">
                                                                
                                                                Gitee
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://git.acwing.com/AsanoSaki">
                                                                
                                                                AcGit
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a href="https://asanosaki.github.io">
                                                                
                                                                BLOG
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://asanosaki.blog.csdn.net">
                                                                
                                                                CSDN
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://www.acwing.com/user/myspace/index/82581">
                                                                
                                                                AcWing
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://www.luogu.com.cn/user/459347">
                                                                
                                                                LuoGu
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://codeforces.com/profile/AsanoSaki">
                                                                
                                                                CodeForces
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://leetcode.cn/u/asanosaki">
                                                                
                                                                LeetCode
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://space.bilibili.com/12300056">
                                                                
                                                                Bilibili
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://www.youtube.com/@AsanoSaki0417">
                                                                
                                                                YouTube
                                                            </a>
                                                        </li>
                                                    
                                                </ul>
                                            
                                        </li>
                                    
                                        <li>
                                            
                                                <a>
                                            
                                                
                                                    <i class="fa fa-heart"></i>
                                                
                                                Favorite
                                            </a>
                                            
                                                <ul class="sub-menu">
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://www.cnki.net">
                                                                
                                                                CNKI
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://www.runoob.com">
                                                                
                                                                Runoob
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://wallhaven.cc">
                                                                
                                                                Wallhaven
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://mirror.yibook.org">
                                                                
                                                                Zlibrary
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="http://www.flysheep6.com">
                                                                
                                                                Flysheep
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://thrift.apache.org">
                                                                
                                                                Thrift
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://developer.mozilla.org/en-US">
                                                                
                                                                MDN
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://v5.bootcss.com">
                                                                
                                                                Bootstrap
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://reactjs.org">
                                                                
                                                                React
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://hub.docker.com">
                                                                
                                                                DockerHub
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://fontawesome.com.cn">
                                                                
                                                                FontAwesome
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://www.online-convert.com">
                                                                
                                                                File-Convert
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://pytorch.org">
                                                                
                                                                PyTorch
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai">
                                                                
                                                                D2L
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="http://www.cvtutorials.com">
                                                                
                                                                CVTutorials
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://oi-wiki.org">
                                                                
                                                                OI-Wiki
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://board.xcpcio.com">
                                                                
                                                                XCPC-Board
                                                            </a>
                                                        </li>
                                                    
                                                        <li>
                                                            <a target="_blank" rel="noopener" href="https://kr-demo.candinya.com/posts/Kratos-Rebirth-Manual">
                                                                
                                                                Kratos-MNL
                                                            </a>
                                                        </li>
                                                    
                                                </ul>
                                            
                                        </li>
                                    
                                
                            </ul>
                        </nav>
                    </div>
                </div>
            </header>
            <header id="kratos-mobile-topnav" class="kratos-topnav">
                <div class="container">
                    <div class="color-logo"><a href="/">AsanoSaki</a></div>
                    <div class="nav-toggle">
                        <a class="kratos-nav-toggle js-kratos-nav-toggle">
                            <i></i>
                        </a>
                    </div>
                </div>
            </header>
        </div>
        <div class="kratos-start kratos-hero-2">
            <!-- <div class="kratos-overlay"></div> -->
            <div class="kratos-cover kratos-cover-2 text-center">
                <div class="desc desc2 animate-box">
                    <a href="/">
                        <h2>AsanoSaki</h2> <br />
                        <span>In summer, the sea breeze tastes like ice cream.</span>
                    </a>
                </div>
            </div>
        </div>

        <div id="kratos-blog-post">
            <div class="container">
                <div id="main" class="row">
                    

        

            <section class="col-md-8">

        

            <article itemscope itemtype="https://schema.org/Article">
    
    <link itemprop="mainEntityOfPage" href="https://asanosaki.github.io/posts/48394.html">
    <div class="kratos-hentry kratos-post-inner clearfix">
        <header class="kratos-entry-header">
            
                <h1 class="kratos-entry-title text-center" itemprop="name headline">PyTorch深度学习入门(CIFAR10分类)</h1>
            
            
            <ul class="kratos-post-meta text-center">
                <li><time datetime="2022-12-01T10:22:00.000Z" itemprop="datePublished"><i class="fa fa-calendar"></i> 2022-12-01</time></li>
                <li itemprop="author" itemscope itemtype="https://schema.org/Person">
                    <i class="fa fa-user"></i> Author <span itemprop="name">AsanoSaki</span>
                </li>
                <li>
                    <i class="fa fa-edit"></i> 
                    
                    
                        ~41.76K
                    
                    words
                </li>
                
            </ul>
        </header>
        <div class="kratos-post-content">
            
            <div id="expire-alert" class="alert alert-warning hidden" role="alert">
                <div class="icon"><i class="fa fa-warning"></i></div>
                <div class="text"><p>本文最后编辑于 <time datetime="1685104691493"></time> 前，其中的内容可能需要更新。</p></div>
            </div>
            
            
            
                <div class="kratos-post-inner-toc toc-div-class" >
                    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0"><span class="toc-number">1.</span> <span class="toc-text">1. 常用函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="toc-number">2.</span> <span class="toc-text">2. 数据加载</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Dataset"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 Dataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-DataLoader"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 DataLoader</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-TensorBoard"><span class="toc-number">3.</span> <span class="toc-text">3. TensorBoard</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-add-scalar"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 add_scalar</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-add-image"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 add_image</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Transform"><span class="toc-number">4.</span> <span class="toc-text">4. Transform</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Transform%E7%9A%84%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95"><span class="toc-number">4.1.</span> <span class="toc-text">4.1 Transform的概念与基本用法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Transform%E7%9A%84%E5%B8%B8%E7%94%A8%E7%B1%BB"><span class="toc-number">4.2.</span> <span class="toc-text">4.2 Transform的常用类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Torchvision%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-number">5.</span> <span class="toc-text">5. Torchvision数据集使用方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CTorch-NN%E5%9F%BA%E6%9C%AC%E9%AA%A8%E6%9E%B6%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">6.</span> <span class="toc-text">6. 神经网络Torch.NN基本骨架的使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Convolution-Layers%E4%B8%8EPooling-Layers"><span class="toc-number">7.</span> <span class="toc-text">7. Convolution Layers与Pooling Layers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-Convolution-Layers"><span class="toc-number">7.1.</span> <span class="toc-text">7.1 Convolution Layers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-Pooling-Layers"><span class="toc-number">7.2.</span> <span class="toc-text">7.2 Pooling Layers</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-Non-linear-Activations%E4%B8%8ELinear-Layers"><span class="toc-number">8.</span> <span class="toc-text">8. Non-linear Activations与Linear Layers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-Non-linear-Activations"><span class="toc-number">8.1.</span> <span class="toc-text">8.1 Non-linear Activations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-Linear-Layers"><span class="toc-number">8.2.</span> <span class="toc-text">8.2 Linear Layers</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA%E5%B0%8F%E5%AE%9E%E6%88%98"><span class="toc-number">9.</span> <span class="toc-text">9. 神经网络模型搭建小实战</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-1-Sequential"><span class="toc-number">9.1.</span> <span class="toc-text">9.1 Sequential</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-%E5%B0%8F%E5%AE%9E%E6%88%98"><span class="toc-number">9.2.</span> <span class="toc-text">9.2 小实战</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">10.</span> <span class="toc-text">10. 损失函数与反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-1-Loss-Functions"><span class="toc-number">10.1.</span> <span class="toc-text">10.1 Loss Functions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-Backward"><span class="toc-number">10.2.</span> <span class="toc-text">10.2 Backward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-3-Optimizer"><span class="toc-number">10.3.</span> <span class="toc-text">10.3 Optimizer</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-%E7%8E%B0%E6%9C%89%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%BF%E7%94%A8%E5%8F%8A%E4%BF%AE%E6%94%B9"><span class="toc-number">11.</span> <span class="toc-text">11. 现有网络模型的使用及修改</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-1-VGG16%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">11.1.</span> <span class="toc-text">11.1 VGG16模型的使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-2-%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%AF%BB%E5%8F%96"><span class="toc-number">11.2.</span> <span class="toc-text">11.2 模型的保存与读取</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-%E5%AE%8C%E6%95%B4%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">12.</span> <span class="toc-text">12. 完整训练模型的方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#12-1-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%97%B6%E7%9A%84%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="toc-number">12.1.</span> <span class="toc-text">12.1 训练模型时的注意事项</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-2-%E4%BD%BF%E7%94%A8GPU%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83"><span class="toc-number">12.2.</span> <span class="toc-text">12.2 使用GPU进行训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-3-CIFAR10-Net-Simple-v3"><span class="toc-number">12.3.</span> <span class="toc-text">12.3 CIFAR10_Net_Simple_v3</span></a></li></ol></li></ol>
                </div>
            
            <hr />
            <div itemprop="articleBody"><blockquote>
<p>通过 CIFAR10 数据集的分类问题初入门 Deep Learning，也是开坑 AI 系列的第一篇文章。<br>
相关环境的搭建可以转至：<a href="/posts/15428.html">Anaconda与PyTorch安装教程</a>。</p>
</blockquote>
<span id="more"></span>
<h2 id="1-常用函数">1. 常用函数</h2>
<p>（1）路径函数</p>
<p>在 <code>os</code> 模块中常用的路径相关函数有：</p>
<ul>
<li><code>os.listdir(path)</code>：将 <code>path</code> 目录下的内容列成一个 <code>list</code>。</li>
<li><code>os.path.join(path1, path2)</code>：拼接路径：<code>path1\path2</code>。</li>
</ul>
<p>例如：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">dir_path = <span class="string">&#x27;dataset/hymenoptera_data/train/ants_image&#x27;</span></span><br><span class="line">img_path_list = os.listdir(dir_path)</span><br><span class="line">img_full_path = os.path.join(dir_path, img_path_list[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(img_path_list)  <span class="comment"># [&#x27;0013035.jpg&#x27;, &#x27;1030023514_aad5c608f9.jpg&#x27;, ...]</span></span><br><span class="line"><span class="built_in">print</span>(img_full_path)  <span class="comment"># dataset/hymenoptera_data/train/ants_image\0013035.jpg</span></span><br></pre></td></tr></table></figure>
<p>（2）辅助函数</p>
<ul>
<li><code>dir()</code>：不带参数时，返回当前范围内的变量、方法和定义的类型列表；带参数时，返回参数的属性、方法列表。</li>
<li><code>help(func)</code>：查看函数 <code>func</code> 的使用说明。</li>
</ul>
<p>例如：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">dir</span>(torch))  <span class="comment"># [&#x27;AVG&#x27;, &#x27;AggregationType&#x27;, ..., &#x27;cuda&#x27;, ...]</span></span><br><span class="line"><span class="built_in">help</span>(torch.cuda.is_available)  <span class="comment"># Help on function is_available in module torch.cuda: is_available() -&gt; bool...</span></span><br></pre></td></tr></table></figure>
<h2 id="2-数据加载">2. 数据加载</h2>
<h3 id="2-1-Dataset">2.1 Dataset</h3>
<p>数据读取和预处理是进行机器学习的首要操作，PyTorch 提供了很多方法来完成数据的读取和预处理。</p>
<p>其中 Dataset 表示数据集，<code>torch.utils.data.Dataset</code> 是代表这一数据的抽象类。你可以自己定义你的数据类，继承和重写这个抽象类，非常简单，只需要定义 <code>__len__</code> 和 <code>__getitem__</code> 这个两个函数即可，例如：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyData</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root_dir, label_dir</span>):</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.label_dir = label_dir</span><br><span class="line">        self.path = os.path.join(self.root_dir, self.label_dir + <span class="string">&#x27;_image&#x27;</span>)</span><br><span class="line">        self.img_path_list = os.listdir(self.path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        img_path = self.img_path_list[idx]</span><br><span class="line">        img_full_path = os.path.join(self.root_dir, self.label_dir + <span class="string">&#x27;_image&#x27;</span>, img_path)</span><br><span class="line">        img = Image.<span class="built_in">open</span>(img_full_path)</span><br><span class="line">        label = self.label_dir</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_path_list)</span><br><span class="line"></span><br><span class="line">root_dir = <span class="string">&#x27;dataset/hymenoptera_data/train&#x27;</span></span><br><span class="line">ants_label_dir = <span class="string">&#x27;ants&#x27;</span></span><br><span class="line"></span><br><span class="line">ants_data = MyData(root_dir, ants_label_dir)</span><br><span class="line">img, label = ants_data[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(img, label)</span><br><span class="line">img.show()</span><br></pre></td></tr></table></figure>
<p>通过上面的方式，可以定义我们需要的数据类，可以通过迭代的方式来获取每一个数据，但这样很难实现取 batch、shuffle 或者是多线程去读取数据。</p>
<h3 id="2-2-DataLoader">2.2 DataLoader</h3>
<p><code>torch.utils.data.DataLoader</code> 构建可迭代的数据装载器，我们在训练的时候，每一个 <code>for</code> 循环，每一次 iteration，就是从 <code>DataLoader</code> 中获取一个 <code>batch_size</code> 大小的数据的。打个比方如果 <code>Dataset</code> 是一副完整的扑克牌，那么 <code>DataLoader</code> 就是抽取几张组成的一部分扑克牌。</p>
<p><code>DataLoader</code> 的参数很多，但我们常用的主要有以下几个：</p>
<ul>
<li><code>dataset</code>：<code>Dataset</code> 类，决定从哪个数据集读取数据。</li>
<li><code>batch_size</code>：批大小。</li>
<li><code>num_works</code>：是否多进程读取机制。</li>
<li><code>shuffle</code>：每个 Epoch 是否乱序。</li>
<li><code>drop_last</code>：当样本数不能被 <code>batch_size</code> 整除时，是否舍弃最后一批数据。</li>
</ul>
<p>要理解这个 <code>drop_last</code>，首先，得先理解 Epoch、Iteration 和 Batch_size 的概念：</p>
<ul>
<li>Epoch：所有训练样本都已输入到模型中，称为一个 Epoch。</li>
<li>Iteration：一批样本输入到模型中，称为一个 Iteration。</li>
<li>Batch_size：一批样本的大小，决定一个 Epoch 有多少个 Iteration。</li>
</ul>
<p><code>DataLoader</code> 的作用就是构建一个数据装载器，根据我们提供的 <code>batch_size</code> 的大小，将数据样本分成一个个的 Batch 去训练模型，而这个分的过程中需要把数据取到，这个就是借助 <code>Dataset</code> 的 <code>__getitem__</code> 方法。</p>
<p>例如：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyData</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root_dir, label_dir, transform</span>):</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.label_dir = label_dir</span><br><span class="line">        self.path = os.path.join(self.root_dir, self.label_dir + <span class="string">&#x27;_image&#x27;</span>)</span><br><span class="line">        self.img_path_list = os.listdir(self.path)</span><br><span class="line">        self.transform = transform  <span class="comment"># transform 的方式</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        img_path = self.img_path_list[idx]</span><br><span class="line">        img_full_path = os.path.join(self.root_dir, self.label_dir + <span class="string">&#x27;_image&#x27;</span>, img_path)</span><br><span class="line">        img = Image.<span class="built_in">open</span>(img_full_path).convert(<span class="string">&#x27;RGB&#x27;</span>)  <span class="comment"># 先将图片转换成三通道</span></span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            img = self.transform(img)</span><br><span class="line">        label = self.label_dir</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_path_list)</span><br><span class="line"></span><br><span class="line">root_dir = <span class="string">&#x27;dataset/hymenoptera_data/train&#x27;</span></span><br><span class="line">ants_label_dir = <span class="string">&#x27;ants&#x27;</span></span><br><span class="line"></span><br><span class="line">trans_dataset = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">83</span>, <span class="number">100</span>)),  <span class="comment"># tensor 大小必须统一</span></span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">ants_data = MyData(root_dir, ants_label_dir, trans_dataset)</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(dataset=ants_data, batch_size=<span class="number">10</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>, drop_last=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">    imgs, labels = data</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(imgs))</span><br><span class="line">    <span class="built_in">print</span>(imgs[<span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(labels)</span><br><span class="line">    <span class="built_in">print</span>(labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>接下来使用 CIFAR10 数据集再展示一次 <code>DataLoader</code> 的用法：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">test_set = datasets.CIFAR10(root=<span class="string">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class="literal">False</span>, transform=transforms.ToTensor(), download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_loader = DataLoader(dataset=test_set, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>, drop_last=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):  <span class="comment"># 循环两个 epoch</span></span><br><span class="line">    <span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_loader):  <span class="comment"># step 表示第几个 batch</span></span><br><span class="line">        imgs, targets = data</span><br><span class="line">        writer.add_images(<span class="string">&#x27;Epoch_&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(epoch), imgs, step)  <span class="comment"># 注意是 add_images，图像默认格式为 NCHW</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p>PS：部分看不懂的代码可以先去学后面的 <code>transform</code> 以及 <code>tensorboard</code>。</p>
<h2 id="3-TensorBoard">3. TensorBoard</h2>
<h3 id="3-1-add-scalar">3.1 add_scalar</h3>
<p>TensorBoard 原本是 TensorFlow 的可视化工具，PyTorch 从1.2.0开始支持 TensorBoard。之前的版本也可以使用 TensorBoardX 代替。</p>
<p>先进入 Anaconda 的 PyTorch 环境，安装 TensorBoard：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda activate PyTorch</span><br><span class="line">pip install tensorboard</span><br></pre></td></tr></table></figure>
<p>在项目根目录下新建一个文件夹 <code>logs</code>，TensorBoard 的工作流程简单来说是将代码运行过程中的，某些你关心的数据保存在这个文件夹中（由代码中的 <code>writer</code> 完成），再读取这个文件夹中的数据，用浏览器显示出来（在命令行运行 TensorBoard 完成）。</p>
<p>我们先绘制一个 <code>y = x</code> 的图像，运行以下代码：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;y=x&#x27;</span>, x, x)  <span class="comment"># tag=&#x27;y=x&#x27;, scalar_value=x, global_step=x</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p><code>add_scalar</code> 函数主要有三个参数：</p>
<ul>
<li><code>tag</code>：数据标识符，可以理解为数据图像的标题。</li>
<li><code>scalar_value</code>：保存的值，即纵轴上的值。</li>
<li><code>global_step</code>：记录的步长，即横轴的值，一般会设置一个不断增加的 <code>step</code>。</li>
</ul>
<p>运行后会看到 <code>logs</code> 文件夹下生成了一个文件，然后我们在 PyCharm 终端的 PyTorch 环境中打开 TensorBoard（要在当前项目中进入 PyTorch 环境，否则 <code>--logdir</code> 的路径就不能用相对路径了）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir logs</span><br></pre></td></tr></table></figure>
<p>打开 <code>http://localhost:6006/</code> 即可看到绘制的图像。</p>
<p>如果因为某些原因导致端口冲突可以指定端口：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir logs --port 6007</span><br></pre></td></tr></table></figure>
<h3 id="3-2-add-image">3.2 add_image</h3>
<p><code>add_image</code> 函数主要有三个参数：</p>
<ul>
<li><code>tag</code>：同 <code>add_scalar</code>。</li>
<li><code>img_tensor</code>：图像数据，类型必须是 <code>torch.Tensor</code>、<code>numpy.ndarry</code> 或 <code>string/blobname</code>。</li>
<li><code>global_step</code>：同 <code>add_scalar</code>。</li>
</ul>
<p>可以看到传入的图片数据有类型限制，目前还没学到 <code>torch.Tensor</code> 类型，以 <code>numpy.ndarry</code> 为例，因此我们需要先安装一下 NumPy，还是在 PyTorch 环境中安装：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install numpy</span><br></pre></td></tr></table></figure>
<p>使用 <code>PIL</code> 打开一个图像，将其转换成 NumPy 数组：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&#x27;dataset/hymenoptera_data/train/ants_image/0013035.jpg&#x27;</span></span><br><span class="line">img_PIL = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line">img_array = np.array(img_PIL)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(img_array))  <span class="comment"># &lt;class &#x27;numpy.ndarray&#x27;&gt;</span></span><br><span class="line"><span class="built_in">print</span>(img_array.shape)  <span class="comment"># (512, 768, 3)</span></span><br></pre></td></tr></table></figure>
<p>可以看到图片的形状是三维的数据，前两个数据分别表示高度和宽度，第三个数据表示通道数，可以记为 <code>(H, W, C)</code>，简写为 <code>HWC</code>。</p>
<p><code>add_image</code> 函数传入图片时格式默认为 <code>CHW</code>，如果格式不匹配需要设定函数中的 <code>dataformats</code> 参数，例如：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line">img_path = <span class="string">&#x27;dataset/hymenoptera_data/train/ants_image/0013035.jpg&#x27;</span></span><br><span class="line">img_PIL = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line">img_array = np.array(img_PIL)</span><br><span class="line"></span><br><span class="line">writer.add_image(<span class="string">&#x27;img_test&#x27;</span>, img_array, <span class="number">1</span>, dataformats=<span class="string">&#x27;HWC&#x27;</span>)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p>运行后打开 TensorBoard 即可在 IMAGES 页面下看到图片。</p>
<h2 id="4-Transform">4. Transform</h2>
<h3 id="4-1-Transform的概念与基本用法">4.1 Transform的概念与基本用法</h3>
<p><code>transforms</code> 在计算机视觉工具包 <code>torchvision</code> 下，包含了很多种对图像数据进行变换的类，这些都是在我们进行图像数据读入步骤中必不可少的，通过图像变换可以将图片变成不同的类型，或者可以通过旋转、裁切等手段对图像数据集的图像进行变换，起到扩充数据集与数据增强的作用。</p>
<p><code>transforms</code> 主要使用的类为：<code>transforms.ToTensor</code>，该类能够将 <code>PIL.Image</code> 或者 <code>ndarray</code> 类型的数据转换为 <code>tensor</code>，并且归一化至 <code>[0, 1]</code>。注意归一化至 <code>[0, 1]</code> 是直接除以255，若自己的 <code>ndarray</code> 数据尺度有变化，则需要自行修改。</p>
<p>为什么需要 <code>tensor</code> 数据类型？因为它包装了反向传播神经网络所需要的一些基础的参数，因此在神经网络中需要将图片类型转换为 <code>tensor</code> 类型进行训练。</p>
<p>例如：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&#x27;dataset/hymenoptera_data/train/ants_image/0013035.jpg&#x27;</span></span><br><span class="line">img_PIL = Image.<span class="built_in">open</span>(img_path)  <span class="comment"># &lt;class &#x27;PIL.JpegImagePlugin.JpegImageFile&#x27;&gt;</span></span><br><span class="line"></span><br><span class="line">tensor_trans = transforms.ToTensor()  <span class="comment"># 创建 ToTensor 的实例对象</span></span><br><span class="line">img_tensor1 = tensor_trans(img_PIL)  <span class="comment"># 将 PIL Image 转换成 tensor</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(img_tensor1))  <span class="comment"># &lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br><span class="line"></span><br><span class="line">img_cv = cv2.imread(img_path)  <span class="comment"># &lt;class &#x27;numpy.ndarray&#x27;&gt;</span></span><br><span class="line">img_tensor2 = tensor_trans(img_cv)  <span class="comment"># 将 OpenCV Image 转换成 tensor</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(img_tensor2))</span><br></pre></td></tr></table></figure>
<h3 id="4-2-Transform的常用类">4.2 Transform的常用类</h3>
<ul>
<li><code>transforms.Compose</code>：<code>Compose</code> 能够将多种变换组合在一起。例如下面的代码可以先将 <code>PIL.Image</code> 中心裁切，然后再转换成 <code>tensor</code>：</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">img_path = <span class="string">&#x27;dataset/hymenoptera_data/train/ants_image/0013035.jpg&#x27;</span></span><br><span class="line">img_PIL = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line"></span><br><span class="line">trans = transforms.Compose([</span><br><span class="line">    transforms.CenterCrop(<span class="number">100</span>),</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">img_trans = trans(img_PIL)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>transforms.CenterCrop</code>：需要传入参数 <code>size</code>，表示以 <code>(size, size)</code> 的大小从中心裁剪，参数也可以为 <code>(height, width)</code>。例如：</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">img_PIL.show()</span><br><span class="line"></span><br><span class="line">trans_centercrop = transforms.CenterCrop((<span class="number">100</span>, <span class="number">150</span>))</span><br><span class="line">img_centercrop = trans_centercrop(img_PIL)</span><br><span class="line">img_centercrop.show()</span><br></pre></td></tr></table></figure>
<ul>
<li><code>transforms.RandomCrop</code>：需要传入参数 <code>size</code>，表示以 <code>(size, size)</code> 的大小随机裁剪，参数也可以为 <code>(height, width)</code>。</li>
<li><code>transforms.Normalize(mean, std)</code>：对数据按通道进行标准化，即先减均值 <code>mean</code>，再除以标准差 <code>std</code>，注意是 <code>HWC</code> 格式，处理公式为：<code>output[channel] = (input[channel] - mean[channel]) / std[channel]</code>，例如：</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">trans_tensor = transforms.ToTensor()</span><br><span class="line">img_tensor = trans_tensor(img_PIL)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果 input 的范围是[0, 1]，那么用该参数归一化后的范围就变为[-1, 1]</span></span><br><span class="line">trans_norm = transforms.Normalize([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>])</span><br><span class="line">img_norm = trans_norm(img_tensor)</span><br><span class="line"><span class="built_in">print</span>(img_norm)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>transforms.Resize</code>：需要传入参数 <code>(height, width)</code> 和 <code> interpolation</code>，表示重置图像的分辨率为 <code>(h, w)</code>，也可以传入一个整数 <code>size</code>，这样会将较短的那条边缩放至 <code>size</code>，另一条边按原图大小等比例缩放。<code>interpolation</code> 为插值方法选择，默认为 <code>PIL.Image.BILINEAR</code>，例如：</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">trans_tensor = transforms.ToTensor()</span><br><span class="line">img_tensor = trans_tensor(img_PIL)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(img_tensor.size())  <span class="comment"># torch.Size([3, 512, 768])，tensor 图像使用 size() 获取大小，PIL 图像使用 size</span></span><br><span class="line"></span><br><span class="line">trans_resize = transforms.Resize((<span class="number">256</span>, <span class="number">300</span>))</span><br><span class="line">img_resize = trans_resize(img_tensor)</span><br><span class="line"><span class="built_in">print</span>(img_resize.size())  <span class="comment"># torch.Size([3, 256, 300])，修改比例</span></span><br><span class="line"></span><br><span class="line">trans_resize = transforms.Resize(<span class="number">30</span>)</span><br><span class="line">img_resize = trans_resize(img_tensor)</span><br><span class="line"><span class="built_in">print</span>(img_resize.size())  <span class="comment"># torch.Size([3, 30, 45])，与原图等比例</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>transforms.ToPILImage</code>：：将 <code>tensor</code> 或者 <code>ndarray</code> 的数据转换为 <code>PIL.Image</code> 类型数据，参数 <code>mode</code> 默认为 <code>None</code>，表示1通道， <code>mode=3</code> 表示3通道，默认转换为 <code>RGB</code>，4通道默认转换为 <code>RGBA</code>。</li>
</ul>
<h2 id="5-Torchvision数据集使用方法">5. Torchvision数据集使用方法</h2>
<p>Torchvision 官方文档 <a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/datasets.html">Torchvision</a> 中的 <code>torchvision.datasets</code> 就是 Torchvision 提供的标准数据集，其中有很多已经构建和训练好的网络模型，在不同的领域下各自有着很优秀的性能。</p>
<p>我们以 CIFAR10 为例，该数据集包括了60000张32*32像素的图像，总共有10个类别，每个类别有6000张图像，其中有50000张图像为训练图像，10000张为测试图像。其使用说明如下图所示：</p>
<ul>
<li><code>root</code>：数据集存放的路径。</li>
<li><code>train</code>：如果为 <code>True</code>，创建的数据集就为训练集，否则创建的数据集就为测试集。</li>
<li><code>transform</code>：使用 <code>transforms</code> 中的变换操作对数据集进行变换。</li>
<li><code>target_transform</code>：对 <code>target</code> 进行 <code>transform</code>。</li>
<li><code>download</code>：如果为 <code>True</code>，就会自动从网上下载这个数据集，否则就不会下载。</li>
</ul>
<p>例如：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>)</span><br><span class="line">test_data = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(train_data[<span class="number">0</span>])  <span class="comment"># (&lt;PIL.Image.Image image mode=RGB size=32x32 at 0x24011FC4F40&gt;, 6)</span></span><br></pre></td></tr></table></figure>
<p>刚开始运行时可以看到正在从网上下载数据集，如果下载速度非常慢可以复制链接去迅雷之类的地方下载，下载好后自己创建设定的路径，将数据集放过来即可。</p>
<p>然后设置断点，用 Debug 模式运行一下代码，我们可以查看一下数据集的内容，数据集 <code>train_data</code> 中的 <code>classes</code> 表示图像的种类，<code>classes_to_idx</code> 表示将种类映射为整数，<code>targets</code> 表示每张图像对应的种类编号，试着输出一下第一张图的信息：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">img, target = train_data[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(img)  <span class="comment"># &lt;PIL.Image.Image image mode=RGB size=32x32 at 0x1EEAEC32190&gt;</span></span><br><span class="line"><span class="built_in">print</span>(target)  <span class="comment"># 6</span></span><br><span class="line"><span class="built_in">print</span>(train_data.classes[target])  <span class="comment"># frog</span></span><br><span class="line">img.show()  <span class="comment"># 图像显示为青蛙</span></span><br></pre></td></tr></table></figure>
<p>现在展示如何使用 <code>transform</code> 参数，假设我们需要将数据集的图像都转换成 <code>tensor</code> 类型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">trans_dataset = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class="literal">True</span>, transform=trans_dataset, download=<span class="literal">True</span>)</span><br><span class="line">test_data = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class="literal">False</span>, transform=trans_dataset, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">img, target = train_data[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(img))  <span class="comment"># &lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="6-神经网络Torch-NN基本骨架的使用">6. 神经网络Torch.NN基本骨架的使用</h2>
<p><code>torch.nn</code> 能够帮助我们更优雅地训练神经网络，使神经网络代码更加简洁和灵活。官方文档：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html">Torch.NN</a>。</p>
<p>在文档中可以看到第一块内容叫做 <code>Container</code>（容器），这就相当于神经网络的骨架，<code>Container</code> 之后的东西就用于往骨架里面填充，如 Convolution Layers（卷积层）、Pooling Layers（池化层），有卷积神经网络基础的小伙伴对这些词应该都很熟悉了。</p>
<p><code>Container</code> 中有六个模块：<code>Module</code>、<code>Sequential</code>、<code>ModuleList</code>、<code>ModuleDict</code>、<code>ParameterList</code>、<code>ParameterDict</code>，其中最常用的为 <code>Module</code>，这是所有神经网络的最基本的类，其基本的构造方式如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  <span class="comment"># 初始化</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):  <span class="comment"># 前向传播</span></span><br><span class="line">        x = F.relu(self.conv1(x))  <span class="comment"># 将 x 进行第一层卷积后用 ReLU 激活函数输出</span></span><br><span class="line">        <span class="keyword">return</span> F.relu(self.conv2(x))  <span class="comment"># 将处理后的 x 再进行第二层卷积后用 ReLU 处理后返回最后结果</span></span><br></pre></td></tr></table></figure>
<p>现在我们尝试自己创建一个简单的神经网络，并输出前向传播的结果：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Network</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  <span class="comment"># 初始化</span></span><br><span class="line">        <span class="built_in">super</span>(Network, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = <span class="built_in">input</span> + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">network = Network()</span><br><span class="line">x = torch.tensor(<span class="number">1.0</span>)  <span class="comment"># x 为 tensor 类型</span></span><br><span class="line">output = network(x)  <span class="comment"># Module 中的 __call__ 函数会调用 forward 函数</span></span><br><span class="line"><span class="built_in">print</span>(output)  <span class="comment"># tensor(2.)</span></span><br></pre></td></tr></table></figure>
<p>我们以 <code>Conv2d</code> 函数为例，该函数的官方文档：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html#torch.nn.functional.conv2d">TORCH.NN.FUNCTIONAL.CONV2D</a>。</p>
<p>该函数有以下几个参数：</p>
<ul>
<li><code>input</code>：输入的图像，<code>size</code> 为 <code>(mini_batch, in_channels, height, width)</code>。</li>
<li><code>weight</code>：卷积核的大小，<code>size</code> 为 <code>(out_channels, in_channels/groups, height, width)</code>。</li>
<li><code>bias</code>：偏置，默认为 <code>None</code>。</li>
<li><code>stride</code>：步长，用来控制卷积核移动间隔，如果为 <code>x</code> 则水平和竖直方向的步长都为 <code>x</code>，如果为 <code>(x, y)</code> 则竖直方向步长为 <code>x</code>，水平方向步长为 <code>y</code>。</li>
<li><code>padding</code>：在输入图像的边沿进行扩边操作，以保证图像输入输出前后的尺寸大小不变，在 PyTorch 的卷积层定义中，默认的 <code>padding</code> 为零填充，即在边缘填充0。</li>
<li><code>padding_mode</code>：扩边的方式。</li>
<li><code>dilation</code>：设定了取数之间的间隔。</li>
</ul>
<p>例如：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([</span><br><span class="line">    [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">kernel = torch.tensor([</span><br><span class="line">    [<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>))  <span class="comment"># batch_size = 1，channel = 1</span></span><br><span class="line">kernel = torch.reshape(kernel, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">output = F.conv2d(<span class="built_in">input</span>, kernel, stride=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="comment"># tensor([[[[15, 16],</span></span><br><span class="line"><span class="comment">#           [ 6, 15]]]])</span></span><br><span class="line"></span><br><span class="line">output = F.conv2d(<span class="built_in">input</span>, kernel, stride=<span class="number">1</span>, bias=torch.tensor([<span class="number">3</span>]))  <span class="comment"># 注意 bias 必须也是矩阵</span></span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="comment"># tensor([[[[18, 19],</span></span><br><span class="line"><span class="comment">#           [ 9, 18]]]])</span></span><br></pre></td></tr></table></figure>
<h2 id="7-Convolution-Layers与Pooling-Layers">7. Convolution Layers与Pooling Layers</h2>
<p>由于图像是二维的，因此基本上最常用到的就是二维的卷积层和池化层：<code>torch.nn.Conv2d</code>、<code>torch.nn.MaxPool2d</code>，官方文档：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d">torch.nn.Conv2d</a>、<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#pooling-layers">Pooling Layers</a>。</p>
<h3 id="7-1-Convolution-Layers">7.1 Convolution Layers</h3>
<p>卷积运算能够<strong>提取输入图像的不同特征</strong>，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。</p>
<p><code>torch.nn.Conv2d</code> 的主要参数有以下几个：</p>
<ul>
<li><code>in_channels</code>：输入图像的通道数，彩色图像一般都是三通道。</li>
<li><code>out_channels</code>：通过卷积后产生的输出图像的通道数。</li>
<li><code>kernel_size</code>：可以是一个数或一个元组，表示卷积核的大小，卷积核的参数是从数据的分布中采样得到的，这些数是多少无所谓，因为在神经网络训练的过程中就是对这些参数进行不断地调整。</li>
<li><code>stride</code>：步长。</li>
<li><code>padding</code>：填充。</li>
<li><code>padding_mode</code>：填充模式，有 <code>zeros</code>、<code>reflect</code>、<code>replicate</code>、<code>circular</code>，默认为 <code>zeros</code>。</li>
<li><code>dilation</code>：可以是一个数或一个元组，表示卷积核各个元素间的距离，也称空洞卷积。</li>
<li><code>group</code>：一般设置为1，基本用不到。</li>
<li><code>bias</code>：偏置，一般设置为 <code>True</code>。</li>
</ul>
<p>例如以下代码构建了一个只有一层卷积层的神经网络，该卷积层的输入和输出通道数都为三通道，卷积核大小为3*3，步长为1，无填充，然后用 CIFAR10 测试数据集进行测试：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">test_data = datasets.CIFAR10(<span class="string">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class="literal">False</span>, transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">data_loader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Network</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Network, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">3</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.conv1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">network = Network()</span><br><span class="line"><span class="built_in">print</span>(network)  <span class="comment"># Network((conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1)))</span></span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    output = network(imgs)</span><br><span class="line">    writer.add_images(<span class="string">&#x27;input&#x27;</span>, imgs, step)</span><br><span class="line">    writer.add_images(<span class="string">&#x27;output&#x27;</span>, output, step)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p>运行后可以打开 TensorBoard 查看一下效果。</p>
<h3 id="7-2-Pooling-Layers">7.2 Pooling Layers</h3>
<p>Pooling Layers 中的 <code>MaxPool</code> 表示最大池化，也称上采样；<code>MaxUnpool</code> 表示最小池化，也称下采样；<code>AvgPool</code> 表示平均池化。其中最常用的为 <code>MaxPool2d</code>，官方文档：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d">torch.nn.MaxPool2d</a>。</p>
<p>最大池化的目的是<strong>保留输入数据的特征，同时减小特征的数据量</strong>。</p>
<p><code>torch.nn.MaxPool2d</code> 的主要参数有以下几个：</p>
<ul>
<li><code>kernel_size</code>：用来取最大值的窗口（池化核）大小，和之前的卷积核类似。</li>
<li><code>stride</code>：步长，注意默认值为 <code>kernel_size</code>。</li>
<li><code>padding</code>：填充，和 <code>Conv2d</code> 一样。</li>
<li><code>dilation</code>：池化核中各个元素间的距离，和 <code>Conv2d</code> 一样。</li>
<li><code>return_indices</code>：如果为 <code>True</code>，表示返回值中包含最大值位置的索引。注意这个最大值指的是在所有窗口中产生的最大值，如果窗口产生的最大值总共有5个，就会有5个返回值。</li>
<li><code>ceil_mode</code>：如果为 <code>True</code>，表示在计算输出结果形状的时候，使用向上取整，否则默认向下取整。</li>
</ul>
<p>输出图像的形状的计算公式可以在官方文档中查看。</p>
<p>接下来我们用代码实现这个池化层：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Network</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Network, self).__init__()</span><br><span class="line">        self.maxpool1 = nn.MaxPool2d(kernel_size=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.maxpool1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([</span><br><span class="line">    [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">], dtype=torch.float32)  <span class="comment"># 注意池化层读入的数据需要为浮点型</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">network = Network()</span><br><span class="line"><span class="built_in">print</span>(network)  <span class="comment"># Network((maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False))</span></span><br><span class="line"></span><br><span class="line">output = network(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="comment"># tensor([[[[2., 3.],</span></span><br><span class="line"><span class="comment">#           [4., 2.]]]])</span></span><br></pre></td></tr></table></figure>
<p>我们用图像来试试效果：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">test_data = datasets.CIFAR10(<span class="string">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class="literal">False</span>, transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">data_loader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Network</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Network, self).__init__()</span><br><span class="line">        self.maxpool1 = nn.MaxPool2d(kernel_size=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.maxpool1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">network = Network()</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    output = network(imgs)</span><br><span class="line">    writer.add_images(<span class="string">&#x27;input&#x27;</span>, imgs, step)</span><br><span class="line">    writer.add_images(<span class="string">&#x27;output&#x27;</span>, output, step)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p>运行后可以打开 TensorBoard 查看一下效果。</p>
<h2 id="8-Non-linear-Activations与Linear-Layers">8. Non-linear Activations与Linear Layers</h2>
<h3 id="8-1-Non-linear-Activations">8.1 Non-linear Activations</h3>
<p>非线性激活的目的是为了在网络中引入一些<strong>非线性特征</strong>，因为非线性特征越多才能训练出符合各种曲线（特征）的模型。</p>
<p>非线性激活函数官方文档：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">Non-linear Activations</a>。</p>
<p>有深度学习基础的同学应该知道最常用的非线性激活函数就是 ReLU 和 Sigmoid 函数，多分类问题会在输出层使用 Softmax 函数（如果损失函数使用的是交叉熵误差函数 <code>CrossEntropyLoss</code> 则会自动计算 Softmax，无需创建 Softmax 层）。这三个函数在 PyTorch 中分别为 <code>nn.ReLU</code>、<code>nn.Sigmoid</code> 和 <code>nn.Softmax</code>。</p>
<p>这两个函数的输入都是只需指明 <code>batch_size</code> 即可，在 PyTorch1.0 之后的版本任何形状的数据都能被计算，无需指定 <code>batch_size</code>。</p>
<p><code>nn.ReLU</code> 只有一个需要设置的参数 <code>inplace</code>，如果为 <code>True</code> 表示计算结果直接替换到输入数据上，例如：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = -<span class="number">1</span></span><br><span class="line">nn.ReLU(<span class="built_in">input</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># input = 0</span></span><br></pre></td></tr></table></figure>
<p>构建 ReLU 层代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Network</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Network, self).__init__()</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.relu1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">network = Network()</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([</span><br><span class="line">    [<span class="number">1</span>, -<span class="number">0.5</span>],</span><br><span class="line">    [-<span class="number">1</span>, <span class="number">3</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">output = network(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="comment"># tensor([[1., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 3.]])</span></span><br></pre></td></tr></table></figure>
<p>由于 ReLU 对图像处理的直观效果不明显，我们使用 Sigmoid 对图像进行处理：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Network</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Network, self).__init__()</span><br><span class="line">        self.sigmoid1 = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.sigmoid1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">test_data = datasets.CIFAR10(<span class="string">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class="literal">False</span>, transform=transforms.ToTensor())</span><br><span class="line">data_loader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">network = Network()</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    output = network(imgs)</span><br><span class="line">    writer.add_images(<span class="string">&#x27;input&#x27;</span>, imgs, step)</span><br><span class="line">    writer.add_images(<span class="string">&#x27;output&#x27;</span>, output, step)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<h3 id="8-2-Linear-Layers">8.2 Linear Layers</h3>
<p>线性层官方文档：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#linear-layers">Linear Layers</a>。</p>
<p>PyTorch 的 <code>nn.Linear</code> 是用于设置网络中的全连接层的，需要注意的是全连接层的输入与输出都是二维张量，一般形状为：<code>[batch_size, size]</code>，不同于卷积层要求输入输出是四维张量，因此在将图像传入全连接层之前一般都会展开成一维的。</p>
<p><code>nn.Linear</code> 有三个参数分别如下：</p>
<ul>
<li><code>in_features</code>：指的是输入的二维张量的大小，即输入的 <code>[batch_size, size]</code> 中的 <code>size</code>。</li>
<li><code>out_features</code>：指的是输出的二维张量的大小，即输出的二维张量的形状为 <code>[batch_size, output_size]</code>，当然，它也代表了该全连接层的神经元个数。从输入输出的张量的 <code>shape</code> 角度来理解，相当于一个输入为 <code>[batch_size, in_features]</code> 的张量变换成了 <code>[batch_size, out_features]</code> 的输出张量。</li>
<li><code>bias</code>：偏置，相当于 <code>y = ax + b</code> 中的 <code>b</code>。</li>
</ul>
<p>代码示例如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Network</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Network, self).__init__()</span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">24</span>, <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.linear1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([</span><br><span class="line">    [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)  <span class="comment"># torch.Size([3, 8])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.flatten(<span class="built_in">input</span>)  <span class="comment"># 将 input 拉平成一维</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)  <span class="comment"># torch.Size([24])</span></span><br><span class="line"></span><br><span class="line">network = Network()</span><br><span class="line"></span><br><span class="line">output = network(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape)  <span class="comment"># torch.Size([30])</span></span><br></pre></td></tr></table></figure>
<h2 id="9-神经网络模型搭建小实战">9. 神经网络模型搭建小实战</h2>
<h3 id="9-1-Sequential">9.1 Sequential</h3>
<p><code>torch.nn.Sequential</code> 是一个 Sequential 容器，能够在容器中嵌套各种实现神经网络中具体功能相关的类，来完成对神经网络模型的搭建。模块的加入一般有两种方式，一种是直接嵌套，另一种是以 <code>OrderedDict</code> 有序字典的方式进行传入，这两种方式的唯一区别是：</p>
<ul>
<li>使用 <code>OrderedDict</code> 搭建的模型的每个模块都有我们自定义的名字。</li>
<li>直接嵌套默认使用从零开始的数字序列作为每个模块的名字。</li>
</ul>
<p>（1）直接嵌套方法的代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">20</span>, <span class="number">64</span>, <span class="number">5</span>),</span><br><span class="line">    nn.ReLU()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"><span class="comment"># Sequential(</span></span><br><span class="line"><span class="comment">#   (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#   (1): ReLU()</span></span><br><span class="line"><span class="comment">#   (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#   (3): ReLU()</span></span><br><span class="line"><span class="comment"># )</span></span><br></pre></td></tr></table></figure>
<p>（2）使用 <code>OrderedDict</code> 的代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"></span><br><span class="line">model = nn.Sequential(OrderedDict([</span><br><span class="line">    (<span class="string">&#x27;Conv1&#x27;</span>, nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>)),</span><br><span class="line">    (<span class="string">&#x27;ReLU1&#x27;</span>, nn.ReLU()),</span><br><span class="line">    (<span class="string">&#x27;Conv2&#x27;</span>, nn.Conv2d(<span class="number">20</span>, <span class="number">64</span>, <span class="number">5</span>)),</span><br><span class="line">    (<span class="string">&#x27;ReLU2&#x27;</span>, nn.ReLU())</span><br><span class="line">]))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"><span class="comment"># Sequential(</span></span><br><span class="line"><span class="comment">#   (Conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#   (ReLU1): ReLU()</span></span><br><span class="line"><span class="comment">#   (Conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#   (ReLU2): ReLU()</span></span><br><span class="line"><span class="comment"># )</span></span><br></pre></td></tr></table></figure>
<h3 id="9-2-小实战">9.2 小实战</h3>
<p>由于代码很简单，都是学过的内容进行组装，因此直接看代码：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CIFAR10_Network</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CIFAR10_Network, self).__init__()</span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),  <span class="comment"># [32, 32, 32]</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),  <span class="comment"># [32, 16, 16]</span></span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),  <span class="comment"># [32, 16, 16]</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),  <span class="comment"># [32, 8, 8]</span></span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),  <span class="comment"># [64, 8, 8]</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),  <span class="comment"># [64, 4, 4]</span></span><br><span class="line">            nn.Flatten(),  <span class="comment"># [1024]</span></span><br><span class="line">            nn.Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">64</span>),  <span class="comment"># [64]</span></span><br><span class="line">            nn.Linear(in_features=<span class="number">64</span>, out_features=<span class="number">10</span>) <span class="comment"># [10]</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.model(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">network = CIFAR10_Network()</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)  <span class="comment"># 返回一个包含了从标准正态分布中抽取的一组随机数的张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)  <span class="comment"># torch.Size([64, 3, 32, 32])</span></span><br><span class="line">output = network(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape)  <span class="comment"># torch.Size([64, 10])</span></span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line">writer.add_graph(network, <span class="built_in">input</span>)  <span class="comment"># 生成计算图</span></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p>使用 <code>add_graph</code> 函数可以在 TensorBoard 中生成神经网络的计算图，通过计算图可以很清晰地看到每一层计算时数据流入流出的结果，双击相应的标签可以进一步深入查看更详细的信息。</p>
<h2 id="10-损失函数与反向传播">10. 损失函数与反向传播</h2>
<h3 id="10-1-Loss-Functions">10.1 Loss Functions</h3>
<p>具有深度学习理论基础的同学对损失函数和反向传播一定不陌生，在此不详细展开理论介绍。损失函数是指用于计算标签值和预测值之间差异的函数，在机器学习过程中，有多种损失函数可供选择，典型的有距离向量，绝对值向量等。使用损失函数的流程概括如下：</p>
<ol>
<li>计算实际输出和目标之间的差距。</li>
<li>为我们更新输出提供一定的依据（反向传播）。</li>
</ol>
<p>损失函数的官方文档：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#loss-functions">Loss Functions</a>。</p>
<p>（1）<code>nn.L1Loss</code>：平均绝对误差（MAE，Mean Absolute Error），计算方法很简单，取预测值和真实值的绝对误差的平均数即可。</p>
<p>PyTorch1.13中 <code>nn.L1Loss</code> 数据形状规定如下：</p>
<ul>
<li><code>Input</code>：<code>(*)</code>，means any number of dimensions.</li>
<li><code>Target</code>：<code>(*)</code>，same shape as the input.</li>
<li><code>Output</code>：scalar. If <code>reduction</code> is <code>none</code>, then <code>(*)</code>, same shape as the input.</li>
</ul>
<p>早先的版本需要指定 <code>batch_size</code> 大小，现在不需要了。可以设置参数 <code>reduction</code>，默认为 <code>mean</code>，即取平均值，也可以设置为 <code>sum</code>，顾名思义就是取和。</p>
<p>测试代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>])</span><br><span class="line">target = torch.tensor([<span class="number">4.0</span>, -<span class="number">2.0</span>, <span class="number">5.0</span>])</span><br><span class="line"></span><br><span class="line">loss = nn.L1Loss()</span><br><span class="line">result = loss(<span class="built_in">input</span>, target)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result)  <span class="comment"># tensor(3.)</span></span><br><span class="line"></span><br><span class="line">loss = nn.L1Loss(reduction=<span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line">result = loss(<span class="built_in">input</span>, target)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result)  <span class="comment"># tensor(9.)</span></span><br></pre></td></tr></table></figure>
<p>（2）<code>nn.MSELoss</code>：均方误差（MSE，Mean Squared Error），即预测值和真实值之差的平方和的平均数。</p>
<p>该损失函数的用法与 <code>nn.L1Loss</code> 相似，代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>])</span><br><span class="line">target = torch.tensor([<span class="number">4.0</span>, -<span class="number">2.0</span>, <span class="number">5.0</span>])</span><br><span class="line"></span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line">result = loss(<span class="built_in">input</span>, target)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result)  <span class="comment"># tensor(9.6667)</span></span><br><span class="line"></span><br><span class="line">loss = nn.MSELoss(reduction=<span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line">result = loss(<span class="built_in">input</span>, target)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result)  <span class="comment"># tensor(29.)</span></span><br></pre></td></tr></table></figure>
<p>（3）<code>nn.CrossEntropyLoss</code>：交叉熵误差，训练分类 C 个类别的模型的时候较常用这个损失函数，一般用在 Softmax 层后面，计算公式较为复杂，可以在官网中查看。</p>
<p>测试代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([<span class="number">0.1</span>, <span class="number">0.7</span>, <span class="number">0.2</span>])</span><br><span class="line">target = torch.tensor(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line">result = loss(<span class="built_in">input</span>, target)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result)  <span class="comment"># tensor(0.7679)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([<span class="number">0.8</span>, <span class="number">0.1</span>, <span class="number">0.1</span>])</span><br><span class="line">result = loss(<span class="built_in">input</span>, target)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result)  <span class="comment"># tensor(1.3897)</span></span><br></pre></td></tr></table></figure>
<h3 id="10-2-Backward">10.2 Backward</h3>
<p>接下来以 CIFAR10 数据集为例，用上一节搭建的神经网络先设置 <code>batch_size</code> 为1，看一下输出结果：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CIFAR10_Network</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CIFAR10_Network, self).__init__()</span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),  <span class="comment"># [32, 32, 32]</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),  <span class="comment"># [32, 16, 16]</span></span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),  <span class="comment"># [32, 16, 16]</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),  <span class="comment"># [32, 8, 8]</span></span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),  <span class="comment"># [64, 8, 8]</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),  <span class="comment"># [64, 4, 4]</span></span><br><span class="line">            nn.Flatten(),  <span class="comment"># [1024]</span></span><br><span class="line">            nn.Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">64</span>),  <span class="comment"># [64]</span></span><br><span class="line">            nn.Linear(in_features=<span class="number">64</span>, out_features=<span class="number">10</span>) <span class="comment"># [10]</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.model(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">network = CIFAR10_Network()</span><br><span class="line"></span><br><span class="line">test_data = datasets.CIFAR10(<span class="string">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class="literal">False</span>, transform=transforms.ToTensor())</span><br><span class="line">data_loader = DataLoader(test_data, batch_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    output = network(imgs)</span><br><span class="line">    output_loss = loss(output, targets)</span><br><span class="line">    <span class="built_in">print</span>(output)</span><br><span class="line">    <span class="built_in">print</span>(targets)</span><br><span class="line">    <span class="built_in">print</span>(output_loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[ 0.1252, -0.1069, -0.0747,  0.0232,  0.0852,  0.1019,  0.0688, -0.1068,</span></span><br><span class="line"><span class="comment">#           0.0854, -0.0740]], grad_fn=&lt;AddmmBackward0&gt;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor(2.2960, grad_fn=&lt;NllLossBackward0&gt;)</span></span><br></pre></td></tr></table></figure>
<p>现在我们来尝试解决第二个问题，即损失函数如何为我们更新输出提供一定的依据（反向传播）。</p>
<p>例如对于卷积层来说，其中卷积核中的每个参数就是我们需要调整的，每个参数具有一个属性 <code>grad</code> 表示梯度，反向传播时每一个要更新的参数都会求出对应的梯度，在优化的过程中就可以根据这个梯度对参数进行优化，最终达到降低损失函数值的目的。</p>
<p>PyTorch 中对损失函数计算出的结果使用 <code>backward</code> 函数即可计算出梯度：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CIFAR10_Network</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CIFAR10_Network, self).__init__()</span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            <span class="comment"># Layers</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.model(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">network = CIFAR10_Network()</span><br><span class="line"></span><br><span class="line">test_data = datasets.CIFAR10(<span class="string">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class="literal">False</span>, transform=transforms.ToTensor())</span><br><span class="line">data_loader = DataLoader(test_data, batch_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    output = network(imgs)</span><br><span class="line">    output_loss = loss(output, targets)</span><br><span class="line">    output_loss.backward()  <span class="comment"># 反向传播</span></span><br></pre></td></tr></table></figure>
<p>我们在计算反向传播之前设置断点，然后可以在 PyCharm 下方的变量区域通过目录 <code>network/model/Protected Attributes/_modules/'0'/weight/grad</code> 查看到某一层参数的梯度，在反向传播之前为 <code>None</code>，执行反向传播的代码后可以看到 <code>grad</code> 处有数值了。</p>
<p>我们有了各个节点参数的梯度，接下来就可以选用一个合适的优化器，来对这些参数进行优化。</p>
<h3 id="10-3-Optimizer">10.3 Optimizer</h3>
<p>优化器 <code>torch.optim</code> 的官方文档：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html">TORCH.OPTIM</a>。</p>
<p>优化器主要是在模型训练阶段对模型的可学习参数进行更新，常用优化器有：SGD、RMSprop、Adam等。优化器初始化时传入传入模型的可学习参数，以及其他超参数如 <code>lr</code>、<code>momentum</code> 等，例如：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">optimizer = optim.Adam([var1, var2], lr=<span class="number">0.0001</span>)</span><br></pre></td></tr></table></figure>
<p>在训练过程中先调用 <code>optimizer.zero_grad()</code> 清空梯度，再调用 <code>loss.backward()</code> 反向传播，最后调用 <code>optimizer.step()</code> 更新模型参数，例如：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    output = network(imgs)</span><br><span class="line">    loss = loss_function(output, targets)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<p>接下来我们来训练20轮神经网络，看看损失函数值的变化：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CIFAR10_Network</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CIFAR10_Network, self).__init__()</span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            <span class="comment"># Layers</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.model(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">network = CIFAR10_Network()</span><br><span class="line"></span><br><span class="line">test_data = datasets.CIFAR10(<span class="string">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class="literal">False</span>, transform=transforms.ToTensor())</span><br><span class="line">data_loader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">loss_function = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(network.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):  <span class="comment"># 学习20轮</span></span><br><span class="line">    total_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        output = network(imgs)</span><br><span class="line">        loss = loss_function(output, targets)</span><br><span class="line">        total_loss += loss</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(total_loss)</span><br></pre></td></tr></table></figure>
<p>可以看到每一轮所有 <code>batch</code> 的损失函数值的总和确实在不断降低了。</p>
<h2 id="11-现有网络模型的使用及修改">11. 现有网络模型的使用及修改</h2>
<h3 id="11-1-VGG16模型的使用">11.1 VGG16模型的使用</h3>
<p>我们以 VGG16 为例，该网络模型是用于大规模图像识别的超深度卷积神经网络，官方文档：<a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/models/generated/torchvision.models.vgg16.html#torchvision.models.vgg16">VGG16</a>。</p>
<p>该网络模型主要有以下参数：</p>
<ul>
<li><code>weights</code>：可以设置成 <code>torchvision.models.VGG16_Weights.DEFAULT</code>，<code>DEFAULT</code> 表示自动使用最新的数据。老版本为 <code>pretrained</code>，如果为 <code>True</code>，表示使用预先训练好的权重，在官网可以看到这个权重是在 <code>ImageNet-1K</code> 数据集训练的，默认为不使用预先训练好的权重。</li>
<li><code>progress</code>：如果为 <code>True</code>，则显示下载的进度条，默认为 <code>True</code>。</li>
</ul>
<p>注意，下载网络时默认的下载路径是 <code>C:\Users\&lt;username&gt;\.cache</code>，因此在下载模型前，我们需要修改路径：打开 <code>D:\Anaconda3_Environments\envs\PyTorch\Lib\site-packages\torch</code> 中的 <code>hub.py</code> 文件，搜索 <code>load_state_dict_from_url</code>，然后修改 <code>model_dir</code> 即可：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model_dir: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="string">&#x27;D:\\Anaconda3_Environments\\envs\\PyTorch\\Torch-model&#x27;</span></span><br></pre></td></tr></table></figure>
<p>然后我们输出一下这个网络模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line">vgg = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(vgg)</span><br><span class="line"><span class="comment"># VGG(</span></span><br><span class="line"><span class="comment">#   (features): Sequential(</span></span><br><span class="line"><span class="comment">#     (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span></span><br><span class="line"><span class="comment">#     (1): ReLU(inplace=True)</span></span><br><span class="line"><span class="comment">#     (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span></span><br><span class="line"><span class="comment">#     (3): ReLU(inplace=True)</span></span><br><span class="line"><span class="comment">#     (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="comment">#     ......</span></span><br><span class="line"><span class="comment">#   )</span></span><br><span class="line"><span class="comment">#   (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))</span></span><br><span class="line"><span class="comment">#   (classifier): Sequential(</span></span><br><span class="line"><span class="comment">#     (0): Linear(in_features=25088, out_features=4096, bias=True)</span></span><br><span class="line"><span class="comment">#     (1): ReLU(inplace=True)</span></span><br><span class="line"><span class="comment">#     (2): Dropout(p=0.5, inplace=False)</span></span><br><span class="line"><span class="comment">#     (3): Linear(in_features=4096, out_features=4096, bias=True)</span></span><br><span class="line"><span class="comment">#     (4): ReLU(inplace=True)</span></span><br><span class="line"><span class="comment">#     (5): Dropout(p=0.5, inplace=False)</span></span><br><span class="line"><span class="comment">#     (6): Linear(in_features=4096, out_features=1000, bias=True)</span></span><br><span class="line"><span class="comment">#   )</span></span><br><span class="line"><span class="comment"># )</span></span><br></pre></td></tr></table></figure>
<p>可以看到这个模型的分类结果为1000类，那么假如我们需要分类 CIFAR10 该如何应用这个网络模型呢？一种方法就是直接将最后一层 <code>Linear</code> 的 <code>out_features</code> 改为10，还有一种方法就是再添加一层 <code>in_features=1000, out_features=10</code> 的 <code>Linear</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">vgg = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)</span><br><span class="line"></span><br><span class="line">vgg.classifier.add_module(<span class="string">&#x27;add_linear&#x27;</span>, nn.Linear(in_features=<span class="number">1000</span>, out_features=<span class="number">10</span>))  <span class="comment"># 在 classifier 中加一层 Linear</span></span><br><span class="line"><span class="comment"># vgg.classifier[6] = nn.Linear(in_features=4096, out_features=10)  # 修改 classifier 的最后一层 Linear</span></span><br><span class="line"></span><br><span class="line">test_data = datasets.CIFAR10(<span class="string">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class="literal">False</span>, transform=transforms.ToTensor())</span><br><span class="line">data_loader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">loss_function = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(vgg.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    total_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        output = vgg(imgs)</span><br><span class="line">        loss = loss_function(output, targets)</span><br><span class="line">        total_loss += loss</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(total_loss)</span><br></pre></td></tr></table></figure>
<p>可以看到效果是比之前自己构建的网络模型好很多的。</p>
<h3 id="11-2-模型的保存与读取">11.2 模型的保存与读取</h3>
<p>我们在对某些模型进行修改后可能想将其保存下来，方便以后用到时无需再构建一遍网络，可以按以下的方式将整个模型保存到路径 <code>models/CIFAR10_VGG16.pth</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">model = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)</span><br><span class="line">model.classifier.add_module(<span class="string">&#x27;add_linear&#x27;</span>, nn.Linear(in_features=<span class="number">1000</span>, out_features=<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">torch.save(model, <span class="string">&#x27;models/CIFAR10_VGG16.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>其对应的加载模型的方式为：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = torch.load(<span class="string">&#x27;models/CIFAR10_VGG16.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>还有一种保存方式是将模型中的参数保存成字典的形式，官方建议使用该方式：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;models/CIFAR10_VGG16_STATE.pkl&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>其对应的加载模型的方式为：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = torchvision.models.vgg16()</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;models/CIFAR10_VGG16_STATE.pkl&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>注意如果是保存自己构建的网络模型，需要在模型的类的源代码中将该类导入进来，例如在 <code>test_save.py</code> 中用以下代码保存自己的网络：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNetwork, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.conv1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">model = MyNetwork()</span><br><span class="line">torch.save(model, <span class="string">&#x27;models/My_Network.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>在 <code>test_load.py</code> 中导入时需要这样写：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> test_save <span class="keyword">import</span> MyNetwork</span><br><span class="line"></span><br><span class="line">model = torch.load(<span class="string">&#x27;models/My_Network.pth&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>
<h2 id="12-完整训练模型的方法">12. 完整训练模型的方法</h2>
<h3 id="12-1-训练模型时的注意事项">12.1 训练模型时的注意事项</h3>
<p>（1）通常我们会将超参数的设置放在一起，使代码更加直观且方便修改：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line">LEARNING_RATE = <span class="number">0.01</span></span><br><span class="line">EPOCH = <span class="number">10</span></span><br></pre></td></tr></table></figure>
<p>（2）我们在每一轮 epoch 中会先对训练集进行训练，然后使用测试集进行正确率的测试，因此一般我们会记录总共训练的次数 <code>total_train_step</code> 以及总共测试的次数 <code>total_test_step</code>，方便后续绘图使用。</p>
<p>（3）在开始训练之前一般需要将模型设置成训练状态，在测试之前需要设置成评估状态，这两种状态会影响少部分的层例如 <code>Dropout</code> 和 <code>BatchNorm</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.train()</span><br><span class="line">    <span class="comment"># training</span></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="comment"># evaluation</span></span><br></pre></td></tr></table></figure>
<p>（4）在分类问题中计算准确率一般用以下方法：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor([</span><br><span class="line">    [<span class="number">0.3</span>, <span class="number">0.7</span>],</span><br><span class="line">    [<span class="number">0.6</span>, <span class="number">0.4</span>]</span><br><span class="line">])  <span class="comment"># 假设两个物体二分类的结果</span></span><br><span class="line"></span><br><span class="line">b = torch.tensor([<span class="number">0</span>, <span class="number">0</span>])  <span class="comment"># 正确的标签</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a.argmax(dim=<span class="number">1</span>)) <span class="comment"># tensor([1, 0])，在第1维上取最大值，即对每一行求最大值，将最大值作为分类结果</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a.argmax(dim=<span class="number">1</span>) == b)  <span class="comment"># tensor([False,  True])，与标签进行比较，第一个物体的结果与标签不符，第二个和标签相符</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>((a.argmax(dim=<span class="number">1</span>) == b).<span class="built_in">sum</span>())  <span class="comment"># tensor(1)，将所有物体与标签的比较结果求和就是 True 的数量，也就是预测正确的数量</span></span><br></pre></td></tr></table></figure>
<p>（5）测试时不能对模型进行任何干扰，即在测试的时候神经网络不能产生梯度，因此在每次测试前需要加上以下代码：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># evaluation</span></span><br></pre></td></tr></table></figure>
<h3 id="12-2-使用GPU进行训练">12.2 使用GPU进行训练</h3>
<p>前提：电脑有 NVIDIA 显卡，配置好了 CUDA，可以使用 <code>torch.cuda.is_available()</code> 来检查 CUDA 是否可用。</p>
<p>使用 GPU 训练的时候，需要将 Module 对象和 Tensor 类型的数据转移到 GPU 上进行计算，一般来说即为将网络模型、数据、损失函数放到 GPU 上计算。</p>
<p>使用 GPU 训练的方式有两种，第一种是使用 <code>cuda()</code> 函数，例如：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 网络模型</span></span><br><span class="line">model = MyNetwork()</span><br><span class="line">model = model.cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">loss_function = nn.CrossEntropyLoss()</span><br><span class="line">loss_function = loss_function.cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据</span></span><br><span class="line"><span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    imgs = imgs.cuda()</span><br><span class="line">    targets = targets.cuda()</span><br></pre></td></tr></table></figure>
<p>另一种是使用 <code>to(device)</code>，<code>device</code> 就是我们选择用来训练模型的设备，该方式与 <code>cuda()</code> 有一点细微的差别如下：</p>
<ul>
<li>对于 Tensor 类型的数据（图像、标签等），使用 <code>to(device)</code> 之后，需要接收返回值，返回值才是正确设置了 <code>device</code> 的 Tensor。</li>
<li>对于 Module 对象（网络模型、损失函数），只用调用 <code>to(device)</code> 就可以将模型设置为指定的 <code>device</code>，不必接收返回值，当然接收返回值也是可以的。</li>
</ul>
<p>例如：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)  <span class="comment"># &#x27;cuda:0&#x27; 表示第 0 号 GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络模型</span></span><br><span class="line">model = MyNetwork()</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">loss_function = nn.CrossEntropyLoss()</span><br><span class="line">loss_function.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据</span></span><br><span class="line"><span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    imgs = imgs.to(device)</span><br><span class="line">    targets = targets.to(device)</span><br></pre></td></tr></table></figure>
<p>注意如果加载在 GPU 上训练好的模型，然后想在 CPU 上使用，需要映射回 CPU：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = torch.load(<span class="string">&#x27;models/AFTER_TRAININGS_MODEL.pth&#x27;</span>, map_location=torch.device(<span class="string">&#x27;cpu&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h3 id="12-3-CIFAR10-Net-Simple-v3">12.3 CIFAR10_Net_Simple_v3</h3>
<p>最后放上经过自己调参达到88%左右的正确率的模型和训练代码吧：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CIFAR10_Net_Simple_v3</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CIFAR10_Net_Simple_v3, self).__init__()</span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),  <span class="comment"># [32, 32, 32]</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">32</span>),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),  <span class="comment"># [32, 32, 32]</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">32</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),  <span class="comment"># [32, 16, 16]</span></span><br><span class="line"></span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),  <span class="comment"># [64, 16, 16]</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),  <span class="comment"># [64, 16, 16]</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),  <span class="comment"># [64, 8, 8]</span></span><br><span class="line"></span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),  <span class="comment"># [128, 16, 16]</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">128</span>, out_channels=<span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),  <span class="comment"># [128, 16, 16]</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),  <span class="comment"># [128, 4, 4]</span></span><br><span class="line"></span><br><span class="line">            nn.Flatten(),  <span class="comment"># [2048]</span></span><br><span class="line">            nn.Dropout(p=<span class="number">0.4</span>, inplace=<span class="literal">False</span>),</span><br><span class="line"></span><br><span class="line">            nn.Linear(in_features=<span class="number">2048</span>, out_features=<span class="number">64</span>),  <span class="comment"># [64]</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.4</span>, inplace=<span class="literal">False</span>),</span><br><span class="line"></span><br><span class="line">            nn.Linear(in_features=<span class="number">64</span>, out_features=<span class="number">10</span>) <span class="comment"># [10]</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.model(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># model = CIFAR10_Net_Simple_v3()</span></span><br><span class="line"><span class="comment"># torch.save(model, &#x27;../models/CIFAR10_Net_Simple_v3.pth&#x27;)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.dataset <span class="keyword">import</span> ConcatDataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> util.CIFAR10_Net_Simple_v3 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">LEARNING_RATE = <span class="number">0.01</span></span><br><span class="line">EPOCH = <span class="number">150</span></span><br><span class="line">SHOW_INFO_STEP = <span class="number">200</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练设备</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)  <span class="comment"># &#x27;cuda:0&#x27; 表示第 0 号 GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据增强</span></span><br><span class="line">trans = transforms.Compose([</span><br><span class="line">    transforms.RandomCrop(<span class="number">32</span>, padding=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]),</span><br><span class="line">    transforms.RandomHorizontalFlip(p=<span class="number">0.5</span>),</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">train_data = datasets.CIFAR10(<span class="string">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class="literal">True</span>, transform=trans)</span><br><span class="line">test_data = datasets.CIFAR10(<span class="string">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class="literal">False</span>, transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 扩充训练集</span></span><br><span class="line"><span class="comment"># trans_train_data = datasets.CIFAR10(&#x27;dataset/CIFAR10&#x27;, train=True, transform=trans)</span></span><br><span class="line"><span class="comment"># train_data = ConcatDataset([train_data, trans_train_data])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE)</span><br><span class="line"></span><br><span class="line">train_data_len = <span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_len = <span class="built_in">len</span>(test_data)</span><br><span class="line"></span><br><span class="line">model = torch.load(<span class="string">&#x27;models/CIFAR10_Net_Simple_v3.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line">loss_function = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=<span class="number">0.9</span>)</span><br><span class="line">scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[<span class="number">8</span>, <span class="number">16</span>, <span class="number">24</span>, <span class="number">32</span>], gamma=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs/CIFAR10_Net_Simple_v3_Aug_Mom_logs&#x27;</span>)</span><br><span class="line"></span><br><span class="line">model.to(device)</span><br><span class="line">loss_function.to(device)</span><br><span class="line"></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;---------- The &#123;&#125; epoch of training begins ----------&#x27;</span>.<span class="built_in">format</span>(epoch))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Learning rate: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(optimizer.state_dict()[<span class="string">&#x27;param_groups&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">    train_loss = <span class="number">0.0</span></span><br><span class="line">    train_acc = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataloader):</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        imgs = imgs.to(device)</span><br><span class="line">        targets = targets.to(device)</span><br><span class="line">        output = model(imgs)</span><br><span class="line"></span><br><span class="line">        acc = (output.argmax(dim=<span class="number">1</span>)==targets).<span class="built_in">float</span>().<span class="built_in">sum</span>()</span><br><span class="line">        loss = loss_function(output, targets)</span><br><span class="line">        train_loss += loss.item()</span><br><span class="line">        train_acc += acc</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_train_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> total_train_step % SHOW_INFO_STEP == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;The number of training: &#123;&#125;, Loss: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_train_step, loss.item()))</span><br><span class="line"></span><br><span class="line">    train_acc /= train_data_len</span><br><span class="line"></span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;train_loss&#x27;</span>, train_loss, epoch)</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;train_acc&#x27;</span>, train_acc, epoch)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;The number of epoch: &#123;&#125;, train_loss: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, train_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;The number of epoch: &#123;&#125;, train_acc: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, train_acc))</span><br><span class="line"></span><br><span class="line">    test_loss = <span class="number">0.0</span></span><br><span class="line">    test_acc = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_dataloader):</span><br><span class="line">            imgs, targets = data</span><br><span class="line">            imgs = imgs.to(device)</span><br><span class="line">            targets = targets.to(device)</span><br><span class="line">            output = model(imgs)</span><br><span class="line"></span><br><span class="line">            acc = (output.argmax(dim=<span class="number">1</span>) == targets).<span class="built_in">float</span>().<span class="built_in">sum</span>()</span><br><span class="line">            loss = loss_function(output, targets)</span><br><span class="line">            test_loss += loss.item()</span><br><span class="line">            test_acc += acc</span><br><span class="line"></span><br><span class="line">            total_test_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        test_acc /= test_data_len</span><br><span class="line"></span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;test_loss&#x27;</span>, test_loss, epoch)</span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;test_acc&#x27;</span>, test_acc, epoch)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;The number of epoch: &#123;&#125;, test_loss: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, test_loss))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;The number of epoch: &#123;&#125;, test_acc: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, test_acc))</span><br><span class="line"></span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;learning_rate&#x27;</span>, optimizer.state_dict()[<span class="string">&#x27;param_groups&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>], epoch)</span><br><span class="line">        scheduler.step()</span><br><span class="line"></span><br><span class="line">torch.save(model, <span class="string">&#x27;models/CIFAR10_Net_Simple_v3_Aug_Mom_150TRAININGS.pth&#x27;</span>)</span><br><span class="line"><span class="comment"># torch.save(model.state_dict(), &#x27;models/CIFAR10_Net_Simple_v3_Aug_Mom_STATE.pkl&#x27;)</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p>至此已经成功入门 PyTorch 啦！可以正式进入 Deep Learning 的学习啦！</p>
</div>
        </div>
        
        <footer class="kratos-entry-footer clearfix">
            
                <div class="post-like-donate text-center clearfix" id="post-like-donate">
                
                
                    <a class="share" href="javascript:;"><i class="fa fa-share-alt"></i> Share</a>
                    <div class="share-wrap" style="display: none;">
    <div class="share-group">
        <a href="javascript:;" class="share-plain qq" onclick="share('qq');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-qq"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain qzone" onclick="share('qzone');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-star"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain weixin pop style-plain" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-weixin"></i>
            </div>
            <div class="share-int">
                <div class="qrcode" id="wechat-qr"></div>
                <p>打开微信“扫一扫”，打开网页后点击屏幕右上角分享按钮</p>
            </div>
        </a>
        <a href="javascript:;" class="share-plain weibo" onclick="share('weibo');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-weibo"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain facebook style-plain" onclick="share('facebook');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-facebook"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain twitter style-plain" onclick="share('twitter');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-twitter"></i>
            </div>
        </a>
    </div>
    <script type="text/javascript">
        $(()=>{
            new QRCode("wechat-qr", {
                text: "https://asanosaki.github.io/posts/48394.html",
                width: 150,
                height: 150,
                correctLevel : QRCode.CorrectLevel.H
            });
        });
        function share(dest) {
            const qqBase        = "https://connect.qq.com/widget/shareqq/index.html?";
            const weiboBase     = "https://service.weibo.com/share/share.php?";
            const qzoneBase     = "https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?";
            const facebookBase  = "https://www.facebook.com/sharer/sharer.php?";
            const twitterBase   = "https://twitter.com/intent/tweet?";
            const hostUrl       = "https://asanosaki.github.io/posts/48394.html";
            const title         = "「PyTorch深度学习入门(CIFAR10分类)」";
            const excerpt       = `通过 CIFAR10 数据集的分类问题初入门 Deep Learning，也是开坑 AI 系列的第一篇文章。
相关环境的搭建可以转至：Anaconda与PyTorch安装教程。`;
            let _URL;
            switch (dest) {
                case "qq"       : _URL = qqBase+"url="+hostUrl+"&title="+title+"&desc=&summary="+excerpt+"&site=cxpy";     break;
                case "weibo"    : _URL = weiboBase+"url="+hostUrl+"&title="+title+excerpt;                                 break;
                case "qzone"    : _URL = qzoneBase+"url="+hostUrl+"&title="+title+"&desc=&summary="+excerpt+"&site=cxpy";  break;
                case "facebook" : _URL = facebookBase+"u="+hostUrl;                                                        break;
                case "twitter"  : _URL = twitterBase+"text="+title+excerpt+"&url="+hostUrl;                                break;
            }
            window.open(_URL);
        };
    </script>
</div>
                
                </div>
            
            <div class="footer-tag clearfix">
                <div class="pull-left">
                <i class="fa fa-tags"></i>
                    <a class="tag-none-link" href="/tags/AI/" rel="tag">AI</a>
                </div>
                <div class="pull-date">
                    <time datetime="2023-05-26T12:38:11.493Z" itemprop="dateModified">Last edited: 2023-05-26</time>
                </div>
            </div>
        </footer>
    </div>
    
        <nav class="navigation post-navigation clearfix" role="navigation">
            
            <div class="nav-previous clearfix">
                <a title=" Kratos-Rebirth主题修改部分细节教程" href="/posts/9012.html">&lt; Previous</a>
            </div>
            
            
            <div class="nav-next clearfix">
                <a title=" 英语日常学习记录" href="/posts/4115.html">Next &gt;</a>
            </div>
            
        </nav>
    
    
</article>

        

            </section>

        

                
            

<section id="kratos-widget-area" class="col-md-4 hidden-xs hidden-sm">
    <!-- 文章和页面根据splitter来分割，没有的话就从头开始设置为sticky -->
    
    
                <aside id="krw-about" class="widget widget-kratos-about clearfix">
    <div class="photo-background"></div>
    <div class="photo-wrapper clearfix">
        <div class="photo-wrapper-tip text-center">
            <img class="about-photo" src="/images/head.webp" loading="lazy" decoding="auto" />
        </div>
    </div>
    <div class="textwidget">
        <p class="text-center"></p>
    </div>
    <div class="site-meta">
        <a class="meta-item" href="/archives/">
            <span class="title">
                Articles
            </span>
            <span class="count">
                45
            </span>
        </a>
        <a class="meta-item" href="/categories/">
            <span class="title">
                Classifications
            </span>
            <span class="count">
                10
            </span>
        </a>
        <a class="meta-item" href="/tags/">
            <span class="title">
                Tags
            </span>
            <span class="count">
                10
            </span>
        </a>
    </div>
</aside>
            
                    <div class="sticky-area">
                
                    <aside id="krw-toc" class="widget widget-kratos-toc clearfix toc-div-class" >
    <div class="photo-background"></div>
    <h4 class="widget-title no-after">
        <i class="fa fa-compass"></i>
        Contents
        <span class="toc-progress-bar" role="progressbar" aria-label="阅读进度："></span>
    </h4>
    <div class="textwidget">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0"><span class="toc-text">1. 常用函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="toc-text">2. 数据加载</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Dataset"><span class="toc-text">2.1 Dataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-DataLoader"><span class="toc-text">2.2 DataLoader</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-TensorBoard"><span class="toc-text">3. TensorBoard</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-add-scalar"><span class="toc-text">3.1 add_scalar</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-add-image"><span class="toc-text">3.2 add_image</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Transform"><span class="toc-text">4. Transform</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Transform%E7%9A%84%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95"><span class="toc-text">4.1 Transform的概念与基本用法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Transform%E7%9A%84%E5%B8%B8%E7%94%A8%E7%B1%BB"><span class="toc-text">4.2 Transform的常用类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Torchvision%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-text">5. Torchvision数据集使用方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CTorch-NN%E5%9F%BA%E6%9C%AC%E9%AA%A8%E6%9E%B6%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-text">6. 神经网络Torch.NN基本骨架的使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Convolution-Layers%E4%B8%8EPooling-Layers"><span class="toc-text">7. Convolution Layers与Pooling Layers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-Convolution-Layers"><span class="toc-text">7.1 Convolution Layers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-Pooling-Layers"><span class="toc-text">7.2 Pooling Layers</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-Non-linear-Activations%E4%B8%8ELinear-Layers"><span class="toc-text">8. Non-linear Activations与Linear Layers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-Non-linear-Activations"><span class="toc-text">8.1 Non-linear Activations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-Linear-Layers"><span class="toc-text">8.2 Linear Layers</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA%E5%B0%8F%E5%AE%9E%E6%88%98"><span class="toc-text">9. 神经网络模型搭建小实战</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-1-Sequential"><span class="toc-text">9.1 Sequential</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-%E5%B0%8F%E5%AE%9E%E6%88%98"><span class="toc-text">9.2 小实战</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-text">10. 损失函数与反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-1-Loss-Functions"><span class="toc-text">10.1 Loss Functions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-Backward"><span class="toc-text">10.2 Backward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-3-Optimizer"><span class="toc-text">10.3 Optimizer</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-%E7%8E%B0%E6%9C%89%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%BF%E7%94%A8%E5%8F%8A%E4%BF%AE%E6%94%B9"><span class="toc-text">11. 现有网络模型的使用及修改</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-1-VGG16%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-text">11.1 VGG16模型的使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-2-%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%AF%BB%E5%8F%96"><span class="toc-text">11.2 模型的保存与读取</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-%E5%AE%8C%E6%95%B4%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-text">12. 完整训练模型的方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#12-1-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%97%B6%E7%9A%84%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="toc-text">12.1 训练模型时的注意事项</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-2-%E4%BD%BF%E7%94%A8GPU%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83"><span class="toc-text">12.2 使用GPU进行训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-3-CIFAR10-Net-Simple-v3"><span class="toc-text">12.3 CIFAR10_Net_Simple_v3</span></a></li></ol></li></ol>
    </div>
</aside>
                
                
  <aside id="krw-categories" class="widget widget-kratos-categories clearfix">
    <h4 class="widget-title"><i class="fa fa-folder"></i>Contents</h4>
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AI/">AI</a><span class="category-list-count">17</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Essay/">Essay</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/MySQL/">MySQL</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Network/">Network</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Others/">Others</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Web/">Web</a><span class="category-list-count">7</span></li></ul>
  </aside>


            
                
  <aside id="krw-tags" class="widget widget-kratos-tags clearfix">
    <h4 class="widget-title"><i class="fa fa-tags"></i>Tags aggregation</h4>
      <div class="tag-clouds">
        <a href="/tags/AI/" style="font-size: 0.8em;">AI</a> <a href="/tags/C/" style="font-size: 0.6em;">C++</a> <a href="/tags/Essay/" style="font-size: 0.6em;">Essay</a> <a href="/tags/Hexo/" style="font-size: 0.64em;">Hexo</a> <a href="/tags/Linux/" style="font-size: 0.68em;">Linux</a> <a href="/tags/MySQL/" style="font-size: 0.6em;">MySQL</a> <a href="/tags/Network/" style="font-size: 0.6em;">Network</a> <a href="/tags/Others/" style="font-size: 0.72em;">Others</a> <a href="/tags/Python/" style="font-size: 0.64em;">Python</a> <a href="/tags/Web/" style="font-size: 0.76em;">Web</a>
      </div>
  </aside>

            
                
  <aside id="krw-posts" class="widget widget-kratos-posts">
  <h4 class="widget-title"><i class="fa fa-file"></i>Latest articles</h4>
  <div class="tab-content">
      <ul class="list-group">
        
        
          
          
            <a class="list-group-item" href="/posts/41844.html"><i class="fa  fa-book"></i> Web学习笔记-React（Redux）</a>
            
          
        
          
          
            <a class="list-group-item" href="/posts/25510.html"><i class="fa  fa-book"></i> Web学习笔记-React（路由）</a>
            
          
        
          
          
            <a class="list-group-item" href="/posts/18290.html"><i class="fa  fa-book"></i> Web学习笔记-React（组合Components）</a>
            
          
        
          
          
            <a class="list-group-item" href="/posts/21781.html"><i class="fa  fa-book"></i> DeepLabV3Plus核心代码详解</a>
            
          
        
          
          
            <a class="list-group-item" href="/posts/23991.html"><i class="fa  fa-book"></i> KMeans聚类与PCA主成分分析</a>
            
          
        
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
      </ul>
  </div>
  </aside>

            
    </div>
</section>
        
        </div>
    </div>
</div>
<footer>
    <div id="footer"  class="ap-lrc"  >
        <div class="container">
            <div class="row">
                <div class="col-md-6 col-md-offset-3 footer-list text-center">
                    <ul class="kratos-social-icons">
                        <li><a target="_blank" rel="nofollow" href="https://weibo.com/u/1952449115"><i class="fa fa-weibo"></i></a></li>
                        <li><a href="mailto:mail@1195595343@qq.com"><i class="fa fa-envelope"></i></a></li>
                        
                        
                        
                        
                        
                        <li><a target="_blank" rel="nofollow" href="https://github.com/AsanoSaki"><i class="fa fa-github"></i></a></li>
                        
                    </ul>
                    <ul class="kratos-copyright">
                        <div>
                            <li>&copy; 2023 AsanoSaki Copyright.</li>
                            <li>This site is running<span id="span_dt">Loading...</span></li>
                        </div>
                        <div>
                            <li>Theme <a href="https://github.com/Candinya/Kratos-Rebirth" target="_blank">Kratos:Rebirth</a></li>
                            <li>Site built with&nbsp;<i class="fa fa-heart throb" style="color:#d43f57"></i>&nbsp;by AsanoSaki.</li>
                        </div>
                        <div>
                            <li>Powered by <a href="https://hexo.io" target="_blank" rel="nofollow">Hexo</a></li>
                            <li>Hosted on <a href="https://github.com/" target="_blank">Github Pages</a></li>
                        </div>
                        <div>
                            
                            
                        </div>
                    </ul>
                </div>
            </div>
        </div>
        <div class="kr-tool text-center">
            <div class="tool">
                
                    <div class="box search-box">
                        <a href="/search/">
                            <span class="fa fa-search"></span>
                        </a>
                    </div>
                
                
                    <div class="box theme-box" id="darkmode-switch">
                        <span class="fa fa-adjust"></span>
                    </div>
                
                
            </div>
            <div class="box gotop-box">
                <span class="fa fa-chevron-up"></span>
            </div>
        </div>
    </div>
</footer>
</div>
</div>

        <script defer src="/vendors/bootstrap@3.3.4/dist/js/bootstrap.min.js"></script>
<script defer src="/vendors/nprogress@0.2.0/nprogress.js"></script>
<script>
    if (!window.kr) {
        window.kr = {};
    }
    window.kr.notMobile = (!(navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i)));
    window.kr.siteRoot = "/";
</script>


    <script async src="/js/candy.min.js"></script>



    <script defer src="/vendors/aplayer@1.10.1/dist/APlayer.min.js"></script>
    


    <script defer src="/vendors/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>

<script defer src="/vendors/clipboard@2.0.6/dist/clipboard.min.js"></script>
<script defer src="/js/kratosr.min.js"></script>
<script defer src="/js/pjax.min.js"></script>



<!-- Extra support for third-party plguins  -->


    </body>
</html>