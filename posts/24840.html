<!DOCTYPE html>
<html lang="zh-CN">
    <head>
  <!-- 元数据 -->
  <meta charset="utf-8">
  <link rel="icon" href="/images/favicon.ico">
  
  <title>D2L学习笔记-计算机视觉 | AsanoSaki</title>
  
  <meta name="author" content="AsanoSaki" />
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="robots" content="index,follow" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <meta name="format-detection" content="telphone=no, email=no" />
  
    <meta name="keywords" content="AI" />
  
  <meta name="description" content="D2L学习笔记-计算机视觉">
<meta property="og:type" content="article">
<meta property="og:title" content="D2L学习笔记-计算机视觉">
<meta property="og:url" content="https://asanosaki.github.io/posts/24840.html">
<meta property="og:site_name" content="AsanoSaki">
<meta property="og:description" content="D2L学习笔记-计算机视觉">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://asanosaki.github.io/images/favicon.ico">
<meta property="article:published_time" content="2023-03-10T02:24:00.000Z">
<meta property="article:modified_time" content="2023-11-26T08:01:43.777Z">
<meta property="article:author" content="AsanoSaki">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://asanosaki.github.io/images/favicon.ico">
  <link rel="alternate" href="atom.xml" type="application/atom+xml">
  <!-- 站点验证相关 -->
  
    
    
    
  
  <!-- 样式表文件 -->
  <link rel="stylesheet" id="kratos-css" href="/css/kratosr.min.css" media="all"></script>
  
    <link rel="stylesheet" id="darkmode-css" href="/css/kr-color-dark.min.css" media="(prefers-color-scheme: dark)"></script>
    <script src="/js/kr-dark.min.js"></script>
  
  
    <link rel="stylesheet" id="highlight-css" href="/css/highlight/light.min.css" media="all"></script>
  
  <link rel="stylesheet" id="fontawe-css" href="/vendors/font-awesome@4.7.0/css/font-awesome.min.css" media="all"></script>
  <link rel="stylesheet" id="nprogress-css" href="/vendors/nprogress@0.2.0/nprogress.css" media="all"></script>
  
  
  
    <link rel="stylesheet" href="/vendors/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"></script>
  
  <!-- 不得不预先加载的一些JS文件 -->
  <script src="/vendors/jquery@3.6.0/dist/jquery.min.js"></script>
  
    <script src="/vendors/qrcode_js@1.0.0/qrcode.min.js"></script>
  
  
  <style>
    
      .kratos-cover.kratos-cover-2 {
        background-image: url('/images/banner2.jpg');
      }
    
    
      @media(min-width:768px) {
        body.custom-background {
          background-image: url('/images/opacity.png');
        }
      }
    
  </style>
  
<meta name="generator" content="Hexo 5.4.2"></head>


    <body class="custom-background">
        <div id="kratos-wrapper">
    <div id="kratos-page">
        <div id="kratos-header">
            <header id="kratos-desktop-topnav" class="kratos-topnav">
                <div class="container">
                    <div class="nav-header">
                        <nav id="kratos-menu-wrap">
                            <ul id="kratos-primary-menu" class="sf-menu">
                                
                                    
                                    
                                        
                                            <li><a href="/"><i class="fa fa-home"></i>Home</a></li>
                                        
                                    
                                        
                                            <li><a href="/archives/"><i class="fa fa-file"></i>Archives</a></li>
                                        
                                    
                                        
                                            <li>
                                                <a><i class="fa fa-paw"></i>Friends</a>
                                                <ul class="sub-menu">
                                                    
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://angels-d.github.io">Angels-D</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="http://syh521.cn">SYH</a></li>
                                                        
                                                    
                                                </ul>
                                            </li>
                                        
                                    
                                        
                                            <li>
                                                <a><i class="fa fa-link"></i>My Links</a>
                                                <ul class="sub-menu">
                                                    
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://github.com/AsanoSaki">GitHub</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://gitee.com/asanosaki">Gitee</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://git.acwing.com/AsanoSaki">AcGit</a></li>
                                                        
                                                    
                                                        
                                                            <li><a href="https://asanosaki.github.io">BLOG</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://asanosaki.blog.csdn.net">CSDN</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://www.acwing.com/user/myspace/index/82581">AcWing</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://www.luogu.com.cn/user/459347">LuoGu</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://codeforces.com/profile/AsanoSaki">CodeForces</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://leetcode.cn/u/asanosaki">LeetCode</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://space.bilibili.com/12300056">Bilibili</a></li>
                                                        
                                                    
                                                </ul>
                                            </li>
                                        
                                    
                                        
                                            <li>
                                                <a><i class="fa fa-heart"></i>Favorite</a>
                                                <ul class="sub-menu">
                                                    
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://www.runoob.com">Runoob</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://wallhaven.cc">Wallhaven</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="http://www.flysheep6.com">Flysheep</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://thrift.apache.org">Thrift</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://developer.mozilla.org/zh-CN">MDN</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://v5.bootcss.com">Bootstrap</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://zh-hans.react.dev">React</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://www.nextjs.cn">Next.js</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://cn.vuejs.org">Vue.js</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://docs.djangoproject.com/zh-hans/4.2">Django</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle">Spring Boot</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://hub.docker.com">Docker Hub</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://fontawesome.com.cn">Font Awesome</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://www.online-convert.com">File Convert</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://pytorch.org">PyTorch</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai">D2L</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://board.xcpcio.com">XCPC Board</a></li>
                                                        
                                                    
                                                        
                                                            <li><a target="_blank" rel="noopener" href="https://kr-demo.candinya.com/posts/Kratos-Rebirth-Manual">Kratos MNL</a></li>
                                                        
                                                    
                                                </ul>
                                            </li>
                                        
                                    
                                
                            </ul>
                        </nav>
                    </div>
                </div>
            </header>
            <header id="kratos-mobile-topnav" class="kratos-topnav">
                <div class="container">
                    <div class="color-logo"><a href="/">AsanoSaki</a></div>
                    <div class="nav-toggle">
                        <a class="kratos-nav-toggle js-kratos-nav-toggle">
                            <i></i>
                        </a>
                    </div>
                </div>
            </header>
        </div>
        <div class="kratos-start kratos-hero-2">
            <!-- <div class="kratos-overlay"></div> -->
            <div class="kratos-cover kratos-cover-2 text-center">
                <div class="desc desc2 animate-box">
                    <a href="/">
                        <h2>AsanoSaki</h2> <br />
                        <span>夏天的海边有冰淇淋口味的海风~</span>
                    </a>
                </div>
            </div>
        </div>

        <div id="kratos-blog-post">
            <div class="container">
                <div id="main" class="row">
                    

        

            <section class="col-md-8">

        

            <article itemscope itemtype="https://schema.org/Article">
    
    <link itemprop="mainEntityOfPage" href="https://asanosaki.github.io/posts/24840.html">
    <div class="kratos-hentry kratos-post-inner clearfix">
        <header class="kratos-entry-header">
            
                <h1 class="kratos-entry-title text-center" itemprop="name headline">D2L学习笔记-计算机视觉</h1>
            
            
            <ul class="kratos-post-meta text-center">
                <li><time datetime="2023-03-10T02:24:00.000Z" itemprop="datePublished"><i class="fa fa-calendar"></i> 2023-03-10</time></li>
                <li itemprop="author" itemscope itemtype="https://schema.org/Person">
                    <i class="fa fa-user"></i> Author <span itemprop="name">AsanoSaki</span>
                </li>
                
                    <li>
                        <i class="fa fa-edit"></i> 
                        
                        
                            ~52.12K
                        
                        words
                    </li>
                
                
            </ul>
        </header>
        <div class="kratos-post-content">
            
            <div id="expire-alert" class="alert alert-warning hidden" role="alert">
                <div class="icon"><i class="fa fa-warning"></i></div>
                <div class="text"><p>本文最后编辑于 <time datetime="1700985703777"></time> 前，其中的内容可能需要更新。</p></div>
            </div>
            
            
            
                <div class="kratos-post-inner-toc toc-div-class" >
                    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%B9%BF"><span class="toc-text">1. 图像增广</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%BE%AE%E8%B0%83"><span class="toc-text">2. 微调</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%92%8C%E8%BE%B9%E7%95%8C%E6%A1%86"><span class="toc-text">3. 目标检测和边界框</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">4. 目标检测数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E9%94%9A%E6%A1%86"><span class="toc-text">5. 锚框</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B"><span class="toc-text">6. 多尺度目标检测</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E5%A4%9A%E5%B0%BA%E5%BA%A6%E9%94%9A%E6%A1%86"><span class="toc-text">6.1 多尺度锚框</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E5%A4%9A%E5%B0%BA%E5%BA%A6%E6%A3%80%E6%B5%8B"><span class="toc-text">6.2 多尺度检测</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E5%8C%BA%E5%9F%9F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88R-CNN%EF%BC%89%E7%B3%BB%E5%88%97"><span class="toc-text">7. 区域卷积神经网络（R-CNN）系列</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-R-CNN"><span class="toc-text">7.1 R-CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-Fast-R-CNN"><span class="toc-text">7.2 Fast R-CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-Faster-R-CNN"><span class="toc-text">7.3 Faster R-CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-4-Mask-R-CNN"><span class="toc-text">7.4 Mask R-CNN</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E5%8D%95%E5%8F%91%E5%A4%9A%E6%A1%86%E6%A3%80%E6%B5%8B%EF%BC%88SSD%EF%BC%89"><span class="toc-text">8. 单发多框检测（SSD）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-%E7%B1%BB%E5%88%AB%E9%A2%84%E6%B5%8B%E5%B1%82%E4%B8%8E%E8%BE%B9%E7%95%8C%E6%A1%86%E9%A2%84%E6%B5%8B%E5%B1%82"><span class="toc-text">8.1 类别预测层与边界框预测层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-%E8%BF%9E%E7%BB%93%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%9A%84%E9%A2%84%E6%B5%8B"><span class="toc-text">8.2 连结多尺度的预测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3-%E9%AB%98%E5%92%8C%E5%AE%BD%E5%87%8F%E5%8D%8A%E5%9D%97"><span class="toc-text">8.3 高和宽减半块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-4-%E5%9F%BA%E6%9C%AC%E7%BD%91%E7%BB%9C%E5%9D%97"><span class="toc-text">8.4 基本网络块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-5-%E5%AE%8C%E6%95%B4%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-text">8.5 完整的模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-6-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-text">8.6 训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-7-%E9%A2%84%E6%B5%8B%E7%9B%AE%E6%A0%87"><span class="toc-text">8.7 预测目标</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">9. 语义分割和数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-1-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2"><span class="toc-text">9.1 图像分割和实例分割</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-Pascal-VOC2012-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">9.2 Pascal VOC2012 语义分割数据集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF"><span class="toc-text">10. 转置卷积</span></a></li></ol>
                </div>
            
            <hr />
            <div itemprop="articleBody"><blockquote>
<p>李沐动手学深度学习（PyTorch）课程学习笔记第七章：计算机视觉。</p>
</blockquote>
<span id="more"></span>
<h2 id="1-图像增广">1. 图像增广</h2>
<p>图像增广在对训练图像进行一系列的随机变化之后，生成相似但不同的训练样本，从而扩大了训练集的规模。此外，应用图像增广的原因是，随机改变训练样本可以减少模型对某些属性的依赖，从而提高模型的泛化能力。例如，我们可以以不同的方式裁剪图像，使感兴趣的对象出现在不同的位置，减少模型对于对象出现位置的依赖。我们还可以调整亮度、颜色等因素来降低模型对颜色的敏感度。</p>
<p>下面的代码有50%的几率使图像向左或向右翻转：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trans = torchvision.transforms.RandomHorizontalFlip()</span><br></pre></td></tr></table></figure>
<p>有50%的几率向上或向下翻转，注意，上下翻转图像不如左右图像翻转那样常用，需要根据数据集的特征考虑是否可以将图像上下翻转：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trans = torchvision.transforms.RandomVerticalFlip()</span><br></pre></td></tr></table></figure>
<p>随机裁剪一个面积为原始面积10%到100%的区域，该区域的宽高比从0.5~2之间随机取值。然后，区域的宽度和高度都被缩放到200像素：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trans = torchvision.transforms.RandomResizedCrop((<span class="number">200</span>, <span class="number">200</span>), scale=(<span class="number">0.1</span>, <span class="number">1</span>), ratio=(<span class="number">0.5</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p>我们可以改变图像颜色的四个方面：亮度、对比度、饱和度和色调。在下面的示例中，我们随机更改图像的亮度，随机值为原始图像的50%(1 - 0.5)到150%(1 + 0.5)之间：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trans = torchvision.transforms.ColorJitter(brightness=<span class="number">0.5</span>, contrast=<span class="number">0</span>, saturation=<span class="number">0</span>, hue=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>在实践中，我们将结合多种图像增广方法。我们可以通过使用一个 <code>Compose</code> 实例来综合上面定义的不同的图像增广方法，并将它们应用到每个图像：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">trans = transforms.Compose([</span><br><span class="line">    transforms.RandomHorizontalFlip(p=<span class="number">0.5</span>),  <span class="comment"># 50%的概率使图片水平翻转</span></span><br><span class="line">    transforms.ColorJitter(brightness=<span class="number">0.5</span>, contrast=<span class="number">0.5</span>, saturation=<span class="number">0.5</span>, hue=<span class="number">0.5</span>),</span><br><span class="line">    transforms.ToTensor()])</span><br></pre></td></tr></table></figure>
<p>图像增广可以直接作用在图像数据上，也可以在使用 <code>torchvision.datasets</code> 导入数据集的时候通过 <code>transform</code> 参数指定：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = trans(X)</span><br><span class="line"></span><br><span class="line">cifar_train = torchvision.datasets.CIFAR10(root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="2-微调">2. 微调</h2>
<p>微调（fine-tuning）是迁移学习（transfer learning）中的常见技巧，微调包括以下四个步骤：</p>
<ul>
<li>在源数据集（例如 ImageNet 数据集）上预训练神经网络模型，即源模型。</li>
<li>创建一个新的神经网络模型，即目标模型。这将复制源模型上的所有模型设计及其参数（输出层除外）。我们假定这些模型参数包含从源数据集中学到的知识，这些知识也将适用于目标数据集。我们还假设源模型的输出层与源数据集的标签密切相关；因此不在目标模型中使用该层。</li>
<li>向目标模型添加输出层，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。</li>
<li>在目标数据集（如椅子数据集）上训练目标模型。输出层将从头开始进行训练，而所有其他层的参数将根据源模型的参数进行微调。</li>
</ul>
<p>当目标数据集比源数据集小得多时，微调有助于提高模型的泛化能力。</p>
<p>我们将在一个 CIFAR10 数据集上微调 ResNet-18 模型。该模型已在 ImageNet 数据集上进行了预训练：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------- Data ----------</span></span><br><span class="line">train_trans = transforms.Compose([</span><br><span class="line">    transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">    transforms.RandomHorizontalFlip(),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    <span class="comment"># 使用ImageNet的RGB通道的均值和标准差，以标准化每个通道</span></span><br><span class="line">    transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                         [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])</span><br><span class="line"></span><br><span class="line">test_trans = transforms.Compose([</span><br><span class="line">    transforms.Resize([<span class="number">256</span>, <span class="number">256</span>]),</span><br><span class="line">    transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                         [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">cifar_train = torchvision.datasets.CIFAR10(root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">True</span>, transform=train_trans, download=<span class="literal">True</span>)</span><br><span class="line">cifar_test = torchvision.datasets.CIFAR10(root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">False</span>, transform=test_trans, download=<span class="literal">True</span>)</span><br><span class="line">train_iter = data.DataLoader(cifar_train, batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line">test_iter = data.DataLoader(cifar_test, batch_size, shuffle=<span class="literal">False</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------- ResNet-18 ----------</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_model</span>(<span class="params">num_classes, use_pretrained=<span class="literal">True</span></span>):</span><br><span class="line">    net = models.resnet18(pretrained=use_pretrained)</span><br><span class="line">    net.fc = nn.Linear(net.fc.in_features, num_classes)</span><br><span class="line">    nn.init.xavier_uniform_(net.fc.weight)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">net = resnet_model(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------- Train ----------</span></span><br><span class="line"><span class="comment"># 如果param_group=True，输出层中的模型参数将使用十倍的学习率</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, train_iter, test_iter, num_epochs, lr, device, wd, param_group=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    loss_function = nn.CrossEntropyLoss()</span><br><span class="line">    loss_function.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> param_group:</span><br><span class="line">        params_1x = [param <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters() <span class="keyword">if</span> name <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&quot;fc.weight&quot;</span>, <span class="string">&quot;fc.bias&quot;</span>]]</span><br><span class="line">        optimizer = torch.optim.SGD([&#123;<span class="string">&#x27;params&#x27;</span>: params_1x&#125;,</span><br><span class="line">                                     &#123;<span class="string">&#x27;params&#x27;</span>: net.fc.parameters(), <span class="string">&#x27;lr&#x27;</span>: lr * <span class="number">10</span>&#125;], lr=lr, weight_decay=wd)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd)</span><br><span class="line"></span><br><span class="line">    writer = SummaryWriter(<span class="string">&#x27;../logs/FineTune_CIFAR10_train_log&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    best_acc = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        net.train()</span><br><span class="line">        train_loss = []</span><br><span class="line">        train_acc = []</span><br><span class="line">        <span class="keyword">for</span> img, label <span class="keyword">in</span> tqdm(train_iter):</span><br><span class="line">            img, label = img.to(device), label.to(device)</span><br><span class="line">            label_hat = net(img)</span><br><span class="line"></span><br><span class="line">            loss = loss_function(label_hat, label)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            label_hat = label_hat.argmax(axis=<span class="number">1</span>)</span><br><span class="line">            acc = (label_hat.<span class="built_in">type</span>(label.dtype) == label).<span class="built_in">float</span>().mean()</span><br><span class="line">            train_loss.append(loss.item())</span><br><span class="line">            train_acc.append(acc)</span><br><span class="line"></span><br><span class="line">        train_loss = <span class="built_in">sum</span>(train_loss) / <span class="built_in">len</span>(train_loss)</span><br><span class="line">        train_acc = <span class="built_in">sum</span>(train_acc) / <span class="built_in">len</span>(train_acc)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;[ Train | <span class="subst">&#123;epoch + <span class="number">1</span>:03d&#125;</span>/<span class="subst">&#123;num_epochs:03d&#125;</span> ] loss = <span class="subst">&#123;train_loss:<span class="number">.5</span>f&#125;</span>, acc = <span class="subst">&#123;train_acc:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        valid_loss = []</span><br><span class="line">        valid_acc = []</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">for</span> img, label <span class="keyword">in</span> tqdm(test_iter):</span><br><span class="line">                img, label = img.to(device), label.to(device)</span><br><span class="line">                label_hat = net(img)</span><br><span class="line">                loss = loss_function(label_hat, label)</span><br><span class="line">                label_hat = label_hat.argmax(axis=<span class="number">1</span>)</span><br><span class="line">                acc = (label_hat.<span class="built_in">type</span>(label.dtype) == label).<span class="built_in">float</span>().mean()</span><br><span class="line">                valid_loss.append(loss.item())</span><br><span class="line">                valid_acc.append(acc)</span><br><span class="line"></span><br><span class="line">        valid_loss = <span class="built_in">sum</span>(valid_loss) / <span class="built_in">len</span>(valid_loss)</span><br><span class="line">        valid_acc = <span class="built_in">sum</span>(valid_acc) / <span class="built_in">len</span>(valid_acc)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;[ Valid | <span class="subst">&#123;epoch + <span class="number">1</span>:03d&#125;</span>/<span class="subst">&#123;num_epochs:03d&#125;</span> ] loss = <span class="subst">&#123;valid_loss:<span class="number">.5</span>f&#125;</span>, acc = <span class="subst">&#123;valid_acc:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;train_loss&#x27;</span>, train_loss, epoch + <span class="number">1</span>)</span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;train_acc&#x27;</span>, train_acc, epoch + <span class="number">1</span>)</span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;valid_loss&#x27;</span>, valid_loss, epoch + <span class="number">1</span>)</span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;valid_acc&#x27;</span>, valid_acc, epoch + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> valid_acc &gt; best_acc:</span><br><span class="line">            best_acc = valid_acc</span><br><span class="line">            torch.save(net.state_dict(), <span class="string">&#x27;../save/FineTune_CIFAR10_train.params&#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;saving model with acc &#123;:.3f&#125;&#x27;</span>.<span class="built_in">format</span>(best_acc))</span><br><span class="line"></span><br><span class="line">    writer.close()</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">lr, wd, num_epochs = <span class="number">0.0005</span>, <span class="number">0.001</span>, <span class="number">50</span></span><br><span class="line"></span><br><span class="line">train(net, train_iter, test_iter, num_epochs, lr, device, wd, param_group=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="3-目标检测和边界框">3. 目标检测和边界框</h2>
<p>在图像分类任务中，我们假设图像中只有一个主要物体对象，我们只关注如何识别其类别。然而，很多时候图像里有多个我们感兴趣的目标，我们不仅想知道它们的类别，还想得到它们在图像中的<strong>具体位置</strong>。在计算机视觉里，我们将这类任务称为<strong>目标检测</strong>（object detection）或<strong>目标识别</strong>（object recognition）。</p>
<p>下面加载本节将使用的示例图像。图像左边是一只狗，右边是一只猫。它们是这张图像里的两个主要目标：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]</span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">plt.figure(dpi=<span class="number">100</span>, figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">img = plt.imread(<span class="string">&#x27;../images/catdog.jpg&#x27;</span>)</span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>在目标检测中，我们通常使用边界框（bounding box）来描述对象的空间位置。边界框是矩形的，由矩形左上角的以及右下角的 <code>x</code> 和 <code>y</code> 坐标决定。另一种常用的边界框表示方法是边界框中心的 <code>(x, y)</code> 轴坐标以及框的宽度和高度。</p>
<p>在这里，我们定义在这两种表示法之间进行转换的函数：<code>box_corner_to_center</code> 从两角表示法转换为中心宽度表示法，而 <code>box_center_to_corner</code> 反之亦然。输入参数 <code>boxes</code> 可以是长度为4的张量，也可以是形状为 <code>(N, 4)</code> 的二维张量，其中 N 是边界框的数量。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">box_corner_to_center</span>(<span class="params">boxes</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;从（左上，右下）转换到（中间，宽度，高度）&quot;&quot;&quot;</span></span><br><span class="line">    x1, y1, x2, y2 = boxes[:, <span class="number">0</span>], boxes[:, <span class="number">1</span>], boxes[:, <span class="number">2</span>], boxes[:, <span class="number">3</span>]</span><br><span class="line">    cx = (x1 + x2) / <span class="number">2</span></span><br><span class="line">    cy = (y1 + y2) / <span class="number">2</span></span><br><span class="line">    w = x2 - x1</span><br><span class="line">    h = y2 - y1</span><br><span class="line">    boxes = torch.stack((cx, cy, w, h), dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> boxes</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">box_center_to_corner</span>(<span class="params">boxes</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;从（中间，宽度，高度）转换到（左上，右下）&quot;&quot;&quot;</span></span><br><span class="line">    cx, cy, w, h = boxes[:, <span class="number">0</span>], boxes[:, <span class="number">1</span>], boxes[:, <span class="number">2</span>], boxes[:, <span class="number">3</span>]</span><br><span class="line">    x1 = cx - <span class="number">0.5</span> * w</span><br><span class="line">    y1 = cy - <span class="number">0.5</span> * h</span><br><span class="line">    x2 = cx + <span class="number">0.5</span> * w</span><br><span class="line">    y2 = cy + <span class="number">0.5</span> * h</span><br><span class="line">    boxes = torch.stack((x1, y1, x2, y2), dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> boxes</span><br></pre></td></tr></table></figure>
<p>我们将根据坐标信息定义图像中狗和猫的边界框。图像中坐标的原点是图像的左上角，向右的方向为 <code>x</code> 轴的正方向，向下的方向为 <code>y</code> 轴的正方向：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># bbox是边界框的英文缩写</span></span><br><span class="line">dog_bbox, cat_bbox = [<span class="number">60.0</span>, <span class="number">45.0</span>, <span class="number">378.0</span>, <span class="number">516.0</span>], [<span class="number">400.0</span>, <span class="number">112.0</span>, <span class="number">655.0</span>, <span class="number">493.0</span>]</span><br><span class="line">boxes = torch.tensor((dog_bbox, cat_bbox))</span><br><span class="line"><span class="built_in">print</span>(box_center_to_corner(box_corner_to_center(boxes)) == boxes)</span><br><span class="line"><span class="comment"># tensor([[True, True, True, True],</span></span><br><span class="line"><span class="comment">#         [True, True, True, True]])</span></span><br></pre></td></tr></table></figure>
<p>我们可以将边界框在图中画出，以检查其是否准确。画之前，我们定义一个辅助函数 <code>bbox_to_rect</code>。它将边界框表示成 <code>matplotlib</code> 的边界框格式，在图像上添加边界框之后，我们可以看到两个物体的主要轮廓基本上在两个框内：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bbox_to_rect</span>(<span class="params">bbox, color</span>):</span><br><span class="line">    <span class="comment"># 将边界框(左上x, 左上y, 右下x, 右下y)格式转换成matplotlib格式：(xy=(左上x, 左上y), width=宽, height=高)</span></span><br><span class="line">    <span class="keyword">return</span> plt.Rectangle(xy=(bbox[<span class="number">0</span>], bbox[<span class="number">1</span>]), width=bbox[<span class="number">2</span>]-bbox[<span class="number">0</span>], height=bbox[<span class="number">3</span>]-bbox[<span class="number">1</span>],</span><br><span class="line">                         fill=<span class="literal">False</span>, edgecolor=color, linewidth=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">fig = plt.imshow(img)</span><br><span class="line">fig.axes.add_patch(bbox_to_rect(dog_bbox, <span class="string">&#x27;blue&#x27;</span>))</span><br><span class="line">fig.axes.add_patch(bbox_to_rect(cat_bbox, <span class="string">&#x27;red&#x27;</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="4-目标检测数据集">4. 目标检测数据集</h2>
<p>目标检测领域没有像 MNIST 和 Fashion-MNIST 那样的小数据集。为了快速测试目标检测模型，我们收集并标记了一个小型数据集。首先，我们拍摄了一组香蕉的照片，并生成了1000张不同角度和大小的香蕉图像。然后，我们在一些背景图片的随机位置上放一张香蕉的图像。最后，我们在图片上为这些香蕉标记了边界框。</p>
<p>包含所有图像和 CSV 标签文件的香蕉检测数据集可以直接从互联网下载，通过 <code>read_data_bananas</code> 函数，我们读取香蕉检测数据集的图像和标签。该数据集的 CSV 文件内含目标类别标签和位于左上角和右下角的真实边界框坐标：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]</span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span></span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line">d2l.DATA_HUB[<span class="string">&#x27;banana-detection&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;banana-detection.zip&#x27;</span>, <span class="string">&#x27;5de26c8fce5ccdea9f91267273464dc968d20d72&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_data_bananas</span>(<span class="params">is_train=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;读取香蕉检测数据集中的图像和标签&quot;&quot;&quot;</span></span><br><span class="line">    data_dir = d2l.download_extract(<span class="string">&#x27;banana-detection&#x27;</span>)  <span class="comment"># 路径为../data</span></span><br><span class="line">    csv_fname = os.path.join(data_dir, <span class="string">&#x27;bananas_train&#x27;</span> <span class="keyword">if</span> is_train <span class="keyword">else</span> <span class="string">&#x27;bananas_val&#x27;</span>, <span class="string">&#x27;label.csv&#x27;</span>)</span><br><span class="line">    csv_data = pd.read_csv(csv_fname)</span><br><span class="line">    csv_data = csv_data.set_index(<span class="string">&#x27;img_name&#x27;</span>)</span><br><span class="line">    images, targets = [], []</span><br><span class="line">    <span class="keyword">for</span> img_name, target <span class="keyword">in</span> csv_data.iterrows():</span><br><span class="line">        images.append(torchvision.io.read_image(os.path.join(data_dir, <span class="string">&#x27;bananas_train&#x27;</span> <span class="keyword">if</span> is_train <span class="keyword">else</span> <span class="string">&#x27;bananas_val&#x27;</span>, <span class="string">&#x27;images&#x27;</span>, <span class="string">f&#x27;<span class="subst">&#123;img_name&#125;</span>&#x27;</span>)))</span><br><span class="line">        <span class="comment"># 这里的target为：(类别, 左上角x, 左上角y, 右下角x, 右下角y)，其中所有图像都具有相同的香蕉类（索引为0）</span></span><br><span class="line">        targets.append(<span class="built_in">list</span>(target))</span><br><span class="line">    <span class="keyword">return</span> images, torch.tensor(targets).unsqueeze(<span class="number">1</span>) / <span class="number">256</span></span><br></pre></td></tr></table></figure>
<p>以下 <code>BananasDataset</code> 类别将允许我们创建一个自定义 <code>Dataset</code> 实例来加载香蕉检测数据集：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BananasDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;一个用于加载香蕉检测数据集的自定义数据集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, is_train</span>):</span><br><span class="line">        self.features, self.labels = read_data_bananas(is_train)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;read &#x27;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(self.features)) + (<span class="string">f&#x27; training examples&#x27;</span> <span class="keyword">if</span> is_train <span class="keyword">else</span> <span class="string">f&#x27; validation examples&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> (self.features[idx].<span class="built_in">float</span>(), self.labels[idx])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.features)</span><br></pre></td></tr></table></figure>
<p>最后，我们定义 <code>load_data_bananas</code> 函数，来为训练集和测试集返回两个数据加载器实例。对于测试集，无须按随机顺序读取它：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_bananas</span>(<span class="params">batch_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;加载香蕉检测数据集&quot;&quot;&quot;</span></span><br><span class="line">    train_iter = DataLoader(BananasDataset(is_train=<span class="literal">True</span>), batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    val_iter = DataLoader(BananasDataset(is_train=<span class="literal">False</span>), batch_size)</span><br><span class="line">    <span class="keyword">return</span> train_iter, val_iter</span><br></pre></td></tr></table></figure>
<p>让我们读取一个小批量，并打印其中的图像和标签的形状。图像的小批量的形状为：<code>(批量大小, 通道数, 高度, 宽度)</code>，它与我们之前图像分类任务中的相同。标签的小批量的形状为：<code>(批量大小, M, 5)</code>，其中 M 是数据集的任何图像中边界框可能出现的最大数量。</p>
<p>小批量计算虽然高效，但它要求每张图像含有相同数量的边界框，以便放在同一个批量中。通常来说，图像可能拥有不同数量个边界框；因此，在达到 M 之前，边界框少于 M 的图像将被非法边界框填充。这样，每个边界框的标签将被长度为5的数组表示。数组中的第一个元素是边界框中对象的类别，其中-1表示用于填充的非法边界框。数组的其余四个元素是边界框左上角和右下角的 <code>(x, y)</code> 坐标值（值域在0~1之间）。对于香蕉数据集而言，由于每张图像上只有一个边界框，因此 <code>M = 1</code>。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch_size, edge_size = <span class="number">32</span>, <span class="number">256</span></span><br><span class="line">train_iter, val_iter = load_data_bananas(batch_size)</span><br><span class="line">features, labels = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_iter))</span><br><span class="line"><span class="built_in">print</span>(features.shape, labels.shape)  <span class="comment"># torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 5])</span></span><br></pre></td></tr></table></figure>
<p>接下来让我们展示10幅带有真实边界框的图像。我们可以看到在所有这些图像中香蕉的旋转角度、大小和位置都有所不同。当然，这只是一个简单的人工数据集，实践中真实世界的数据集通常要复杂得多：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># d2l.show_images()函数的实现</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_images</span>(<span class="params">imgs, num_rows, num_cols, titles=<span class="literal">None</span>, scale=<span class="number">1.5</span></span>):</span><br><span class="line">    figsize = (num_cols * scale, num_rows * scale)</span><br><span class="line">    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)</span><br><span class="line">    axes = axes.flatten()</span><br><span class="line">    <span class="keyword">for</span> i, (ax, img) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes, imgs)):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            img = img.numpy()</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        ax.imshow(img)</span><br><span class="line">        ax.axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        ax.axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">if</span> titles:</span><br><span class="line">            ax.set_title(titles[i])</span><br><span class="line">    <span class="keyword">return</span> axes</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bbox_to_rect</span>(<span class="params">bbox, color</span>):</span><br><span class="line">    <span class="keyword">return</span> plt.Rectangle(xy=(bbox[<span class="number">0</span>], bbox[<span class="number">1</span>]), width=bbox[<span class="number">2</span>]-bbox[<span class="number">0</span>], height=bbox[<span class="number">3</span>]-bbox[<span class="number">1</span>],</span><br><span class="line">                         fill=<span class="literal">False</span>, edgecolor=color, linewidth=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">imgs = (features[<span class="number">0</span>:<span class="number">10</span>].permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)) / <span class="number">255</span></span><br><span class="line">axes = show_images(imgs, <span class="number">2</span>, <span class="number">5</span>, scale=<span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> ax, label <span class="keyword">in</span> <span class="built_in">zip</span>(axes, labels[<span class="number">0</span>:<span class="number">10</span>]):</span><br><span class="line">    ax.add_patch(bbox_to_rect(label[<span class="number">0</span>][<span class="number">1</span>:<span class="number">5</span>] * edge_size, color=<span class="string">&#x27;white&#x27;</span>))</span><br><span class="line">    <span class="comment"># d2l.show_bboxes(ax, [label[0][1:5] * edge_size], colors=[&#x27;w&#x27;])  # 功能与上一行相同</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="5-锚框">5. 锚框</h2>
<p>由于本节难度较大，因此详细分析见：<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_computer-vision/anchor.html">D2L-计算机视觉-锚框</a>。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">torch.set_printoptions(<span class="number">2</span>)  <span class="comment"># 精简输出精度</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multibox_prior</span>(<span class="params">data, sizes, ratios</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成以每个像素为中心具有不同形状的锚框&quot;&quot;&quot;</span></span><br><span class="line">    in_height, in_width = data.shape[-<span class="number">2</span>:]</span><br><span class="line">    device, num_sizes, num_ratios = data.device, <span class="built_in">len</span>(sizes), <span class="built_in">len</span>(ratios)</span><br><span class="line">    boxes_per_pixel = (num_sizes + num_ratios - <span class="number">1</span>)</span><br><span class="line">    size_tensor = torch.tensor(sizes, device=device)</span><br><span class="line">    ratio_tensor = torch.tensor(ratios, device=device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 为了将锚点移动到像素的中心，需要设置偏移量。</span></span><br><span class="line">    <span class="comment"># 因为一个像素的高为1且宽为1，我们选择偏移我们的中心0.5</span></span><br><span class="line">    offset_h, offset_w = <span class="number">0.5</span>, <span class="number">0.5</span></span><br><span class="line">    steps_h = <span class="number">1.0</span> / in_height  <span class="comment"># 在y轴上缩放步长</span></span><br><span class="line">    steps_w = <span class="number">1.0</span> / in_width  <span class="comment"># 在x轴上缩放步长</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成锚框的所有中心点</span></span><br><span class="line">    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h</span><br><span class="line">    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w</span><br><span class="line">    shift_y, shift_x = torch.meshgrid(center_h, center_w, indexing=<span class="string">&#x27;ij&#x27;</span>)</span><br><span class="line">    shift_y, shift_x = shift_y.reshape(-<span class="number">1</span>), shift_x.reshape(-<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(center_h.shape)  <span class="comment"># torch.Size([561])</span></span><br><span class="line">    <span class="built_in">print</span>(shift_y.shape)  <span class="comment"># torch.Size([408408])</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成“boxes_per_pixel”个高和宽，之后用于创建锚框的四角坐标(xmin, xmax, ymin, ymax)</span></span><br><span class="line">    w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[<span class="number">0</span>]),</span><br><span class="line">                   sizes[<span class="number">0</span>] * torch.sqrt(ratio_tensor[<span class="number">1</span>:]))) * in_height / in_width  <span class="comment"># 处理矩形输入</span></span><br><span class="line">    h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[<span class="number">0</span>]),</span><br><span class="line">                   sizes[<span class="number">0</span>] / torch.sqrt(ratio_tensor[<span class="number">1</span>:])))</span><br><span class="line">    <span class="built_in">print</span>(w.shape)  <span class="comment"># torch.Size([5])</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 除以2来获得半高和半宽作为中心点到左上和右下的偏移量，repeat(a, b)表示在行上复制a倍，在列上复制b倍</span></span><br><span class="line">    anchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(in_height * in_width, <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">    <span class="built_in">print</span>(anchor_manipulations.shape)  <span class="comment"># torch.Size([2042040, 4])</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每个中心点都将有“boxes_per_pixel”个锚框，所以生成含所有锚框中心的网格，重复了“boxes_per_pixel”次</span></span><br><span class="line">    out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y], dim=<span class="number">1</span>).repeat_interleave(boxes_per_pixel, dim=<span class="number">0</span>)</span><br><span class="line">    <span class="built_in">print</span>(out_grid.shape)  <span class="comment"># torch.Size([2042040, 4])</span></span><br><span class="line">    output = out_grid + anchor_manipulations</span><br><span class="line">    <span class="keyword">return</span> output.unsqueeze(<span class="number">0</span>)  <span class="comment"># 增加batch维度</span></span><br><span class="line"></span><br><span class="line">img = plt.imread(<span class="string">&#x27;../images/catdog.jpg&#x27;</span>)</span><br><span class="line">h, w = img.shape[:<span class="number">2</span>]</span><br><span class="line"><span class="built_in">print</span>(h, w)  <span class="comment"># 561 728</span></span><br><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">3</span>, h, w))</span><br><span class="line">Y = multibox_prior(X, sizes=[<span class="number">0.75</span>, <span class="number">0.5</span>, <span class="number">0.25</span>], ratios=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0.5</span>])</span><br><span class="line"><span class="built_in">print</span>(Y.shape)  <span class="comment"># torch.Size([1, 2042040, 4])</span></span><br><span class="line"></span><br><span class="line">boxes = Y.reshape(h, w, <span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(boxes[<span class="number">250</span>, <span class="number">250</span>, <span class="number">0</span>, :])  <span class="comment"># tensor([0.06, 0.07, 0.63, 0.82])</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_bboxes</span>(<span class="params">axes, bboxes, labels=<span class="literal">None</span>, colors=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;显示所有边界框&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_list</span>(<span class="params">obj, default_values=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> obj <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            obj = default_values</span><br><span class="line">        <span class="keyword">elif</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(obj, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            obj = [obj]</span><br><span class="line">        <span class="keyword">return</span> obj</span><br><span class="line"></span><br><span class="line">    labels = _make_list(labels)</span><br><span class="line">    colors = _make_list(colors, [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;c&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> i, bbox <span class="keyword">in</span> <span class="built_in">enumerate</span>(bboxes):</span><br><span class="line">        color = colors[i % <span class="built_in">len</span>(colors)]</span><br><span class="line">        rect = d2l.bbox_to_rect(bbox.detach().numpy(), color)</span><br><span class="line">        axes.add_patch(rect)</span><br><span class="line">        <span class="keyword">if</span> labels <span class="keyword">and</span> <span class="built_in">len</span>(labels) &gt; i:</span><br><span class="line">            text_color = <span class="string">&#x27;k&#x27;</span> <span class="keyword">if</span> color == <span class="string">&#x27;w&#x27;</span> <span class="keyword">else</span> <span class="string">&#x27;w&#x27;</span></span><br><span class="line">            axes.text(rect.xy[<span class="number">0</span>], rect.xy[<span class="number">1</span>], labels[i], va=<span class="string">&#x27;center&#x27;</span>, ha=<span class="string">&#x27;center&#x27;</span>,</span><br><span class="line">                      fontsize=<span class="number">9</span>, color=text_color, bbox=<span class="built_in">dict</span>(facecolor=color, lw=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">plt.figure(dpi=<span class="number">100</span>)</span><br><span class="line">bbox_scale = torch.tensor((w, h, w, h))  <span class="comment"># 用于将坐标值从0~1复原为0~w(h)</span></span><br><span class="line"><span class="comment"># fig = plt.imshow(img)</span></span><br><span class="line"><span class="comment"># show_bboxes(fig.axes, boxes[250, 250, :, :] * bbox_scale,</span></span><br><span class="line"><span class="comment">#             [&#x27;s=0.75, r=1&#x27;, &#x27;s=0.5, r=1&#x27;, &#x27;s=0.25, r=1&#x27;, &#x27;s=0.75, r=2&#x27;, &#x27;s=0.75, r=0.5&#x27;])</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">box_iou</span>(<span class="params">boxes1, boxes2</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算两个锚框或边界框列表中成对的交并比&quot;&quot;&quot;</span></span><br><span class="line">    box_area = <span class="keyword">lambda</span> boxes: ((boxes[:, <span class="number">2</span>] - boxes[:, <span class="number">0</span>]) * (boxes[:, <span class="number">3</span>] - boxes[:, <span class="number">1</span>]))</span><br><span class="line">    <span class="comment"># boxes1.shape: (boxes1的数量, 4)</span></span><br><span class="line">    <span class="comment"># boxes2.shape: (boxes2的数量, 4)</span></span><br><span class="line">    <span class="comment"># areas1.shape: (boxes1的数量,)</span></span><br><span class="line">    <span class="comment"># areas2.shape: (boxes2的数量,)</span></span><br><span class="line">    areas1 = box_area(boxes1)</span><br><span class="line">    areas2 = box_area(boxes2)</span><br><span class="line">    <span class="comment"># inter_upperlefts.shape, inter_lowerrights.shape, inters.shape: (boxes1的数量, boxes2的数量, 2)</span></span><br><span class="line">    inter_upperlefts = torch.<span class="built_in">max</span>(boxes1[:, <span class="literal">None</span>, :<span class="number">2</span>], boxes2[:, :<span class="number">2</span>])</span><br><span class="line">    inter_lowerrights = torch.<span class="built_in">min</span>(boxes1[:, <span class="literal">None</span>, <span class="number">2</span>:], boxes2[:, <span class="number">2</span>:])</span><br><span class="line">    inters = (inter_lowerrights - inter_upperlefts).clamp(<span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># inter_areas.shape, union_areas.shape: (boxes1的数量, boxes2的数量)</span></span><br><span class="line">    inter_areas = inters[:, :, <span class="number">0</span>] * inters[:, :, <span class="number">1</span>]</span><br><span class="line">    union_areas = areas1[:, <span class="literal">None</span>] + areas2 - inter_areas</span><br><span class="line">    <span class="keyword">return</span> inter_areas / union_areas</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">assign_anchor_to_bbox</span>(<span class="params">ground_truth, anchors, device, iou_threshold=<span class="number">0.5</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将最接近的真实边界框分配给锚框&quot;&quot;&quot;</span></span><br><span class="line">    num_anchors, num_gt_boxes = anchors.shape[<span class="number">0</span>], ground_truth.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 位于第i行和第j列的元素x_ij是锚框i和真实边界框j的IoU</span></span><br><span class="line">    jaccard = box_iou(anchors, ground_truth)</span><br><span class="line">    <span class="comment"># 对于每个锚框，分配的真实边界框的张量</span></span><br><span class="line">    anchors_bbox_map = torch.full((num_anchors,), -<span class="number">1</span>, dtype=torch.long, device=device)</span><br><span class="line">    <span class="comment"># 根据阈值，决定是否分配真实边界框</span></span><br><span class="line">    max_ious, indices = torch.<span class="built_in">max</span>(jaccard, dim=<span class="number">1</span>)</span><br><span class="line">    anc_i = torch.nonzero(max_ious &gt;= iou_threshold).reshape(-<span class="number">1</span>)</span><br><span class="line">    box_j = indices[max_ious &gt;= iou_threshold]</span><br><span class="line">    anchors_bbox_map[anc_i] = box_j</span><br><span class="line">    col_discard = torch.full((num_anchors,), -<span class="number">1</span>)</span><br><span class="line">    row_discard = torch.full((num_gt_boxes,), -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_gt_boxes):</span><br><span class="line">        max_idx = torch.argmax(jaccard)</span><br><span class="line">        box_idx = (max_idx % num_gt_boxes).long()</span><br><span class="line">        anc_idx = (max_idx / num_gt_boxes).long()</span><br><span class="line">        anchors_bbox_map[anc_idx] = box_idx</span><br><span class="line">        jaccard[:, box_idx] = col_discard</span><br><span class="line">        jaccard[anc_idx, :] = row_discard</span><br><span class="line">    <span class="keyword">return</span> anchors_bbox_map</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">offset_boxes</span>(<span class="params">anchors, assigned_bb, eps=<span class="number">1e-6</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;对锚框偏移量的转换&quot;&quot;&quot;</span></span><br><span class="line">    c_anc = d2l.box_corner_to_center(anchors)</span><br><span class="line">    c_assigned_bb = d2l.box_corner_to_center(assigned_bb)</span><br><span class="line">    offset_xy = <span class="number">10</span> * (c_assigned_bb[:, :<span class="number">2</span>] - c_anc[:, :<span class="number">2</span>]) / c_anc[:, <span class="number">2</span>:]</span><br><span class="line">    offset_wh = <span class="number">5</span> * torch.log(eps + c_assigned_bb[:, <span class="number">2</span>:] / c_anc[:, <span class="number">2</span>:])</span><br><span class="line">    offset = torch.cat([offset_xy, offset_wh], dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> offset</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multibox_target</span>(<span class="params">anchors, labels</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用真实边界框标记锚框&quot;&quot;&quot;</span></span><br><span class="line">    batch_size, anchors = labels.shape[<span class="number">0</span>], anchors.squeeze(<span class="number">0</span>)</span><br><span class="line">    batch_offset, batch_mask, batch_class_labels = [], [], []</span><br><span class="line">    device, num_anchors = anchors.device, anchors.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">        label = labels[i, :, :]</span><br><span class="line">        anchors_bbox_map = assign_anchor_to_bbox(label[:, <span class="number">1</span>:], anchors, device)</span><br><span class="line">        bbox_mask = ((anchors_bbox_map &gt;= <span class="number">0</span>).<span class="built_in">float</span>().unsqueeze(-<span class="number">1</span>)).repeat(<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="comment"># 将类标签和分配的边界框坐标初始化为零</span></span><br><span class="line">        class_labels = torch.zeros(num_anchors, dtype=torch.long, device=device)</span><br><span class="line">        assigned_bb = torch.zeros((num_anchors, <span class="number">4</span>), dtype=torch.float32, device=device)</span><br><span class="line">        <span class="comment"># 使用真实边界框来标记锚框的类别</span></span><br><span class="line">        <span class="comment"># 如果一个锚框没有被分配，标记其为背景（值为零）</span></span><br><span class="line">        indices_true = torch.nonzero(anchors_bbox_map &gt;= <span class="number">0</span>)</span><br><span class="line">        bb_idx = anchors_bbox_map[indices_true]</span><br><span class="line">        class_labels[indices_true] = label[bb_idx, <span class="number">0</span>].long() + <span class="number">1</span></span><br><span class="line">        assigned_bb[indices_true] = label[bb_idx, <span class="number">1</span>:]</span><br><span class="line">        <span class="comment"># 偏移量转换</span></span><br><span class="line">        offset = offset_boxes(anchors, assigned_bb) * bbox_mask</span><br><span class="line">        batch_offset.append(offset.reshape(-<span class="number">1</span>))</span><br><span class="line">        batch_mask.append(bbox_mask.reshape(-<span class="number">1</span>))</span><br><span class="line">        batch_class_labels.append(class_labels)</span><br><span class="line">    bbox_offset = torch.stack(batch_offset)</span><br><span class="line">    bbox_mask = torch.stack(batch_mask)</span><br><span class="line">    class_labels = torch.stack(batch_class_labels)</span><br><span class="line">    <span class="keyword">return</span> (bbox_offset, bbox_mask, class_labels)</span><br><span class="line"></span><br><span class="line">ground_truth = torch.tensor([[<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.08</span>, <span class="number">0.52</span>, <span class="number">0.92</span>],</span><br><span class="line">                             [<span class="number">1</span>, <span class="number">0.55</span>, <span class="number">0.2</span>, <span class="number">0.9</span>, <span class="number">0.88</span>]])</span><br><span class="line">anchors = torch.tensor([[<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>], [<span class="number">0.15</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.4</span>],</span><br><span class="line">                        [<span class="number">0.63</span>, <span class="number">0.05</span>, <span class="number">0.88</span>, <span class="number">0.98</span>], [<span class="number">0.66</span>, <span class="number">0.45</span>, <span class="number">0.8</span>, <span class="number">0.8</span>],</span><br><span class="line">                        [<span class="number">0.57</span>, <span class="number">0.3</span>, <span class="number">0.92</span>, <span class="number">0.9</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># fig = plt.imshow(img)</span></span><br><span class="line"><span class="comment"># show_bboxes(fig.axes, ground_truth[:, 1:] * bbox_scale, [&#x27;dog&#x27;, &#x27;cat&#x27;], &#x27;k&#x27;)</span></span><br><span class="line"><span class="comment"># show_bboxes(fig.axes, anchors * bbox_scale, [&#x27;0&#x27;, &#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;, &#x27;4&#x27;])</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回的结果中有三个元素，都是张量格式。第一个元素包含了为每个锚框标记的四个偏移值。注意负类锚框的偏移量被标记为零</span></span><br><span class="line"><span class="comment"># 第二个元素是掩码（mask）变量，形状为（批量大小，锚框数的四倍）</span></span><br><span class="line"><span class="comment"># 第三个元素包含标记的输入锚框的类别</span></span><br><span class="line">labels = multibox_target(anchors.unsqueeze(<span class="number">0</span>), ground_truth.unsqueeze(<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(labels[<span class="number">2</span>])  <span class="comment"># tensor([[0, 1, 2, 0, 2]])</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">offset_inverse</span>(<span class="params">anchors, offset_preds</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;根据带有预测偏移量的锚框来预测边界框&quot;&quot;&quot;</span></span><br><span class="line">    anc = d2l.box_corner_to_center(anchors)</span><br><span class="line">    pred_bbox_xy = (offset_preds[:, :<span class="number">2</span>] * anc[:, <span class="number">2</span>:] / <span class="number">10</span>) + anc[:, :<span class="number">2</span>]</span><br><span class="line">    pred_bbox_wh = torch.exp(offset_preds[:, <span class="number">2</span>:] / <span class="number">5</span>) * anc[:, <span class="number">2</span>:]</span><br><span class="line">    pred_bbox = torch.cat((pred_bbox_xy, pred_bbox_wh), dim=<span class="number">1</span>)</span><br><span class="line">    predicted_bbox = d2l.box_center_to_corner(pred_bbox)</span><br><span class="line">    <span class="keyword">return</span> predicted_bbox</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nms</span>(<span class="params">boxes, scores, iou_threshold</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;对预测边界框的置信度进行排序&quot;&quot;&quot;</span></span><br><span class="line">    B = torch.argsort(scores, dim=-<span class="number">1</span>, descending=<span class="literal">True</span>)</span><br><span class="line">    keep = []  <span class="comment"># 保留预测边界框的指标</span></span><br><span class="line">    <span class="keyword">while</span> B.numel() &gt; <span class="number">0</span>:</span><br><span class="line">        i = B[<span class="number">0</span>]</span><br><span class="line">        keep.append(i)</span><br><span class="line">        <span class="keyword">if</span> B.numel() == <span class="number">1</span>: <span class="keyword">break</span></span><br><span class="line">        iou = box_iou(boxes[i, :].reshape(-<span class="number">1</span>, <span class="number">4</span>), boxes[B[<span class="number">1</span>:], :].reshape(-<span class="number">1</span>, <span class="number">4</span>)).reshape(-<span class="number">1</span>)</span><br><span class="line">        inds = torch.nonzero(torch.as_tensor(iou &lt;= iou_threshold, dtype=torch.float32)).reshape(-<span class="number">1</span>)</span><br><span class="line">        B = B[inds + <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(keep, device=boxes.device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multibox_detection</span>(<span class="params">cls_probs, offset_preds, anchors, nms_threshold=<span class="number">0.5</span>, pos_threshold=<span class="number">0.009999999</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用非极大值抑制来预测边界框&quot;&quot;&quot;</span></span><br><span class="line">    device, batch_size = cls_probs.device, cls_probs.shape[<span class="number">0</span>]</span><br><span class="line">    anchors = anchors.squeeze(<span class="number">0</span>)</span><br><span class="line">    num_classes, num_anchors = cls_probs.shape[<span class="number">1</span>], cls_probs.shape[<span class="number">2</span>]</span><br><span class="line">    out = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        conf, class_id = torch.<span class="built_in">max</span>(cls_prob[<span class="number">1</span>:], <span class="number">0</span>)</span><br><span class="line">        predicted_bb = offset_inverse(anchors, offset_pred)</span><br><span class="line">        keep = nms(predicted_bb, conf, nms_threshold)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 找到所有的non_keep索引，并将类设置为背景</span></span><br><span class="line">        all_idx = torch.arange(num_anchors, dtype=torch.long, device=device)</span><br><span class="line">        combined = torch.cat((keep, all_idx))</span><br><span class="line">        uniques, counts = combined.unique(return_counts=<span class="literal">True</span>)</span><br><span class="line">        non_keep = uniques[counts == <span class="number">1</span>]</span><br><span class="line">        all_id_sorted = torch.cat((keep, non_keep))</span><br><span class="line">        class_id[non_keep] = -<span class="number">1</span></span><br><span class="line">        class_id = class_id[all_id_sorted]</span><br><span class="line">        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]</span><br><span class="line">        <span class="comment"># pos_threshold是一个用于非背景预测的阈值</span></span><br><span class="line">        below_min_idx = (conf &lt; pos_threshold)</span><br><span class="line">        class_id[below_min_idx] = -<span class="number">1</span></span><br><span class="line">        conf[below_min_idx] = <span class="number">1</span> - conf[below_min_idx]</span><br><span class="line">        pred_info = torch.cat((class_id.unsqueeze(<span class="number">1</span>), conf.unsqueeze(<span class="number">1</span>), predicted_bb), dim=<span class="number">1</span>)</span><br><span class="line">        out.append(pred_info)</span><br><span class="line">    <span class="keyword">return</span> torch.stack(out)</span><br><span class="line"></span><br><span class="line">anchors = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.08</span>, <span class="number">0.52</span>, <span class="number">0.92</span>], [<span class="number">0.08</span>, <span class="number">0.2</span>, <span class="number">0.56</span>, <span class="number">0.95</span>],</span><br><span class="line">                        [<span class="number">0.15</span>, <span class="number">0.3</span>, <span class="number">0.62</span>, <span class="number">0.91</span>], [<span class="number">0.55</span>, <span class="number">0.2</span>, <span class="number">0.9</span>, <span class="number">0.88</span>]])</span><br><span class="line">offset_preds = torch.tensor([<span class="number">0</span>] * anchors.numel())</span><br><span class="line">cls_probs = torch.tensor([[<span class="number">0</span>] * <span class="number">4</span>,  <span class="comment"># 背景的预测概率</span></span><br><span class="line">                          [<span class="number">0.9</span>, <span class="number">0.8</span>, <span class="number">0.7</span>, <span class="number">0.1</span>],  <span class="comment"># 狗的预测概率</span></span><br><span class="line">                          [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.9</span>]])  <span class="comment"># 猫的预测概率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># fig = plt.imshow(img)</span></span><br><span class="line"><span class="comment"># show_bboxes(fig.axes, anchors * bbox_scale, [&#x27;dog=0.9&#x27;, &#x27;dog=0.8&#x27;, &#x27;dog=0.7&#x27;, &#x27;cat=0.9&#x27;])</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回结果的形状是（批量大小，锚框的数量，6）</span></span><br><span class="line"><span class="comment"># 第一个元素是预测的类索引，从0开始（0代表狗，1代表猫），值-1表示背景或在非极大值抑制中被移除了</span></span><br><span class="line"><span class="comment"># 第二个元素是预测的边界框的置信度</span></span><br><span class="line"><span class="comment"># 其余四个元素分别是预测边界框左上角和右下角的坐标（范围介于0~1之间）</span></span><br><span class="line">output = multibox_detection(cls_probs.unsqueeze(<span class="number">0</span>),</span><br><span class="line">                            offset_preds.unsqueeze(<span class="number">0</span>),</span><br><span class="line">                            anchors.unsqueeze(<span class="number">0</span>),</span><br><span class="line">                            nms_threshold=<span class="number">0.5</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="comment"># tensor([[[ 0.00,  0.90,  0.10,  0.08,  0.52,  0.92],</span></span><br><span class="line"><span class="comment">#          [ 1.00,  0.90,  0.55,  0.20,  0.90,  0.88],</span></span><br><span class="line"><span class="comment">#          [-1.00,  0.80,  0.08,  0.20,  0.56,  0.95],</span></span><br><span class="line"><span class="comment">#          [-1.00,  0.70,  0.15,  0.30,  0.62,  0.91]]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除-1类别（背景）的预测边界框后，我们可以输出由非极大值抑制保存的最终预测边界框</span></span><br><span class="line">fig = plt.imshow(img)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> output[<span class="number">0</span>].detach().numpy():</span><br><span class="line">    <span class="keyword">if</span> i[<span class="number">0</span>] == -<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    label = (<span class="string">&#x27;dog=&#x27;</span>, <span class="string">&#x27;cat=&#x27;</span>)[<span class="built_in">int</span>(i[<span class="number">0</span>])] + <span class="built_in">str</span>(i[<span class="number">1</span>])</span><br><span class="line">    show_bboxes(fig.axes, [torch.tensor(i[<span class="number">2</span>:]) * bbox_scale], label)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="6-多尺度目标检测">6. 多尺度目标检测</h2>
<p>在上一节中，我们以输入图像的每个像素为中心，生成了多个锚框。基本而言，这些锚框代表了图像不同区域的样本。然而，如果为每个像素都生成的锚框，我们最终可能会得到太多需要计算的锚框。想象一个561*728的输入图像，如果以每个像素为中心生成五个形状不同的锚框，就需要在图像上标记和预测超过200万个锚框（561*728*5）。</p>
<h3 id="6-1-多尺度锚框">6.1 多尺度锚框</h3>
<p>减少图像上的锚框数量并不困难。比如，我们可以在输入图像中均匀采样一小部分像素，并以它们为中心生成锚框。此外，在不同尺度下，我们可以生成不同数量和不同大小的锚框。直观地说，比起较大的目标，较小的目标在图像上出现的可能性更多样。例如，1*1、1*2和2*2的目标可以分别以4、2和1种可能的方式出现在2*2的图像上。因此，当使用较小的锚框检测较小的物体时，我们可以采样更多的区域，而对于较大的物体，我们可以采样较少的区域。</p>
<p>为了演示如何在多个尺度下生成锚框，让我们先读取一张图像。它的高度和宽度分别为561和728像素：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">img = plt.imread(<span class="string">&#x27;../images/catdog.jpg&#x27;</span>)</span><br><span class="line">h, w = img.shape[:<span class="number">2</span>]</span><br><span class="line"><span class="built_in">print</span>(h, w)  <span class="comment"># 561 728</span></span><br></pre></td></tr></table></figure>
<p><code>display_anchors</code> 函数定义如下。我们在<strong>特征图</strong>（fmap）上生成锚框（anchors），每个单位（像素）作为锚框的中心。由于锚框中的 <code>(x, y)</code> 轴坐标值（anchors）已经被除以特征图（fmap）的宽度和高度，因此这些值介于0和1之间，表示特征图中锚框的相对位置。</p>
<p>由于锚框（anchors）的中心分布于特征图（fmap）上的所有单位，因此这些中心必须根据其相对空间位置在任何输入图像上均匀分布。更具体地说，给定特征图的宽度和高度 <code>fmap_w</code> 和 <code>fmap_h</code>，以下函数将均匀地对任何输入图像中 <code>fmap_h</code> 行和 <code>fmap_w</code> 列中的像素进行采样。以这些均匀采样的像素为中心，将会生成大小为 <code>s</code>（假设列表 <code>s</code> 的长度为1）且宽高比（ratios）不同的锚框：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">display_anchors</span>(<span class="params">fmap_w, fmap_h, s</span>):</span><br><span class="line">    plt.figure(dpi=<span class="number">100</span>)</span><br><span class="line">    <span class="comment"># 前两个维度上的值不影响输出</span></span><br><span class="line">    fmap = torch.zeros((<span class="number">1</span>, <span class="number">10</span>, fmap_h, fmap_w))</span><br><span class="line">    anchors = d2l.multibox_prior(fmap, sizes=s, ratios=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0.5</span>])</span><br><span class="line">    bbox_scale = torch.tensor((w, h, w, h))</span><br><span class="line">    d2l.show_bboxes(plt.imshow(img).axes, anchors[<span class="number">0</span>] * bbox_scale)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>首先，让我们考虑探测小目标。为了在显示时更容易分辨，在这里具有不同中心的锚框不会重叠：锚框的尺度设置为0.15，特征图的高度和宽度设置为4。我们可以看到，图像上4行和4列的锚框的中心是均匀分布的：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display_anchors(fmap_w=<span class="number">4</span>, fmap_h=<span class="number">4</span>, s=[<span class="number">0.15</span>])</span><br></pre></td></tr></table></figure>
<p>然后，我们将特征图的高度和宽度减小一半，然后使用较大的锚框来检测较大的目标。当尺度设置为0.4时，一些锚框将彼此重叠：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display_anchors(fmap_w=<span class="number">2</span>, fmap_h=<span class="number">2</span>, s=[<span class="number">0.4</span>])</span><br></pre></td></tr></table></figure>
<p>最后，我们进一步将特征图的高度和宽度减小一半，然后将锚框的尺度增加到0.8。此时，锚框的中心即是图像的中心：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display_anchors(fmap_w=<span class="number">1</span>, fmap_h=<span class="number">1</span>, s=[<span class="number">0.8</span>])</span><br></pre></td></tr></table></figure>
<h3 id="6-2-多尺度检测">6.2 多尺度检测</h3>
<p>既然我们已经生成了多尺度的锚框，我们就将使用它们来检测不同尺度下各种大小的目标。下面，我们介绍一种基于 CNN 的多尺度目标检测方法，将在第8节（SSD）中实现。</p>
<p>在某种规模上，假设我们有 <code>c</code> 张形状为 <code>h * w</code> 的特征图。使用上一小节中的方法，我们生成了 <code>hw</code> 组锚框，其中每组都有 <code>a</code> 个中心相同的锚框。例如，在上一小节实验的第一个尺度上，给定10个（通道数量）<code>4 * 4</code> 的特征图，我们生成了16组锚框，每组包含3个中心相同的锚框。接下来，每个锚框都根据真实值边界框来标记了类和偏移量。在当前尺度下，目标检测模型需要预测输入图像上 <code>hw</code> 组锚框类别和偏移量，其中不同组锚框具有不同的中心。</p>
<p>假设此处的 <code>c</code> 张特征图是 CNN 基于输入图像的正向传播算法获得的中间输出。既然每张特征图上都有 <code>hw</code> 个不同的空间位置，那么相同空间位置可以看作含有 <code>c</code> 个单元。根据感受野的定义，特征图在相同空间位置的 <code>c</code> 个单元在输入图像上的感受野相同：它们表征了同一感受野内的输入图像信息。因此，我们可以将特征图在同一空间位置的 <code>c</code> 个单元变换为使用此空间位置生成的 <code>a</code> 个锚框类别和偏移量。本质上，我们用输入图像在某个感受野区域内的信息，来预测输入图像上与该区域位置相近的锚框类别和偏移量。</p>
<p>当不同层的特征图在输入图像上分别拥有不同大小的感受野时，它们可以用于检测不同大小的目标。例如，我们可以设计一个神经网络，其中靠近输出层的特征图单元具有更宽的感受野，这样它们就可以从输入图像中检测到较大的目标。</p>
<p>简言之，我们可以利用深层神经网络在多个层次上对图像进行分层表示，从而实现多尺度目标检测。在第8节我们将通过一个具体的例子来说明它是如何工作的。</p>
<h2 id="7-区域卷积神经网络（R-CNN）系列">7. 区域卷积神经网络（R-CNN）系列</h2>
<h3 id="7-1-R-CNN">7.1 R-CNN</h3>
<p>R-CNN 首先从输入图像中选取若干（例如2000个）提议区域（如锚框也是一种选取方法），并标注它们的类别和边界框（如偏移量）。然后，用卷积神经网络对每个提议区域进行前向传播以抽取其特征。接下来，我们用每个提议区域的特征来预测类别和边界框。具体来说，R-CNN 包括以下四个步骤：</p>
<ol>
<li>对输入图像使用选择性搜索来选取多个高质量的提议区域。这些提议区域通常是在多个尺度下选取的，并具有不同的形状和大小。每个提议区域都将被标注类别和真实边界框；</li>
<li>选择一个预训练的卷积神经网络，并将其在输出层之前截断。将每个提议区域变形为网络需要的输入尺寸，并通过前向传播输出抽取的提议区域特征；</li>
<li>将每个提议区域的特征连同其标注的类别作为一个样本。训练多个支持向量机对目标分类，其中每个支持向量机用来判断样本是否属于某一个类别；</li>
<li>将每个提议区域的特征连同其标注的边界框作为一个样本，训练线性回归模型来预测真实边界框。</li>
</ol>
<p>尽管 R-CNN 模型通过预训练的卷积神经网络有效地抽取了图像特征，但它的速度很慢。想象一下，我们可能从一张图像中选出上千个提议区域，这需要上千次的卷积神经网络的前向传播来执行目标检测。这种庞大的计算量使得 R-CNN 在现实世界中难以被广泛应用。</p>
<h3 id="7-2-Fast-R-CNN">7.2 Fast R-CNN</h3>
<p>R-CNN 的主要性能瓶颈在于，对每个提议区域，卷积神经网络的前向传播是独立的，而没有共享计算。由于这些区域通常有重叠，独立的特征抽取会导致重复的计算。Fast R-CNN 对 R-CNN 的主要改进之一，是仅在<strong>整张图像</strong>上执行卷积神经网络的前向传播。Fast R-CNN 的主要计算如下：</p>
<ol>
<li>与 R-CNN 相比，Fast R-CNN 用来提取特征的入卷积神经网络的输入是整个图像，而不是各个提议区域。此外，这个网络通常会参与训练。设输入为一张图像，将卷积神经网络的输出的形状记为 <code>1 * c * h1 * w1</code>；</li>
<li>假设选择性搜索生成了 <code>n</code> 个提议区域。这些形状各异的提议区域在卷积神经网络的输出上分别标出了形状各异的兴趣区域。然后，这些感兴趣的区域需要进一步抽取出<strong>形状相同</strong>的特征（比如指定高度 <code>h2</code> 和宽度 <code>w2</code>），以便于连结后输出。为了实现这一目标，Fast R-CNN 引入了<strong>兴趣区域汇聚层</strong>（RoI pooling）：将卷积神经网络的输出和提议区域作为输入，输出连结后的各个提议区域抽取的特征，形状为 <code>n * c * h2 * w2</code>；</li>
<li>通过全连接层将输出形状变换为 <code>n * d</code>，其中超参数 <code>d</code> 取决于模型设计；</li>
<li>预测 <code>n</code> 个提议区域中每个区域的类别和边界框。更具体地说，在预测类别和边界框时，将全连接层的输出分别转换为形状为 <code>n * q</code>（<code>q</code> 是类别的数量）的输出和形状为 <code>n * 4</code> 的输出。其中预测类别时使用 Softmax 回归。</li>
</ol>
<p>下面，我们演示了兴趣区域汇聚层的计算方法。假设卷积神经网络抽取的特征 <code>X</code> 的高度和宽度都是4，且只有单通道：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line">X = torch.arange(<span class="number">16.</span>).reshape(<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"><span class="comment"># tensor([[[[ 0.,  1.,  2.,  3.],</span></span><br><span class="line"><span class="comment">#           [ 4.,  5.,  6.,  7.],</span></span><br><span class="line"><span class="comment">#           [ 8.,  9., 10., 11.],</span></span><br><span class="line"><span class="comment">#           [12., 13., 14., 15.]]]])</span></span><br></pre></td></tr></table></figure>
<p>让我们进一步假设输入图像的高度和宽度都是40像素，且选择性搜索在此图像上生成了两个提议区域。每个区域由5个元素表示：区域目标类别、左上角和右下角的 <code>(x, y)</code> 坐标：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rois = torch.Tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">20</span>, <span class="number">20</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">10</span>, <span class="number">30</span>, <span class="number">30</span>]])</span><br></pre></td></tr></table></figure>
<p>由于 <code>X</code> 的高和宽是输入图像高和宽的1/10，因此，两个提议区域的坐标先按 <code>spatial_scale</code> 乘以0.1。然后，在 <code>X</code> 上分别标出这两个兴趣区域 <code>X[:, :, 0:3, 0:3]</code> 和 <code>X[:, :, 1:4, 0:4]</code>。最后，在 <code>2 * 2</code> 的兴趣区域汇聚层中，每个兴趣区域被划分为子窗口网格，并进一步抽取相同形状 <code>2 * 2</code> 的特征：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torchvision.ops.roi_pool(X, rois, output_size=(<span class="number">2</span>, <span class="number">2</span>), spatial_scale=<span class="number">0.1</span>))</span><br><span class="line"><span class="comment"># tensor([[[[ 5.,  6.],</span></span><br><span class="line"><span class="comment">#           [ 9., 10.]]],</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#         [[[ 9., 11.],</span></span><br><span class="line"><span class="comment">#           [13., 15.]]]])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="7-3-Faster-R-CNN">7.3 Faster R-CNN</h3>
<p>为了较精确地检测目标结果，Fast R-CNN 模型通常需要在选择性搜索中生成大量的提议区域。Faster R-CNN 提出将选择性搜索替换为<strong>区域提议网络</strong>（region proposal network），从而减少提议区域的生成数量，并保证目标检测的精度。具体来说，区域提议网络的计算步骤如下：</p>
<ol>
<li>使用填充为1的 <code>3 * 3</code> 的卷积层变换卷积神经网络的输出，并将输出通道数记为 <code>c</code>。这样，卷积神经网络为图像抽取的特征图中的每个单元均得到一个长度为 <code>c</code> 的新特征；</li>
<li>以特征图的每个像素为中心，生成多个不同大小和宽高比的锚框并标注它们；</li>
<li>使用锚框中心单元长度为 <code>c</code> 的特征，分别预测该锚框的二元类别（含目标还是背景）和边界框；</li>
<li>使用非极大值抑制，从预测类别为目标的预测边界框中移除相似的结果。最终输出的预测边界框即是兴趣区域汇聚层所需的提议区域。</li>
</ol>
<p>值得一提的是，区域提议网络作为 Faster R-CNN 模型的一部分，是和整个模型一起训练得到的。换句话说，Faster R-CNN 的目标函数不仅包括目标检测中的类别和边界框预测，还包括区域提议网络中锚框的二元类别和边界框预测。作为端到端训练的结果，区域提议网络能够学习到如何生成高质量的提议区域，从而在减少了从数据中学习的提议区域的数量的情况下，仍保持目标检测的精度。</p>
<h3 id="7-4-Mask-R-CNN">7.4 Mask R-CNN</h3>
<p>如果在训练集中还标注了每个目标在图像上的<strong>像素级位置</strong>，那么 Mask R-CNN 能够有效地利用这些详尽的标注信息进一步提升目标检测的精度。</p>
<p>Mask R-CNN 是基于 Faster R-CNN 修改而来的。具体来说，Mask R-CNN 将兴趣区域汇聚层替换为了兴趣区域对齐层（RoI Align），使用双线性插值（bilinear interpolation）来保留特征图上的空间信息，从而更适于像素级预测。兴趣区域对齐层的输出包含了所有与兴趣区域的形状相同的特征图。它们不仅被用于预测每个兴趣区域的类别和边界框，还通过额外的全卷积网络预测目标的像素级位置。本章的后续章节将更详细地介绍如何使用全卷积网络预测图像中像素级的语义。</p>
<h2 id="8-单发多框检测（SSD）">8. 单发多框检测（SSD）</h2>
<p>SSD 模型主要由基础网络组成，其后是几个多尺度特征块。基本网络用于从输入图像中提取特征，因此它可以使用深度卷积神经网络。单发多框检测论文中选用了在分类层之前截断的 VGG，现在也常用 ResNet 替代。我们可以设计基础网络，使它输出的高和宽较大。这样一来，基于该特征图生成的锚框数量较多，可以用来检测尺寸较小的目标。接下来的每个多尺度特征块将上一层提供的特征图的高和宽缩小（如减半），并使特征图中每个单元在输入图像上的感受野变得更广阔。</p>
<p>回想一下在第6节中，通过深度神经网络分层表示图像的多尺度目标检测的设计。由于接近顶部的多尺度特征图较小，但具有较大的感受野，它们适合检测较少但较大的物体。简而言之，通过多尺度特征块，单发多框检测生成不同大小的锚框，并通过预测边界框的类别和偏移量来检测大小不同的目标，因此这是一个多尺度目标检测模型。</p>
<h3 id="8-1-类别预测层与边界框预测层">8.1 类别预测层与边界框预测层</h3>
<p>设目标类别的数量为 <code>q</code>。这样一来，锚框有 <code>q + 1</code> 个类别，其中第0类是背景。在某个尺度下，设特征图的高和宽分别为 <code>h</code> 和 <code>w</code>。如果以其中每个单元为中心生成 <code>a</code> 个锚框，那么我们需要对 <code>hwa</code> 个锚框进行分类。如果使用全连接层作为输出，很容易导致模型参数过多。回忆 NiN 一节介绍的使用卷积层的通道来输出类别预测的方法，单发多框检测采用同样的方法来降低模型复杂度。</p>
<p>具体来说，类别预测层使用一个保持输入高和宽的卷积层。这样一来，输出和输入在特征图宽和高上的空间坐标一一对应。考虑输出和输入同一空间坐标 <code>(x, y)</code>：输出特征图上 <code>(x, y)</code> 坐标的通道里包含了以输入特征图 <code>(x, y)</code> 坐标为中心生成的所有锚框的类别预测。因此输出通道数为 <code>a * (q + 1)</code>。</p>
<p>类别预测层的定义如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="comment"># num_inputs为输入通道数，(num_classes + 1)表示还有一个背景类，因为需要预测每个锚框是哪个类因此输出通道要乘以num_anchors</span></span><br><span class="line"><span class="comment"># 即对于输入的每一个像素，它的输出通道数就是以该像素为中心的num_anchors个锚框的预测值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cls_predictor</span>(<span class="params">num_inputs, num_anchors, num_classes</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Conv2d(num_inputs, num_anchors * (num_classes + <span class="number">1</span>), kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>边界框预测层的设计与类别预测层的设计类似。唯一不同的是，这里需要为每个锚框预测4个偏移量，而不是 <code>q + 1</code> 个类别：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测锚框和真实边界框的offset，对每一个锚框有4个预测值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bbox_predictor</span>(<span class="params">num_inputs, num_anchors</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Conv2d(num_inputs, num_anchors * <span class="number">4</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="8-2-连结多尺度的预测">8.2 连结多尺度的预测</h3>
<p>单发多框检测使用多尺度特征图来生成锚框并预测其类别和偏移量。在不同的尺度下，特征图的形状或以同一单元为中心的锚框的数量可能会有所不同。因此，不同尺度下预测输出的形状可能会有所不同。</p>
<p>在以下示例中，我们为同一个小批量构建两个不同比例（<code>Y1</code> 和 <code>Y2</code>）的特征图，其中 <code>Y2</code> 的高度和宽度是 <code>Y1</code> 的一半。以类别预测为例，假设 <code>Y1</code> 和 <code>Y2</code> 的每个单元分别生成了5个和3个锚框。进一步假设目标类别的数量为10，对于特征图 <code>Y1</code> 和 <code>Y2</code>，类别预测输出中的通道数分别为 <code>5 * (10 + 1) = 55</code> 和 <code>3 * (10 + 1) = 33</code>，其中任一输出的形状是 <code>(批量大小, 通道数, 高度, 宽度)</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x, block</span>):</span><br><span class="line">    <span class="keyword">return</span> block(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Y1为对输入的400(20*20)个像素都会做55(5*(10+1))个预测</span></span><br><span class="line"><span class="comment"># 因此在不同尺度下的预测除了batch维之外另外三个维度都会发生变化</span></span><br><span class="line">Y1 = forward(torch.zeros((<span class="number">2</span>, <span class="number">8</span>, <span class="number">20</span>, <span class="number">20</span>)), cls_predictor(<span class="number">8</span>, <span class="number">5</span>, <span class="number">10</span>))</span><br><span class="line">Y2 = forward(torch.zeros((<span class="number">2</span>, <span class="number">16</span>, <span class="number">10</span>, <span class="number">10</span>)), cls_predictor(<span class="number">16</span>, <span class="number">3</span>, <span class="number">10</span>))</span><br><span class="line"><span class="built_in">print</span>(Y1.shape, Y2.shape)  <span class="comment"># torch.Size([2, 55, 20, 20]) torch.Size([2, 33, 10, 10])</span></span><br></pre></td></tr></table></figure>
<p>除了批量大小这一维度外，其他三个维度都具有不同的尺寸。为了将这两个预测输出链接起来以提高计算效率，我们将把这些张量转换为更一致的格式。</p>
<p>通道维包含中心相同的锚框的预测结果。我们首先将通道维移到最后一维。因为不同尺度下批量大小仍保持不变，我们可以将预测结果转成二维的 <code>(批量大小, 高 * 宽 * 通道数)</code> 的格式，以方便之后在维度1上的连结。这样一来，尽管 <code>Y1</code> 和 <code>Y2</code> 在通道数、高度和宽度方面具有不同的大小，我们仍然可以在同一个小批量的两个不同尺度上连接这两个预测输出：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># start_dim=1表示将后面三个维度展平成一维</span></span><br><span class="line"><span class="comment"># 把通道放最后表示对于每个像素的预测是连续值，否则展平后每个像素的预测就不是连续的</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">flatten_pred</span>(<span class="params">pred</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.flatten(pred.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>), start_dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">concat_preds</span>(<span class="params">preds</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.cat([flatten_pred(p) <span class="keyword">for</span> p <span class="keyword">in</span> preds], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(concat_preds([Y1, Y2]).shape)  <span class="comment"># torch.Size([2, 25300])</span></span><br></pre></td></tr></table></figure>
<h3 id="8-3-高和宽减半块">8.3 高和宽减半块</h3>
<p>高和宽减半块将输入特征图的高度和宽度减半，会扩大每个单元在其输出特征图中的感受野，该模块此前已在 VGG 中使用过：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 高宽减半块，该模块将输入特征图的高度和宽度减半</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">down_sample_blk</span>(<span class="params">in_channels, out_channels</span>):</span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">        blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        blk.append(nn.BatchNorm2d(out_channels))</span><br><span class="line">        blk.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    blk.append(nn.MaxPool2d(<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(forward(torch.zeros((<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>, <span class="number">20</span>)), down_sample_blk(<span class="number">3</span>, <span class="number">10</span>)).shape)  <span class="comment"># torch.Size([2, 10, 10, 10])</span></span><br></pre></td></tr></table></figure>
<h3 id="8-4-基本网络块">8.4 基本网络块</h3>
<p>基本网络块用于从输入图像中抽取特征。为了计算简洁，我们构造了一个小的基础网络，该网络串联3个高和宽减半块，并逐步将通道数翻倍：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">base_net</span>():</span><br><span class="line">    blk = []</span><br><span class="line">    num_filters = [<span class="number">3</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(num_filters) - <span class="number">1</span>):</span><br><span class="line">        blk.append(down_sample_blk(num_filters[i], num_filters[i + <span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(forward(torch.zeros((<span class="number">2</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>)), base_net()).shape)  <span class="comment"># torch.Size([2, 64, 32, 32])</span></span><br></pre></td></tr></table></figure>
<h3 id="8-5-完整的模型">8.5 完整的模型</h3>
<p>完整的单发多框检测模型由<strong>五个模块</strong>组成，每个块生成的特征图既用于生成锚框，又用于预测这些锚框的类别和偏移量。在这五个模块中，第一个是<strong>基本网络块</strong>，第二个到第四个是<strong>高和宽减半块</strong>，最后一个模块使用<strong>全局最大池化层</strong>将高度和宽度都降到1。从技术上讲，第二到第五个区块都是 SSD 中的<strong>多尺度特征块</strong>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_blk</span>(<span class="params">i</span>):</span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">        blk = base_net()</span><br><span class="line">    <span class="keyword">elif</span> i == <span class="number">1</span>:</span><br><span class="line">        blk = down_sample_blk(<span class="number">64</span>, <span class="number">128</span>)</span><br><span class="line">    <span class="keyword">elif</span> i == <span class="number">4</span>:</span><br><span class="line">        blk = nn.AdaptiveMaxPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        blk = down_sample_blk(<span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure>
<p>现在我们为每个块定义前向传播。与图像分类任务不同，此处的输出包括：CNN 特征图 <code>Y</code>、在当前尺度下根据 <code>Y</code> 生成的锚框、预测的这些锚框的类别和偏移量（基于 <code>Y</code>）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为每个块定义前向传播，此处的cls_predictor和bbox_predictor为已经构造好的卷积层</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">blk_forward</span>(<span class="params">X, blk, size, ratio, cls_predictor, bbox_predictor</span>):</span><br><span class="line">    Y = blk(X)  <span class="comment"># feature map</span></span><br><span class="line">    anchors = d2l.multibox_prior(Y, sizes=size, ratios=ratio)</span><br><span class="line">    cls_preds = cls_predictor(Y)</span><br><span class="line">    bbox_preds = bbox_predictor(Y)</span><br><span class="line">    <span class="keyword">return</span> (Y, anchors, cls_preds, bbox_preds)</span><br></pre></td></tr></table></figure>
<p>超参数的设置过程可以看：<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_computer-vision/ssd.html">单发多框检测（SSD）</a>。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sizes = [[<span class="number">0.2</span>, <span class="number">0.272</span>], [<span class="number">0.37</span>, <span class="number">0.447</span>], [<span class="number">0.54</span>, <span class="number">0.619</span>], [<span class="number">0.71</span>, <span class="number">0.79</span>], [<span class="number">0.88</span>, <span class="number">0.961</span>]]</span><br><span class="line">ratios = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0.5</span>]] * <span class="number">5</span></span><br><span class="line">num_anchors = <span class="built_in">len</span>(sizes[<span class="number">0</span>]) + <span class="built_in">len</span>(ratios[<span class="number">0</span>]) - <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>现在，我们就可以按如下方式定义完整的模型 <code>TinySSD</code> 了：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TinySSD</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(TinySSD, self).__init__(**kwargs)</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        idx_to_in_channels = [<span class="number">64</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">128</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">            <span class="comment"># 即赋值语句self.blk_i=get_blk(i)</span></span><br><span class="line">            <span class="built_in">setattr</span>(self, <span class="string">f&#x27;blk_<span class="subst">&#123;i&#125;</span>&#x27;</span>, get_blk(i))</span><br><span class="line">            <span class="built_in">setattr</span>(self, <span class="string">f&#x27;cls_<span class="subst">&#123;i&#125;</span>&#x27;</span>, cls_predictor(idx_to_in_channels[i], num_anchors, num_classes))</span><br><span class="line">            <span class="built_in">setattr</span>(self, <span class="string">f&#x27;bbox_<span class="subst">&#123;i&#125;</span>&#x27;</span>, bbox_predictor(idx_to_in_channels[i], num_anchors))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        anchors, cls_preds, bbox_preds = [<span class="literal">None</span>] * <span class="number">5</span>, [<span class="literal">None</span>] * <span class="number">5</span>, [<span class="literal">None</span>] * <span class="number">5</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">            <span class="comment"># getattr(self, &#x27;blk_%d&#x27;%i)即访问self.blk_i</span></span><br><span class="line">            X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(</span><br><span class="line">                X, <span class="built_in">getattr</span>(self, <span class="string">f&#x27;blk_<span class="subst">&#123;i&#125;</span>&#x27;</span>), sizes[i], ratios[i],</span><br><span class="line">                <span class="built_in">getattr</span>(self, <span class="string">f&#x27;cls_<span class="subst">&#123;i&#125;</span>&#x27;</span>), <span class="built_in">getattr</span>(self, <span class="string">f&#x27;bbox_<span class="subst">&#123;i&#125;</span>&#x27;</span>))</span><br><span class="line">            <span class="comment"># print(f&#x27;anchors[&#123;i&#125;], cls_preds[&#123;i&#125;], bbox_preds[&#123;i&#125;]:&#x27;, anchors[i].shape, cls_preds[i].shape, bbox_preds[i].shape)</span></span><br><span class="line">        anchors = torch.cat(anchors, dim=<span class="number">1</span>)</span><br><span class="line">        cls_preds = concat_preds(cls_preds)</span><br><span class="line">        cls_preds = cls_preds.reshape(cls_preds.shape[<span class="number">0</span>], -<span class="number">1</span>, self.num_classes + <span class="number">1</span>)</span><br><span class="line">        bbox_preds = concat_preds(bbox_preds)</span><br><span class="line">        <span class="keyword">return</span> anchors, cls_preds, bbox_preds</span><br><span class="line"></span><br><span class="line"><span class="comment"># f_map: (32, 3, 256, 256)-&gt;(32, 64, 32, 32)-&gt;(32, 128, 16, 16)-&gt;(32, 128, 8, 8)-&gt;(32, 128, 4, 4)-&gt;(32, 128, 1, 1)</span></span><br><span class="line"><span class="comment"># anchors[0], cls_preds[0], bbox_preds[0]: torch.Size([1, 4096, 4]) torch.Size([32, 8, 32, 32]) torch.Size([32, 16, 32, 32])</span></span><br><span class="line"><span class="comment"># anchors[1], cls_preds[1], bbox_preds[1]: torch.Size([1, 1024, 4]) torch.Size([32, 8, 16, 16]) torch.Size([32, 16, 16, 16])</span></span><br><span class="line"><span class="comment"># anchors[2], cls_preds[2], bbox_preds[2]: torch.Size([1, 256, 4]) torch.Size([32, 8, 8, 8]) torch.Size([32, 16, 8, 8])</span></span><br><span class="line"><span class="comment"># anchors[3], cls_preds[3], bbox_preds[3]: torch.Size([1, 64, 4]) torch.Size([32, 8, 4, 4]) torch.Size([32, 16, 4, 4])</span></span><br><span class="line"><span class="comment"># anchors[4], cls_preds[4], bbox_preds[4]: torch.Size([1, 4, 4]) torch.Size([32, 8, 1, 1]) torch.Size([32, 16, 1, 1])</span></span><br><span class="line">net = TinySSD(num_classes=<span class="number">1</span>)</span><br><span class="line">X = torch.zeros((<span class="number">32</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>))</span><br><span class="line">anchors, cls_preds, bbox_preds = net(X)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output anchors:&#x27;</span>, anchors.shape)  <span class="comment"># output anchors: torch.Size([1, 5444, 4])</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output class preds:&#x27;</span>, cls_preds.shape)  <span class="comment"># output class preds: torch.Size([32, 5444, 2])</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output bbox preds:&#x27;</span>, bbox_preds.shape)  <span class="comment"># output bbox preds: torch.Size([32, 21776])</span></span><br></pre></td></tr></table></figure>
<h3 id="8-6-训练模型">8.6 训练模型</h3>
<p>首先读取数据集和设置超参数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">lr, num_epochs, batch_size = <span class="number">0.2</span>, <span class="number">20</span>, <span class="number">32</span></span><br><span class="line">train_iter, valid_iter = d2l.load_data_bananas(batch_size)</span><br></pre></td></tr></table></figure>
<p>然后定义损失函数和评价函数，目标检测有两种类型的损失。第一种有关<strong>锚框类别</strong>的损失：我们可以简单地复用之前图像分类问题里一直使用的交叉熵损失函数来计算；第二种有关<strong>正类锚框偏移量</strong>的损失：预测偏移量是一个回归问题。但是，对于这个回归问题，我们在这里不使用平方损失，而是使用 L1 范数损失，即预测值和真实值之差的绝对值。掩码变量 <code>bbox_masks</code> 令负类锚框和填充锚框不参与损失的计算。最后，我们将锚框类别和偏移量的损失相加，以获得模型的最终损失函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cls_loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">bbox_loss = nn.L1Loss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calc_loss</span>(<span class="params">cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks</span>):</span><br><span class="line">    batch_size, num_classes = cls_preds.shape[<span class="number">0</span>], cls_preds.shape[<span class="number">2</span>]</span><br><span class="line">    cls = cls_loss(cls_preds.reshape(-<span class="number">1</span>, num_classes), cls_labels.reshape(-<span class="number">1</span>)).reshape(batch_size, -<span class="number">1</span>).mean(dim=<span class="number">1</span>)</span><br><span class="line">    bbox = bbox_loss(bbox_preds * bbox_masks, bbox_labels * bbox_masks).mean(dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> cls + bbox</span><br></pre></td></tr></table></figure>
<p>我们可以沿用准确率评价分类结果。由于偏移量使用了 L1 范数损失，我们使用平均绝对误差来（MAE）评价边界框的预测结果。这些预测结果是从生成的锚框及其预测偏移量中获得的：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cls_eval</span>(<span class="params">cls_preds, cls_labels</span>):</span><br><span class="line">    <span class="comment"># 由于类别预测结果放在最后一维，argmax需要指定最后一维</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>((cls_preds.argmax(dim=-<span class="number">1</span>).<span class="built_in">type</span>(cls_labels.dtype) == cls_labels).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bbox_eval</span>(<span class="params">bbox_preds, bbox_labels, bbox_masks</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>((torch.<span class="built_in">abs</span>((bbox_labels - bbox_preds) * bbox_masks)).<span class="built_in">sum</span>())</span><br></pre></td></tr></table></figure>
<p>最后是训练模型，在训练模型时，我们需要在模型的前向传播过程中生成多尺度锚框 <code>anchors</code>，并预测其类别 <code>cls_preds</code> 和偏移量 <code>bbox_preds</code>。然后，我们根据标签信息 <code>label</code> 为生成的锚框标记类别 <code>cls_labels</code> 和偏移量 <code>bbox_labels</code>。最后，我们根据类别和偏移量的预测和标注值计算损失函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, train_iter, valid_iter, num_epochs, lr, device</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=<span class="number">5e-4</span>)</span><br><span class="line"></span><br><span class="line">    writer = SummaryWriter(<span class="string">&#x27;../logs/SSD_train_log&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    best_acc = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        net.train()</span><br><span class="line">        train_loss, train_acc, train_bbox_err = [], [], []</span><br><span class="line">        <span class="keyword">for</span> feature, label <span class="keyword">in</span> tqdm(train_iter):</span><br><span class="line">            feature, label = feature.to(device), label.to(device)</span><br><span class="line">            <span class="comment"># 生成多尺度的锚框，为每个锚框预测类别和偏移量</span></span><br><span class="line">            anchors, cls_preds, bbox_preds = net(feature)</span><br><span class="line">            <span class="comment"># 为每个锚框标注类别和偏移量</span></span><br><span class="line">            bbox_labels, bbox_masks, cls_labels = d2l.multibox_target(anchors, label)</span><br><span class="line">            <span class="comment"># 根据类别和偏移量的预测和标注值计算损失函数</span></span><br><span class="line">            loss = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks)</span><br><span class="line"></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.mean().backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            acc = cls_eval(cls_preds, cls_labels) / cls_labels.numel()</span><br><span class="line">            bbox_mae = bbox_eval(bbox_preds, bbox_labels, bbox_masks) / bbox_labels.numel()</span><br><span class="line"></span><br><span class="line">            train_loss.append(loss.mean())</span><br><span class="line">            train_acc.append(acc)</span><br><span class="line">            train_bbox_err.append(bbox_mae)</span><br><span class="line"></span><br><span class="line">        train_loss = <span class="built_in">sum</span>(train_loss) / <span class="built_in">len</span>(train_loss)</span><br><span class="line">        train_acc = <span class="built_in">sum</span>(train_acc) / <span class="built_in">len</span>(train_acc)</span><br><span class="line">        train_bbox_err = <span class="built_in">sum</span>(train_bbox_err) / <span class="built_in">len</span>(train_bbox_err)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;[ Train | <span class="subst">&#123;epoch + <span class="number">1</span>:03d&#125;</span>/<span class="subst">&#123;num_epochs:03d&#125;</span> ] loss = <span class="subst">&#123;train_loss:<span class="number">.5</span>f&#125;</span>, acc = <span class="subst">&#123;train_acc:<span class="number">.5</span>f&#125;</span>, bbox_err = <span class="subst">&#123;train_bbox_err:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        valid_loss, valid_acc, valid_bbox_err = [], [], []</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">for</span> feature, label <span class="keyword">in</span> tqdm(valid_iter):</span><br><span class="line">                feature, label = feature.to(device), label.to(device)</span><br><span class="line">                anchors, cls_preds, bbox_preds = net(feature)</span><br><span class="line">                bbox_labels, bbox_masks, cls_labels = d2l.multibox_target(anchors, label)</span><br><span class="line"></span><br><span class="line">                loss = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks)</span><br><span class="line">                acc = cls_eval(cls_preds, cls_labels) / cls_labels.numel()</span><br><span class="line">                bbox_mae = bbox_eval(bbox_preds, bbox_labels, bbox_masks) / bbox_labels.numel()</span><br><span class="line"></span><br><span class="line">                valid_loss.append(loss.mean())</span><br><span class="line">                valid_acc.append(acc)</span><br><span class="line">                valid_bbox_err.append(bbox_mae)</span><br><span class="line"></span><br><span class="line">        valid_loss = <span class="built_in">sum</span>(valid_loss) / <span class="built_in">len</span>(valid_loss)</span><br><span class="line">        valid_acc = <span class="built_in">sum</span>(valid_acc) / <span class="built_in">len</span>(valid_acc)</span><br><span class="line">        valid_bbox_err = <span class="built_in">sum</span>(valid_bbox_err) / <span class="built_in">len</span>(valid_bbox_err)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;[ Valid | <span class="subst">&#123;epoch + <span class="number">1</span>:03d&#125;</span>/<span class="subst">&#123;num_epochs:03d&#125;</span> ] loss = <span class="subst">&#123;valid_loss:<span class="number">.5</span>f&#125;</span>, acc = <span class="subst">&#123;valid_acc:<span class="number">.5</span>f&#125;</span>, bbox_err = <span class="subst">&#123;valid_bbox_err:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        writer.add_scalars(<span class="string">&#x27;train&#x27;</span>, &#123;<span class="string">&#x27;loss&#x27;</span>: train_loss,</span><br><span class="line">                                     <span class="string">&#x27;acc&#x27;</span>: train_acc,</span><br><span class="line">                                     <span class="string">&#x27;bbox_err&#x27;</span>: train_bbox_err&#125;, epoch + <span class="number">1</span>)</span><br><span class="line">        writer.add_scalars(<span class="string">&#x27;valid&#x27;</span>, &#123;<span class="string">&#x27;loss&#x27;</span>: valid_loss,</span><br><span class="line">                                     <span class="string">&#x27;acc&#x27;</span>: valid_acc,</span><br><span class="line">                                     <span class="string">&#x27;bbox_err&#x27;</span>: valid_bbox_err&#125;, epoch + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> valid_acc &gt; best_acc:</span><br><span class="line">            best_acc = valid_acc</span><br><span class="line">            torch.save(net.state_dict(), <span class="string">&#x27;../save/SSD_train.params&#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;saving model with acc &#123;:.3f&#125;&#x27;</span>.<span class="built_in">format</span>(best_acc))</span><br><span class="line"></span><br><span class="line">    writer.close()</span><br><span class="line"></span><br><span class="line">train(net, train_iter, valid_iter, num_epochs, lr, device)</span><br></pre></td></tr></table></figure>
<h3 id="8-7-预测目标">8.7 预测目标</h3>
<p>在预测阶段，我们希望能把图像里面所有我们感兴趣的目标检测出来。在下面，我们读取并调整测试图像的大小，然后将其转成卷积层需要的四维格式。使用 <code>multibox_detection</code> 函数，我们可以根据锚框及其预测偏移量得到预测边界框，然后通过非极大值抑制来移除相似的预测边界框。最后，我们筛选所有置信度不低于0.9的边界框，做为最终输出：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">X = torchvision.io.read_image(<span class="string">&#x27;../images/banana.jpg&#x27;</span>).unsqueeze(<span class="number">0</span>).<span class="built_in">float</span>()  <span class="comment"># 将其转成卷积层需要的四维格式</span></span><br><span class="line">img = X.squeeze(<span class="number">0</span>).permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).long()  <span class="comment"># (h, w, c)</span></span><br><span class="line">net.to(device)</span><br><span class="line">net.load_state_dict(torch.load(<span class="string">&#x27;../save/SSD_train.params&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">x, net, device</span>):</span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line">    anchors, cls_preds, bbox_preds = net(X.to(device))</span><br><span class="line">    cls_probs = F.softmax(cls_preds, dim=<span class="number">2</span>).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    output = d2l.multibox_detection(cls_probs, bbox_preds, anchors)</span><br><span class="line">    idx = [i <span class="keyword">for</span> i, row <span class="keyword">in</span> <span class="built_in">enumerate</span>(output[<span class="number">0</span>]) <span class="keyword">if</span> row[<span class="number">0</span>] != -<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> output[<span class="number">0</span>, idx]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">display</span>(<span class="params">img, output, threshold</span>):</span><br><span class="line">    plt.figure(dpi=<span class="number">100</span>)</span><br><span class="line">    fig = plt.imshow(img)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> output:</span><br><span class="line">        score = <span class="built_in">float</span>(row[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> score &lt; threshold:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        h, w = img.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">        bbox = [row[<span class="number">2</span>:<span class="number">6</span>] * torch.tensor((w, h, w, h), device=row.device)]</span><br><span class="line">        d2l.show_bboxes(fig.axes, bbox, <span class="string">&#x27;%.2f&#x27;</span> % score, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">output = predict(X, net, device)</span><br><span class="line">display(img, output.cpu(), threshold=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<h2 id="9-语义分割和数据集">9. 语义分割和数据集</h2>
<p>在前几节中讨论的目标检测问题中，我们一直使用方形边界框来标注和预测图像中的目标。本节将探讨<strong>语义分割</strong>（semantic segmentation）问题，它重点关注于如何将图像分割成属于不同语义类别的区域。与目标检测不同，语义分割可以识别并理解图像中<strong>每一个像素</strong>的内容：其语义区域的标注和预测是像素级的。与目标检测相比，语义分割标注的像素级的边框显然更加精细。</p>
<h3 id="9-1-图像分割和实例分割">9.1 图像分割和实例分割</h3>
<p>计算机视觉领域还有2个与语义分割相似的重要问题，即<strong>图像分割</strong>（image segmentation）和<strong>实例分割</strong>（instance segmentation）。我们在这里将它们同语义分割简单区分一下：</p>
<ul>
<li>图像分割将图像划分为若干组成区域，这类问题的方法通常利用图像中像素之间的相关性。它在训练时不需要有关图像像素的标签信息，在预测时也无法保证分割出的区域具有我们希望得到的语义。以图像 <code>catdog.jpg</code> 作为输入，图像分割可能会将狗分为两个区域：一个覆盖以黑色为主的嘴和眼睛，另一个覆盖以黄色为主的其余部分身体。</li>
<li>实例分割也叫同时检测并分割（simultaneous detection and segmentation），它研究如何识别图像中各个目标实例的像素级区域。与语义分割不同，实例分割不仅需要区分语义，还要<strong>区分不同的目标实例</strong>。例如，如果图像中有两条狗，则实例分割需要区分像素属于的两条狗中的哪一条。</li>
</ul>
<h3 id="9-2-Pascal-VOC2012-语义分割数据集">9.2 Pascal VOC2012 语义分割数据集</h3>
<p>最重要的语义分割数据集之一是 Pascal VOC2012，下面我们深入了解一下这个数据集：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">d2l.DATA_HUB[<span class="string">&#x27;voc2012&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;VOCtrainval_11-May-2012.tar&#x27;</span>, <span class="string">&#x27;4e443f8a2eca6b1dac8a6c57641b67dd40621a49&#x27;</span>)</span><br><span class="line">voc_dir = d2l.download_extract(<span class="string">&#x27;voc2012&#x27;</span>, <span class="string">&#x27;VOCdevkit/VOC2012&#x27;</span>)  <span class="comment"># 提取出的数据集位于../data/VOCdevkit/VOC2012</span></span><br></pre></td></tr></table></figure>
<p>进入路径 <code>../data/VOCdevkit/VOC2012</code> 之后，我们可以看到数据集的不同组件。<code>ImageSets/Segmentation</code> 路径包含用于训练和测试样本的文本文件，而 <code>JPEGImages</code> 和 <code>SegmentationClass</code> 路径分别存储着每个示例的输入图像和标签。此处的标签也采用图像格式，其尺寸和它所标注的输入图像的尺寸相同。此外，标签中颜色相同的像素属于同一个语义类别。下面将 <code>read_voc_images</code> 函数定义为将所有输入的图像和标签读入内存：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">read_voc_images</span>(<span class="params">voc_dir, is_train=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;读取所有VOC图像并标注&quot;&quot;&quot;</span></span><br><span class="line">    txt_fname = os.path.join(voc_dir, <span class="string">&#x27;ImageSets&#x27;</span>, <span class="string">&#x27;Segmentation&#x27;</span>, <span class="string">&#x27;train.txt&#x27;</span> <span class="keyword">if</span> is_train <span class="keyword">else</span> <span class="string">&#x27;val.txt&#x27;</span>)</span><br><span class="line">    mode = torchvision.io.image.ImageReadMode.RGB</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(txt_fname, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        images = f.read().split()</span><br><span class="line">    features, labels = [], []</span><br><span class="line">    <span class="keyword">for</span> i, fname <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">        features.append(torchvision.io.read_image(os.path.join(voc_dir, <span class="string">&#x27;JPEGImages&#x27;</span>, <span class="string">f&#x27;<span class="subst">&#123;fname&#125;</span>.jpg&#x27;</span>)))</span><br><span class="line">        labels.append(torchvision.io.read_image(os.path.join(voc_dir, <span class="string">&#x27;SegmentationClass&#x27;</span> , <span class="string">f&#x27;<span class="subst">&#123;fname&#125;</span>.png&#x27;</span>), mode))</span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br><span class="line"></span><br><span class="line">train_features, train_labels = read_voc_images(voc_dir, <span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(train_features))  <span class="comment"># 1464</span></span><br><span class="line"><span class="built_in">print</span>(train_features[<span class="number">0</span>].shape, train_labels[<span class="number">0</span>].shape)  <span class="comment"># torch.Size([3, 281, 500]) torch.Size([3, 281, 500])</span></span><br></pre></td></tr></table></figure>
<p>下面我们绘制前5个输入图像及其标签。在标签图像中，白色和黑色分别表示边框和背景，而其他颜色则对应不同的类别：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">imgs = train_features[<span class="number">0</span>:<span class="number">5</span>] + train_labels[<span class="number">0</span>:<span class="number">5</span>]</span><br><span class="line">imgs = [img.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>) <span class="keyword">for</span> img <span class="keyword">in</span> imgs]  <span class="comment"># 将通道放到最后一维</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(imgs), imgs[<span class="number">0</span>].shape)  <span class="comment"># 10 torch.Size([281, 500, 3])</span></span><br><span class="line">d2l.show_images(imgs, <span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>接下来，我们列举 RGB 颜色值和类名：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">VOC_COLORMAP = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">128</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">128</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">128</span>, <span class="number">0</span>, <span class="number">128</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">128</span>, <span class="number">128</span>], [<span class="number">128</span>, <span class="number">128</span>, <span class="number">128</span>], [<span class="number">64</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">192</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">64</span>, <span class="number">128</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">192</span>, <span class="number">128</span>, <span class="number">0</span>], [<span class="number">64</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">192</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">64</span>, <span class="number">128</span>, <span class="number">128</span>], [<span class="number">192</span>, <span class="number">128</span>, <span class="number">128</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">64</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">64</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">192</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">192</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">64</span>, <span class="number">128</span>]]</span><br><span class="line"></span><br><span class="line">VOC_CLASSES = [<span class="string">&#x27;background&#x27;</span>, <span class="string">&#x27;aeroplane&#x27;</span>, <span class="string">&#x27;bicycle&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;boat&#x27;</span>, <span class="string">&#x27;bottle&#x27;</span>, <span class="string">&#x27;bus&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;chair&#x27;</span>, <span class="string">&#x27;cow&#x27;</span>, <span class="string">&#x27;diningtable&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;motorbike&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;person&#x27;</span>, <span class="string">&#x27;potted plant&#x27;</span>, <span class="string">&#x27;sheep&#x27;</span>, <span class="string">&#x27;sofa&#x27;</span>, <span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;tv/monitor&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>通过上面定义的两个常量，我们可以方便地查找标签中每个像素的类索引。我们定义了 <code>voc_colormap2label</code> 函数来构建从上述 RGB 颜色值到类别索引的映射，而 <code>voc_label_indices</code> 函数将 RGB 值映射到在 Pascal VOC2012 数据集中的类别索引：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">voc_colormap2label</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;构建从RGB到VOC类别索引的映射&quot;&quot;&quot;</span></span><br><span class="line">    colormap2label = torch.zeros(<span class="number">256</span> ** <span class="number">3</span>, dtype=torch.long)</span><br><span class="line">    <span class="keyword">for</span> i, colormap <span class="keyword">in</span> <span class="built_in">enumerate</span>(VOC_COLORMAP):</span><br><span class="line">        colormap2label[(colormap[<span class="number">0</span>] * <span class="number">256</span> + colormap[<span class="number">1</span>]) * <span class="number">256</span> + colormap[<span class="number">2</span>]] = i  <span class="comment"># 哈希映射</span></span><br><span class="line">    <span class="keyword">return</span> colormap2label</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">voc_label_indices</span>(<span class="params">colormap, colormap2label</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将VOC标签中的RGB值映射到它们的类别索引&quot;&quot;&quot;</span></span><br><span class="line">    colormap = colormap.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).numpy().astype(<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">    idx = ((colormap[:, :, <span class="number">0</span>] * <span class="number">256</span> + colormap[:, :, <span class="number">1</span>]) * <span class="number">256</span> + colormap[:, :, <span class="number">2</span>])</span><br><span class="line">    <span class="keyword">return</span> colormap2label[idx]</span><br></pre></td></tr></table></figure>
<p>例如，在第一张样本图像中，飞机头部区域的类别索引为1，而背景索引为0：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">colormap2label = voc_colormap2label()</span><br><span class="line">y = voc_label_indices(train_labels[<span class="number">0</span>], colormap2label)</span><br><span class="line"><span class="built_in">print</span>(y[<span class="number">105</span>:<span class="number">115</span>, <span class="number">130</span>:<span class="number">140</span>], VOC_CLASSES[<span class="number">1</span>])</span><br><span class="line"><span class="comment"># tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],</span></span><br><span class="line"><span class="comment">#         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]) aeroplane</span></span><br></pre></td></tr></table></figure>
<p>之前的实验我们通过再缩放图像使其符合模型的输入形状。然而在语义分割中，这样做需要将预测的像素类别重新映射回原始尺寸的输入图像。这样的映射可能不够精确，尤其在不同语义的分割区域。为了避免这个问题，我们将图像<strong>裁剪为固定尺寸</strong>，而不是再缩放。具体来说，我们使用图像增广中的随机裁剪，裁剪输入图像和标签的相同区域：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">voc_rand_crop</span>(<span class="params">feature, label, height, width</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;随机裁剪特征和标签图像&quot;&quot;&quot;</span></span><br><span class="line">    rect = torchvision.transforms.RandomCrop.get_params(feature, (height, width))</span><br><span class="line">    feature = torchvision.transforms.functional.crop(feature, *rect)</span><br><span class="line">    label = torchvision.transforms.functional.crop(label, *rect)</span><br><span class="line">    <span class="keyword">return</span> feature, label</span><br><span class="line"></span><br><span class="line">imgs = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    imgs += voc_rand_crop(train_features[<span class="number">0</span>], train_labels[<span class="number">0</span>], <span class="number">200</span>, <span class="number">300</span>)</span><br><span class="line">imgs = [img.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>) <span class="keyword">for</span> img <span class="keyword">in</span> imgs]</span><br><span class="line">d2l.show_images(imgs[::<span class="number">2</span>] + imgs[<span class="number">1</span>::<span class="number">2</span>], <span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>我们通过继承高级 API 提供的 <code>Dataset</code> 类，自定义了一个语义分割数据集类 <code>VOCSegDataset</code>。通过实现 <code>__getitem__</code> 函数，我们可以任意访问数据集中索引为 <code>idx</code> 的输入图像及其每个像素的类别索引。由于数据集中有些图像的尺寸可能小于随机裁剪所指定的输出尺寸，这些样本可以通过自定义的 <code>filter</code> 函数移除掉。此外，我们还定义了 <code>normalize_image</code> 函数，从而对输入图像的 RGB 三个通道的值分别做标准化：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VOCSegDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;一个用于加载VOC数据集的自定义数据集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, is_train, crop_size, voc_dir</span>):</span><br><span class="line">        self.transform = torchvision.transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">        self.crop_size = crop_size</span><br><span class="line">        features, labels = read_voc_images(voc_dir, is_train=is_train)</span><br><span class="line">        self.features = [self.normalize_image(feature) <span class="keyword">for</span> feature <span class="keyword">in</span> self.<span class="built_in">filter</span>(features)]</span><br><span class="line">        self.labels = self.<span class="built_in">filter</span>(labels)</span><br><span class="line">        self.colormap2label = voc_colormap2label()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;read &#x27;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(self.features)) + <span class="string">&#x27; examples&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normalize_image</span>(<span class="params">self, img</span>):</span><br><span class="line">        <span class="keyword">return</span> self.transform(img.<span class="built_in">float</span>() / <span class="number">255</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 过滤掉比裁切大小还小的图像</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">filter</span>(<span class="params">self, imgs</span>):</span><br><span class="line">        <span class="keyword">return</span> [img <span class="keyword">for</span> img <span class="keyword">in</span> imgs <span class="keyword">if</span> (img.shape[<span class="number">1</span>] &gt;= self.crop_size[<span class="number">0</span>] <span class="keyword">and</span></span><br><span class="line">                                        img.shape[<span class="number">2</span>] &gt;= self.crop_size[<span class="number">1</span>])]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        feature, label = voc_rand_crop(self.features[idx], self.labels[idx], *self.crop_size)</span><br><span class="line">        <span class="keyword">return</span> (feature, voc_label_indices(label, self.colormap2label))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.features)</span><br><span class="line"></span><br><span class="line">crop_size = (<span class="number">320</span>, <span class="number">480</span>)</span><br><span class="line">voc_train, voc_valid = VOCSegDataset(<span class="literal">True</span>, crop_size, voc_dir), VOCSegDataset(<span class="literal">False</span>, crop_size, voc_dir)</span><br></pre></td></tr></table></figure>
<p>最后，我们定义以下 <code>load_data_voc</code> 函数来下载并读取 Pascal VOC2012 语义分割数据集。它返回训练集和测试集的数据迭代器：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_voc</span>(<span class="params">batch_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;加载VOC语义分割数据集&quot;&quot;&quot;</span></span><br><span class="line">    train_iter = DataLoader(voc_train, batch_size, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line">    valid_iter = DataLoader(voc_valid, batch_size, drop_last=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> train_iter, valid_iter</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_iter, valid_iter = load_data_voc(batch_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> features, labels <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(features.shape)  <span class="comment"># torch.Size([64, 3, 320, 480])</span></span><br><span class="line">    <span class="built_in">print</span>(labels.shape)  <span class="comment"># torch.Size([64, 320, 480])</span></span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h2 id="10-转置卷积">10. 转置卷积</h2>
</div>
        </div>
        
        <footer class="kratos-entry-footer clearfix">
            
                <div class="post-like-donate text-center clearfix" id="post-like-donate">
                
                
                    <a class="share" href="javascript:;"><i class="fa fa-share-alt"></i> Share</a>
                    <div class="share-wrap" style="display: none;">
    <div class="share-group">
        <a href="javascript:;" class="share-plain qq" onclick="share('qq');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-qq"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain qzone" onclick="share('qzone');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-star"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain weixin pop style-plain" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-weixin"></i>
            </div>
            <div class="share-int">
                <div class="qrcode" id="wechat-qr"></div>
                <p>打开微信“扫一扫”，打开网页后点击屏幕右上角分享按钮</p>
            </div>
        </a>
        <a href="javascript:;" class="share-plain weibo" onclick="share('weibo');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-weibo"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain facebook style-plain" onclick="share('facebook');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-facebook"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain twitter style-plain" onclick="share('twitter');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-twitter"></i>
            </div>
        </a>
    </div>
    <script type="text/javascript">
        $(()=>{
            new QRCode("wechat-qr", {
                text: "https://asanosaki.github.io/posts/24840.html",
                width: 150,
                height: 150,
                correctLevel : QRCode.CorrectLevel.H
            });
        });
        function share(dest) {
            const qqBase        = "https://connect.qq.com/widget/shareqq/index.html?";
            const weiboBase     = "https://service.weibo.com/share/share.php?";
            const qzoneBase     = "https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?";
            const facebookBase  = "https://www.facebook.com/sharer/sharer.php?";
            const twitterBase   = "https://twitter.com/intent/tweet?";
            const hostUrl       = "https://asanosaki.github.io/posts/24840.html";
            const title         = "「D2L学习笔记-计算机视觉」";
            const excerpt       = `李沐动手学深度学习（PyTorch）课程学习笔记第七章：计算机视觉。`;
            let _URL;
            switch (dest) {
                case "qq"       : _URL = qqBase+"url="+hostUrl+"&title="+title+"&desc=&summary="+excerpt+"&site=cxpy";     break;
                case "weibo"    : _URL = weiboBase+"url="+hostUrl+"&title="+title+excerpt;                                 break;
                case "qzone"    : _URL = qzoneBase+"url="+hostUrl+"&title="+title+"&desc=&summary="+excerpt+"&site=cxpy";  break;
                case "facebook" : _URL = facebookBase+"u="+hostUrl;                                                        break;
                case "twitter"  : _URL = twitterBase+"text="+title+excerpt+"&url="+hostUrl;                                break;
            }
            window.open(_URL);
        };
    </script>
</div>
                
                </div>
            
            <div class="footer-tag clearfix">
                <div class="pull-left">
                <i class="fa fa-tags"></i>
                    <a class="tag-none-link" href="/tags/AI/" rel="tag">AI</a>
                </div>
                <div class="pull-date">
                    <time datetime="2023-11-26T08:01:43.777Z" itemprop="dateModified">Last edited: 2023-11-26</time>
                </div>
            </div>
        </footer>
    </div>
    
        <nav class="navigation post-navigation clearfix" role="navigation">
            
            <div class="nav-previous clearfix">
                <a title=" D2L学习笔记-现代卷积神经网络" href="/posts/21165.html">&lt; Previous</a>
            </div>
            
            
            <div class="nav-next clearfix">
                <a title=" Python路径操作模块pathlib教程" href="/posts/55.html">Next &gt;</a>
            </div>
            
        </nav>
    
    
</article>

        

            </section>

        

                
            

<section id="kratos-widget-area" class="col-md-4 hidden-xs hidden-sm">
    <!-- 文章和页面根据splitter来分割，没有的话就从头开始设置为sticky -->
    
    
                <aside id="krw-about" class="widget widget-kratos-about clearfix">
    <div class="photo-background"></div>
    <div class="photo-wrapper clearfix">
        <div class="photo-wrapper-tip text-center">
            <img class="about-photo" src="/images/head.webp" loading="lazy" decoding="auto" />
        </div>
    </div>
    <div class="textwidget">
        <p class="text-center"></p>
    </div>
    <div class="site-meta">
        <a class="meta-item" href="/archives/">
            <span class="title">
                Articles
            </span>
            <span class="count">
                89
            </span>
        </a>
        <a class="meta-item" href="/categories/">
            <span class="title">
                Classifications
            </span>
            <span class="count">
                10
            </span>
        </a>
        <a class="meta-item" href="/tags/">
            <span class="title">
                Tags
            </span>
            <span class="count">
                10
            </span>
        </a>
    </div>
</aside>
            
                    <div class="sticky-area">
                
                    <aside id="krw-toc" class="widget widget-kratos-toc clearfix toc-div-class" >
    <div class="photo-background"></div>
    <h4 class="widget-title no-after">
        <i class="fa fa-compass"></i>
        Contents
        <span class="toc-progress-bar" role="progressbar" aria-label="阅读进度："></span>
    </h4>
    <div class="textwidget">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%B9%BF"><span class="toc-text">1. 图像增广</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%BE%AE%E8%B0%83"><span class="toc-text">2. 微调</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%92%8C%E8%BE%B9%E7%95%8C%E6%A1%86"><span class="toc-text">3. 目标检测和边界框</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">4. 目标检测数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E9%94%9A%E6%A1%86"><span class="toc-text">5. 锚框</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B"><span class="toc-text">6. 多尺度目标检测</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E5%A4%9A%E5%B0%BA%E5%BA%A6%E9%94%9A%E6%A1%86"><span class="toc-text">6.1 多尺度锚框</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E5%A4%9A%E5%B0%BA%E5%BA%A6%E6%A3%80%E6%B5%8B"><span class="toc-text">6.2 多尺度检测</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E5%8C%BA%E5%9F%9F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88R-CNN%EF%BC%89%E7%B3%BB%E5%88%97"><span class="toc-text">7. 区域卷积神经网络（R-CNN）系列</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-R-CNN"><span class="toc-text">7.1 R-CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-Fast-R-CNN"><span class="toc-text">7.2 Fast R-CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-Faster-R-CNN"><span class="toc-text">7.3 Faster R-CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-4-Mask-R-CNN"><span class="toc-text">7.4 Mask R-CNN</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E5%8D%95%E5%8F%91%E5%A4%9A%E6%A1%86%E6%A3%80%E6%B5%8B%EF%BC%88SSD%EF%BC%89"><span class="toc-text">8. 单发多框检测（SSD）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-%E7%B1%BB%E5%88%AB%E9%A2%84%E6%B5%8B%E5%B1%82%E4%B8%8E%E8%BE%B9%E7%95%8C%E6%A1%86%E9%A2%84%E6%B5%8B%E5%B1%82"><span class="toc-text">8.1 类别预测层与边界框预测层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-%E8%BF%9E%E7%BB%93%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%9A%84%E9%A2%84%E6%B5%8B"><span class="toc-text">8.2 连结多尺度的预测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3-%E9%AB%98%E5%92%8C%E5%AE%BD%E5%87%8F%E5%8D%8A%E5%9D%97"><span class="toc-text">8.3 高和宽减半块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-4-%E5%9F%BA%E6%9C%AC%E7%BD%91%E7%BB%9C%E5%9D%97"><span class="toc-text">8.4 基本网络块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-5-%E5%AE%8C%E6%95%B4%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-text">8.5 完整的模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-6-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-text">8.6 训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-7-%E9%A2%84%E6%B5%8B%E7%9B%AE%E6%A0%87"><span class="toc-text">8.7 预测目标</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">9. 语义分割和数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-1-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2"><span class="toc-text">9.1 图像分割和实例分割</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-Pascal-VOC2012-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">9.2 Pascal VOC2012 语义分割数据集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF"><span class="toc-text">10. 转置卷积</span></a></li></ol>
    </div>
</aside>
                
                
  <aside id="krw-categories" class="widget widget-kratos-categories clearfix">
    <h4 class="widget-title"><i class="fa fa-folder"></i>Contents</h4>
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AI/">AI</a><span class="category-list-count">18</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Essay/">Essay</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Interview/">Interview</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Others/">Others</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">18</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Web/">Web</a><span class="category-list-count">9</span></li></ul>
  </aside>


            
                
  <aside id="krw-tags" class="widget widget-kratos-tags clearfix">
    <h4 class="widget-title"><i class="fa fa-tags"></i>Tags aggregation</h4>
      <div class="tag-clouds">
        <a href="/tags/AI/" style="font-size: 0.77em;">AI</a> <a href="/tags/C/" style="font-size: 0.6em;">C++</a> <a href="/tags/Essay/" style="font-size: 0.6em;">Essay</a> <a href="/tags/Hexo/" style="font-size: 0.63em;">Hexo</a> <a href="/tags/Interview/" style="font-size: 0.67em;">Interview</a> <a href="/tags/Java/" style="font-size: 0.8em;">Java</a> <a href="/tags/Linux/" style="font-size: 0.67em;">Linux</a> <a href="/tags/Others/" style="font-size: 0.7em;">Others</a> <a href="/tags/Python/" style="font-size: 0.77em;">Python</a> <a href="/tags/Web/" style="font-size: 0.73em;">Web</a>
      </div>
  </aside>

            
                
  <aside id="krw-posts" class="widget widget-kratos-posts">
  <h4 class="widget-title"><i class="fa fa-file"></i>Latest articles</h4>
  <div class="tab-content">
      <ul class="list-group">
        
        
          
          
            <a class="list-group-item" href="/posts/32409.html"><i class="fa  fa-book"></i> Determined AI部署与使用教程</a>
            
          
        
          
          
            <a class="list-group-item" href="/posts/5710.html"><i class="fa  fa-book"></i> SpringBoot学习笔记-实现微服务：匹配系统（下）</a>
            
          
        
          
          
            <a class="list-group-item" href="/posts/5710.html"><i class="fa  fa-book"></i> SpringBoot学习笔记-实现微服务：匹配系统（中）</a>
            
          
        
          
          
            <a class="list-group-item" href="/posts/2329.html"><i class="fa  fa-book"></i> Spring面试题总结</a>
            
          
        
          
          
            <a class="list-group-item" href="/posts/53737.html"><i class="fa  fa-book"></i> Java进阶面试题总结</a>
            
          
        
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
      </ul>
  </div>
  </aside>

            
    </div>
</section>
        
        </div>
    </div>
</div>
<footer>
    <div id="footer"  >
        <div class="container">
            <div class="row">
                <div class="col-md-6 col-md-offset-3 footer-list text-center">
                    <ul class="kratos-social-icons">
                        <!-- Keep for compatibility -->
                        <li><a target="_blank" rel="nofollow" href="https://weibo.com/u/1952449115"><i class="fa fa-weibo"></i></a></li>
                        <li><a href="mailto:mail@1195595343@qq.com"><i class="fa fa-envelope"></i></a></li>
                        
                        
                        
                        
                        
                        <li><a target="_blank" rel="nofollow" href="https://github.com/AsanoSaki"><i class="fa fa-github"></i></a></li>
                        
                        <!-- New links -->
                        
                    </ul>
                    <ul class="kratos-copyright">
                        <div>
                            <li>&copy; 2024 AsanoSaki Copyright.</li>
                            <li>This site is running<span id="span_dt">Loading...</span></li>
                        </div>
                        <div>
                            <li>Theme <a href="https://github.com/Candinya/Kratos-Rebirth" target="_blank">Kratos:Rebirth</a></li>
                            <li>Site built with&nbsp;<i class="fa fa-heart throb" style="color:#d43f57"></i>&nbsp;by AsanoSaki.</li>
                        </div>
                        <div>
                            <li>Powered by <a href="https://hexo.io" target="_blank" rel="nofollow">Hexo</a></li>
                            <li>Hosted on <a href="https://github.com/" target="_blank">Github Pages</a></li>
                        </div>
                        <div>
                            
                            
                        </div>
                    </ul>
                </div>
            </div>
        </div>
        <div class="kr-tool text-center">
            <div class="tool">
                
                    <div class="box search-box">
                        <a href="/search/">
                            <span class="fa fa-search"></span>
                        </a>
                    </div>
                
                
                    <div class="box theme-box" id="darkmode-switch">
                        <span class="fa fa-adjust"></span>
                    </div>
                
                
                
            </div>
            <div class="box gotop-box">
                <span class="fa fa-chevron-up"></span>
            </div>
        </div>
    </div>
</footer>
</div>
</div>

        <script defer src="/vendors/bootstrap@3.3.4/dist/js/bootstrap.min.js"></script>
<script defer src="/vendors/nprogress@0.2.0/nprogress.js"></script>
<script>
    if (!window.kr) {
        window.kr = {};
    }
    window.kr.notMobile = (!(navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i)));
    window.kr.siteRoot = "/";
</script>





    <script defer src="/vendors/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>

<script defer src="/js/kratosr.min.js"></script>
<script defer src="/js/pjax.min.js"></script>



<!-- Extra support for third-party plguins  -->


    </body>
</html>