{
    "version": "https://jsonfeed.org/version/1",
    "title": "AsanoSaki",
    "description": "",
    "home_page_url": "https://asanosaki.github.io",
    "items": [
        {
            "id": "https://asanosaki.github.io/posts/23991.html",
            "url": "https://asanosaki.github.io/posts/23991.html",
            "title": "KMeans聚类与PCA主成分分析",
            "date_published": "2023-08-09T02:03:00.000Z",
            "content_html": "<blockquote>\n<p>介绍二维数据与高维数据的 K-Means 聚类算法以及高维数据的 PCA 主成分分析方法。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-二维数据K-Means聚类\">1. 二维数据K-Means聚类</h2>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.cluster <span class=\"keyword\">import</span> KMeans, MiniBatchKMeans</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.decomposition <span class=\"keyword\">import</span> PCA</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 以center为中心产生随机分布的数据</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_random_data</span>(<span class=\"params\">center, data_num, data_dim=<span class=\"number\">2</span>, fix_seed=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> fix_seed:</span><br><span class=\"line\">        np.random.seed(<span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 产生-2~2的随机数</span></span><br><span class=\"line\">    offset = np.random.rand(data_num, data_dim) * <span class=\"number\">4</span> - <span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> center + offset</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">centers = [[<span class=\"number\">2</span>, <span class=\"number\">8</span>], [<span class=\"number\">5</span>, <span class=\"number\">2</span>], [<span class=\"number\">9</span>, <span class=\"number\">6</span>]]</span><br><span class=\"line\">center_num = <span class=\"built_in\">len</span>(centers)</span><br><span class=\"line\">data_num = <span class=\"number\">500</span>  <span class=\"comment\"># 每个center产生的样本数量</span></span><br><span class=\"line\"></span><br><span class=\"line\">data = np.zeros([data_num * center_num, <span class=\"number\">2</span>])</span><br><span class=\"line\"><span class=\"keyword\">for</span> i, center <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(centers):</span><br><span class=\"line\">    data[i * data_num:(i + <span class=\"number\">1</span>) * data_num] = get_random_data(center, data_num)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 显示数据</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(data.shape)  <span class=\"comment\"># (1500, 2)</span></span><br><span class=\"line\">plt.scatter(data[:, <span class=\"number\">0</span>], data[:, <span class=\"number\">1</span>], s=<span class=\"number\">5</span>, color=<span class=\"string\">&#x27;c&#x27;</span>)</span><br><span class=\"line\">plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># ---------- KMeans聚类 ----------</span></span><br><span class=\"line\">kms = KMeans(n_clusters=<span class=\"number\">3</span>, init=<span class=\"string\">&#x27;k-means++&#x27;</span>)  <span class=\"comment\"># 聚出3类</span></span><br><span class=\"line\">kms.fit(data)  <span class=\"comment\"># 模型拟合</span></span><br><span class=\"line\">centers = kms.cluster_centers_  <span class=\"comment\"># 计算聚类中心</span></span><br><span class=\"line\">labels = kms.labels_  <span class=\"comment\"># 聚类后每个样本的类别标签</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># ---------- MiniBatchKMeans聚类 ----------</span></span><br><span class=\"line\"><span class=\"comment\"># mbk = MiniBatchKMeans(init=&#x27;k-means++&#x27;, n_clusters=3, batch_size=32, random_state=0)</span></span><br><span class=\"line\"><span class=\"comment\"># mbk.fit(data)</span></span><br><span class=\"line\"><span class=\"comment\"># centers = mbk.cluster_centers_</span></span><br><span class=\"line\"><span class=\"comment\"># labels = mbk.labels_</span></span><br><span class=\"line\"></span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">8</span>, <span class=\"number\">6</span>))</span><br><span class=\"line\">plt.scatter(data[:, <span class=\"number\">0</span>], data[:, <span class=\"number\">1</span>], s=<span class=\"number\">5</span>, c=labels, cmap=<span class=\"string\">&#x27;Accent&#x27;</span>, alpha=<span class=\"number\">0.5</span>)  <span class=\"comment\"># 画出聚类后带标签的样本</span></span><br><span class=\"line\">plt.scatter(centers[:, <span class=\"number\">0</span>], centers[:, <span class=\"number\">1</span>], marker=<span class=\"string\">&#x27;*&#x27;</span>, s=<span class=\"number\">120</span>, c=[<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>], cmap=<span class=\"string\">&#x27;Accent&#x27;</span>)  <span class=\"comment\"># 画出聚类中心</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-高维数据PCA后聚类\">2. 高维数据PCA后聚类</h2>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 产生高维数据</span></span><br><span class=\"line\">hd_centers = [</span><br><span class=\"line\">    [<span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">6</span>, <span class=\"number\">1</span>, <span class=\"number\">5</span>, <span class=\"number\">3</span>],</span><br><span class=\"line\">    [<span class=\"number\">5</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">5</span>, <span class=\"number\">5</span>, <span class=\"number\">9</span>, <span class=\"number\">3</span>, <span class=\"number\">6</span>, <span class=\"number\">6</span>, <span class=\"number\">5</span>],</span><br><span class=\"line\">    [<span class=\"number\">3</span>, <span class=\"number\">8</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>, <span class=\"number\">7</span>, <span class=\"number\">1</span>, <span class=\"number\">8</span>, <span class=\"number\">8</span>, <span class=\"number\">5</span>, <span class=\"number\">4</span>]</span><br><span class=\"line\">]</span><br><span class=\"line\">hd_center_num = <span class=\"built_in\">len</span>(hd_centers)</span><br><span class=\"line\">hd_data_num = <span class=\"number\">500</span></span><br><span class=\"line\"></span><br><span class=\"line\">hd_data = np.zeros([hd_data_num * hd_center_num, <span class=\"number\">10</span>])</span><br><span class=\"line\"><span class=\"keyword\">for</span> i, center <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(hd_centers):</span><br><span class=\"line\">    hd_data[i * hd_data_num:(i + <span class=\"number\">1</span>) * hd_data_num] = get_random_data(center, hd_data_num, data_dim=<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(hd_data.shape)  <span class=\"comment\"># (1500, 10)</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># ---------- PCA主成分分析降维 ----------</span></span><br><span class=\"line\">pca = PCA(n_components=<span class=\"number\">2</span>)  <span class=\"comment\"># 降成二维</span></span><br><span class=\"line\">pca.fit(hd_data)</span><br><span class=\"line\">pca_data = pca.transform(hd_data)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(pca_data.shape)  <span class=\"comment\"># (1500, 2)</span></span><br><span class=\"line\">plt.scatter(pca_data[:, <span class=\"number\">0</span>], pca_data[:, <span class=\"number\">1</span>], s=<span class=\"number\">5</span>, color=<span class=\"string\">&#x27;c&#x27;</span>)</span><br><span class=\"line\">plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># ---------- KMeans聚类 ----------</span></span><br><span class=\"line\">kms = KMeans(n_clusters=<span class=\"number\">3</span>, init=<span class=\"string\">&#x27;k-means++&#x27;</span>)</span><br><span class=\"line\">kms.fit(pca_data)  <span class=\"comment\"># 模型拟合</span></span><br><span class=\"line\">centers = kms.cluster_centers_  <span class=\"comment\"># 计算聚类中心</span></span><br><span class=\"line\">labels = kms.labels_  <span class=\"comment\"># 聚类后每个样本的类别标签</span></span><br><span class=\"line\"></span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">8</span>, <span class=\"number\">6</span>))</span><br><span class=\"line\">plt.scatter(pca_data[:, <span class=\"number\">0</span>], pca_data[:, <span class=\"number\">1</span>], s=<span class=\"number\">5</span>, c=labels, cmap=<span class=\"string\">&#x27;Accent&#x27;</span>, alpha=<span class=\"number\">0.5</span>)  <span class=\"comment\"># 画出聚类后带标签的样本</span></span><br><span class=\"line\">plt.scatter(centers[:, <span class=\"number\">0</span>], centers[:, <span class=\"number\">1</span>], marker=<span class=\"string\">&#x27;*&#x27;</span>, s=<span class=\"number\">120</span>, c=[<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>], cmap=<span class=\"string\">&#x27;Accent&#x27;</span>)  <span class=\"comment\"># 画出聚类中心</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-高维数据聚类并计算与中心的相似度\">3. 高维数据聚类并计算与中心的相似度</h2>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 产生高维数据</span></span><br><span class=\"line\">hd_centers = [</span><br><span class=\"line\">    [<span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">6</span>, <span class=\"number\">1</span>, <span class=\"number\">5</span>, <span class=\"number\">3</span>],</span><br><span class=\"line\">    [<span class=\"number\">5</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">5</span>, <span class=\"number\">5</span>, <span class=\"number\">9</span>, <span class=\"number\">3</span>, <span class=\"number\">6</span>, <span class=\"number\">6</span>, <span class=\"number\">5</span>],</span><br><span class=\"line\">    [<span class=\"number\">3</span>, <span class=\"number\">8</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>, <span class=\"number\">7</span>, <span class=\"number\">1</span>, <span class=\"number\">8</span>, <span class=\"number\">8</span>, <span class=\"number\">5</span>, <span class=\"number\">4</span>]</span><br><span class=\"line\">]</span><br><span class=\"line\">hd_center_num = <span class=\"built_in\">len</span>(hd_centers)</span><br><span class=\"line\">hd_data_num = <span class=\"number\">500</span></span><br><span class=\"line\"></span><br><span class=\"line\">hd_data = np.zeros([hd_data_num * hd_center_num, <span class=\"number\">10</span>])</span><br><span class=\"line\"><span class=\"keyword\">for</span> i, center <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(hd_centers):</span><br><span class=\"line\">    hd_data[i * hd_data_num:(i + <span class=\"number\">1</span>) * hd_data_num] = get_random_data(center, hd_data_num, data_dim=<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(hd_data.shape)  <span class=\"comment\"># (1500, 10)</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 对原始高维数据聚类</span></span><br><span class=\"line\">kms = KMeans(n_clusters=<span class=\"number\">3</span>, init=<span class=\"string\">&#x27;k-means++&#x27;</span>)</span><br><span class=\"line\">kms.fit(hd_data)  <span class=\"comment\"># 模型拟合</span></span><br><span class=\"line\">centers = kms.cluster_centers_  <span class=\"comment\"># 计算聚类中心</span></span><br><span class=\"line\">labels = kms.labels_  <span class=\"comment\"># 聚类后每个样本的类别标签</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(centers.shape)  <span class=\"comment\"># (3, 10)，3个聚类中心的高维向量</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(labels[:<span class=\"number\">10</span>])  <span class=\"comment\"># [1 1 1 1 1 1 1 1 1 1]，第1类的标签</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(labels[<span class=\"number\">500</span>:<span class=\"number\">510</span>])  <span class=\"comment\"># [2 2 2 2 2 2 2 2 2 2]，第2类的标签</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(labels[<span class=\"number\">1490</span>:])  <span class=\"comment\"># [0 0 0 0 0 0 0 0 0 0]，第3类的标签</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算每个向量分别与3个聚类中心的余弦相似度</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">CosineSimilarity</span>(<span class=\"params\">x, y</span>):</span><br><span class=\"line\">    normalized_x = x / np.linalg.norm(x, axis=-<span class=\"number\">1</span>, keepdims=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    normalized_y = y / np.linalg.norm(y, axis=-<span class=\"number\">1</span>, keepdims=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.<span class=\"built_in\">sum</span>(normalized_x * normalized_y, axis=-<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">data_cnt = hd_data_num * hd_center_num</span><br><span class=\"line\">true_cnt = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i, data <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(hd_data):</span><br><span class=\"line\">    score_0 = CosineSimilarity(data, centers[<span class=\"number\">0</span>])</span><br><span class=\"line\">    score_1 = CosineSimilarity(data, centers[<span class=\"number\">1</span>])</span><br><span class=\"line\">    score_2 = CosineSimilarity(data, centers[<span class=\"number\">2</span>])</span><br><span class=\"line\">    max_idx = np.argmax([score_0, score_1, score_2])</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;ID: <span class=\"subst\">&#123;i&#125;</span>, Score0: <span class=\"subst\">&#123;score_0:<span class=\"number\">.2</span>f&#125;</span>, Score1: <span class=\"subst\">&#123;score_1:<span class=\"number\">.2</span>f&#125;</span>, Score2: <span class=\"subst\">&#123;score_2:<span class=\"number\">.2</span>f&#125;</span>, Pred_label: <span class=\"subst\">&#123;max_idx&#125;</span>&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> i &lt; <span class=\"number\">500</span>:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> max_idx == <span class=\"number\">1</span>:</span><br><span class=\"line\">            true_cnt = true_cnt + <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">elif</span> i &lt; <span class=\"number\">1000</span>:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> max_idx == <span class=\"number\">2</span>:</span><br><span class=\"line\">            true_cnt = true_cnt + <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> max_idx == <span class=\"number\">0</span>:</span><br><span class=\"line\">            true_cnt = true_cnt + <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"comment\"># ID: 0, Score0: 0.85, Score1: 0.98, Score2: 0.85, Pred_label: 1</span></span><br><span class=\"line\"><span class=\"comment\"># ID: 1, Score0: 0.84, Score1: 0.92, Score2: 0.79, Pred_label: 1</span></span><br><span class=\"line\"><span class=\"comment\"># ID: 2, Score0: 0.87, Score1: 0.93, Score2: 0.90, Pred_label: 1</span></span><br><span class=\"line\"><span class=\"comment\"># ...</span></span><br><span class=\"line\"><span class=\"comment\"># ID: 500, Score0: 0.85, Score1: 0.78, Score2: 0.99, Pred_label: 2</span></span><br><span class=\"line\"><span class=\"comment\"># ID: 501, Score0: 0.85, Score1: 0.75, Score2: 0.98, Pred_label: 2</span></span><br><span class=\"line\"><span class=\"comment\"># ID: 502, Score0: 0.83, Score1: 0.71, Score2: 0.98, Pred_label: 2</span></span><br><span class=\"line\"><span class=\"comment\"># ...</span></span><br><span class=\"line\"><span class=\"comment\"># ID: 1000, Score0: 0.99, Score1: 0.81, Score2: 0.87, Pred_label: 0</span></span><br><span class=\"line\"><span class=\"comment\"># ID: 1001, Score0: 0.98, Score1: 0.77, Score2: 0.83, Pred_label: 0</span></span><br><span class=\"line\"><span class=\"comment\"># ID: 1002, Score0: 0.99, Score1: 0.76, Score2: 0.87, Pred_label: 0</span></span><br><span class=\"line\"><span class=\"comment\"># ...</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;All: <span class=\"subst\">&#123;data_cnt&#125;</span>, True: <span class=\"subst\">&#123;true_cnt&#125;</span>, Acc: <span class=\"subst\">&#123;true_cnt / data_cnt:<span class=\"number\">.2</span>f&#125;</span>&#x27;</span>)  <span class=\"comment\"># All: 1500, True: 1499, Acc: 1.00</span></span><br></pre></td></tr></table></figure>\n",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/21944.html",
            "url": "https://asanosaki.github.io/posts/21944.html",
            "title": "分割图像的着色与相似度匹配",
            "date_published": "2023-08-08T06:14:00.000Z",
            "content_html": "<blockquote>\n<p>介绍图像分割后产生的 Mask 灰度图的着色以及相似度匹配计算。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-分割图像着色\">1. 分割图像着色</h2>\n<p>以 SAM 分割为例，我们分割出来产生的 <code>masks</code> 为一个 List，长度为分割出来的类别数，List 中的每个元素为一个 Dict，记录了分割目标的面积、边界框等信息，其中的 <code>segmentation</code> 字段为分割出来的二值图，宽高与原图一致，目标像素点为 True，否则为 False。</p>\n<p>我们实现两种方式分别对分割出来的 <code>masks</code> 以及保存下来的若干张分割图像进行合并与上色。灰度图像和伪彩色图像都对应一个索引表，这个索引表又叫调色板。图像的像素值就是索引，灰度图的索引表为：</p>\n<table>\n    <thead>\n        <tr>\n            <th>像素值</th>\n            <th>R</th>\n            <th>G</th>\n            <th>B</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>0</td>\n            <td>0</td>\n            <td>0</td>\n            <td>0</td>\n        </tr>\n        <tr>\n            <td>1</td>\n            <td>1</td>\n            <td>1</td>\n            <td>1</td>\n        </tr>\n        <tr>\n            <td>2</td>\n            <td>2</td>\n            <td>2</td>\n            <td>2</td>\n        </tr>\n        <tr>\n            <td>3</td>\n            <td>3</td>\n            <td>3</td>\n            <td>3</td>\n        </tr>\n        <tr>\n            <td>...</td>\n            <td>...</td>\n            <td>...</td>\n            <td>...</td>\n        </tr>\n        <tr>\n            <td>255</td>\n            <td>255</td>\n            <td>255</td>\n            <td>255</td>\n        </tr>\n    </tbody>\n</table>\n<p>索引表不同的像素值对应的 RGB 值就是该像素的颜色，灰度图像的索引表中的 RGB 值都与像素值相同。同理，只要修改这些 RGB 数值，就可以显示伪彩色图像了。注意调色板的索引从0-255，因此，调色板的每个索引对应的 RGB 值都要进行设置。</p>\n<p>代码如下：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 调色板</span></span><br><span class=\"line\">_palette = []</span><br><span class=\"line\">color_num = <span class=\"number\">100</span>  <span class=\"comment\"># 不同的颜色数量</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(color_num // <span class=\"number\">4</span>):</span><br><span class=\"line\">    _palette.append([(<span class=\"number\">255</span> + i * <span class=\"number\">8</span>) % <span class=\"number\">256</span>, (<span class=\"number\">50</span> + i * <span class=\"number\">8</span>) % <span class=\"number\">256</span>, (<span class=\"number\">100</span> + i * <span class=\"number\">8</span>) % <span class=\"number\">256</span>])</span><br><span class=\"line\">    _palette.append([(<span class=\"number\">100</span> + i * <span class=\"number\">8</span>) % <span class=\"number\">256</span>, (<span class=\"number\">50</span> + i * <span class=\"number\">8</span>) % <span class=\"number\">256</span>, (<span class=\"number\">255</span> + i * <span class=\"number\">8</span>) % <span class=\"number\">256</span>])</span><br><span class=\"line\">    _palette.append([(<span class=\"number\">33</span> + i * <span class=\"number\">8</span>) % <span class=\"number\">256</span>, (<span class=\"number\">133</span> + i * <span class=\"number\">8</span>) % <span class=\"number\">256</span>, (<span class=\"number\">233</span> + i * <span class=\"number\">8</span>) % <span class=\"number\">256</span>])</span><br><span class=\"line\">    _palette.append([(<span class=\"number\">68</span> + i * <span class=\"number\">8</span>) % <span class=\"number\">256</span>, (<span class=\"number\">218</span> + i * <span class=\"number\">8</span>) % <span class=\"number\">256</span>, (<span class=\"number\">138</span> + i * <span class=\"number\">8</span>) % <span class=\"number\">256</span>])</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(color_num, <span class=\"number\">256</span>):  <span class=\"comment\"># 补上后面的灰度值索引</span></span><br><span class=\"line\">    _palette.append([i, i, i])</span><br><span class=\"line\">color_palette = np.array(_palette, dtype=<span class=\"string\">&#x27;uint8&#x27;</span>).reshape(-<span class=\"number\">1</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(color_palette.shape)  # (256, 3)</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 给mask上色</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">colorize_mask</span>(<span class=\"params\">mask</span>):</span><br><span class=\"line\">    mask = Image.fromarray(mask.astype(np.uint8))</span><br><span class=\"line\">    mask = mask.convert(mode=<span class=\"string\">&#x27;P&#x27;</span>)</span><br><span class=\"line\">    mask.putpalette(color_palette)</span><br><span class=\"line\">    <span class=\"comment\"># mask = mask.convert(mode=&#x27;RGB&#x27;)</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> mask</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 给mask上色并保存</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">save_colorize_mask</span>(<span class=\"params\">mask, output_dir, file_name</span>):</span><br><span class=\"line\">    save_mask = colorize_mask(mask)</span><br><span class=\"line\">    save_mask.save(os.path.join(output_dir, file_name))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 显示SAM分割出来的masks</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">show_origin_masks</span>(<span class=\"params\">masks</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(masks) == <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span></span><br><span class=\"line\">    sorted_masks = <span class=\"built_in\">sorted</span>(masks, key=(<span class=\"keyword\">lambda</span> x: x[<span class=\"string\">&#x27;area&#x27;</span>]), reverse=<span class=\"literal\">True</span>)  <span class=\"comment\"># 按面积从大到小排序</span></span><br><span class=\"line\">    <span class=\"comment\"># ax = plt.gca()  # 在原本的图片上绘制</span></span><br><span class=\"line\">    <span class=\"comment\"># ax.set_autoscale_on(False)  # 在原本的图片上绘制</span></span><br><span class=\"line\"></span><br><span class=\"line\">    img = np.ones((sorted_masks[<span class=\"number\">0</span>][<span class=\"string\">&#x27;segmentation&#x27;</span>].shape[<span class=\"number\">0</span>], sorted_masks[<span class=\"number\">0</span>][<span class=\"string\">&#x27;segmentation&#x27;</span>].shape[<span class=\"number\">1</span>], <span class=\"number\">4</span>))  <span class=\"comment\"># img.shape: (1080, 1920, 4)</span></span><br><span class=\"line\">    img[:, :, <span class=\"number\">3</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> mask <span class=\"keyword\">in</span> sorted_masks:</span><br><span class=\"line\">        m = mask[<span class=\"string\">&#x27;segmentation&#x27;</span>]  <span class=\"comment\"># (1080, 1920)的True or False矩阵</span></span><br><span class=\"line\">        color_mask = np.concatenate([np.random.random(<span class=\"number\">3</span>), [<span class=\"number\">0.35</span>]])  <span class=\"comment\"># 第4维表示透明度</span></span><br><span class=\"line\">        img[m] = color_mask</span><br><span class=\"line\">    <span class=\"comment\"># ax.imshow(img)  # 在原本的图片上绘制</span></span><br><span class=\"line\">    plt.axis(<span class=\"string\">&#x27;off&#x27;</span>)</span><br><span class=\"line\">    plt.imshow(img)</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 显示目录img_masks_path中的masks</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">show_img_masks</span>(<span class=\"params\">masks_dir, height, width, threshold</span>):</span><br><span class=\"line\">    mask = np.zeros((height, width))</span><br><span class=\"line\">    img_mask_names = os.listdir(masks_dir)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> idx, img_mask_name <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(img_mask_names):</span><br><span class=\"line\">        img_mask = Image.<span class=\"built_in\">open</span>(os.path.join(masks_dir, img_mask_name))</span><br><span class=\"line\">        img_mask = np.asarray(img_mask, dtype=np.bool_)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> np.<span class=\"built_in\">sum</span>(img_mask) &lt; threshold:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        mask[img_mask] = idx + <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">    Image.fromarray(mask).show()  <span class=\"comment\"># 上色前的mask</span></span><br><span class=\"line\">    mask = colorize_mask(mask)</span><br><span class=\"line\">    mask.show()  <span class=\"comment\"># 上色后的mask</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 读取原始图像</span></span><br><span class=\"line\">image = Image.<span class=\"built_in\">open</span>(<span class=\"string\">&#x27;../images/people.jpg&#x27;</span>)</span><br><span class=\"line\">width, height = image.size</span><br><span class=\"line\">image.show()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 读取SAM分割的若干masks图像进行合并上色显示</span></span><br><span class=\"line\">masks_dir = <span class=\"string\">&#x27;../images/people_sam/&#x27;</span></span><br><span class=\"line\">threshold = <span class=\"number\">200</span></span><br><span class=\"line\">show_img_masks(masks_dir, height, width, threshold)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 展示上色前后的mask</span></span><br><span class=\"line\">test_mask_path = <span class=\"string\">&#x27;../images/test_mask.png&#x27;</span></span><br><span class=\"line\">test_mask = Image.<span class=\"built_in\">open</span>(test_mask_path)</span><br><span class=\"line\">test_mask.show()</span><br><span class=\"line\">test_mask = np.array(test_mask, dtype=np.uint8)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(test_mask.shape)  <span class=\"comment\"># (1080, 1920)</span></span><br><span class=\"line\"></span><br><span class=\"line\">mask_gray = Image.fromarray(test_mask, <span class=\"string\">&#x27;P&#x27;</span>)</span><br><span class=\"line\">mask_gray.show()</span><br><span class=\"line\">mask_color = colorize_mask(test_mask)</span><br><span class=\"line\">mask_color.show()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">list</span>(np.unique(mask_color)))  <span class=\"comment\"># [0, 1, 2, 3, 4, 5, 6, ..., 44]，可以看成类别</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"2-分割图相似度匹配\">2. 分割图相似度匹配</h2>\n<p><strong>区域相似度</strong>（Region Similarity）：为了测量基于区域的分割相似度，即错误像素的数量，我们使用 Jaccard 索引 𝒥 表示， 𝒥 定义为预测的分割输出 Mask 和真值 Mask 之间的交并比 IoU（Intersection over Union），Jaccard 索引提供了关于错误分类像素的直观的信息。</p>\n<p><strong>边沿精度</strong>（Contour Accuracy）：边沿精度即计算 F-score，F-score 评估的是预测 Mask 的边界是否与真值 Mask 的边界对应。首先应提取预测 Mask 和真值 Mask 的边界元素坐标，将边界上的元素置为 True，非边界的元素置为 False。F-score 被定义为<strong>精度</strong>和<strong>召回率</strong>的调和平均数。</p>\n<p><strong>精度</strong>（Precision，P，也称查准率）：分母应是<strong>预测</strong> Mask 的边界元素总数，分子则是在预测 Mask 为边界的那些元素中真正属于真值的。换句话说，预测 Mask 假设有100个元素为边界元素，但实际上可能只有70个存在于真值图的对应位置上，即70个真值的正样本被正确（True）预测为 Positive，属于 True Positive（TP），所以此时的查准率为70%，剩下的30个元素是错误（False）预测为 Positive，属于 False Positive（FP）。</p>\n<p><strong>召回率</strong>（Recall，R，也称查全率）：分母是<strong>真值</strong> Mask 的边界元素总数，分子表示多少个本质的正样本被预测出来。例如真值 Mask 的边界有140个元素，但实际的预测 Mask 中只有70个真值的正样本被正确（True）预测为 Positive（TP），还有70个被错误（False）预测为 Negative（False Negative），那么此时的 Recall 为50%。</p>\n<p>J &amp; F 指标的计算代码如下：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\"><span class=\"keyword\">from</span> skimage.morphology <span class=\"keyword\">import</span> binary_dilation, disk</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">db_eval_iou</span>(<span class=\"params\">annotation, segmentation</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    Compute region similarity as the Jaccard Index.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">        annotation   (ndarray): binary annotation   map.</span></span><br><span class=\"line\"><span class=\"string\">        segmentation (ndarray): binary segmentation map.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Return:</span></span><br><span class=\"line\"><span class=\"string\">        jaccard (float): region similarity</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    annotation = annotation.astype(np.bool_)</span><br><span class=\"line\">    segmentation = segmentation.astype(np.bool_)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> np.isclose(np.<span class=\"built_in\">sum</span>(annotation), <span class=\"number\">0</span>) <span class=\"keyword\">and</span> np.isclose(np.<span class=\"built_in\">sum</span>(segmentation), <span class=\"number\">0</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> np.<span class=\"built_in\">sum</span>((annotation &amp; segmentation)) / np.<span class=\"built_in\">sum</span>((annotation | segmentation), dtype=np.float32)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">db_eval_boundary</span>(<span class=\"params\">foreground_mask, gt_mask, bound_th=<span class=\"number\">0.008</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    Compute mean, recall and decay from per-frame evaluation.</span></span><br><span class=\"line\"><span class=\"string\">    Calculates precision/recall for boundaries between foreground_mask and</span></span><br><span class=\"line\"><span class=\"string\">    gt_mask using morphological operators to speed it up.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">        foreground_mask (ndarray): binary segmentation image.</span></span><br><span class=\"line\"><span class=\"string\">        gt_mask         (ndarray): binary annotated image.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">        F (float): boundaries F-measure</span></span><br><span class=\"line\"><span class=\"string\">        P (float): boundaries precision</span></span><br><span class=\"line\"><span class=\"string\">        R (float): boundaries recall</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> np.atleast_3d(foreground_mask).shape[<span class=\"number\">2</span>] == <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">    bound_pix = bound_th <span class=\"keyword\">if</span> bound_th &gt;= <span class=\"number\">1</span> <span class=\"keyword\">else</span> \\</span><br><span class=\"line\">        np.ceil(bound_th * np.linalg.norm(foreground_mask.shape))  <span class=\"comment\"># np.linalg.norm计算范数，默认为L2范数</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Get the pixel boundaries of both masks</span></span><br><span class=\"line\">    fg_boundary = seg2bmap(foreground_mask)  <span class=\"comment\"># 将边界置为True</span></span><br><span class=\"line\">    gt_boundary = seg2bmap(gt_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">    fg_dil = binary_dilation(fg_boundary, disk(bound_pix))  <span class=\"comment\"># 二值化膨胀</span></span><br><span class=\"line\">    gt_dil = binary_dilation(gt_boundary, disk(bound_pix))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Get the intersection</span></span><br><span class=\"line\">    gt_match = gt_boundary * fg_dil  <span class=\"comment\"># 计算GT中与FG边缘匹配的像素</span></span><br><span class=\"line\">    fg_match = fg_boundary * gt_dil  <span class=\"comment\"># 计算FG中与GT边缘匹配的像素</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Area of the intersection</span></span><br><span class=\"line\">    n_fg = np.<span class=\"built_in\">sum</span>(fg_boundary)  <span class=\"comment\"># FG边缘像素数量</span></span><br><span class=\"line\">    n_gt = np.<span class=\"built_in\">sum</span>(gt_boundary)  <span class=\"comment\"># GT边缘像素数量</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">#% Compute precision and recall</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> n_fg == <span class=\"number\">0</span> <span class=\"keyword\">and</span> n_gt &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">        precision = <span class=\"number\">1</span></span><br><span class=\"line\">        recall = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">elif</span> n_fg &gt; <span class=\"number\">0</span> <span class=\"keyword\">and</span> n_gt == <span class=\"number\">0</span>:</span><br><span class=\"line\">        precision = <span class=\"number\">0</span></span><br><span class=\"line\">        recall = <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">elif</span> n_fg == <span class=\"number\">0</span> <span class=\"keyword\">and</span> n_gt == <span class=\"number\">0</span>:</span><br><span class=\"line\">        precision = <span class=\"number\">1</span></span><br><span class=\"line\">        recall = <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        precision = np.<span class=\"built_in\">sum</span>(fg_match) / <span class=\"built_in\">float</span>(n_fg)</span><br><span class=\"line\">        recall = np.<span class=\"built_in\">sum</span>(gt_match) / <span class=\"built_in\">float</span>(n_gt)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Compute F measure</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> precision + recall == <span class=\"number\">0</span>:</span><br><span class=\"line\">        F_score = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        F_score = <span class=\"number\">2</span> * precision * recall / (precision + recall)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> F_score</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">seg2bmap</span>(<span class=\"params\">seg, width=<span class=\"literal\">None</span>, height=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    From a segmentation, compute a binary boundary map with 1 pixel wide</span></span><br><span class=\"line\"><span class=\"string\">    boundaries.  The boundary pixels are offset by 1/2 pixel towards the</span></span><br><span class=\"line\"><span class=\"string\">    origin from the actual segment boundary.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">        seg     : Segments labeled from 1..k.</span></span><br><span class=\"line\"><span class=\"string\">        width      :\tWidth of desired bmap  &lt;= seg.shape[1]</span></span><br><span class=\"line\"><span class=\"string\">        height  :    Height of desired bmap &lt;= seg.shape[0]</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">        bmap (ndarray):    Binary boundary map.</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    seg = seg.astype(np.bool_)</span><br><span class=\"line\">    seg[seg &gt; <span class=\"number\">0</span>] = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> np.atleast_3d(seg).shape[<span class=\"number\">2</span>] == <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">    width = seg.shape[<span class=\"number\">1</span>] <span class=\"keyword\">if</span> width <span class=\"keyword\">is</span> <span class=\"literal\">None</span> <span class=\"keyword\">else</span> width</span><br><span class=\"line\">    height = seg.shape[<span class=\"number\">0</span>] <span class=\"keyword\">if</span> height <span class=\"keyword\">is</span> <span class=\"literal\">None</span> <span class=\"keyword\">else</span> height</span><br><span class=\"line\"></span><br><span class=\"line\">    h,w = seg.shape[:<span class=\"number\">2</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    ar1 = <span class=\"built_in\">float</span>(width) / <span class=\"built_in\">float</span>(height)</span><br><span class=\"line\">    ar2 = <span class=\"built_in\">float</span>(w) / <span class=\"built_in\">float</span>(h)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"keyword\">not</span> (width &gt; w | height &gt; h | <span class=\"built_in\">abs</span>(ar1 - ar2) &gt; <span class=\"number\">0.01</span>),\\</span><br><span class=\"line\">            <span class=\"string\">&#x27;Can&#x27;</span><span class=\"string\">&#x27;t convert %dx%d seg to %dx%d bmap.&#x27;</span>%(w, h, width, height)</span><br><span class=\"line\"></span><br><span class=\"line\">    e = np.zeros_like(seg)</span><br><span class=\"line\">    s = np.zeros_like(seg)</span><br><span class=\"line\">    se = np.zeros_like(seg)</span><br><span class=\"line\"></span><br><span class=\"line\">    e[:, :-<span class=\"number\">1</span>] = seg[:, <span class=\"number\">1</span>:]</span><br><span class=\"line\">    s[:-<span class=\"number\">1</span>, :] = seg[<span class=\"number\">1</span>:, :]</span><br><span class=\"line\">    se[:-<span class=\"number\">1</span>, :-<span class=\"number\">1</span>] = seg[<span class=\"number\">1</span>:, <span class=\"number\">1</span>:]</span><br><span class=\"line\"></span><br><span class=\"line\">    b = seg^e | seg^s | seg^se</span><br><span class=\"line\">    b[-<span class=\"number\">1</span>, :] = seg[-<span class=\"number\">1</span>, :]^e[-<span class=\"number\">1</span>, :]</span><br><span class=\"line\">    b[:, -<span class=\"number\">1</span>] = seg[:, -<span class=\"number\">1</span>]^s[:, -<span class=\"number\">1</span>]</span><br><span class=\"line\">    b[-<span class=\"number\">1</span>, -<span class=\"number\">1</span>] = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> w == width <span class=\"keyword\">and</span> h == height:</span><br><span class=\"line\">        bmap = b</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        bmap = np.zeros((height, width))</span><br><span class=\"line\">        <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(w):</span><br><span class=\"line\">            <span class=\"keyword\">for</span> y <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(h):</span><br><span class=\"line\">                <span class=\"keyword\">if</span> b[y, x]:</span><br><span class=\"line\">                    j = <span class=\"number\">1</span> + math.floor((y - <span class=\"number\">1</span>) + height / h)</span><br><span class=\"line\">                    i = <span class=\"number\">1</span> + math.floor((x - <span class=\"number\">1</span>) + width / h)</span><br><span class=\"line\">                    bmap[j, i] = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> bmap</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">database_path = <span class=\"string\">&#x27;../data/mask_database/&#x27;</span></span><br><span class=\"line\">test_path = <span class=\"string\">&#x27;../data/mask_test/&#x27;</span></span><br><span class=\"line\">database_img_name_list = os.listdir(database_path)</span><br><span class=\"line\">test_img_name_list = os.listdir(test_path)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> test_img_name <span class=\"keyword\">in</span> test_img_name_list:</span><br><span class=\"line\">    test_img = Image.<span class=\"built_in\">open</span>(test_path + test_img_name)</span><br><span class=\"line\">    h, w = test_img.size</span><br><span class=\"line\">    test_img = np.asarray(test_img)</span><br><span class=\"line\"></span><br><span class=\"line\">    best_iou, best_F, best_iou_dbname, best_F_dbname = <span class=\"number\">0.0</span>, <span class=\"number\">0.0</span>, <span class=\"literal\">None</span>, <span class=\"literal\">None</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> database_img_name <span class=\"keyword\">in</span> database_img_name_list:</span><br><span class=\"line\">        database_img = Image.<span class=\"built_in\">open</span>(database_path + database_img_name).resize((h, w))</span><br><span class=\"line\">        database_img = np.asarray(database_img)</span><br><span class=\"line\"></span><br><span class=\"line\">        iou = db_eval_iou(test_img, database_img)</span><br><span class=\"line\">        F_score = db_eval_boundary(test_img, database_img)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> iou &gt; best_iou:</span><br><span class=\"line\">            best_iou = iou</span><br><span class=\"line\">            best_iou_dbname = database_img_name</span><br><span class=\"line\">        <span class=\"keyword\">if</span> F_score &gt; best_F:</span><br><span class=\"line\">            best_F = F_score</span><br><span class=\"line\">            best_F_dbname = database_img_name</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;[<span class=\"subst\">&#123;test_img_name&#125;</span>] best iou: <span class=\"subst\">&#123;best_iou:<span class=\"number\">.4</span>f&#125;</span> (<span class=\"subst\">&#123;best_iou_dbname&#125;</span>), best F: <span class=\"subst\">&#123;best_F:<span class=\"number\">.4</span>f&#125;</span> (<span class=\"subst\">&#123;best_F_dbname&#125;</span>)&#x27;</span>)</span><br><span class=\"line\">    <span class=\"comment\"># [0_70.png] best iou: 0.7433 (0_9_156_197_151.png), best F: 0.2681 (0_2_152_212_77.png)</span></span><br><span class=\"line\">    <span class=\"comment\"># [0_71.png] best iou: 0.9399 (0_3_140_238_157.png), best F: 0.7001 (0_3_140_238_157.png)</span></span><br><span class=\"line\">    <span class=\"comment\"># [0_72.png] best iou: 0.7066 (0_10_190_185_95.png), best F: 0.2735 (0_7_93_244_223.png)</span></span><br><span class=\"line\">    <span class=\"comment\"># [0_75.png] best iou: 0.9089 (0_5_241_130_227.png), best F: 0.5160 (0_5_241_130_227.png)</span></span><br><span class=\"line\">    <span class=\"comment\"># [0_77.png] best iou: 0.5177 (0_18_252_79_113.png), best F: 0.2171 (0_1_245_116_182.png)</span></span><br><span class=\"line\">    <span class=\"comment\"># [0_78.png] best iou: 0.7393 (0_4_251_231_252.png), best F: 0.2872 (0_9_156_197_151.png)</span></span><br></pre></td></tr></table></figure>\n",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/6828.html",
            "url": "https://asanosaki.github.io/posts/6828.html",
            "title": "DeAOT视频追踪论文阅读笔记",
            "date_published": "2023-08-08T05:58:00.000Z",
            "content_html": "<blockquote>\n<p>本文记录 DeAOT 视频追踪论文的阅读笔记。<br>\n涉及的相关知识点为：AOT（Associating Objects with Transformers for Video Object Segmentation）、DeAOT（Decoupling Features in Hierarchical Propagation for Video Object Segmentation）、FPN（Feature Pyramid Networks for Object Detection）、Depth-wise Convolution、DropPath、GroupNorm。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-相关知识\">1. 相关知识</h2>\n<h3 id=\"1-1-Depth-wise卷积\">1.1 Depth-wise卷积</h3>\n<p>Depth-wise（DW）卷积与 Point-wise（PW）卷积，合起来被称作 Depth-wise Separable Convolution，该结构和常规卷积操作类似，可用来提取特征，但相比于常规卷积操作，其<strong>参数量和运算成本较低</strong>。所以在一些轻量级网络中会碰到这种结构，如 MobileNet。</p>\n<p>Depth-wise Convolution 的一个卷积核负责一个通道，即一个通道只被一个单通道的卷积核卷积，而常规卷积每个卷积核是同时操作输入图片的每个通道，即每个卷积核的通道数与图片的通道数相同。</p>\n<p>Depth-wise Convolution 完成后的 Feature Map 数量与输入层的通道数相同，无法扩展 Feature Map。而且这种运算对输入层的每个通道<strong>独立</strong>进行卷积运算，没有有效地利用不同通道在相同空间位置上的特征信息。因此需要Point-wise Convolution 来将这些 Feature Map 进行组合生成新的 Feature Map。</p>\n<p>Depth-wise Convolution 代码如下：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Depth-wise卷积，输出维度和输入维度相同</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">DWConv2d</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, indim, dropout=<span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"comment\"># 当groups=in_channel时，是在做Depth-wise Conv</span></span><br><span class=\"line\">        self.conv = nn.Conv2d(indim, indim, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), padding=<span class=\"number\">1</span>, groups=indim, bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        self.dropout = nn.Dropout2d(p=dropout, inplace=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):  <span class=\"comment\"># x.shape: (bsz, c, h, w)</span></span><br><span class=\"line\">        out = self.dropout(self.conv(x))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out</span><br><span class=\"line\"></span><br><span class=\"line\">dwconv2d = DWConv2d(indim=<span class=\"number\">3</span>)</span><br><span class=\"line\">x = torch.randn(<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">10</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dwconv2d(x).shape)  <span class=\"comment\"># torch.Size([1, 3, 10, 10])</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"1-2-DropPath\">1.2 DropPath</h3>\n<p>DropPath 是一种针对<strong>分支</strong>网络而提出的网络正则化方法，作用是将深度学习网络中的多分支结构随机删除。DropPath 作用的是网络分支，而 DropOut 作用的是 Feature Map，DropConnect 作用的是参数。</p>\n<p>简单来说，DropPath 的输出是随机将一个 batch 中所有的神经元均设置为0；而在 DropOut 中，是在每个 batch 中随机选择神经元设置为0。</p>\n<p>DropPath 代码如下：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">DropPath</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, drop_prob=<span class=\"literal\">None</span>, batch_dim=<span class=\"number\">0</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(DropPath, self).__init__()</span><br><span class=\"line\">        self.drop_prob = drop_prob  <span class=\"comment\"># 丢弃率，假设是0.5</span></span><br><span class=\"line\">        self.batch_dim = batch_dim  <span class=\"comment\"># batch在第几维，假设是0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.drop_path(x, self.drop_prob)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">drop_path</span>(<span class=\"params\">self, x, drop_prob</span>):  <span class=\"comment\"># x.shape: (hw, bsz, c)</span></span><br><span class=\"line\">        <span class=\"comment\"># 丢弃率为0或者不是在训练时直接返回x</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> drop_prob == <span class=\"number\">0.</span> <span class=\"keyword\">or</span> <span class=\"keyword\">not</span> self.training:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\">        keep_prob = <span class=\"number\">1</span> - drop_prob  <span class=\"comment\"># 保持率，0.5</span></span><br><span class=\"line\">        shape = [<span class=\"number\">1</span> <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(x.ndim)]  <span class=\"comment\"># [1, 1, 1]</span></span><br><span class=\"line\">        shape[self.batch_dim] = x.shape[self.batch_dim]  <span class=\"comment\"># [bsz, 1, 1]</span></span><br><span class=\"line\">        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)  <span class=\"comment\"># 0~1之间的均匀分布</span></span><br><span class=\"line\">        random_tensor.floor_()  <span class=\"comment\"># 下取整，随机出来的大于等于0.5的数都为1，确定保留哪些batch</span></span><br><span class=\"line\">        output = x.div(keep_prob) * random_tensor  <span class=\"comment\"># 除以keep_prob是为了让训练和测试时的期望保持一致</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br><span class=\"line\"></span><br><span class=\"line\">droppath = DropPath(drop_prob=<span class=\"number\">0.5</span>, batch_dim=<span class=\"number\">0</span>)</span><br><span class=\"line\">a = torch.randn(<span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(droppath(a))</span><br><span class=\"line\"><span class=\"comment\"># tensor([[[-0.6209, -5.4889, -1.9857],</span></span><br><span class=\"line\"><span class=\"comment\">#          [ 0.1626,  6.0644,  0.8875]],</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\">#         [[ 0.0000, -0.0000,  0.0000],</span></span><br><span class=\"line\"><span class=\"comment\">#          [ 0.0000, -0.0000,  0.0000]],</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\">#         [[-0.1041,  0.4921,  0.3389],</span></span><br><span class=\"line\"><span class=\"comment\">#          [-2.0490, -0.0399, -0.1521]]])</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"1-3-Group-Normalization\">1.3 Group Normalization</h3>\n<p>BN 全名是 Batch Normalization，见名知意，其是一种归一化方式，而且是以 batch 的维度做归一化，那么问题就来了，此归一化方式如果使用过小的 batch size 会导致其性能下降，一般来说每个 GPU 上的 batch size 设为32最合适，但是对于一些其他深度学习任务 batch size 往往只有1或2，比如目标检测，图像分割，视频分类上，输入的图像数据很大，较大的 batch size 显存吃不消。</p>\n<p>另外，Batch Normalization 是在 batch 这个维度上做 Normalization，但是这个维度并<strong>不是固定不变的</strong>，比如训练和测试时一般不一样，一般都是训练的时候在训练集上通过<strong>滑动平均</strong>预先计算好平均（mean），和方差（variance）参数，在测试的时候，不再计算这些值，而是直接调用这些预计算好的参数来用。但是，当训练数据和测试数据分布有差别时，训练机上预计算好的数据并不能代表测试数据，这就导致在训练、验证、测试这三个阶段存在 inconsistency（不一致性）。</p>\n<p>Group Normalization（GN）首先将 channel 分为许多组（group），对每一组做归一化，即先将 feature 的维度由 <code>[N, C, H, W]</code> reshape 为 <code>[N, G, C/G, H, W]</code>，归一化的维度为 <code>[C/G, H, W]</code>。事实上，GN 的极端情况就是 LN（Layer Normalization）和 IN（Instance Normalization），分别对应 <code>G = 1</code> 和 <code>G = C</code>，作者在论文中给出 G 设为32较好。</p>\n<p>Group Normalization 代码如下：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 自己实现GN</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">GroupNorm</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, num_channels, num_groups=<span class=\"number\">32</span>, eps=<span class=\"number\">1e-5</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(GroupNorm, self).__init__()</span><br><span class=\"line\">        self.gamma = nn.Parameter(torch.ones(<span class=\"number\">1</span>, num_groups, <span class=\"number\">1</span>))</span><br><span class=\"line\">        self.beta = nn.Parameter(torch.zeros(<span class=\"number\">1</span>, num_groups, <span class=\"number\">1</span>))</span><br><span class=\"line\">        self.num_groups = num_groups</span><br><span class=\"line\">        self.eps = eps</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):  <span class=\"comment\"># x.shape: (N, C, H, W)</span></span><br><span class=\"line\">        N, C, H, W = x.size()</span><br><span class=\"line\">        G = self.num_groups</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> C % G == <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">        x = x.view(N, G, -<span class=\"number\">1</span>)</span><br><span class=\"line\">        mean = x.mean(-<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\">        std = x.std(-<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        x = self.gamma * (x - mean) / (std + self.eps) + self.beta</span><br><span class=\"line\">        x = x.view(N, C, H, W)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\">x = torch.randn(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\">gn = GroupNorm(<span class=\"number\">2</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">x_gn = gn(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x_gn)</span><br><span class=\"line\"><span class=\"comment\"># tensor([[[[-0.1165, -0.5803,  0.7635],</span></span><br><span class=\"line\"><span class=\"comment\">#           [-0.2374, -1.3281,  1.4988]],</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\">#          [[ 0.0982, -0.3936, -1.3941],</span></span><br><span class=\"line\"><span class=\"comment\">#           [-0.5678,  1.2321,  1.0253]]]], grad_fn=&lt;ViewBackward0&gt;)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x_gn.mean((<span class=\"number\">2</span>, <span class=\"number\">3</span>)))  <span class=\"comment\"># tensor([[-4.9671e-09,  9.9341e-09]], grad_fn=&lt;MeanBackward1&gt;)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x_gn.var((<span class=\"number\">2</span>, <span class=\"number\">3</span>)))  <span class=\"comment\"># tensor([[1.0000, 1.0000]], grad_fn=&lt;VarBackward0&gt;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用torch.nn中的GN</span></span><br><span class=\"line\">torch_gn = nn.GroupNorm(num_groups=<span class=\"number\">2</span>, num_channels=<span class=\"number\">2</span>)</span><br><span class=\"line\">x_torch_gn = torch_gn(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x_torch_gn)</span><br><span class=\"line\"><span class=\"comment\"># tensor([[[[-0.1277, -0.6357,  0.8363],</span></span><br><span class=\"line\"><span class=\"comment\">#           [-0.2601, -1.4548,  1.6419]],</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\">#          [[ 0.1076, -0.4312, -1.5272],</span></span><br><span class=\"line\"><span class=\"comment\">#           [-0.6220,  1.3497,  1.1232]]]], grad_fn=&lt;NativeGroupNormBackward0&gt;)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x_torch_gn.mean((<span class=\"number\">2</span>, <span class=\"number\">3</span>)))  <span class=\"comment\"># tensor([[-2.9802e-08,  1.9868e-08]], grad_fn=&lt;MeanBackward1&gt;)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x_torch_gn.var((<span class=\"number\">2</span>, <span class=\"number\">3</span>)))  <span class=\"comment\"># tensor([[1.2000, 1.2000]], grad_fn=&lt;VarBackward0&gt;)</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"1-4-FPN特征金字塔网络\">1.4 FPN特征金字塔网络</h3>\n<p>目标的多尺度一直是目标检测算法极为棘手的问题。像 Fast R-CNN，YOLO 这些只是利用深层网络进行检测的算法，是很难把小目标物体检测好的。因为小目标物体本身的像素就比较少，随着下采样的累积，它的特征更容易被丢失。</p>\n<p>特征金字塔网络（Feature Pyramid Network，FPN）是一个在特征尺度的金字塔操作，它是通过将<strong>自底向上</strong>（Bottom-up）和<strong>自顶向下</strong>（Top-down）的特征图进行<strong>融合</strong>来实现特征金字塔操作的。FPN 提供的是一个特征融合的机制，并没有引入太多的参数，实现了以增加极小计算代价的情况下提升对多尺度目标的检测能力。</p>\n<p>自底向上即是卷积网络的前向过程，我们可以选择不同的骨干网络，例如 ResNet-50 或者 ResNet-101。前向网络的返回值依次是 C2、C3、C4、C5，是每次池化之后得到的 Feature Map。</p>\n<p>通过自底向上路径，FPN 得到了四组 Feature Map。浅层的 Feature Map，例如 C2 含有更多的底层信息（纹理，颜色等），而深层的 Feature Map 如 C5 含有更多的语义信息。为了将这四组倾向不同特征的 Feature Map 组合起来，FPN 使用了自顶向下及<strong>横向连接</strong>的策略，最终得到 P2、P3、P4、P5 四个输出。</p>\n<p>最后，FPN 在 P2、P3、P4、P5 之后均接了一个 3*3 Conv 操作，该卷积操作是为了减轻上采样的<strong>混叠效应</strong>（aliasing effect）。</p>\n<p>FPN 和 U-Net 最大的不同是它的多个层级的都会有各自的输出层，而每个输出层都有不同尺度的感受野。一个比较粗暴的方式是每一层都预测所有的样本，而另一个更好的选择是根据一些可能存在的先验知识选择一个最好的层。</p>\n<p>FPN 代码如下：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Bottleneck</span>(nn.Module):</span><br><span class=\"line\">    expansion = <span class=\"number\">4</span>  <span class=\"comment\"># 残差块第3个卷积层的通道膨胀倍率</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, in_channels, channels, stride=<span class=\"number\">1</span>, downsample=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Bottleneck, self).__init__()</span><br><span class=\"line\">        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=<span class=\"number\">1</span>, bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        self.bn1 = nn.BatchNorm2d(channels)</span><br><span class=\"line\">        self.conv2 = nn.Conv2d(channels, channels, kernel_size=<span class=\"number\">3</span>, stride=stride, padding=<span class=\"number\">1</span>, bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        self.bn2 = nn.BatchNorm2d(channels)</span><br><span class=\"line\">        self.conv3 = nn.Conv2d(channels, self.expansion * channels, kernel_size=<span class=\"number\">1</span>, bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        self.bn3 = nn.BatchNorm2d(self.expansion * channels)</span><br><span class=\"line\">        self.relu = nn.ReLU(inplace=<span class=\"literal\">True</span>)</span><br><span class=\"line\">        self.downsample = downsample</span><br><span class=\"line\">        self.stride = stride</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        residual = x  <span class=\"comment\"># 将原始输入暂存为shortcut的输出</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.downsample <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:  <span class=\"comment\"># 如果需要下采样，那么shortcut后：H/2，W/2。C：out_channel -&gt; 4 * out_channel</span></span><br><span class=\"line\">            residual = self.downsample(x)</span><br><span class=\"line\"></span><br><span class=\"line\">        out = self.relu(self.bn1(self.conv1(x)))</span><br><span class=\"line\">        out = self.relu(self.bn2(self.conv2(out)))</span><br><span class=\"line\">        out = self.bn3(self.conv3(out))</span><br><span class=\"line\"></span><br><span class=\"line\">        out += residual  <span class=\"comment\"># 残差连接</span></span><br><span class=\"line\">        out = self.relu(out)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> out</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">FPN</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, block, layers</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(FPN, self).__init__()</span><br><span class=\"line\">        self.inchannels = <span class=\"number\">64</span></span><br><span class=\"line\"></span><br><span class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>, kernel_size=<span class=\"number\">7</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">3</span>, bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        self.bn1 = nn.BatchNorm2d(<span class=\"number\">64</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        self.relu = nn.ReLU(inplace=<span class=\"literal\">True</span>)</span><br><span class=\"line\">        self.maxpool = nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Bottom-up layers</span></span><br><span class=\"line\">        self.layer1 = self._make_layer(block, <span class=\"number\">64</span>, layers[<span class=\"number\">0</span>])</span><br><span class=\"line\">        self.layer2 = self._make_layer(block, <span class=\"number\">128</span>, layers[<span class=\"number\">1</span>], stride=<span class=\"number\">2</span>)</span><br><span class=\"line\">        self.layer3 = self._make_layer(block, <span class=\"number\">256</span>, layers[<span class=\"number\">2</span>], stride=<span class=\"number\">2</span>)</span><br><span class=\"line\">        self.layer4 = self._make_layer(block, <span class=\"number\">512</span>, layers[<span class=\"number\">3</span>], stride=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Top layer</span></span><br><span class=\"line\">        self.toplayer = nn.Conv2d(<span class=\"number\">2048</span>, <span class=\"number\">256</span>, kernel_size=<span class=\"number\">1</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">0</span>)  <span class=\"comment\"># Reduce channels</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Lateral layers</span></span><br><span class=\"line\">        self.latlayer1 = nn.Conv2d(<span class=\"number\">1024</span>, <span class=\"number\">256</span>, kernel_size=<span class=\"number\">1</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">0</span>)</span><br><span class=\"line\">        self.latlayer2 = nn.Conv2d(<span class=\"number\">512</span>, <span class=\"number\">256</span>, kernel_size=<span class=\"number\">1</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">0</span>)</span><br><span class=\"line\">        self.latlayer3 = nn.Conv2d(<span class=\"number\">256</span>, <span class=\"number\">256</span>, kernel_size=<span class=\"number\">1</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Smooth layers</span></span><br><span class=\"line\">        self.smooth1 = nn.Conv2d(<span class=\"number\">256</span>, <span class=\"number\">256</span>, kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">        self.smooth2 = nn.Conv2d(<span class=\"number\">256</span>, <span class=\"number\">256</span>, kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">        self.smooth3 = nn.Conv2d(<span class=\"number\">256</span>, <span class=\"number\">256</span>, kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> m <span class=\"keyword\">in</span> self.modules():</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(m, nn.Conv2d):</span><br><span class=\"line\">                n = m.kernel_size[<span class=\"number\">0</span>] * m.kernel_size[<span class=\"number\">1</span>] * m.out_channels</span><br><span class=\"line\">                m.weight.data.normal_(<span class=\"number\">0</span>, math.sqrt(<span class=\"number\">2.</span> / n))</span><br><span class=\"line\">            <span class=\"keyword\">elif</span> <span class=\"built_in\">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class=\"line\">                m.weight.data.fill_(<span class=\"number\">1</span>)</span><br><span class=\"line\">                m.bias.data.zero_()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_make_layer</span>(<span class=\"params\">self, block, channel, block_num, stride=<span class=\"number\">1</span></span>):</span><br><span class=\"line\">        downsample = <span class=\"literal\">None</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> stride != <span class=\"number\">1</span> <span class=\"keyword\">or</span> self.inchannels != channel * block.expansion:</span><br><span class=\"line\">            downsample = nn.Sequential(</span><br><span class=\"line\">                nn.Conv2d(self.inchannels, block.expansion * channel, kernel_size=<span class=\"number\">1</span>, stride=stride, bias=<span class=\"literal\">False</span>),</span><br><span class=\"line\">                nn.BatchNorm2d(block.expansion * channel)</span><br><span class=\"line\">            )</span><br><span class=\"line\">        layers = []</span><br><span class=\"line\">        layers.append(block(self.inchannels, channel, stride, downsample))</span><br><span class=\"line\">        self.inchannels = channel * block.expansion</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, block_num):</span><br><span class=\"line\">            layers.append(block(self.inchannels, channel))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> nn.Sequential(*layers)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_upsample_add</span>(<span class=\"params\">self, x, y</span>):  <span class=\"comment\"># 将x上采样成y的size后与y相加</span></span><br><span class=\"line\">        _, _, H, W = y.size()</span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.interpolate(x, size=(H, W), mode=<span class=\"string\">&#x27;bilinear&#x27;</span>) + y</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):  <span class=\"comment\"># (bsz, 3, h, w)</span></span><br><span class=\"line\">        <span class=\"comment\"># Bottom-up</span></span><br><span class=\"line\">        x = self.conv1(x)  <span class=\"comment\"># (bsz, 64, h/2, w/2)</span></span><br><span class=\"line\">        x = self.bn1(x)</span><br><span class=\"line\">        x = self.relu(x)</span><br><span class=\"line\">        c1 = self.maxpool(x)  <span class=\"comment\"># (bsz, 64, h/4, w/4)</span></span><br><span class=\"line\"></span><br><span class=\"line\">        c2 = self.layer1(c1)  <span class=\"comment\"># (bsz, 256, h/4, w/4)</span></span><br><span class=\"line\">        c3 = self.layer2(c2)  <span class=\"comment\"># (bsz, 512, h/8, w/8)</span></span><br><span class=\"line\">        c4 = self.layer3(c3)  <span class=\"comment\"># (bsz, 1024, h/16, w/16)</span></span><br><span class=\"line\">        c5 = self.layer4(c4)  <span class=\"comment\"># (bsz, 2048, h/32, w/32)</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Top-down</span></span><br><span class=\"line\">        p5 = self.toplayer(c5)  <span class=\"comment\"># (bsz, 256, h/32, w/32)</span></span><br><span class=\"line\">        p4 = self._upsample_add(p5, self.latlayer1(c4))  <span class=\"comment\"># (bsz, 256, h/16, w/16)</span></span><br><span class=\"line\">        p3 = self._upsample_add(p4, self.latlayer2(c3))  <span class=\"comment\"># (bsz, 256, h/8, w/8)</span></span><br><span class=\"line\">        p2 = self._upsample_add(p3, self.latlayer3(c2))  <span class=\"comment\"># (bsz, 256, h/4, w/4)</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Smooth</span></span><br><span class=\"line\">        p4 = self.smooth1(p4)  <span class=\"comment\"># (bsz, 256, h/16, w/16)</span></span><br><span class=\"line\">        p3 = self.smooth2(p3)  <span class=\"comment\"># (bsz, 256, h/8, w/8)</span></span><br><span class=\"line\">        p2 = self.smooth3(p2)  <span class=\"comment\"># (bsz, 256, h/4, w/4)</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> p2, p3, p4, p5</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">FPN101</span>():</span><br><span class=\"line\">    <span class=\"keyword\">return</span> FPN(Bottleneck, [<span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">fpn_101 = FPN101()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">input</span> = torch.randn(<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">256</span>, <span class=\"number\">256</span>)</span><br><span class=\"line\">output_p2, output_p3, output_p4, output_p5 = fpn_101(<span class=\"built_in\">input</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(output_p2.shape)  <span class=\"comment\"># torch.Size([1, 256, 64, 64])</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(output_p3.shape)  <span class=\"comment\"># torch.Size([1, 256, 32, 32])</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(output_p4.shape)  <span class=\"comment\"># torch.Size([1, 256, 16, 16])</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(output_p5.shape)  <span class=\"comment\"># torch.Size([1, 256, 8, 8])</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"2-半监督VOS与AOT模型\">2. 半监督VOS与AOT模型</h2>\n<h3 id=\"2-1-VOS与AOT简介\">2.1 VOS与AOT简介</h3>\n<p>视频对象分割（VOS）旨在识别和分割给定视频中的一个或多个感兴趣的对象，半监督 VOS 需要算法在给定一帧或多帧的<strong>对象注释掩码</strong>的情况下跟踪和分割整个视频序列中的对象。</p>\n<p>此前最先进的方法学习用单个正目标解码特征，因此必须在多目标场景下单独匹配和分割每个目标，消耗多倍的计算资源。我们提出 Associating Objects with Transformers（AOT）方法来<strong>统一匹配和解码多个对象</strong>。AOT 采用 Identification 机制将<strong>多个目标</strong>关联到<strong>同一高维嵌入空间</strong>中。因此可以像处理单个对象一样高效地同时处理多个对象的匹配和分割解码。</p>\n<p>AOT 方法将<strong>分层传播</strong>引入到 VOS 中。分层传播可以逐渐将 ID 信息从过去的帧传播到当前帧，并将当前帧的特征从 object-agnostic（对象不可知）转移到 object-specific（对象特定）。</p>\n<h3 id=\"2-2-ID-机制\">2.2 ID 机制</h3>\n<p>ID 机制为每个目标分配唯一的 ID 信息，并将任意数量（要求小于预定义的大量）目标的 mask 嵌入到同一高维空间中。因此，网络可以学习所有目标之间的关联或相关性。此外，可以利用分配的 ID 信息直接解码多对象分割。</p>\n<p>我们初始化一个身份库（ID Bank），其中存储 M 个具有 C 维的识别向量。为了嵌入多个不同的目标掩码，每个目标将被<strong>随机</strong>分配一个不同的识别向量。</p>\n<h3 id=\"2-3-Long-Short-Term-Transformer（LSTT）\">2.3 Long Short-Term Transformer（LSTT）</h3>\n<p>本文设计长短期 Transformer（LSTT）用于构建<strong>分层对象匹配和传播</strong>。每个 LSTT 块都利用<strong>长期注意力</strong>来匹配<strong>第一帧</strong>的嵌入，并利用<strong>短期注意力</strong>来匹配<strong>多个附近帧</strong>的嵌入。与仅利用一个注意力层的方法相比，我们发现分层注意力结构在<strong>关联多个对象</strong>方面更有效。</p>\n<p>LSTT 首先采用自注意力层，负责学习<strong>当前帧内目标之间</strong>的关联或相关性。此外，LSTT 还引入了长期注意力和短期注意力，前者用于聚合来自长期<strong>记忆帧的目标信息</strong>，后者能够从邻近的短期帧学习<strong>时间平滑性</strong>。所有注意力模块都是以<strong>多头注意力</strong>的形式实现的，即多个注意力模块后跟串联和线性投影。</p>\n<p>长期注意力负责将目标的信息从过去的记忆帧（包含参考帧和存储的预测帧）聚合到当前帧。由于当前帧和过去帧之间的时间间隔是可变的并且可以是长期的，因此<strong>时间平滑性难以保证</strong>。因此，长期注意力采用<strong>非局部</strong>注意力。</p>\n<p>短期注意力用于聚合每个当前帧位置的时空邻域中的信息。直观上，多个连续视频帧之间的图像变化始终是<strong>平滑且连续</strong>的。因此，连续帧中的目标匹配和传播可以<strong>限制在小的时空邻域内</strong>，从而比非局部过程具有更好的效率。</p>\n<h2 id=\"三、DeAOT\">三、DeAOT</h2>\n<p>本文重点是为<strong>半监督视频对象分割</strong>（VOS）开发一种更有效的分层传播方法。在 AOT 方法中 object-specific 信息的增加将不可避免地导致深层传播层中 object-agnostic 的视觉信息的丢失。为了解决这样的问题并进一步促进视觉嵌入的学习，本文提出了一种<strong>分层传播中的解耦特征</strong>（DeAOT）方法。DeAOT 通过在两个<strong>独立</strong>的分支中处理 object-agnostic 和 object-specific 的嵌入来解耦它们的分层传播。其次，为了补偿双分支传播的额外计算，设计了一种用于构造分层传播的有效模块 GPM（门控传播模块），它是通过<strong>单头注意力</strong>精心设计的。</p>\n<h3 id=\"3-1-分层双分支传播\">3.1 分层双分支传播</h3>\n<p>DeAOT 在两个<strong>并行分支</strong>中传播对象的视觉特征（visual features）和掩码（mask features）特征。具体来说，视觉分支负责匹配对象、收集过去的视觉信息并提炼对象特征。为了重新识别对象，ID 分支<strong>重用</strong>视觉分支计算的<strong>匹配图</strong>（注意力图），将 ID 嵌入（由 AOT 中的 ID 机制编码）从过去的帧传播到当前帧。两个分支共享相同的具有 L 个传播层的层次结构。</p>\n<h3 id=\"3-2-门控传播模块GPM\">3.2 门控传播模块GPM</h3>\n<p>门控传播函数首先通过使用<strong>条件门</strong>来增强基于注意力的传播，此外，我们利用 Depth-wise 卷积以<strong>轻量级</strong>方式增强局部空间上下文的建模。</p>\n<p>门控传播模块由三种门控传播组成：自传播、长期传播、短期传播。与 LSTT 相比，GPM 去掉了前馈模块，进一步节省了计算量和参数。所有传播过程都采用门控传播函数。</p>\n",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/6211.html",
            "url": "https://asanosaki.github.io/posts/6211.html",
            "title": "Kaggle项目实战",
            "date_published": "2023-05-30T01:45:00.000Z",
            "content_html": "<blockquote>\n<p>记录 Kaggle 中的一些经典竞赛，也当做自己的练手小项目。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-实战Kaggle比赛：预测房价\">1. 实战Kaggle比赛：预测房价</h2>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 比赛链接：https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> hashlib</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> tarfile</span><br><span class=\"line\"><span class=\"keyword\">import</span> zipfile</span><br><span class=\"line\"><span class=\"keyword\">import</span> requests</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils <span class=\"keyword\">import</span> data</span><br><span class=\"line\"></span><br><span class=\"line\">DATA_HUB = <span class=\"built_in\">dict</span>()</span><br><span class=\"line\">DATA_URL = <span class=\"string\">&#x27;http://d2l-data.s3-accelerate.amazonaws.com/&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">download</span>(<span class=\"params\">name, cache_dir=os.path.join(<span class=\"params\"><span class=\"string\">&#x27;..&#x27;</span>, <span class=\"string\">&#x27;data&#x27;</span></span>)</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;下载一个DATA_HUB中的文件，返回本地文件名&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> name <span class=\"keyword\">in</span> DATA_HUB, <span class=\"string\">f&quot;<span class=\"subst\">&#123;name&#125;</span> 不存在于 <span class=\"subst\">&#123;DATA_HUB&#125;</span>&quot;</span></span><br><span class=\"line\">    url, sha1_hash = DATA_HUB[name]</span><br><span class=\"line\">    os.makedirs(cache_dir, exist_ok=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    fname = os.path.join(cache_dir, url.split(<span class=\"string\">&#x27;/&#x27;</span>)[-<span class=\"number\">1</span>])</span><br><span class=\"line\">    <span class=\"keyword\">if</span> os.path.exists(fname):</span><br><span class=\"line\">        sha1 = hashlib.sha1()</span><br><span class=\"line\">        <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(fname, <span class=\"string\">&#x27;rb&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">            <span class=\"keyword\">while</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">                data = f.read(<span class=\"number\">1048576</span>)</span><br><span class=\"line\">                <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> data:</span><br><span class=\"line\">                    <span class=\"keyword\">break</span></span><br><span class=\"line\">                sha1.update(data)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> sha1.hexdigest() == sha1_hash:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> fname  <span class=\"comment\"># 命中缓存</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;正在从<span class=\"subst\">&#123;url&#125;</span>下载<span class=\"subst\">&#123;fname&#125;</span>...&#x27;</span>)</span><br><span class=\"line\">    r = requests.get(url, stream=<span class=\"literal\">True</span>, verify=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(fname, <span class=\"string\">&#x27;wb&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        f.write(r.content)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> fname</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">download_extract</span>(<span class=\"params\">name, folder=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;下载并解压zip/tar文件&quot;&quot;&quot;</span></span><br><span class=\"line\">    fname = download(name)</span><br><span class=\"line\">    base_dir = os.path.dirname(fname)</span><br><span class=\"line\">    data_dir, ext = os.path.splitext(fname)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> ext == <span class=\"string\">&#x27;.zip&#x27;</span>:</span><br><span class=\"line\">        fp = zipfile.ZipFile(fname, <span class=\"string\">&#x27;r&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> ext <span class=\"keyword\">in</span> (<span class=\"string\">&#x27;.tar&#x27;</span>, <span class=\"string\">&#x27;.gz&#x27;</span>):</span><br><span class=\"line\">        fp = tarfile.<span class=\"built_in\">open</span>(fname, <span class=\"string\">&#x27;r&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> <span class=\"literal\">False</span>, <span class=\"string\">&#x27;只有zip/tar文件可以被解压缩&#x27;</span></span><br><span class=\"line\">    fp.extractall(base_dir)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> os.path.join(base_dir, folder) <span class=\"keyword\">if</span> folder <span class=\"keyword\">else</span> data_dir</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">download_all</span>():</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;下载DATA_HUB中的所有文件&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> name <span class=\"keyword\">in</span> DATA_HUB:</span><br><span class=\"line\">        download(name)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 下载并缓存Kaggle房屋数据集</span></span><br><span class=\"line\">DATA_HUB[<span class=\"string\">&#x27;kaggle_house_train&#x27;</span>] = (DATA_URL + <span class=\"string\">&#x27;kaggle_house_pred_train.csv&#x27;</span>, <span class=\"string\">&#x27;585e9cc93e70b39160e7921475f9bcd7d31219ce&#x27;</span>)</span><br><span class=\"line\">DATA_HUB[<span class=\"string\">&#x27;kaggle_house_test&#x27;</span>] = (DATA_URL + <span class=\"string\">&#x27;kaggle_house_pred_test.csv&#x27;</span>, <span class=\"string\">&#x27;fa19780a7b011d9b009e8bff8e99922a8ee2eb90&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用pandas分别加载包含训练数据和测试数据的两个CSV文件</span></span><br><span class=\"line\">train_data = pd.read_csv(download(<span class=\"string\">&#x27;kaggle_house_train&#x27;</span>))</span><br><span class=\"line\">test_data = pd.read_csv(download(<span class=\"string\">&#x27;kaggle_house_test&#x27;</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 训练数据集包括1460个样本，每个样本80个特征和1个标签，而测试数据集包含1459个样本，每个样本80个特征</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(train_data.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(test_data.shape)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 看看前四个和最后两个特征，以及相应标签（房价）</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(train_data.iloc[<span class=\"number\">0</span>:<span class=\"number\">4</span>, [<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, -<span class=\"number\">3</span>, -<span class=\"number\">2</span>, -<span class=\"number\">1</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 在每个样本中，第一个特征是ID，这有助于模型识别每个训练样本。虽然这很方便，但它不携带任何用于预测的信息。因此，在将数据提供给模型之前，我们将其从数据集中删除</span></span><br><span class=\"line\">all_features = pd.concat((train_data.iloc[:, <span class=\"number\">1</span>:-<span class=\"number\">1</span>], test_data.iloc[:, <span class=\"number\">1</span>:]))  <span class=\"comment\"># train数据的最后一列为label</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 在开始建模之前，我们需要对数据进行预处理。首先，我们将所有缺失的值替换为相应特征的平均值</span></span><br><span class=\"line\"><span class=\"comment\"># 若无法获得测试数据，则可根据训练数据计算均值和标准差</span></span><br><span class=\"line\">numeric_features = all_features.dtypes[all_features.dtypes != <span class=\"string\">&#x27;object&#x27;</span>].index  <span class=\"comment\"># 数值类型特征的下标</span></span><br><span class=\"line\">all_features[numeric_features] = all_features[numeric_features].apply(<span class=\"keyword\">lambda</span> x: (x - x.mean()) / (x.std()))  <span class=\"comment\"># 将所有数值特征的均值变成0，方差变成1</span></span><br><span class=\"line\"><span class=\"comment\"># 在标准化数据之后，所有均值消失，因此我们可以将缺失值设置为0</span></span><br><span class=\"line\">all_features[numeric_features] = all_features[numeric_features].fillna(<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 接下来，我们处理离散值。这包括诸如“MSZoning”之类的特征。我们用独热编码替换它们，pandas软件包会自动为我们实现这一点</span></span><br><span class=\"line\"><span class=\"comment\"># dummy_na=True将&#x27;na&#x27;（缺失值）视为有效的特征值，并为其创建指示符特征</span></span><br><span class=\"line\">all_features = pd.get_dummies(all_features, dummy_na=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(all_features.shape)  <span class=\"comment\"># (2919, 331)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 通过values属性，我们可以从pandas格式中提取NumPy格式，并将其转换为张量表示用于训练</span></span><br><span class=\"line\">n_train = train_data.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)</span><br><span class=\"line\">test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)</span><br><span class=\"line\">train_labels = torch.tensor(train_data.SalePrice.values.reshape(-<span class=\"number\">1</span>, <span class=\"number\">1</span>), dtype=torch.float32)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 训练</span></span><br><span class=\"line\">loss_function = nn.MSELoss()</span><br><span class=\"line\">in_features = train_features.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">net = nn.Sequential(nn.Linear(in_features, <span class=\"number\">128</span>),</span><br><span class=\"line\">                    nn.ReLU(),</span><br><span class=\"line\">                    nn.Dropout(<span class=\"number\">0.1</span>),</span><br><span class=\"line\">                    nn.Linear(<span class=\"number\">128</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 房价就像股票价格一样，我们关心的是相对数量，而不是绝对数量，我们更关心相对误差，即：(真实值-预测值)/真实值，解决这个问题的一种方法是用价格预测的对数来衡量差异</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">log_rmse</span>(<span class=\"params\">net, features, labels</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 为了在取对数时进一步稳定该值，将小于1的值设置为1</span></span><br><span class=\"line\">    clipped_preds = torch.clamp(net(features), <span class=\"number\">1</span>, <span class=\"built_in\">float</span>(<span class=\"string\">&#x27;inf&#x27;</span>))  <span class=\"comment\"># 将inf变成1</span></span><br><span class=\"line\">    rmse = torch.sqrt(loss_function(torch.log(clipped_preds), torch.log(labels)))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> rmse.item()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">load_array</span>(<span class=\"params\">data_arrays, batch_size, is_train=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">    dataset = data.TensorDataset(*data_arrays)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">net, train_features, train_labels, test_features, test_labels, num_epochs, learning_rate, weight_decay, batch_size</span>):</span><br><span class=\"line\">    train_ls, test_ls = [], []</span><br><span class=\"line\">    train_iter = load_array((train_features, train_labels), batch_size)</span><br><span class=\"line\"></span><br><span class=\"line\">    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)  <span class=\"comment\"># 这里使用的是Adam优化算法</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">            optimizer.zero_grad()</span><br><span class=\"line\">            loss = loss_function(net(X), y)</span><br><span class=\"line\">            loss.backward()</span><br><span class=\"line\">            optimizer.step()</span><br><span class=\"line\"></span><br><span class=\"line\">        train_ls.append(log_rmse(net, train_features, train_labels))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> test_labels <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            test_ls.append(log_rmse(net, test_features, test_labels))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> train_ls, test_ls</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># K折交叉验证，有助于模型选择和超参数调整，首先需要定义一个函数，在K折交叉验证过程中返回第i折的数据</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_k_fold_data</span>(<span class=\"params\">k, i, X, y</span>):</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> k &gt; <span class=\"number\">1</span></span><br><span class=\"line\">    fold_size = X.shape[<span class=\"number\">0</span>] // k  <span class=\"comment\"># 每一折的大小</span></span><br><span class=\"line\">    X_train, y_train = <span class=\"literal\">None</span>, <span class=\"literal\">None</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(k):  <span class=\"comment\"># 分成k份</span></span><br><span class=\"line\">        idx = <span class=\"built_in\">slice</span>(j * fold_size, (j + <span class=\"number\">1</span>) * fold_size)</span><br><span class=\"line\">        X_part, y_part = X[idx, :], y[idx]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> j == i:  <span class=\"comment\"># 将第i折的数据作为验证数据</span></span><br><span class=\"line\">            X_valid, y_valid = X_part, y_part</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> X_train <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            X_train, y_train = X_part, y_part</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            X_train = torch.cat([X_train, X_part], <span class=\"number\">0</span>)</span><br><span class=\"line\">            y_train = torch.cat([y_train, y_part], <span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X_train, y_train, X_valid, y_valid</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">k_fold</span>(<span class=\"params\">k, X_train, y_train, num_epochs, learning_rate, weight_decay, batch_size</span>):</span><br><span class=\"line\">    train_l_sum, valid_l_sum = <span class=\"number\">0</span>, <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(k):</span><br><span class=\"line\">        data = get_k_fold_data(k, i, X_train, y_train)</span><br><span class=\"line\">        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate, weight_decay, batch_size)</span><br><span class=\"line\">        train_l_sum += train_ls[-<span class=\"number\">1</span>]</span><br><span class=\"line\">        valid_l_sum += valid_ls[-<span class=\"number\">1</span>]</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;折<span class=\"subst\">&#123;i + <span class=\"number\">1</span>&#125;</span>，训练log rmse: <span class=\"subst\">&#123;<span class=\"built_in\">float</span>(train_ls[-<span class=\"number\">1</span>]):f&#125;</span>，验证log rmse: <span class=\"subst\">&#123;<span class=\"built_in\">float</span>(valid_ls[-<span class=\"number\">1</span>]):f&#125;</span>&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> train_l_sum / k, valid_l_sum / k</span><br><span class=\"line\"></span><br><span class=\"line\">k, num_epochs, lr, weight_decay, batch_size = <span class=\"number\">5</span>, <span class=\"number\">250</span>, <span class=\"number\">0.5</span>, <span class=\"number\">1e-3</span>, <span class=\"number\">32</span></span><br><span class=\"line\"></span><br><span class=\"line\">train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr, weight_decay, batch_size)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;<span class=\"subst\">&#123;k&#125;</span>-折验证: 平均训练log rmse: <span class=\"subst\">&#123;<span class=\"built_in\">float</span>(train_l):f&#125;</span>，&#x27;</span><span class=\"string\">f&#x27;平均验证log rmse: <span class=\"subst\">&#123;<span class=\"built_in\">float</span>(valid_l):f&#125;</span>&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用所有数据对其进行训练</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_and_pred</span>(<span class=\"params\">train_features, test_features, train_labels, test_data, num_epochs, lr, weight_decay, batch_size</span>):</span><br><span class=\"line\">    train_ls, _ = train(net, train_features, train_labels, <span class=\"literal\">None</span>, <span class=\"literal\">None</span>, num_epochs, lr, weight_decay, batch_size)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;训练log rmse: <span class=\"subst\">&#123;<span class=\"built_in\">float</span>(train_ls[-<span class=\"number\">1</span>]):f&#125;</span>&#x27;</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 将网络应用于测试集</span></span><br><span class=\"line\">    preds = net(test_features).detach().numpy()</span><br><span class=\"line\">    <span class=\"comment\"># 将其重新格式化以导出到Kaggle，将预测保存在CSV文件中可以简化将结果上传到Kaggle的过程</span></span><br><span class=\"line\">    test_data[<span class=\"string\">&#x27;SalePrice&#x27;</span>] = pd.Series(preds.reshape(<span class=\"number\">1</span>, -<span class=\"number\">1</span>)[<span class=\"number\">0</span>])</span><br><span class=\"line\">    submission = pd.concat([test_data[<span class=\"string\">&#x27;Id&#x27;</span>], test_data[<span class=\"string\">&#x27;SalePrice&#x27;</span>]], axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">    submission.to_csv(<span class=\"string\">&#x27;../data/submission.csv&#x27;</span>, index=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">train_and_pred(train_features, test_features, train_labels, test_data, num_epochs, lr, weight_decay, batch_size)</span><br></pre></td></tr></table></figure>\n",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/9681.html",
            "url": "https://asanosaki.github.io/posts/9681.html",
            "title": "Python遥感常用模块Rasterio与Rioxarray教程",
            "date_published": "2023-05-29T07:52:00.000Z",
            "content_html": "<blockquote>\n<p><code>rasterio</code> 是一个很多模块是基于 <code>GDAL</code> 的 Python 包，可用于处理地理空间栅格数据，例如 GeoTIFF 文件。<code>xarray</code> 是一个为数组提供标签，例如尺寸、坐标和其他特定属性的 Python 包，它使大维数组的工作更加直观。<code>rioxarray</code> 结合了 <code>rasterio</code> 的功能和 <code>xarray</code> 的所有优点。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-Rasterio与Rioxarray安装\">1. Rasterio与Rioxarray安装</h2>\n<p>首先安装 Rasterio 模块，（本人使用 <code>conda</code> 安装时遇到过报错 <code>ImportError: cannot import name 'CRS' from 'pyproj' (unknown location)</code>，是由于 <code>pyproj</code> 模块安装不全，因此建议采用后面的离线安装方式或者之后遇到问题时删除 <code>pyproj</code> 模块后再离线安装该模块）：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install gdal</span><br><span class=\"line\">conda install rasterio</span><br></pre></td></tr></table></figure>\n<p>如果安装失败可以采用离线安装的方式，Rasterio 依赖很多第三方库，所以比较麻烦，按下面的顺序依次安装即可，可以尝试使用 <code>pip</code> 安装或者下载 <code>.whl</code> 文件离线安装（注意对上 Python 版本）：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pyproj</span><br><span class=\"line\">Shapely</span><br><span class=\"line\">GDAL</span><br><span class=\"line\">Fiona</span><br><span class=\"line\">rasterio</span><br></pre></td></tr></table></figure>\n<p>各个模块的链接：<a href=\"https://pypi.org/project/pyproj/\">Pyproj</a>、<a href=\"https://pypi.org/project/shapely/\">Shapely</a>、<a href=\"https://pypi.org/project/GDAL/\">GDAL</a>、<a href=\"https://pypi.org/project/Fiona/\">Fiona</a>、<a href=\"https://pypi.org/project/rasterio/\">Rasterio</a>。</p>\n<p>离线安装指令：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install E:\\GDAL-1.2.10-cp310-cp310-win_amd64.whl</span><br></pre></td></tr></table></figure>\n<p>在 Python 中使用 Anaconda 安装 <code>rioxarray</code> 包时，首先需要安装 <code>GDAL</code> 和 <code>rasterio</code>，然后再安装 <code>rioxarray</code>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install rioxarray</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-使用教程\">2. 使用教程</h2>\n<p>（1）使用 Rioxarray 读取并展示图像：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> rasterio</span><br><span class=\"line\"><span class=\"keyword\">import</span> rioxarray</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\">plt.rcParams[<span class=\"string\">&#x27;font.sans-serif&#x27;</span>] = [<span class=\"string\">&#x27;SimHei&#x27;</span>]</span><br><span class=\"line\">plt.rcParams[<span class=\"string\">&#x27;axes.unicode_minus&#x27;</span>] = <span class=\"literal\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\">img_path = <span class=\"string\">&#x27;../images/tiff_img.tif&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">img = rioxarray.open_rasterio(img_path)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(img.shape)  <span class=\"comment\"># (22, 488, 480)，第一维为通道数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">type</span>(img))  <span class=\"comment\"># &lt;class &#x27;xarray.core.dataarray.DataArray&#x27;&gt;</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">type</span>(img.values))  <span class=\"comment\"># &lt;class &#x27;numpy.ndarray&#x27;&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">fig, axes = plt.subplots(<span class=\"number\">1</span>, <span class=\"number\">2</span>, figsize=(<span class=\"number\">10</span>, <span class=\"number\">6</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> ax <span class=\"keyword\">in</span> axes.flat:</span><br><span class=\"line\">    ax_img = ax.imshow(img[<span class=\"number\">0</span>], cmap=<span class=\"string\">&#x27;viridis&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span> ax, title <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(axes.flat, [<span class=\"string\">&#x27;img1&#x27;</span>, <span class=\"string\">&#x27;img2&#x27;</span>]):</span><br><span class=\"line\">    ax.set_title(title)</span><br><span class=\"line\">fig.colorbar(mappable=ax_img, label=<span class=\"string\">&#x27;FSC&#x27;</span>, orientation=<span class=\"string\">&#x27;horizontal&#x27;</span>, ax=axes, fraction=<span class=\"number\">0.04</span>)  <span class=\"comment\"># 图例，fraction可调整大小</span></span><br><span class=\"line\"></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>也可以用另一种形式展示（注意如果使用 Rasterio 读取图像则无法使用该方式展示图像）：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.figure(dpi=<span class=\"number\">300</span>, figsize=(<span class=\"number\">15</span>, <span class=\"number\">5</span>))</span><br><span class=\"line\">plt.subplots_adjust(hspace=<span class=\"number\">0.2</span>, wspace=<span class=\"number\">0.5</span>)</span><br><span class=\"line\">plt.subplot(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">img[<span class=\"number\">0</span>].plot(cmap=<span class=\"string\">&#x27;terrain&#x27;</span>)  <span class=\"comment\"># getting the first band</span></span><br><span class=\"line\">plt.subplot(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">img[<span class=\"number\">1</span>].plot(cmap=<span class=\"string\">&#x27;terrain&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># plt.savefig(&#x27;1.png&#x27;, dpi=300, bbox_inches=&#x27;tight&#x27;, pad_inches=0)</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>（2）使用 Rasterio 读取图像：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">img = rasterio.<span class=\"built_in\">open</span>(img_path).read()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(img.shape)  <span class=\"comment\"># (22, 488, 480)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">type</span>(img))  <span class=\"comment\"># &lt;class &#x27;numpy.ndarray&#x27;&gt;</span></span><br></pre></td></tr></table></figure>\n<p>（3）转换为 Tensor 类型：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\">img_torch = torch.tensor(np.array(img.values), dtype=torch.float32)  <span class=\"comment\"># Rioxarray转Tensor</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(img_torch.shape)  <span class=\"comment\"># torch.Size([22, 488, 480])</span></span><br><span class=\"line\"></span><br><span class=\"line\">img_torch = torch.tensor(img, dtype=torch.float32)  <span class=\"comment\"># Rasterio转Tensor</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(img_torch.shape)  <span class=\"comment\"># torch.Size([22, 488, 480])</span></span><br></pre></td></tr></table></figure>\n<p>（4）将 TIFF 图像逐像素提取出数据构建 CSV 文件：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> tqdm</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">read_image</span>(<span class=\"params\">img_path</span>):</span><br><span class=\"line\">    img = rasterio.<span class=\"built_in\">open</span>(img_path).read()</span><br><span class=\"line\">    band, height, width = np.shape(img)</span><br><span class=\"line\"></span><br><span class=\"line\">    img_data_list = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> tqdm.trange(height):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> y <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(width):</span><br><span class=\"line\">            temp = img[::, x, y]</span><br><span class=\"line\">            <span class=\"keyword\">if</span> np.array(np.isnan(temp), dtype=np.int8).<span class=\"built_in\">sum</span>() &gt; <span class=\"number\">0</span>:  <span class=\"comment\"># 过滤nan值</span></span><br><span class=\"line\">                <span class=\"keyword\">continue</span></span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                img_data_list.append(temp.tolist())</span><br><span class=\"line\"></span><br><span class=\"line\">    img_arr = np.array(img_data_list)</span><br><span class=\"line\">    img_arr = np.around(img_arr, <span class=\"number\">6</span>)  <span class=\"comment\"># 将数据四舍五入保留6位小数</span></span><br><span class=\"line\">    labels = img_arr[:, <span class=\"number\">0</span>]  <span class=\"comment\"># 第一个特征为标签</span></span><br><span class=\"line\">    dataset = img_arr[:, <span class=\"number\">1</span>:]  <span class=\"comment\"># 之后的特征为训练数据</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(os.path.basename(img_path), <span class=\"string\">&#x27;读取成功!&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># return dataset, labels</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> img_arr</span><br><span class=\"line\"></span><br><span class=\"line\">total_dataset = np.zeros((<span class=\"number\">1</span>, <span class=\"number\">22</span>), dtype=np.float32)</span><br><span class=\"line\">img_data = read_image(img_path)</span><br><span class=\"line\">total_dataset = np.append(total_dataset, img_data, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">total_dataset = np.delete(total_dataset, obj=<span class=\"number\">0</span>, axis=<span class=\"number\">0</span>)  <span class=\"comment\"># 按行(axis=0)删除第一行(obj=0)元素</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(total_dataset, <span class=\"string\">&#x27;\\n&#x27;</span>, np.shape(total_dataset))</span><br><span class=\"line\"><span class=\"comment\"># [[0.570768 0.14354  0.159068 ... 0.458602 1.       0.4     ]</span></span><br><span class=\"line\"><span class=\"comment\">#  [0.307365 0.14354  0.159068 ... 0.458602 1.       0.4     ]</span></span><br><span class=\"line\"><span class=\"comment\">#  [0.005285 0.14354  0.159068 ... 0.428406 1.       0.4     ]</span></span><br><span class=\"line\"><span class=\"comment\">#  ...</span></span><br><span class=\"line\"><span class=\"comment\">#  [0.993229 0.393478 0.370807 ... 0.243081 1.       0.8     ]</span></span><br><span class=\"line\"><span class=\"comment\">#  [0.967867 0.370807 0.356894 ... 0.243081 1.       0.8     ]</span></span><br><span class=\"line\"><span class=\"comment\">#  [0.945627 0.321429 0.305714 ... 0.243081 1.       0.8     ]]</span></span><br><span class=\"line\"><span class=\"comment\">#  (116082, 22)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 一张影像22个波段，每一波段为一种特征，特征名如下，其中FSC既是模型训练时的标签数据也是模型输出数据</span></span><br><span class=\"line\">feature_name = [<span class=\"string\">&#x27;FSC&#x27;</span>, <span class=\"string\">&#x27;SR1&#x27;</span>, <span class=\"string\">&#x27;SR2&#x27;</span>, <span class=\"string\">&#x27;SR3&#x27;</span>, <span class=\"string\">&#x27;SR4&#x27;</span>, <span class=\"string\">&#x27;SR5&#x27;</span>, <span class=\"string\">&#x27;SR6&#x27;</span>, <span class=\"string\">&#x27;SR7&#x27;</span>, <span class=\"string\">&#x27;NDVI&#x27;</span>, <span class=\"string\">&#x27;NDSI&#x27;</span>,</span><br><span class=\"line\">                <span class=\"string\">&#x27;NDFSI&#x27;</span>, <span class=\"string\">&#x27;SensorZenith&#x27;</span>, <span class=\"string\">&#x27;SensorAzimuth&#x27;</span>, <span class=\"string\">&#x27;SolarZenith&#x27;</span>, <span class=\"string\">&#x27;SolarAzimuth&#x27;</span>,</span><br><span class=\"line\">                <span class=\"string\">&#x27;Dem&#x27;</span>, <span class=\"string\">&#x27;Slope&#x27;</span>, <span class=\"string\">&#x27;Aspect&#x27;</span>, <span class=\"string\">&#x27;LST&#x27;</span>, <span class=\"string\">&#x27;A2T&#x27;</span>, <span class=\"string\">&#x27;SC&#x27;</span>, <span class=\"string\">&#x27;LCT&#x27;</span>]</span><br><span class=\"line\">df = pd.DataFrame(total_dataset, columns=feature_name)</span><br><span class=\"line\">df.to_csv(<span class=\"string\">&#x27;../data/MODIS_total_data.csv&#x27;</span>, index=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(df)</span><br><span class=\"line\"><span class=\"comment\">#              FSC       SR1       SR2       SR3  ...       LST       A2T   SC  LCT</span></span><br><span class=\"line\"><span class=\"comment\"># 0       0.570768  0.143540  0.159068  0.165776  ...  0.447205  0.458602  1.0  0.4</span></span><br><span class=\"line\"><span class=\"comment\"># 1       0.307365  0.143540  0.159068  0.165776  ...  0.447205  0.458602  1.0  0.4</span></span><br><span class=\"line\"><span class=\"comment\"># ...          ...       ...       ...       ...  ...       ...       ...  ...  ...</span></span><br><span class=\"line\"><span class=\"comment\"># 116080  0.967867  0.370807  0.356894  0.384162  ...  0.252946  0.243081  1.0  0.8</span></span><br><span class=\"line\"><span class=\"comment\"># 116081  0.945627  0.321429  0.305714  0.327329  ...  0.252946  0.243081  1.0  0.8</span></span><br><span class=\"line\"><span class=\"comment\"># [116082 rows x 22 columns]</span></span><br><span class=\"line\"></span><br><span class=\"line\">train_data, valid_data = train_test_split(df, test_size=<span class=\"number\">0.3</span>, random_state=<span class=\"number\">1</span>)  <span class=\"comment\"># 按7:3的比例划分train_data与valid_data</span></span><br><span class=\"line\">train_data.to_csv(<span class=\"string\">&#x27;../data/MODIS_train_data.csv&#x27;</span>, index=<span class=\"literal\">False</span>)</span><br><span class=\"line\">valid_data.to_csv(<span class=\"string\">&#x27;../data/MODIS_valid_data.csv&#x27;</span>, index=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(train_data)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(valid_data)</span><br><span class=\"line\"><span class=\"comment\">#             FSC       SR1       SR2  ...       A2T        SC  LCT</span></span><br><span class=\"line\"><span class=\"comment\"># 65463  1.000000  0.868261  0.860124  ...  0.306415  0.954102  0.4</span></span><br><span class=\"line\"><span class=\"comment\"># 71636  0.000000  0.074969  0.090683  ...  0.492837  0.021780  0.4</span></span><br><span class=\"line\"><span class=\"comment\"># ...         ...       ...       ...  ...       ...       ...  ...</span></span><br><span class=\"line\"><span class=\"comment\"># 77708  0.836359  0.252298  0.268199  ...  0.400243  1.000000  0.4</span></span><br><span class=\"line\"><span class=\"comment\"># 98539  0.004958  0.048758  0.073168  ...  0.547051  0.000000  0.4</span></span><br><span class=\"line\"><span class=\"comment\"># [81257 rows x 22 columns]</span></span><br><span class=\"line\"><span class=\"comment\">#              FSC       SR1       SR2  ...       A2T        SC  LCT</span></span><br><span class=\"line\"><span class=\"comment\"># 24035   0.907556  0.579814  0.588075  ...  0.332088  1.000000  0.8</span></span><br><span class=\"line\"><span class=\"comment\"># 26625   0.988592  0.708696  0.702981  ...  0.334435  0.999297  0.4</span></span><br><span class=\"line\"><span class=\"comment\"># ...          ...       ...       ...  ...       ...       ...  ...</span></span><br><span class=\"line\"><span class=\"comment\"># 22745   0.000000  0.054348  0.127143  ...  0.494257  0.532436  0.4</span></span><br><span class=\"line\"><span class=\"comment\"># 31068   0.994422  0.562795  0.532174  ...  0.384267  1.000000  0.4</span></span><br><span class=\"line\"><span class=\"comment\"># [34825 rows x 22 columns]</span></span><br></pre></td></tr></table></figure>\n",
            "tags": [
                "Python"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/31783.html",
            "url": "https://asanosaki.github.io/posts/31783.html",
            "title": "Python绘图模块Plotly教程",
            "date_published": "2023-05-29T07:42:00.000Z",
            "content_html": "<blockquote>\n<p>Plotly 是一个快速完善并崛起的交互式的、开源的绘图库库，Python 库则是它的一个重要分支。现已支持超过40种独特的图表类型，涵盖了广泛的统计、金融、地理、科学和三维用例。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-Plotly安装\">1. Plotly安装</h2>\n<p>Python 中可以使用 <code>pip</code> 或者 <code>conda</code> 安装 Plotly：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install plotly</span><br><span class=\"line\">conda install plotly</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-Plotly绘图教程\">2. Plotly绘图教程</h2>\n<h3 id=\"2-1-折线图与散点图\">2.1 折线图与散点图</h3>\n<p>折线图不仅可以表示数量的多少，而且可以反映同一事物在不同时间里的发展变化的情况，易于显示数据变化趋势，可以直观地反映这种变化以及各组之间的差别。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> plotly.graph_objects <span class=\"keyword\">as</span> go</span><br><span class=\"line\"></span><br><span class=\"line\">x = np.arange(<span class=\"number\">5</span>)</span><br><span class=\"line\">y1 = np.random.rand(<span class=\"number\">5</span>) * <span class=\"number\">5</span></span><br><span class=\"line\">y2 = np.random.rand(<span class=\"number\">5</span>) * <span class=\"number\">5</span></span><br><span class=\"line\">y3 = np.random.rand(<span class=\"number\">5</span>) * <span class=\"number\">5</span></span><br><span class=\"line\"></span><br><span class=\"line\">fig = go.Figure(</span><br><span class=\"line\">    data=[</span><br><span class=\"line\">        <span class=\"comment\"># name为图例名，textfont设置字体属性，mode为绘图模式，marker设置颜色否则后续导出图像会丢失颜色（不导出可不设置该参数也有默认颜色）</span></span><br><span class=\"line\">        go.Scatter(name=<span class=\"string\">&#x27;Lines&#x27;</span>, x=x, y=y1, textfont=<span class=\"built_in\">dict</span>(size=<span class=\"number\">25</span>), mode=<span class=\"string\">&#x27;lines&#x27;</span>, marker=<span class=\"built_in\">dict</span>(color=<span class=\"string\">&#x27;#0068C9&#x27;</span>)),</span><br><span class=\"line\">        go.Scatter(name=<span class=\"string\">&#x27;Markers&#x27;</span>, x=x, y=y2, textfont=<span class=\"built_in\">dict</span>(size=<span class=\"number\">25</span>), mode=<span class=\"string\">&#x27;markers&#x27;</span>),</span><br><span class=\"line\">        go.Scatter(name=<span class=\"string\">&#x27;Lines&amp;Markers&#x27;</span>, x=x, y=y3, textfont=<span class=\"built_in\">dict</span>(size=<span class=\"number\">25</span>), mode=<span class=\"string\">&#x27;lines+markers&#x27;</span>),</span><br><span class=\"line\">    ]</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置图像格式</span></span><br><span class=\"line\">fig.update_layout(</span><br><span class=\"line\">    autosize=<span class=\"literal\">False</span>, width=<span class=\"number\">1200</span>, height=<span class=\"number\">650</span>,  <span class=\"comment\"># 取消自动大小，手动设置宽高</span></span><br><span class=\"line\">    title=<span class=\"string\">&#x27;This is title&#x27;</span>,  <span class=\"comment\"># 标题</span></span><br><span class=\"line\">    xaxis=<span class=\"built_in\">dict</span>(title=<span class=\"string\">&#x27;X&#x27;</span>, nticks=<span class=\"number\">5</span>),  <span class=\"comment\"># 设置X轴属性</span></span><br><span class=\"line\">    yaxis=<span class=\"built_in\">dict</span>(title=<span class=\"string\">&#x27;Y&#x27;</span>, nticks=<span class=\"number\">11</span>, <span class=\"built_in\">range</span>=(<span class=\"number\">0</span>, <span class=\"number\">5</span>)),  <span class=\"comment\"># 设置Y轴属性，nticks表示划分为多少段</span></span><br><span class=\"line\">    showlegend=<span class=\"literal\">True</span>  <span class=\"comment\"># 显示图例</span></span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">fig.show()</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-2-饼图\">2.2 饼图</h3>\n<p>饼图用于强调各项数据占总体的占比，强调个体和整体的比较。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fig = go.Figure(</span><br><span class=\"line\">    <span class=\"comment\"># textinfo表示显示内容是百分比还是标签，hoverinfo表示鼠标悬停的显示内容，pull表示每一块往外拉的比例</span></span><br><span class=\"line\">    go.Pie(labels=[<span class=\"string\">&#x27;Train data&#x27;</span>, <span class=\"string\">&#x27;Valid data&#x27;</span>], values=[<span class=\"number\">7</span>, <span class=\"number\">3</span>],</span><br><span class=\"line\">           textinfo=<span class=\"string\">&#x27;percent&#x27;</span>, hoverinfo=<span class=\"string\">&#x27;label+percent&#x27;</span>,</span><br><span class=\"line\">           textfont=<span class=\"built_in\">dict</span>(size=<span class=\"number\">15</span>), pull=[<span class=\"number\">0</span>, <span class=\"number\">0.05</span>],</span><br><span class=\"line\">           title=<span class=\"string\">&#x27;训练集划分&#x27;</span>, titlefont=<span class=\"built_in\">dict</span>(size=<span class=\"number\">18</span>),</span><br><span class=\"line\">           marker=<span class=\"built_in\">dict</span>(colors=[<span class=\"string\">&#x27;#0068C9&#x27;</span>, <span class=\"string\">&#x27;#83C9FF&#x27;</span>]))</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">fig.update_layout(</span><br><span class=\"line\">    autosize=<span class=\"literal\">False</span>, width=<span class=\"number\">600</span>, height=<span class=\"number\">450</span></span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">fig.show()</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-3-直方图\">2.3 直方图</h3>\n<p>直方图虽然也和条形图一样通过矩形的长度表示数值，但他的宽度一般用于表示各组的组距，因此其高度与宽度均有意义，适合展示大量数据集的统计结果，直方图的表示的数据通常是连续排列，而柱状图则是分开排列。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = np.random.rand(<span class=\"number\">1000</span>) * <span class=\"number\">30</span>  <span class=\"comment\"># 生成1000个0-30之间的数</span></span><br><span class=\"line\"></span><br><span class=\"line\">fig = go.Figure(</span><br><span class=\"line\">    data=[</span><br><span class=\"line\">        go.Histogram(name=<span class=\"string\">&#x27;X&#x27;</span>, x=x)</span><br><span class=\"line\">    ]</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">fig.update_layout(</span><br><span class=\"line\">    autosize=<span class=\"literal\">False</span>, width=<span class=\"number\">1350</span>, height=<span class=\"number\">600</span>,</span><br><span class=\"line\">    xaxis=<span class=\"built_in\">dict</span>(title=<span class=\"string\">&#x27;Value&#x27;</span>),</span><br><span class=\"line\">    yaxis=<span class=\"built_in\">dict</span>(title=<span class=\"string\">&#x27;Count&#x27;</span>),</span><br><span class=\"line\">    showlegend=<span class=\"literal\">True</span>,</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">fig.update_traces(opacity=<span class=\"number\">0.6</span>)  <span class=\"comment\"># 设置透明度</span></span><br><span class=\"line\"></span><br><span class=\"line\">fig.show()</span><br></pre></td></tr></table></figure>\n<p>可设置 <code>barmode</code> 参数实现多个直方图覆盖的效果：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x1 = np.random.rand(<span class=\"number\">1000</span>) * <span class=\"number\">30</span></span><br><span class=\"line\">x2 = np.random.rand(<span class=\"number\">500</span>) * <span class=\"number\">30</span></span><br><span class=\"line\"></span><br><span class=\"line\">fig = go.Figure(</span><br><span class=\"line\">    data=[</span><br><span class=\"line\">        go.Histogram(name=<span class=\"string\">&#x27;X1&#x27;</span>, x=x1),</span><br><span class=\"line\">        go.Histogram(name=<span class=\"string\">&#x27;X2&#x27;</span>, x=x2)</span><br><span class=\"line\">    ]</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">fig.update_layout(</span><br><span class=\"line\">    barmode=<span class=\"string\">&#x27;overlay&#x27;</span>,  <span class=\"comment\"># 设置覆盖模式</span></span><br><span class=\"line\">    autosize=<span class=\"literal\">False</span>, width=<span class=\"number\">1350</span>, height=<span class=\"number\">600</span>,</span><br><span class=\"line\">    xaxis=<span class=\"built_in\">dict</span>(title=<span class=\"string\">&#x27;Value&#x27;</span>),</span><br><span class=\"line\">    yaxis=<span class=\"built_in\">dict</span>(title=<span class=\"string\">&#x27;Count&#x27;</span>),</span><br><span class=\"line\">    showlegend=<span class=\"literal\">True</span>,</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">fig.update_traces(opacity=<span class=\"number\">0.6</span>)  <span class=\"comment\"># 设置透明度</span></span><br><span class=\"line\"></span><br><span class=\"line\">fig.show()</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-4-条形图\">2.4 条形图</h3>\n<p>条形图用于比较各组数据的差异性，强调进行个体间的比较。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = np.arange(<span class=\"number\">10</span>)</span><br><span class=\"line\">y = np.random.randint(<span class=\"number\">30</span>, size=<span class=\"number\">10</span>) + <span class=\"number\">1</span>  <span class=\"comment\"># 生成10个1~30的整数</span></span><br><span class=\"line\"></span><br><span class=\"line\">fig = go.Figure(</span><br><span class=\"line\">    data=[</span><br><span class=\"line\">        go.Bar(name=<span class=\"string\">&#x27;Bar1&#x27;</span>, x=x, y=y, textfont=<span class=\"built_in\">dict</span>(size=<span class=\"number\">25</span>))</span><br><span class=\"line\">    ]</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">fig.update_layout(</span><br><span class=\"line\">    autosize=<span class=\"literal\">False</span>, width=<span class=\"number\">800</span>, height=<span class=\"number\">500</span>,</span><br><span class=\"line\">    title=<span class=\"string\">&#x27;Bar&#x27;</span>,</span><br><span class=\"line\">    xaxis=<span class=\"built_in\">dict</span>(title=<span class=\"string\">&#x27;X&#x27;</span>),</span><br><span class=\"line\">    yaxis=<span class=\"built_in\">dict</span>(title=<span class=\"string\">&#x27;Y&#x27;</span>),</span><br><span class=\"line\">    showlegend=<span class=\"literal\">True</span></span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">fig.show()</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-5-热力图\">2.5 热力图</h3>\n<p>热力图是一种特殊的图表，它是一种通过对色块着色来显示数据的统计图表，在绘图时，需要指定每个颜色映射的规则（一般以颜色的强度或色调为标准）；比如颜色越深的表示数值越大、程度越深或者颜色越浅的数值越大、程度越深。热力图适合用于查看总体的情况、观察特殊值或者显示多个变量之间的差异性、检测它们之间是否存在相关性等等。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df = pd.read_csv(<span class=\"string\">&#x27;../data/MODIS/test_data.csv&#x27;</span>, nrows=<span class=\"number\">10</span>)  <span class=\"comment\"># [10 rows x 22 columns]</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(df.head(<span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"comment\">#    FSC       SR1       SR2       SR3  ...       LST       A2T   SC  LCT</span></span><br><span class=\"line\"><span class=\"comment\"># 0  1.0  0.587019  0.551739  0.565093  ...  0.129661  0.205581  1.0  0.8</span></span><br><span class=\"line\"></span><br><span class=\"line\">pearson = df.corr()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(pearson.values.shape)  <span class=\"comment\"># (22, 22)</span></span><br><span class=\"line\">features = df.columns.values  <span class=\"comment\"># 或者features = pearson.index.values</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(features)  <span class=\"comment\"># [&#x27;FSC&#x27; &#x27;SR1&#x27; &#x27;SR2&#x27; &#x27;SR3&#x27; ...]</span></span><br><span class=\"line\"></span><br><span class=\"line\">fig = go.Figure(</span><br><span class=\"line\">    data=[</span><br><span class=\"line\">        go.Heatmap(x=features, y=features, z=pearson.values, colorscale=<span class=\"string\">&#x27;blues&#x27;</span>)</span><br><span class=\"line\">    ]</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">fig.update_layout(</span><br><span class=\"line\">    autosize=<span class=\"literal\">False</span>, width=<span class=\"number\">900</span>, height=<span class=\"number\">900</span>,</span><br><span class=\"line\">    title=<span class=\"string\">&#x27;皮尔逊相关系数热力图&#x27;</span>,</span><br><span class=\"line\">    <span class=\"comment\"># 以下注释的两行代码用于保存本地时调整字体的大小防止显示不全</span></span><br><span class=\"line\">    <span class=\"comment\"># xaxis=dict(title=&#x27;Feature&#x27;, titlefont=dict(size=10), tickfont=dict(size=8)),</span></span><br><span class=\"line\">    <span class=\"comment\"># yaxis=dict(title=&#x27;Feature&#x27;, titlefont=dict(size=10), tickfont=dict(size=8)),</span></span><br><span class=\"line\">    xaxis=<span class=\"built_in\">dict</span>(title=<span class=\"string\">&#x27;Features&#x27;</span>),</span><br><span class=\"line\">    yaxis=<span class=\"built_in\">dict</span>(title=<span class=\"string\">&#x27;Features&#x27;</span>),</span><br><span class=\"line\">    showlegend=<span class=\"literal\">True</span></span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">fig.show()</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-6-导出图像到本地\">2.6 导出图像到本地</h3>\n<p>首先我们需要安装两个依赖项：<code>orca</code> 和 <code>psutil</code>，<code>orca</code> 在 PyPi 存储库中不可用，因此需要使用 <code>conda</code> 安装：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install -c plotly plotly-orca psutil</span><br></pre></td></tr></table></figure>\n<p>或者直接安装 <code>kaleido</code> 模块：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install kaleido</span><br></pre></td></tr></table></figure>\n<p>安装完成后即可使用 Plotly 的 <code>io</code> 库导出图像（格式可以是 SVG、JPG、PNG等）：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> plotly.io <span class=\"keyword\">as</span> pio</span><br><span class=\"line\"></span><br><span class=\"line\">pio.write_image(fig, <span class=\"string\">&#x27;images/figure.svg&#x27;</span>)</span><br></pre></td></tr></table></figure>\n",
            "tags": [
                "Python"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/39408.html",
            "url": "https://asanosaki.github.io/posts/39408.html",
            "title": "动手学深度学习笔记(李沐)-注意力机制",
            "date_published": "2023-05-21T09:57:00.000Z",
            "content_html": "<blockquote>\n<p>李沐动手学深度学习（PyTorch）课程学习笔记第十章：注意力机制。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-注意力提示\">1. 注意力提示</h2>\n<p>注意力机制（Attention Mechanism）是人们在机器学习模型中嵌入的一种特殊结构，用来自动学习和计算<strong>输入数据对输出数据的贡献</strong>大小。</p>\n<p><strong>非自主性提示</strong>是基于环境中物体的突出性和易见性。想象一下，假如我们面前有五个物品：一份报纸、一篇研究论文、一杯咖啡、一本笔记本和一本书，所有纸制品都是黑白印刷的，但咖啡杯是红色的。换句话说，这个咖啡杯在这种视觉环境中是突出和显眼的，不由自主地引起人们的注意，所以我们会把视力最敏锐的地方放到咖啡上。喝咖啡后，我们会变得兴奋并想读书，所以转过头，重新聚焦眼睛，然后看看书，与咖啡杯是由于突出性导致的选择不同，此时选择书是受到了<strong>认知和意识</strong>的控制，因此注意力在基于<strong>自主性提示</strong>去辅助选择时将更为谨慎。受试者的主观意愿推动，选择的力量也就更强大。自主性的与非自主性的注意力提示解释了人类的注意力的方式，下面来看看如何通过这两种注意力提示，用神经网络来设计注意力机制的框架。</p>\n<p>首先，考虑一个相对简单的状况，即只使用非自主性提示。要想将选择偏向于感官输入，则可以简单地使用参数化的全连接层，甚至是非参数化的最大汇聚层或平均汇聚层。</p>\n<p>因此，“是否包含自主性提示”将注意力机制与全连接层或汇聚层区别开来。在注意力机制的背景下，自主性提示被称为<strong>查询</strong>（query）。给定任何查询，注意力机制通过注意力汇聚（attention pooling）将选择引导至感官输入（sensory inputs，例如中间特征表示）。在注意力机制中，这些感官输入被称为<strong>值</strong>（value）。更通俗的解释，每个值都与一个<strong>键</strong>（key）配对，这可以想象为感官输入的非自主提示。可以通过设计注意力汇聚的方式，便于给定的查询（自主性提示）与键（非自主性提示）进行匹配，这将引导得出最匹配的值（感官输入）。</p>\n<p>平均汇聚层可以被视为输入的加权平均值，其中各输入的权重是一样的。实际上，注意力汇聚得到的是加权平均的总和值，其中权重是在给定的查询和不同的键之间计算得出的。为了可视化注意力权重，需要定义一个 <code>show_heatmaps</code> 函数，其输入 <code>matrices</code> 的形状是 <code>(要显示的行数, 要显示的列数, 查询的数目, 键的数目)</code>。下面使用一个简单的例子进行演示，在本例子中，仅当查询和键相同时，注意力权重为1，否则为0：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">show_heatmaps</span>(<span class=\"params\">matrices, xlabel, ylabel, titles=<span class=\"literal\">None</span>, figsize=(<span class=\"params\"><span class=\"number\">8</span>, <span class=\"number\">6</span></span>), cmap=<span class=\"string\">&#x27;Reds&#x27;</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;显示矩阵热图&quot;&quot;&quot;</span></span><br><span class=\"line\">    d2l.use_svg_display()</span><br><span class=\"line\">    num_rows, num_cols = matrices.shape[<span class=\"number\">0</span>], matrices.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\">    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize, sharex=<span class=\"literal\">True</span>, sharey=<span class=\"literal\">True</span>, squeeze=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, (row_axes, row_matrices) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(<span class=\"built_in\">zip</span>(axes, matrices)):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j, (ax, matrix) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(<span class=\"built_in\">zip</span>(row_axes, row_matrices)):</span><br><span class=\"line\">            pcm = ax.imshow(matrix.detach().numpy(), cmap=cmap)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> i == num_rows - <span class=\"number\">1</span>:</span><br><span class=\"line\">                ax.set_xlabel(xlabel)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> j == <span class=\"number\">0</span>:</span><br><span class=\"line\">                ax.set_ylabel(ylabel)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> titles:</span><br><span class=\"line\">                ax.set_title(titles[j])</span><br><span class=\"line\">    fig.colorbar(pcm, ax=axes, shrink=<span class=\"number\">0.6</span>)</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\">attention_weights = torch.eye(<span class=\"number\">10</span>).reshape((<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">10</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\">show_heatmaps(attention_weights, xlabel=<span class=\"string\">&#x27;Keys&#x27;</span>, ylabel=<span class=\"string\">&#x27;Queries&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>此外可以使用 Plotly 绘制热力图：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> plotly.graph_objects <span class=\"keyword\">as</span> go</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">show_plotly_heatmaps</span>(<span class=\"params\">x=<span class=\"literal\">None</span>, y=<span class=\"literal\">None</span>, z=<span class=\"literal\">None</span>, colorscale=<span class=\"string\">&#x27;reds&#x27;</span>, width=<span class=\"number\">600</span>, height=<span class=\"number\">600</span>, title=<span class=\"literal\">None</span>, xtitle=<span class=\"literal\">None</span>, ytitle=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">    heatmap_fig = go.Figure(</span><br><span class=\"line\">        data=[</span><br><span class=\"line\">            go.Heatmap(x=x, y=y, z=z, colorscale=colorscale)</span><br><span class=\"line\">        ]</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    heatmap_fig.update_layout(</span><br><span class=\"line\">        autosize=<span class=\"literal\">False</span>, width=width, height=height,</span><br><span class=\"line\">        title=title,</span><br><span class=\"line\">        xaxis=<span class=\"built_in\">dict</span>(title=xtitle), yaxis=<span class=\"built_in\">dict</span>(title=ytitle),</span><br><span class=\"line\">        showlegend=<span class=\"literal\">True</span></span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    heatmap_fig.show()</span><br><span class=\"line\"></span><br><span class=\"line\">attention_weights = torch.eye(<span class=\"number\">10</span>)</span><br><span class=\"line\">show_plotly_heatmaps(z=attention_weights, title=<span class=\"string\">&#x27;Attention Weights Heatmap&#x27;</span>, xtitle=<span class=\"string\">&#x27;Keys&#x27;</span>, ytitle=<span class=\"string\">&#x27;Queries&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-注意力汇聚：Nadaraya-Watson核回归\">2. 注意力汇聚：Nadaraya-Watson核回归</h2>\n<p>上节介绍了框架下的注意力机制的主要成分：查询（自主提示）和键（非自主提示）之间的交互形成了注意力汇聚；注意力汇聚有选择地聚合了值（感官输入）以生成最终的输出。本节将介绍注意力汇聚的更多细节，以便从宏观上了解注意力机制在实践中的运作方式。具体来说，1964年提出的 Nadaraya-Watson 核回归模型是一个简单但完整的例子，可以用于演示具有注意力机制的机器学习，其理论介绍可见：<a href=\"https://zh-v2.d2l.ai/chapter_attention-mechanisms/nadaraya-waston.html\">注意力汇聚：Nadaraya-Watson核回归</a>。</p>\n<p>首先生成一个非线性函数的人工数据集：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">n_train = <span class=\"number\">50</span>  <span class=\"comment\"># 训练样本数</span></span><br><span class=\"line\">x_train, _ = torch.sort(torch.rand(n_train) * <span class=\"number\">5</span>)  <span class=\"comment\"># 排序后的训练样本</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">2</span> * torch.sin(x) + x**<span class=\"number\">0.8</span></span><br><span class=\"line\"></span><br><span class=\"line\">y_train = f(x_train) + torch.normal(<span class=\"number\">0.0</span>, <span class=\"number\">0.5</span>, (n_train,))  <span class=\"comment\"># 训练样本的输出</span></span><br><span class=\"line\">x_test = torch.arange(<span class=\"number\">0</span>, <span class=\"number\">5</span>, <span class=\"number\">0.1</span>)  <span class=\"comment\"># 测试样本</span></span><br><span class=\"line\">y_truth = f(x_test)  <span class=\"comment\"># 测试样本的真实输出</span></span><br><span class=\"line\">n_test = <span class=\"built_in\">len</span>(x_test)  <span class=\"comment\"># 测试样本数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(n_test)  <span class=\"comment\"># 50</span></span><br></pre></td></tr></table></figure>\n<p>函数 <code>plot_kernel_reg</code> 将绘制所有的训练样本（样本由圆圈表示），不带噪声项的真实数据生成函数（标记为 <code>Truth</code>），以及学习得到的预测函数（标记为 <code>Pred</code>）。先使用最简单的估计器来解决回归问题，即基于平均汇聚来计算所有训练样本输出值的平均值：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">plot_kernel_reg</span>(<span class=\"params\">y_hat</span>):</span><br><span class=\"line\">    fig = go.Figure(</span><br><span class=\"line\">        data=[</span><br><span class=\"line\">            go.Scatter(x=x_test, y=y_truth, mode=<span class=\"string\">&#x27;lines&#x27;</span>, name=<span class=\"string\">&#x27;Truth&#x27;</span>),</span><br><span class=\"line\">            go.Scatter(x=x_test, y=y_hat, mode=<span class=\"string\">&#x27;lines&#x27;</span>, name=<span class=\"string\">&#x27;Pred&#x27;</span>),</span><br><span class=\"line\">            go.Scatter(x=x_train, y=y_train, mode=<span class=\"string\">&#x27;markers&#x27;</span>, name=<span class=\"string\">&#x27;Sample&#x27;</span>, opacity=<span class=\"number\">0.5</span>)</span><br><span class=\"line\">        ]</span><br><span class=\"line\">    )</span><br><span class=\"line\">    fig.update_layout(</span><br><span class=\"line\">        autosize=<span class=\"literal\">False</span>, width=<span class=\"number\">1200</span>, height=<span class=\"number\">800</span>,</span><br><span class=\"line\">        xaxis=<span class=\"built_in\">dict</span>(title=<span class=\"string\">&#x27;x&#x27;</span>), yaxis=<span class=\"built_in\">dict</span>(title=<span class=\"string\">&#x27;y&#x27;</span>),</span><br><span class=\"line\">        showlegend=<span class=\"literal\">True</span></span><br><span class=\"line\">    )</span><br><span class=\"line\">    fig.show()</span><br><span class=\"line\">    <span class=\"comment\"># d2l.plot(x_test, [y_truth, y_hat], &#x27;x&#x27;, &#x27;y&#x27;, legend=[&#x27;Truth&#x27;, &#x27;Pred&#x27;],</span></span><br><span class=\"line\">    <span class=\"comment\">#          xlim=[0, 5], ylim=[-1, 5])</span></span><br><span class=\"line\">    <span class=\"comment\"># d2l.plt.plot(x_train, y_train, &#x27;o&#x27;, alpha=0.5)</span></span><br><span class=\"line\"></span><br><span class=\"line\">y_hat = torch.repeat_interleave(y_train.mean(), n_test)</span><br><span class=\"line\">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure>\n<p>显然，平均汇聚忽略了输入，Nadaraya-Watson 核回归根据输入的位置对输出进行加权，是一个非参数模型。接下来，我们将基于这个非参数的注意力汇聚模型来绘制预测结果。从绘制的结果会发现新的模型预测线是平滑的，并且比平均汇聚的预测更接近真实。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># X_repeat.shape: (n_test, n_train)</span></span><br><span class=\"line\"><span class=\"comment\"># 每一行都包含着相同的测试输入（例如：同样的查询）</span></span><br><span class=\"line\">X_repeat = x_test.repeat_interleave(n_train).reshape((-<span class=\"number\">1</span>, n_train))</span><br><span class=\"line\"><span class=\"comment\"># x_train包含着键，attention_weights.shape: (n_test, n_train)</span></span><br><span class=\"line\"><span class=\"comment\"># 每一行都包含着要在给定的每个查询的值（y_train）之间分配的注意力权重</span></span><br><span class=\"line\">attention_weights = nn.functional.softmax(-(X_repeat - x_train)**<span class=\"number\">2</span> / <span class=\"number\">2</span>, dim=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># y_hat的每个元素都是值的加权平均值，其中的权重是注意力权重</span></span><br><span class=\"line\">y_hat = torch.matmul(attention_weights, y_train)  <span class=\"comment\"># y_hat.shape: torch.Size([50])</span></span><br><span class=\"line\">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure>\n<p>非参数的 Nadaraya-Watson 核回归具有一致性（consistency）的优点：如果有足够的数据，此模型会收敛到最优结果。尽管如此，我们还是可以轻松地将可学习的参数集成到注意力汇聚中。</p>\n<p>为了更有效地计算小批量数据的注意力，我们可以利用深度学习开发框架中提供的批量矩阵乘法：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.ones((<span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\">Y = torch.ones((<span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.bmm(X, Y).shape)  <span class=\"comment\"># torch.Size([2, 1, 6])</span></span><br></pre></td></tr></table></figure>\n<p>在注意力机制的背景中，我们可以使用小批量矩阵乘法来计算小批量数据中的加权平均值：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weights = torch.ones((<span class=\"number\">2</span>, <span class=\"number\">10</span>)) * <span class=\"number\">0.1</span></span><br><span class=\"line\">values = torch.arange(<span class=\"number\">20.0</span>).reshape((<span class=\"number\">2</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.bmm(weights.unsqueeze(<span class=\"number\">1</span>), values.unsqueeze(-<span class=\"number\">1</span>)))</span><br><span class=\"line\"><span class=\"comment\"># tensor([[[ 4.5000]],</span></span><br><span class=\"line\"><span class=\"comment\">#         [[14.5000]]])</span></span><br></pre></td></tr></table></figure>\n<p>定义 Nadaraya-Watson 核回归的带参数版本为：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">NWKernelRegression</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__(**kwargs)</span><br><span class=\"line\">        self.w = nn.Parameter(torch.rand((<span class=\"number\">1</span>,), requires_grad=<span class=\"literal\">True</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, queries, keys, values</span>):</span><br><span class=\"line\">        <span class=\"comment\"># queries和attention_weights的形状为: (查询个数, “键-值”对个数)</span></span><br><span class=\"line\">        queries = queries.repeat_interleave(keys.shape[<span class=\"number\">1</span>]).reshape((-<span class=\"number\">1</span>, keys.shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">        self.attention_weights = nn.functional.softmax(-((queries - keys) * self.w)**<span class=\"number\">2</span> / <span class=\"number\">2</span>, dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"comment\"># values的形状为: (查询个数, “键-值”对个数)</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.bmm(self.attention_weights.unsqueeze(<span class=\"number\">1</span>), values.unsqueeze(-<span class=\"number\">1</span>)).reshape(-<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n<p>接下来，将训练数据集变换为键和值用于训练注意力模型。在带参数的注意力汇聚模型中，任何一个训练样本的输入都会和除自己以外的所有训练样本的“键-值”对进行计算，从而得到其对应的预测输出：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># X_tile.shape: (n_train, n_train)，每一行都包含着相同的训练输入</span></span><br><span class=\"line\">X_tile = x_train.repeat(n_train, <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># Y_tile.shape: (n_train, n_train)，每一行都包含着相同的训练输出</span></span><br><span class=\"line\">Y_tile = y_train.repeat(n_train, <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># keys.shape: (n_train, n_train-1)，将对角线元素筛去</span></span><br><span class=\"line\">keys = X_tile[(<span class=\"number\">1</span> - torch.eye(n_train)).<span class=\"built_in\">type</span>(torch.<span class=\"built_in\">bool</span>)].reshape((n_train, -<span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"comment\"># values.shape: (n_train, n_train-1)，将对角线元素筛去</span></span><br><span class=\"line\">values = Y_tile[(<span class=\"number\">1</span> - torch.eye(n_train)).<span class=\"built_in\">type</span>(torch.<span class=\"built_in\">bool</span>)].reshape((n_train, -<span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<p>训练带参数的注意力汇聚模型时，使用平方损失函数和随机梯度下降：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = NWKernelRegression()</span><br><span class=\"line\">loss_function = nn.MSELoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">optimizer = torch.optim.SGD(net.parameters(), lr=<span class=\"number\">0.01</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">20</span>):</span><br><span class=\"line\">    optimizer.zero_grad()</span><br><span class=\"line\">    loss = loss_function(net(x_train, keys, values), y_train)</span><br><span class=\"line\">    loss.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">    optimizer.step()</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;epoch <span class=\"subst\">&#123;epoch + <span class=\"number\">1</span>&#125;</span>, loss <span class=\"subst\">&#123;<span class=\"built_in\">float</span>(loss.<span class=\"built_in\">sum</span>()):<span class=\"number\">.6</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>训练完带参数的注意力汇聚模型后可以发现：在尝试拟合带噪声的训练数据时，预测结果绘制的线不如之前非参数模型的平滑，因为与非参数的注意力汇聚模型相比，带参数的模型加入可学习的参数后，曲线在注意力权重较大的区域变得更不平滑。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># keys.shape: (n_test, n_train)，每一行包含着相同的训练输入（例如，相同的键）</span></span><br><span class=\"line\">keys = x_train.repeat(n_test, <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># value.shape: (n_test, n_train)</span></span><br><span class=\"line\">values = y_train.repeat(n_test, <span class=\"number\">1</span>)</span><br><span class=\"line\">y_hat = net(x_test, keys, values).detach()</span><br><span class=\"line\">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-注意力评分函数\">3. 注意力评分函数</h2>\n<p>在上一节中使用了高斯核来对查询和键之间的关系建模。高斯核的指数部分可以视为注意力评分函数（attention scoring function），简称<strong>评分函数</strong>（scoring function），然后把这个函数的输出结果输入到 Softmax 函数中进行运算。通过上述步骤，将得到与键对应的值的概率分布（即注意力权重）。最后，注意力汇聚的输出就是基于这些注意力权重的值的加权和。</p>\n<p>选择不同的注意力评分函数会导致不同的注意力汇聚操作。本节将介绍两个流行的评分函数，稍后将用他们来实现更复杂的注意力机制。</p>\n<p>正如上面提到的，Softmax 操作用于输出一个概率分布作为注意力权重。在某些情况下，并非所有的值都应该被纳入到注意力汇聚中。例如，为了在机器翻译中高效处理小批量数据集，某些文本序列被填充了没有意义的特殊词元。为了仅将有意义的词元作为值来获取注意力汇聚，可以指定一个有效序列长度（即词元的个数），以便在计算 Softmax 时过滤掉超出指定范围的位置。下面的 <code>masked_softmax</code> 函数实现了这样的掩蔽 Softmax 操作（masked softmax operation），其中任何超出有效长度的位置都被掩蔽并置为0。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">masked_softmax</span>(<span class=\"params\">X, valid_lens</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;通过在最后一个轴上掩蔽元素来执行softmax操作&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># X: 3D张量，valid_lens: 1D或2D张量</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> valid_lens <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> nn.functional.softmax(X, dim=-<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        shape = X.shape</span><br><span class=\"line\">        <span class=\"keyword\">if</span> valid_lens.dim() == <span class=\"number\">1</span>:</span><br><span class=\"line\">            valid_lens = torch.repeat_interleave(valid_lens, shape[<span class=\"number\">1</span>])  <span class=\"comment\"># [a, b] -&gt; [a, a, ..., b, b, ...]</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            valid_lens = valid_lens.reshape(-<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0</span></span><br><span class=\"line\">        X = d2l.sequence_mask(X.reshape(-<span class=\"number\">1</span>, shape[-<span class=\"number\">1</span>]), valid_lens, value=-<span class=\"number\">1e6</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> nn.functional.softmax(X.reshape(shape), dim=-<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n<p>为了演示此函数是如何工作的，考虑由两个2*4矩阵表示的样本，这两个样本的有效长度分别为2和3。经过掩蔽 Softmax 操作，超出有效长度的值都被掩蔽为0：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(masked_softmax(torch.rand(<span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>), torch.tensor([<span class=\"number\">2</span>, <span class=\"number\">3</span>])))</span><br><span class=\"line\"><span class=\"comment\"># tensor([[[0.3292, 0.6708, 0.0000, 0.0000],</span></span><br><span class=\"line\"><span class=\"comment\">#          [0.5249, 0.4751, 0.0000, 0.0000]],</span></span><br><span class=\"line\"><span class=\"comment\">#         [[0.3104, 0.4577, 0.2318, 0.0000],</span></span><br><span class=\"line\"><span class=\"comment\">#          [0.3227, 0.3408, 0.3365, 0.0000]]])</span></span><br></pre></td></tr></table></figure>\n<p>同样，也可以使用二维张量，为矩阵样本中的每一行指定有效长度：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(masked_softmax(torch.rand(<span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>), torch.tensor([[<span class=\"number\">1</span>, <span class=\"number\">3</span>], [<span class=\"number\">2</span>, <span class=\"number\">4</span>]])))</span><br><span class=\"line\"><span class=\"comment\"># tensor([[[1.0000, 0.0000, 0.0000, 0.0000],</span></span><br><span class=\"line\"><span class=\"comment\">#          [0.4203, 0.2752, 0.3045, 0.0000]],</span></span><br><span class=\"line\"><span class=\"comment\">#         [[0.4234, 0.5766, 0.0000, 0.0000],</span></span><br><span class=\"line\"><span class=\"comment\">#          [0.2979, 0.1618, 0.2246, 0.3157]]])</span></span><br></pre></td></tr></table></figure>\n<p>接下来将介绍加性注意力与缩放点积注意力，其理论分析可见：<a href=\"https://zh-v2.d2l.ai/chapter_attention-mechanisms/attention-scoring-functions.html\">注意力评分函数</a>。</p>\n<p>一般来说，当查询和键是不同长度的矢量时，可以使用加性注意力作为评分函数。将查询和键连结起来后输入到一个多层感知机（MLP）中，感知机包含一个隐藏层，其隐藏单元数是一个超参数。通过使用 <code>tanh</code> 作为激活函数，并且禁用偏置项：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">AdditiveAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;加性注意力&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, key_size, query_size, num_hiddens, dropout, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(AdditiveAttention, self).__init__(**kwargs)</span><br><span class=\"line\">        self.W_k = nn.Linear(key_size, num_hiddens, bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        self.W_q = nn.Linear(query_size, num_hiddens, bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        self.w_v = nn.Linear(num_hiddens, <span class=\"number\">1</span>, bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        self.dropout = nn.Dropout(dropout)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, queries, keys, values, valid_lens</span>):</span><br><span class=\"line\">        queries, keys = self.W_q(queries), self.W_k(keys)  <span class=\"comment\"># (2, 1, 8), (2, 10, 8)</span></span><br><span class=\"line\">        <span class=\"comment\"># 维度扩展后使用广播方式进行求和</span></span><br><span class=\"line\">        <span class=\"comment\"># 扩展后的queries.shape: (batch_size, 查询的个数, 1, num_hidden)</span></span><br><span class=\"line\">        <span class=\"comment\"># 扩展后的key.shape: (batch_size, 1, “键-值”对的个数, num_hiddens)</span></span><br><span class=\"line\">        features = queries.unsqueeze(<span class=\"number\">2</span>) + keys.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">        features = torch.tanh(features)  <span class=\"comment\"># (2, 1, 10, 8)</span></span><br><span class=\"line\">        <span class=\"comment\"># self.w_v仅有一个输出，因此从形状中移除最后那个大小为1的维度</span></span><br><span class=\"line\">        <span class=\"comment\"># scores.shape: (batch_size, 查询的个数, “键-值”对的个数)</span></span><br><span class=\"line\">        scores = self.w_v(features).squeeze(-<span class=\"number\">1</span>)  <span class=\"comment\"># (2, 1, 10, 1) -&gt; (2, 1, 10)</span></span><br><span class=\"line\">        self.attention_weights = masked_softmax(scores, valid_lens)  <span class=\"comment\"># (2, 1, 10)</span></span><br><span class=\"line\">        <span class=\"comment\"># values.shape: (batch_size, “键-值”对的个数, 值的维度)</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.bmm(self.dropout(self.attention_weights), values)</span><br><span class=\"line\"></span><br><span class=\"line\">queries, keys = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">1</span>, (<span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">20</span>)), torch.ones((<span class=\"number\">2</span>, <span class=\"number\">10</span>, <span class=\"number\">2</span>))</span><br><span class=\"line\"><span class=\"comment\"># values的小批量，两个值矩阵是相同的</span></span><br><span class=\"line\">values = torch.arange(<span class=\"number\">40</span>, dtype=torch.float32).reshape(<span class=\"number\">1</span>, <span class=\"number\">10</span>, <span class=\"number\">4</span>).repeat(<span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">valid_lens = torch.tensor([<span class=\"number\">2</span>, <span class=\"number\">6</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">attention = AdditiveAttention(key_size=<span class=\"number\">2</span>, query_size=<span class=\"number\">20</span>, num_hiddens=<span class=\"number\">8</span>, dropout=<span class=\"number\">0.1</span>)</span><br><span class=\"line\">attention.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(attention(queries, keys, values, valid_lens))</span><br><span class=\"line\"><span class=\"comment\"># tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],</span></span><br><span class=\"line\"><span class=\"comment\">#         [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=&lt;BmmBackward0&gt;)</span></span><br></pre></td></tr></table></figure>\n<p>尽管加性注意力包含了可学习的参数，但由于本例子中每个键都是相同的，所以注意力权重是均匀的，由指定的有效长度决定：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># show_plotly_heatmaps函数在第一节中定义</span></span><br><span class=\"line\">show_plotly_heatmaps(z=attention.attention_weights.detach().reshape((<span class=\"number\">2</span>, <span class=\"number\">10</span>)), height=<span class=\"number\">300</span>, xtitle=<span class=\"string\">&#x27;Keys&#x27;</span>, ytitle=<span class=\"string\">&#x27;Queries&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>使用点积可以得到计算效率更高的评分函数，但是点积操作要求查询和键具有相同的长度，为了演示 <code>DotProductAttention</code> 类，我们使用与先前加性注意力例子中相同的键、值和有效长度。对于点积操作，我们令查询的特征维度与键的特征维度大小相同：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">DotProductAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;缩放点积注意力&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, dropout, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(DotProductAttention, self).__init__(**kwargs)</span><br><span class=\"line\">        self.dropout = nn.Dropout(dropout)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># queries.shape: (batch_size, 查询的个数, d)</span></span><br><span class=\"line\">    <span class=\"comment\"># keys.shape: (batch_size, “键-值”对的个数, d)</span></span><br><span class=\"line\">    <span class=\"comment\"># values.shape: (batch_size, “键-值”对的个数, 值的维度)</span></span><br><span class=\"line\">    <span class=\"comment\"># valid_lens.shape: (batch_size,)或者(batch_size, 查询的个数)</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, queries, keys, values, valid_lens=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        d = queries.shape[-<span class=\"number\">1</span>]</span><br><span class=\"line\">        scores = torch.bmm(queries, keys.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)) / math.sqrt(d)</span><br><span class=\"line\">        self.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.bmm(self.dropout(self.attention_weights), values)</span><br><span class=\"line\"></span><br><span class=\"line\">queries = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">1</span>, (<span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>))</span><br><span class=\"line\">attention = DotProductAttention(dropout=<span class=\"number\">0.5</span>)</span><br><span class=\"line\">attention.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(attention(queries, keys, values, valid_lens))</span><br><span class=\"line\"><span class=\"comment\"># tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],</span></span><br><span class=\"line\"><span class=\"comment\">#         [[10.0000, 11.0000, 12.0000, 13.0000]]])</span></span><br></pre></td></tr></table></figure>\n<p>与加性注意力演示相同，由于键包含的是相同的元素，而这些元素无法通过任何查询进行区分，因此获得了均匀的注意力权重。</p>\n<h2 id=\"4-Bahdanau注意力（使用注意力的seq2seq）\">4. Bahdanau注意力（使用注意力的seq2seq）</h2>\n<p>Bahdanau 注意力模型的原理可见：<a href=\"https://zh-v2.d2l.ai/chapter_attention-mechanisms/bahdanau-attention.html\">Bahdanau 注意力</a>。</p>\n<p>下面看看如何定义 Bahdanau 注意力，实现循环神经网络编码器-解码器。其实，我们只需重新定义解码器即可。为了更方便地显示学习的注意力权重，以下 <code>AttentionDecoder</code> 类定义了带有注意力机制解码器的基本接口：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">sys.path.append(<span class=\"string\">&quot;..&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">from</span> util.functions <span class=\"keyword\">import</span> train_seq2seq, predict_seq2seq, bleu, show_plotly_heatmaps</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">AttentionDecoder</span>(d2l.Decoder):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;带有注意力机制解码器的基本接口&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(AttentionDecoder, self).__init__(**kwargs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">    @property</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">attention_weights</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>\n<p>接下来，让我们在接下来的 <code>Seq2SeqAttentionDecoder</code> 类中实现带有 Bahdanau 注意力的循环神经网络解码器。首先，初始化解码器的状态，需要下面的输入：</p>\n<ul>\n<li>编码器在所有时间步的最终层隐状态，将作为注意力的键和值；</li>\n<li>上一时间步的编码器全层隐状态，将作为初始化解码器的隐状态；</li>\n<li>编码器有效长度（排除在注意力池中填充词元）。</li>\n</ul>\n<p>在每个解码时间步骤中，解码器上一个时间步的最终层隐状态将用作查询。因此，注意力输出和输入嵌入都连结为循环神经网络解码器的输入。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Seq2SeqAttentionDecoder</span>(<span class=\"title class_ inherited__\">AttentionDecoder</span>):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, vocab_size, embed_size, num_hiddens, num_layers, dropout=<span class=\"number\">0</span>, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Seq2SeqAttentionDecoder, self).__init__(**kwargs)</span><br><span class=\"line\">        self.attention = d2l.AdditiveAttention(num_hiddens, num_hiddens, num_hiddens, dropout)</span><br><span class=\"line\">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class=\"line\">        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout)</span><br><span class=\"line\">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">init_state</span>(<span class=\"params\">self, enc_outputs, enc_valid_lens, *args</span>):</span><br><span class=\"line\">        <span class=\"comment\"># outputs.shape: (batch_size, num_steps, num_hiddens)</span></span><br><span class=\"line\">        <span class=\"comment\"># hidden_state.shape: (num_layers, batch_size, num_hiddens)</span></span><br><span class=\"line\">        outputs, hidden_state = enc_outputs</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (outputs.permute(<span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">2</span>), hidden_state, enc_valid_lens)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X, state</span>):</span><br><span class=\"line\">        <span class=\"comment\"># enc_outputs.shape: (batch_size, num_steps, num_hiddens)</span></span><br><span class=\"line\">        <span class=\"comment\"># hidden_state.shape: (num_layers, batch_size, num_hiddens)</span></span><br><span class=\"line\">        enc_outputs, hidden_state, enc_valid_lens = state</span><br><span class=\"line\">        X = self.embedding(X).permute(<span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">2</span>)  <span class=\"comment\"># X.shape: (num_steps, batch_size, embed_size)</span></span><br><span class=\"line\">        outputs, self._attention_weights = [], []</span><br><span class=\"line\">        <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> X:</span><br><span class=\"line\">            <span class=\"comment\"># query.shape: (batch_size, 1, num_hiddens)</span></span><br><span class=\"line\">            query = torch.unsqueeze(hidden_state[-<span class=\"number\">1</span>], dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">            <span class=\"comment\"># context.shape: (batch_size, 1, num_hiddens)</span></span><br><span class=\"line\">            context = self.attention(query, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class=\"line\">            <span class=\"comment\"># 在特征维度上连结</span></span><br><span class=\"line\">            x = torch.cat((context, torch.unsqueeze(x, dim=<span class=\"number\">1</span>)), dim=-<span class=\"number\">1</span>)</span><br><span class=\"line\">            <span class=\"comment\"># 将x变形为(1, batch_size, embed_size + num_hiddens)</span></span><br><span class=\"line\">            out, hidden_state = self.rnn(x.permute(<span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">2</span>), hidden_state)</span><br><span class=\"line\">            outputs.append(out)</span><br><span class=\"line\">            self._attention_weights.append(self.attention.attention_weights)</span><br><span class=\"line\">        <span class=\"comment\"># 全连接层变换后，outputs的形状为(num_steps, batch_size, vocab_size)</span></span><br><span class=\"line\">        outputs = self.dense(torch.cat(outputs, dim=<span class=\"number\">0</span>))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> outputs.permute(<span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">2</span>), [enc_outputs, hidden_state, enc_valid_lens]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">    @property</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">attention_weights</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self._attention_weights</span><br></pre></td></tr></table></figure>\n<p>接下来，使用包含7个时间步的4个序列输入的小批量测试 Bahdanau 注意力解码器：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">encoder = d2l.Seq2SeqEncoder(vocab_size=<span class=\"number\">10</span>, embed_size=<span class=\"number\">8</span>, num_hiddens=<span class=\"number\">16</span>, num_layers=<span class=\"number\">2</span>)</span><br><span class=\"line\">encoder.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\">decoder = Seq2SeqAttentionDecoder(vocab_size=<span class=\"number\">10</span>, embed_size=<span class=\"number\">8</span>, num_hiddens=<span class=\"number\">16</span>, num_layers=<span class=\"number\">2</span>)</span><br><span class=\"line\">decoder.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\">X = torch.zeros((<span class=\"number\">4</span>, <span class=\"number\">7</span>), dtype=torch.long)  <span class=\"comment\"># (batch_size, num_steps)</span></span><br><span class=\"line\">state = decoder.init_state(encoder(X), <span class=\"literal\">None</span>)</span><br><span class=\"line\">output, state = decoder(X, state)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(output.shape, <span class=\"built_in\">len</span>(state), state[<span class=\"number\">0</span>].shape, <span class=\"built_in\">len</span>(state[<span class=\"number\">1</span>]), state[<span class=\"number\">1</span>][<span class=\"number\">0</span>].shape)</span><br><span class=\"line\"><span class=\"comment\"># torch.Size([4, 7, 10]) 3 torch.Size([4, 7, 16]) 2 torch.Size([4, 16])</span></span><br></pre></td></tr></table></figure>\n<p>我们在这里指定超参数，实例化一个带有 Bahdanau 注意力的编码器和解码器，并对这个模型进行机器翻译训练：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">embed_size, num_hiddens, num_layers, dropout = <span class=\"number\">32</span>, <span class=\"number\">32</span>, <span class=\"number\">2</span>, <span class=\"number\">0.1</span></span><br><span class=\"line\">batch_size, num_steps, lr, num_epochs = <span class=\"number\">64</span>, <span class=\"number\">10</span>, <span class=\"number\">0.005</span>, <span class=\"number\">300</span></span><br><span class=\"line\">device = torch.device(<span class=\"string\">&#x27;cuda&#x27;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class=\"line\">encoder = d2l.Seq2SeqEncoder(<span class=\"built_in\">len</span>(src_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class=\"line\">decoder = Seq2SeqAttentionDecoder(<span class=\"built_in\">len</span>(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class=\"line\">net = d2l.EncoderDecoder(encoder, decoder)</span><br><span class=\"line\">train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device, <span class=\"string\">&#x27;../logs/Bahdanau_seq2seq_train_log&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>模型训练后，我们用它将几个英语句子翻译成法语并计算它们的 BLEU 分数：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">engs = [<span class=\"string\">&#x27;go .&#x27;</span>, <span class=\"string\">&quot;i lost .&quot;</span>, <span class=\"string\">&#x27;he\\&#x27;s calm .&#x27;</span>, <span class=\"string\">&#x27;i\\&#x27;m home .&#x27;</span>]</span><br><span class=\"line\">fras = [<span class=\"string\">&#x27;va !&#x27;</span>, <span class=\"string\">&#x27;j\\&#x27;ai perdu .&#x27;</span>, <span class=\"string\">&#x27;il est calme .&#x27;</span>, <span class=\"string\">&#x27;je suis chez moi .&#x27;</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> eng, fra <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(engs, fras):</span><br><span class=\"line\">    translation, dec_attention_weight_seq = predict_seq2seq(net, eng, src_vocab, tgt_vocab, num_steps, device, <span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;<span class=\"subst\">&#123;eng&#125;</span> =&gt; <span class=\"subst\">&#123;translation&#125;</span>, bleu <span class=\"subst\">&#123;bleu(translation, fra, k=<span class=\"number\">2</span>):<span class=\"number\">.3</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>训练结束后，下面通过可视化注意力权重会发现每个查询都会在键值对上分配不同的权重，这说明在每个解码步中，输入序列的不同部分被选择性地聚集在注意力池中：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">attention_weights = torch.cat([step[<span class=\"number\">0</span>][<span class=\"number\">0</span>][<span class=\"number\">0</span>] <span class=\"keyword\">for</span> step <span class=\"keyword\">in</span> dec_attention_weight_seq], <span class=\"number\">0</span>).reshape((-<span class=\"number\">1</span>, num_steps))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加上一个包含序列结束词元</span></span><br><span class=\"line\">show_plotly_heatmaps(z=attention_weights[:, :<span class=\"built_in\">len</span>(engs[-<span class=\"number\">1</span>].split()) + <span class=\"number\">1</span>].cpu().detach(), xtitle=<span class=\"string\">&#x27;Keys&#x27;</span>, ytitle=<span class=\"string\">&#x27;Queries&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<h2 id=\"5-多头注意力\">5. 多头注意力</h2>\n<p>在实践中，当给定相同的查询、键和值的集合时，我们希望模型可以基于相同的注意力机制学习到不同的行为，然后将不同的行为作为知识组合起来，捕获序列内各种范围的依赖关系（例如，短距离依赖和长距离依赖关系）。因此，允许注意力机制组合使用查询、键和值的不同<strong>子空间表示</strong>（representation subspaces）可能是有益的。</p>\n<p>为此，与其只使用单独一个注意力汇聚，我们可以用独立学习得到的 <code>h</code> 组不同的<strong>线性投影</strong>（linear projections）来变换查询、键和值。然后，这 <code>h</code> 组变换后的查询、键和值将并行地送到注意力汇聚中。最后，将这 <code>h</code> 个注意力汇聚的输出拼接在一起，并且通过另一个可以学习的线性投影进行变换，以产生最终输出。这种设计被称为<strong>多头注意力</strong>（multihead attention）。对于 <code>h</code> 个注意力汇聚输出，每一个注意力汇聚都被称作一个<strong>头</strong>（head）。基于这种设计，每个头都可能会关注输入的不同部分，可以表示比简单加权平均值更复杂的函数。</p>\n<p>多头注意力模型的原理可见：<a href=\"https://zh-v2.d2l.ai/chapter_attention-mechanisms/multihead-attention.html\">多头注意力</a>。</p>\n<p>在实现过程中通常选择缩放点积注意力作为每一个注意力头：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MultiHeadAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;多头注意力&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, key_size, query_size, value_size, num_hiddens, num_heads, dropout, bias=<span class=\"literal\">False</span>, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(MultiHeadAttention, self).__init__(**kwargs)</span><br><span class=\"line\">        self.num_heads = num_heads</span><br><span class=\"line\">        self.attention = d2l.DotProductAttention(dropout)</span><br><span class=\"line\">        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)</span><br><span class=\"line\">        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)</span><br><span class=\"line\">        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)</span><br><span class=\"line\">        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, queries, keys, values, valid_lens</span>):</span><br><span class=\"line\">        <span class=\"comment\"># queries/keys/values的形状: (batch_size, 查询或者“键-值”对的个数, query_size/key_size/value_size)</span></span><br><span class=\"line\">        <span class=\"comment\"># valid_lens的形状: (batch_size,)或(batch_size, 查询的个数)</span></span><br><span class=\"line\">        <span class=\"comment\"># 经过变换后，输出的queries/keys/values的形状: (batch_size * num_heads, 查询或者“键-值”对的个数, num_hiddens / num_heads)</span></span><br><span class=\"line\">        queries = transpose_qkv(self.W_q(queries), self.num_heads)</span><br><span class=\"line\">        keys = transpose_qkv(self.W_k(keys), self.num_heads)</span><br><span class=\"line\">        values = transpose_qkv(self.W_v(values), self.num_heads)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> valid_lens <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 在轴0，将每一项（标量或者矢量）复制num_heads次</span></span><br><span class=\"line\">            valid_lens = torch.repeat_interleave(valid_lens, repeats=self.num_heads, dim=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># output的形状: (batch_size * num_heads, 查询的个数, num_hiddens / num_heads)</span></span><br><span class=\"line\">        output = self.attention(queries, keys, values, valid_lens)</span><br><span class=\"line\">        <span class=\"comment\"># output_concat的形状: (batch_size, 查询的个数, num_hiddens)</span></span><br><span class=\"line\">        output_concat = transpose_output(output, self.num_heads)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.W_o(output_concat)</span><br></pre></td></tr></table></figure>\n<p>为了能够使多个头并行计算，上面的 <code>MultiHeadAttention</code> 类将使用下面定义的两个转置函数。具体来说，<code>transpose_output</code> 函数反转了 <code>transpose_qkv</code> 函数的操作：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">transpose_qkv</span>(<span class=\"params\">X, num_heads</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;为了多注意力头的并行计算而变换形状&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 输入X的形状: (batch_size, 查询或者“键-值”对的个数, num_hiddens)</span></span><br><span class=\"line\">    X = X.reshape(X.shape[<span class=\"number\">0</span>], X.shape[<span class=\"number\">1</span>], num_heads, -<span class=\"number\">1</span>)  <span class=\"comment\"># (batch_size, 查询或者“键-值”对的个数, num_heads, num_hiddens/num_heads)</span></span><br><span class=\"line\">    X = X.permute(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">3</span>)  <span class=\"comment\"># (batch_size, num_heads, 查询或者“键-值”对的个数, num_hiddens/num_heads)</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> X.reshape(-<span class=\"number\">1</span>, X.shape[<span class=\"number\">2</span>], X.shape[<span class=\"number\">3</span>])  <span class=\"comment\"># (batch_size*num_heads, 查询或者“键-值”对的个数, num_hiddens/num_heads)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">transpose_output</span>(<span class=\"params\">X, num_heads</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;逆转transpose_qkv函数的操作&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 输入X的形状: (batch_size*num_heads, 查询或者“键-值”对的个数, num_hiddens/num_heads)</span></span><br><span class=\"line\">    X = X.reshape(-<span class=\"number\">1</span>, num_heads, X.shape[<span class=\"number\">1</span>], X.shape[<span class=\"number\">2</span>])  <span class=\"comment\"># (batch_size, num_heads, 查询或者“键-值”对的个数, num_hiddens/num_heads)</span></span><br><span class=\"line\">    X = X.permute(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">3</span>)  <span class=\"comment\"># (batch_size, 查询或者“键-值”对的个数, num_heads, num_hiddens/num_heads)</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> X.reshape(X.shape[<span class=\"number\">0</span>], X.shape[<span class=\"number\">1</span>], -<span class=\"number\">1</span>)  <span class=\"comment\"># (batch_size, 查询或者“键-值”对的个数, num_hiddens)</span></span><br></pre></td></tr></table></figure>\n<p>下面使用键和值相同的小例子来测试我们编写的 <code>MultiHeadAttention</code> 类。多头注意力输出的形状是 <code>(batch_size, num_queries, num_hiddens)</code>：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_hiddens, num_heads = <span class=\"number\">100</span>, <span class=\"number\">5</span></span><br><span class=\"line\">attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens, num_hiddens, num_heads, <span class=\"number\">0.5</span>)</span><br><span class=\"line\">attention.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size, num_queries = <span class=\"number\">2</span>, <span class=\"number\">4</span></span><br><span class=\"line\">num_kvpairs, valid_lens = <span class=\"number\">6</span>, torch.tensor([<span class=\"number\">3</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\">X = torch.ones((batch_size, num_queries, num_hiddens))</span><br><span class=\"line\">Y = torch.ones((batch_size, num_kvpairs, num_hiddens))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(attention(X, Y, Y, valid_lens).shape)  <span class=\"comment\"># torch.Size([2, 4, 100])</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"6-自注意力和位置编码\">6. 自注意力和位置编码</h2>\n<p>在深度学习中，经常使用卷积神经网络（CNN）或循环神经网络（RNN）对序列进行编码。想象一下，有了注意力机制之后，我们将词元序列输入注意力池化中，以便同一组词元同时充当查询、键和值。具体来说，每个查询都会关注所有的键-值对并生成一个注意力输出。当查询、键和值来自同一组输入时被称为<strong>自注意力</strong>（self-attention），也被称为<strong>内部注意力</strong>（intra-attention）。本节将使用自注意力进行序列编码，以及如何使用序列的顺序作为补充信息。</p>\n<p>自注意力模型的原理可见：<a href=\"https://zh-v2.d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html\">自注意力和位置编码</a>。</p>\n<p>下面的代码片段是基于多头注意力对一个张量完成自注意力的计算，张量的形状为 <code>(批量大小, 时间步的数目或词元序列的长度, h)</code>，输出与输入的张量形状相同：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"><span class=\"keyword\">from</span> util.functions <span class=\"keyword\">import</span> show_plotly_heatmaps</span><br><span class=\"line\"></span><br><span class=\"line\">num_hiddens, num_heads = <span class=\"number\">100</span>, <span class=\"number\">5</span></span><br><span class=\"line\">attention = d2l.MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens, num_hiddens, num_heads, <span class=\"number\">0.5</span>)</span><br><span class=\"line\">attention.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size, num_queries, valid_lens = <span class=\"number\">2</span>, <span class=\"number\">4</span>, torch.tensor([<span class=\"number\">3</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\">X = torch.ones((batch_size, num_queries, num_hiddens))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(attention(X, X, X, valid_lens).shape)  <span class=\"comment\"># torch.Size([2, 4, 100])</span></span><br></pre></td></tr></table></figure>\n<p>在处理词元序列时，循环神经网络是逐个的重复地处理词元的，而自注意力则因为并行计算而放弃了顺序操作。为了使用序列的顺序信息，通过在输入表示中添加<strong>位置编码</strong>（positional encoding）来注入绝对的或相对的位置信息。位置编码可以通过学习得到也可以直接固定得到。接下来描述的是基于正弦函数和余弦函数的固定位置编码。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">PositionalEncoding</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;位置编码&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, num_hiddens, dropout, max_len=<span class=\"number\">1000</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(PositionalEncoding, self).__init__()</span><br><span class=\"line\">        self.dropout = nn.Dropout(dropout)</span><br><span class=\"line\">        <span class=\"comment\"># 创建一个足够长的P</span></span><br><span class=\"line\">        self.P = torch.zeros((<span class=\"number\">1</span>, max_len, num_hiddens))</span><br><span class=\"line\">        X = torch.arange(max_len, dtype=torch.float32).reshape(-<span class=\"number\">1</span>, <span class=\"number\">1</span>) /\\</span><br><span class=\"line\">            torch.<span class=\"built_in\">pow</span>(<span class=\"number\">10000</span>, torch.arange(<span class=\"number\">0</span>, num_hiddens, <span class=\"number\">2</span>, dtype=torch.float32) / num_hiddens)</span><br><span class=\"line\">        self.P[:, :, <span class=\"number\">0</span>::<span class=\"number\">2</span>] = torch.sin(X)</span><br><span class=\"line\">        self.P[:, :, <span class=\"number\">1</span>::<span class=\"number\">2</span>] = torch.cos(X)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        X = X + self.P[:, :X.shape[<span class=\"number\">1</span>], :].to(X.device)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.dropout(X)</span><br></pre></td></tr></table></figure>\n<p>在位置嵌入矩阵 <code>P</code> 中，行代表词元在序列中的位置，列代表位置编码的不同维度。从下面的例子中可以看到位置嵌入矩阵的第6列和第7列的频率高于第8列和第9列。第6列和第7列之间的偏移量（第8列和第9列相同）是由于正弦函数和余弦函数的交替：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">encoding_dim, num_steps = <span class=\"number\">32</span>, <span class=\"number\">60</span></span><br><span class=\"line\">pos_encoding = PositionalEncoding(encoding_dim, <span class=\"number\">0</span>)</span><br><span class=\"line\">pos_encoding.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\">X = pos_encoding(torch.zeros((<span class=\"number\">1</span>, num_steps, encoding_dim)))</span><br><span class=\"line\">P = pos_encoding.P[:, :X.shape[<span class=\"number\">1</span>], :]</span><br><span class=\"line\">d2l.plot(torch.arange(num_steps), P[<span class=\"number\">0</span>, :, <span class=\"number\">6</span>:<span class=\"number\">10</span>].T, xlabel=<span class=\"string\">&#x27;Row (position)&#x27;</span>, figsize=(<span class=\"number\">8</span>, <span class=\"number\">4</span>),</span><br><span class=\"line\">         legend=[<span class=\"string\">&quot;Col %d&quot;</span> % d <span class=\"keyword\">for</span> d <span class=\"keyword\">in</span> torch.arange(<span class=\"number\">6</span>, <span class=\"number\">10</span>)])</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>通过绘制热力图可以看到，位置编码通过使用三角函数在编码维度上降低频率：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">P = P[<span class=\"number\">0</span>, :, :]</span><br><span class=\"line\">show_plotly_heatmaps(z=P, xtitle=<span class=\"string\">&#x27;Column (encoding dimension)&#x27;</span>, ytitle=<span class=\"string\">&#x27;Row (position)&#x27;</span>, colorscale=<span class=\"string\">&#x27;Blues&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<h2 id=\"7-Transformer\">7. Transformer</h2>\n",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/7592.html",
            "url": "https://asanosaki.github.io/posts/7592.html",
            "title": "动手学深度学习笔记(李沐)-现代循环神经网络",
            "date_published": "2023-04-11T04:58:00.000Z",
            "content_html": "<blockquote>\n<p>李沐动手学深度学习（PyTorch）课程学习笔记第九章：现代循环神经网络。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-门控循环单元（GRU）\">1. 门控循环单元（GRU）</h2>\n<p>在<a href=\"https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/bptt.html\">通过时间反向传播</a>中，我们讨论了如何在循环神经网络中计算梯度，以及矩阵连续乘积可以导致梯度消失或梯度爆炸的问题。下面我们简单思考一下这种梯度异常在实践中的意义，我们可能会遇到以下的情况：</p>\n<ul>\n<li>早期观测值对预测所有未来观测值具有非常重要的意义。考虑一个极端情况，其中第一个观测值包含一个校验和，目标是在序列的末尾辨别校验和是否正确。在这种情况下，第一个词元的影响至关重要。我们希望有某些机制能够<strong>在一个记忆元里存储重要的早期信息</strong>。如果没有这样的机制，我们将不得不给这个观测值指定一个非常大的梯度，因为它会影响所有后续的观测值。</li>\n<li>一些词元没有相关的观测值。例如，在对网页内容进行情感分析时，可能有一些辅助 HTML 代码与网页传达的情绪无关。我们希望有一些机制来<strong>跳过隐状态表示中的此类词元</strong>。</li>\n<li>序列的各个部分之间存在逻辑中断。例如，书的章节之间可能会有过渡存在，或者证券的熊市和牛市之间可能会有过渡存在。在这种情况下，最好有一种方法来<strong>重置我们的内部状态表示</strong>。</li>\n</ul>\n<p>在学术界已经提出了许多方法来解决这类问题。其中最早的方法是<strong>长短期记忆</strong>（long-short-term memory，LSTM），我们将在下一节中讨论。<strong>门控循环单元</strong>（gated recurrent unit，GRU）是一个稍微简化的变体，通常能够提供同等的效果，并且计算的速度明显更快。由于门控循环单元更简单，我们从它开始解读。</p>\n<p>门控循环单元与普通的循环神经网络之间的关键区别在于：前者支持<strong>隐状态的门控</strong>。这意味着模型有专门的机制来确定应该何时更新隐状态，以及应该何时重置隐状态。这些机制是可学习的，并且能够解决了上面列出的问题。例如，如果第一个词元非常重要，模型将学会在第一次观测之后不更新隐状态。同样，模型也可以学会跳过不相关的临时观测。最后，模型还将学会在需要的时候重置隐状态。下面我们将详细讨论各类门控。</p>\n<p>我们首先介绍<strong>重置门</strong>（reset gate）和<strong>更新门</strong>（update gate）。我们把它们设计成 <code>(0, 1)</code> 区间中的向量，这样我们就可以进行<strong>凸组合</strong>。重置门允许我们<strong>控制可能还想记住</strong>的过去状态的数量；更新门将允许我们<strong>控制新状态中有多少个是旧状态的副本</strong>。</p>\n<p>我们从构造这些门控开始。重置门和更新门的输入是由当前时间步的输入和前一时间步的隐状态给出。两个门的输出是由使用 Sigmoid 激活函数的两个全连接层给出。门控循环单元的数学表达详见：<a href=\"https://zh-v2.d2l.ai/chapter_recurrent-modern/gru.html\">门控循环单元（GRU）</a>。</p>\n<h3 id=\"1-1-门控循环单元的从零开始实现\">1.1 门控循环单元的从零开始实现</h3>\n<p>为了更好地理解门控循环单元模型，我们从零开始实现它。首先，我们读取上一章中使用的时间机器数据集：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.tensorboard <span class=\"keyword\">import</span> SummaryWriter</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"><span class=\"keyword\">from</span> tqdm <span class=\"keyword\">import</span> tqdm</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size, num_steps = <span class=\"number\">32</span>, <span class=\"number\">35</span></span><br><span class=\"line\">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure>\n<p>下一步是初始化模型参数。我们从标准差为0.01的高斯分布中提取权重，并将偏置项设为0，超参数 <code>num_hiddens</code> 定义隐藏单元的数量，实例化与更新门、重置门、候选隐状态和输出层相关的所有权重和偏置：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_params</span>(<span class=\"params\">vocab_size, num_hiddens, device</span>):</span><br><span class=\"line\">    num_inputs = num_outputs = vocab_size</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">normal</span>(<span class=\"params\">shape</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.randn(size=shape, device=device) * <span class=\"number\">0.01</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">three</span>():</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (normal((num_inputs, num_hiddens)), normal((num_hiddens, num_hiddens)), torch.zeros(num_hiddens, device=device))</span><br><span class=\"line\"></span><br><span class=\"line\">    W_xz, W_hz, b_z = three()  <span class=\"comment\"># 更新门参数</span></span><br><span class=\"line\">    W_xr, W_hr, b_r = three()  <span class=\"comment\"># 重置门参数</span></span><br><span class=\"line\">    W_xh, W_hh, b_h = three()  <span class=\"comment\"># 候选隐状态参数</span></span><br><span class=\"line\">    <span class=\"comment\"># 输出层参数</span></span><br><span class=\"line\">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class=\"line\">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class=\"line\">    <span class=\"comment\"># 附加梯度</span></span><br><span class=\"line\">    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> params:</span><br><span class=\"line\">        param.requires_grad_(<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> params</span><br></pre></td></tr></table></figure>\n<p>现在我们将定义隐状态的初始化函数 <code>init_gru_state</code>。与从零开始实现 RNN 中定义的 <code>init_rnn_state</code> 函数一样，此函数返回一个形状为 <code>(批量大小, 隐藏单元个数)</code> 的张量，张量的值全部为零：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_gru_state</span>(<span class=\"params\">batch_size, num_hiddens, device</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (torch.zeros((batch_size, num_hiddens), device=device),)</span><br></pre></td></tr></table></figure>\n<p>现在我们准备定义门控循环单元模型，模型的架构与基本的循环神经网络单元是相同的，只是权重更新公式更为复杂：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">gru</span>(<span class=\"params\">inputs, state, params</span>):</span><br><span class=\"line\">    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class=\"line\">    H, = state</span><br><span class=\"line\">    outputs = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X <span class=\"keyword\">in</span> inputs:</span><br><span class=\"line\">        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)  <span class=\"comment\"># @为矩阵乘法，相当于torch.mm</span></span><br><span class=\"line\">        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)</span><br><span class=\"line\">        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)</span><br><span class=\"line\">        H = Z * H + (<span class=\"number\">1</span> - Z) * H_tilda</span><br><span class=\"line\">        Y = H @ W_hq + b_q</span><br><span class=\"line\">        outputs.append(Y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.cat(outputs, dim=<span class=\"number\">0</span>), (H,)</span><br></pre></td></tr></table></figure>\n<p>训练和预测的工作方式与从零开始实现 RNN 完全相同。训练结束后，我们分别打印输出训练集的困惑度，以及前缀 <code>time traveler</code> 和 <code>traveler</code> 的预测序列上的困惑度：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">device = torch.device(<span class=\"string\">&#x27;cuda&#x27;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\">num_hiddens, num_epochs, lr = <span class=\"number\">256</span>, <span class=\"number\">500</span>, <span class=\"number\">1</span></span><br><span class=\"line\">net = d2l.RNNModelScratch(<span class=\"built_in\">len</span>(vocab), num_hiddens, device, get_params, init_gru_state, gru)</span><br><span class=\"line\">train(net, train_iter, vocab, lr, num_epochs, device)  <span class=\"comment\"># 与从零开始实现RNN的train函数相同，此处不再实现</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"1-2-门控循环单元的简洁实现\">1.2 门控循环单元的简洁实现</h3>\n<p>高级 API 包含了前文介绍的所有配置细节，所以我们可以直接实例化门控循环单元模型。这段代码的运行速度要快得多，因为它使用的是编译好的运算符而不是 Python 来处理之前阐述的许多细节：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">RNNModel</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, rnn_layer, vocab_size, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(RNNModel, self).__init__(**kwargs)</span><br><span class=\"line\">        self.rnn = rnn_layer</span><br><span class=\"line\">        self.vocab_size = vocab_size</span><br><span class=\"line\">        self.num_hiddens = self.rnn.hidden_size</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> self.rnn.bidirectional:</span><br><span class=\"line\">            self.num_directions = <span class=\"number\">1</span></span><br><span class=\"line\">            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.num_directions = <span class=\"number\">2</span></span><br><span class=\"line\">            self.linear = nn.Linear(self.num_hiddens * <span class=\"number\">2</span>, self.vocab_size)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, inputs, state</span>):</span><br><span class=\"line\">        X = F.one_hot(inputs.T.long(), self.vocab_size)</span><br><span class=\"line\">        X = X.to(torch.float32)</span><br><span class=\"line\">        Y, state = self.rnn(X, state)</span><br><span class=\"line\">        output = self.linear(Y.reshape((-<span class=\"number\">1</span>, Y.shape[-<span class=\"number\">1</span>])))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, state</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">begin_state</span>(<span class=\"params\">self, device, batch_size=<span class=\"number\">1</span></span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(self.rnn, nn.LSTM):</span><br><span class=\"line\">            <span class=\"keyword\">return</span> torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> (torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device),</span><br><span class=\"line\">                    torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device))</span><br><span class=\"line\"></span><br><span class=\"line\">gru_layer = nn.GRU(<span class=\"built_in\">len</span>(vocab), num_hiddens)</span><br><span class=\"line\">net = RNNModel(gru_layer, <span class=\"built_in\">len</span>(vocab))</span><br><span class=\"line\">net = net.to(device)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_epoch</span>(<span class=\"params\">net, train_iter, loss_function, optimizer, device, use_random_iter</span>):</span><br><span class=\"line\">    state = <span class=\"literal\">None</span></span><br><span class=\"line\">    train_loss = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X, Y <span class=\"keyword\">in</span> tqdm(train_iter):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> state <span class=\"keyword\">is</span> <span class=\"literal\">None</span> <span class=\"keyword\">or</span> use_random_iter:</span><br><span class=\"line\">            state = net.begin_state(batch_size=X.shape[<span class=\"number\">0</span>], device=device)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, nn.Module) <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(state, <span class=\"built_in\">tuple</span>):</span><br><span class=\"line\">                state.detach_()</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                <span class=\"keyword\">for</span> s <span class=\"keyword\">in</span> state:</span><br><span class=\"line\">                    s.detach_()</span><br><span class=\"line\">        y = Y.T.reshape(-<span class=\"number\">1</span>)</span><br><span class=\"line\">        X, y = X.to(device), y.to(device)</span><br><span class=\"line\">        loss_function.to(device)</span><br><span class=\"line\">        y_hat, state = net(X, state)</span><br><span class=\"line\">        loss = loss_function(y_hat, y.long()).mean()</span><br><span class=\"line\">        optimizer.zero_grad()</span><br><span class=\"line\">        loss.backward()</span><br><span class=\"line\">        d2l.grad_clipping(net, <span class=\"number\">1</span>)</span><br><span class=\"line\">        optimizer.step()</span><br><span class=\"line\">        train_loss.append(loss)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> math.exp(<span class=\"built_in\">sum</span>(train_loss) / <span class=\"built_in\">len</span>(train_loss))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">net, train_iter, vocab, lr, num_epochs, device, use_random_iter=<span class=\"literal\">False</span></span>):</span><br><span class=\"line\">    loss_function = nn.CrossEntropyLoss()</span><br><span class=\"line\">    optimizer = torch.optim.SGD(net.parameters(), lr)</span><br><span class=\"line\">    pred = <span class=\"keyword\">lambda</span> prefix: d2l.predict_ch8(prefix, <span class=\"number\">50</span>, net, vocab, device)</span><br><span class=\"line\"></span><br><span class=\"line\">    writer = SummaryWriter(<span class=\"string\">&#x27;../logs/GRU_train_log&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        ppl = train_epoch(net, train_iter, loss_function, optimizer, device, use_random_iter)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (epoch + <span class=\"number\">1</span>) % <span class=\"number\">10</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(pred(<span class=\"string\">&#x27;time traveller&#x27;</span>))</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Perplexity: <span class=\"subst\">&#123;ppl:<span class=\"number\">.1</span>f&#125;</span>&#x27;</span>)</span><br><span class=\"line\">            writer.add_scalar(<span class=\"string\">&#x27;train_loss&#x27;</span>, ppl, epoch + <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(pred(<span class=\"string\">&#x27;time traveller&#x27;</span>))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(pred(<span class=\"string\">&#x27;traveller&#x27;</span>))</span><br><span class=\"line\">    writer.close()</span><br><span class=\"line\"></span><br><span class=\"line\">train(net, train_iter, vocab, lr, num_epochs, device)</span><br><span class=\"line\"><span class=\"comment\"># Perplexity: 1.0</span></span><br><span class=\"line\"><span class=\"comment\"># time travelleryou can show black is white by argument said filby</span></span><br><span class=\"line\"><span class=\"comment\"># travelleryou can show black is white by argument said filby</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"2-长短期记忆网络（LSTM）\">2. 长短期记忆网络（LSTM）</h2>\n<p>长期以来，隐变量模型存在着长期信息保存和短期输入缺失的问题。解决这一问题的最早方法之一是长短期存储器（long short-term memory，LSTM）。它有许多与门控循环单元一样的属性。</p>\n<p>可以说，长短期记忆网络的设计灵感来自于计算机的逻辑门。长短期记忆网络引入了记忆元（memory cell），或简称为单元（cell）。有些文献认为记忆元是隐状态的一种特殊类型，它们与隐状态具有相同的形状，其设计目的是用于记录附加的信息。为了控制记忆元，我们需要许多门。其中一个门用来从单元中输出条目，我们将其称为<strong>输出门</strong>（output gate）。另外一个门用来决定何时将数据读入单元，我们将其称为<strong>输入门</strong>（input gate）。我们还需要一种机制来重置单元的内容，由<strong>遗忘门</strong>（forget gate）来管理，这种设计的动机与门控循环单元相同，能够通过专用机制决定什么时候记忆或忽略隐状态中的输入。</p>\n<p>长短期记忆网络的数学表达详见：<a href=\"https://zh-v2.d2l.ai/chapter_recurrent-modern/lstm.html\">长短期记忆网络（LSTM）</a>。</p>\n<h3 id=\"2-1-长短期记忆网络的从零开始实现\">2.1 长短期记忆网络的从零开始实现</h3>\n<p>现在，我们从零开始实现长短期记忆网络，我们首先加载时光机器数据集：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.tensorboard <span class=\"keyword\">import</span> SummaryWriter</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"><span class=\"keyword\">from</span> tqdm <span class=\"keyword\">import</span> tqdm</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size, num_steps = <span class=\"number\">32</span>, <span class=\"number\">35</span></span><br><span class=\"line\">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure>\n<p>接下来，我们需要定义和初始化模型参数。如前所述，超参数 <code>num_hiddens</code> 定义隐藏单元的数量。我们按照标准差0.01的高斯分布初始化权重，并将偏置项设为0：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_lstm_params</span>(<span class=\"params\">vocab_size, num_hiddens, device</span>):</span><br><span class=\"line\">    num_inputs = num_outputs = vocab_size</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">normal</span>(<span class=\"params\">shape</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.randn(size=shape, device=device) * <span class=\"number\">0.01</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">three</span>():</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (normal((num_inputs, num_hiddens)), normal((num_hiddens, num_hiddens)), torch.zeros(num_hiddens, device=device))</span><br><span class=\"line\"></span><br><span class=\"line\">    W_xi, W_hi, b_i = three()  <span class=\"comment\"># 输入门参数</span></span><br><span class=\"line\">    W_xf, W_hf, b_f = three()  <span class=\"comment\"># 遗忘门参数</span></span><br><span class=\"line\">    W_xo, W_ho, b_o = three()  <span class=\"comment\"># 输出门参数</span></span><br><span class=\"line\">    W_xc, W_hc, b_c = three()  <span class=\"comment\"># 候选记忆元参数</span></span><br><span class=\"line\">    <span class=\"comment\"># 输出层参数</span></span><br><span class=\"line\">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class=\"line\">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class=\"line\">    <span class=\"comment\"># 附加梯度</span></span><br><span class=\"line\">    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> params:</span><br><span class=\"line\">        param.requires_grad_(<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> params</span><br></pre></td></tr></table></figure>\n<p>在初始化函数中，长短期记忆网络的隐状态需要返回一个额外的记忆元，单元的值为0，形状为 <code>(批量大小, 隐藏单元数)</code>。因此，我们得到以下的状态初始化：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_lstm_state</span>(<span class=\"params\">batch_size, num_hiddens, device</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (torch.zeros((batch_size, num_hiddens), device=device),</span><br><span class=\"line\">            torch.zeros((batch_size, num_hiddens), device=device))</span><br></pre></td></tr></table></figure>\n<p>实际模型的定义与我们前面讨论的一样：提供三个门和一个额外的记忆元。请注意，只有隐状态 <code>H</code> 才会传递到输出层，而记忆元 <code>C</code> 不直接参与输出计算：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">lstm</span>(<span class=\"params\">inputs, state, params</span>):</span><br><span class=\"line\">    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q] = params</span><br><span class=\"line\">    (H, C) = state</span><br><span class=\"line\">    outputs = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X <span class=\"keyword\">in</span> inputs:</span><br><span class=\"line\">        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)</span><br><span class=\"line\">        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)</span><br><span class=\"line\">        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)</span><br><span class=\"line\">        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)</span><br><span class=\"line\">        C = F * C + I * C_tilda</span><br><span class=\"line\">        H = O * torch.tanh(C)</span><br><span class=\"line\">        Y = (H @ W_hq) + b_q</span><br><span class=\"line\">        outputs.append(Y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.cat(outputs, dim=<span class=\"number\">0</span>), (H, C)</span><br></pre></td></tr></table></figure>\n<p>让我们通过实例化从零实现 RNN 章节中引入的 <code>RNNModelScratch</code> 类来训练一个长短期记忆网络，就如我们在上一节中所做的一样：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">device = torch.device(<span class=\"string\">&#x27;cuda&#x27;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\">num_hiddens, num_epochs, lr = <span class=\"number\">256</span>, <span class=\"number\">500</span>, <span class=\"number\">1</span></span><br><span class=\"line\">net = d2l.RNNModelScratch(<span class=\"built_in\">len</span>(vocab), num_hiddens, device, get_lstm_params, init_lstm_state, lstm)</span><br><span class=\"line\">train(net, train_iter, vocab, lr, num_epochs, device)  <span class=\"comment\"># 与从零开始实现RNN的train函数相同，此处不再实现</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"2-2-长短期记忆网络的简洁实现\">2.2 长短期记忆网络的简洁实现</h3>\n<p>使用高级 API，我们可以直接实例化 LSTM 模型：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">RNNModel</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, rnn_layer, vocab_size, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(RNNModel, self).__init__(**kwargs)</span><br><span class=\"line\">        self.rnn = rnn_layer</span><br><span class=\"line\">        self.vocab_size = vocab_size</span><br><span class=\"line\">        self.num_hiddens = self.rnn.hidden_size</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> self.rnn.bidirectional:</span><br><span class=\"line\">            self.num_directions = <span class=\"number\">1</span></span><br><span class=\"line\">            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.num_directions = <span class=\"number\">2</span></span><br><span class=\"line\">            self.linear = nn.Linear(self.num_hiddens * <span class=\"number\">2</span>, self.vocab_size)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, inputs, state</span>):</span><br><span class=\"line\">        X = F.one_hot(inputs.T.long(), self.vocab_size)</span><br><span class=\"line\">        X = X.to(torch.float32)</span><br><span class=\"line\">        Y, state = self.rnn(X, state)</span><br><span class=\"line\">        output = self.linear(Y.reshape((-<span class=\"number\">1</span>, Y.shape[-<span class=\"number\">1</span>])))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, state</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">begin_state</span>(<span class=\"params\">self, device, batch_size=<span class=\"number\">1</span></span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(self.rnn, nn.LSTM):</span><br><span class=\"line\">            <span class=\"keyword\">return</span> torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> (torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device),</span><br><span class=\"line\">                    torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device))</span><br><span class=\"line\"></span><br><span class=\"line\">lstm_layer = nn.LSTM(<span class=\"built_in\">len</span>(vocab), num_hiddens)</span><br><span class=\"line\">net = RNNModel(lstm_layer, <span class=\"built_in\">len</span>(vocab))</span><br><span class=\"line\">net = net.to(device)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_epoch</span>(<span class=\"params\">net, train_iter, loss_function, optimizer, device, use_random_iter</span>):</span><br><span class=\"line\">    state = <span class=\"literal\">None</span></span><br><span class=\"line\">    train_loss = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X, Y <span class=\"keyword\">in</span> tqdm(train_iter):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> state <span class=\"keyword\">is</span> <span class=\"literal\">None</span> <span class=\"keyword\">or</span> use_random_iter:</span><br><span class=\"line\">            state = net.begin_state(batch_size=X.shape[<span class=\"number\">0</span>], device=device)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, nn.Module) <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(state, <span class=\"built_in\">tuple</span>):</span><br><span class=\"line\">                state.detach_()</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                <span class=\"keyword\">for</span> s <span class=\"keyword\">in</span> state:</span><br><span class=\"line\">                    s.detach_()</span><br><span class=\"line\">        y = Y.T.reshape(-<span class=\"number\">1</span>)</span><br><span class=\"line\">        X, y = X.to(device), y.to(device)</span><br><span class=\"line\">        loss_function.to(device)</span><br><span class=\"line\">        y_hat, state = net(X, state)</span><br><span class=\"line\">        loss = loss_function(y_hat, y.long()).mean()</span><br><span class=\"line\">        optimizer.zero_grad()</span><br><span class=\"line\">        loss.backward()</span><br><span class=\"line\">        d2l.grad_clipping(net, <span class=\"number\">1</span>)</span><br><span class=\"line\">        optimizer.step()</span><br><span class=\"line\">        train_loss.append(loss)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> math.exp(<span class=\"built_in\">sum</span>(train_loss) / <span class=\"built_in\">len</span>(train_loss))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">net, train_iter, vocab, lr, num_epochs, device, use_random_iter=<span class=\"literal\">False</span></span>):</span><br><span class=\"line\">    loss_function = nn.CrossEntropyLoss()</span><br><span class=\"line\">    optimizer = torch.optim.SGD(net.parameters(), lr)</span><br><span class=\"line\">    pred = <span class=\"keyword\">lambda</span> prefix: d2l.predict_ch8(prefix, <span class=\"number\">50</span>, net, vocab, device)</span><br><span class=\"line\"></span><br><span class=\"line\">    writer = SummaryWriter(<span class=\"string\">&#x27;../logs/LSTM_train_log&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        ppl = train_epoch(net, train_iter, loss_function, optimizer, device, use_random_iter)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (epoch + <span class=\"number\">1</span>) % <span class=\"number\">10</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(pred(<span class=\"string\">&#x27;time traveller&#x27;</span>))</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Perplexity: <span class=\"subst\">&#123;ppl:<span class=\"number\">.1</span>f&#125;</span>&#x27;</span>)</span><br><span class=\"line\">            writer.add_scalar(<span class=\"string\">&#x27;train_loss&#x27;</span>, ppl, epoch + <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(pred(<span class=\"string\">&#x27;time traveller&#x27;</span>))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(pred(<span class=\"string\">&#x27;traveller&#x27;</span>))</span><br><span class=\"line\">    writer.close()</span><br><span class=\"line\"></span><br><span class=\"line\">train(net, train_iter, vocab, lr, num_epochs, device)</span><br><span class=\"line\"><span class=\"comment\"># Perplexity: 1.0</span></span><br><span class=\"line\"><span class=\"comment\"># time traveller for so it will be convenient to speak of himwas e</span></span><br><span class=\"line\"><span class=\"comment\"># travelleryou can show black is white by argument said filby</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"3-深度循环神经网络\">3. 深度循环神经网络</h2>\n<p>到目前为止，我们只讨论了具有一个单向隐藏层的循环神经网络。其中，隐变量和观测值与具体的函数形式的交互方式是相当随意的。只要交互类型建模具有足够的灵活性，这就不是一个大问题。然而，对一个单层来说，这可能具有相当的挑战性。之前在线性模型中，我们通过添加更多的层来解决这个问题。而在循环神经网络中，我们首先需要确定如何添加更多的层，以及在哪里添加额外的非线性，因此这个问题有点棘手。</p>\n<p>事实上，我们可以将多层循环神经网络堆叠在一起，通过对几个简单层的组合，产生了一个灵活的机制。特别是，数据可能与不同层的堆叠有关。例如，我们可能希望保持有关金融市场状况（熊市或牛市）的宏观数据可用，而微观数据只记录较短期的时间动态。</p>\n<p>深度循环神经网络的数学表达详见：<a href=\"https://zh-v2.d2l.ai/chapter_recurrent-modern/deep-rnn.html\">深度循环神经网络</a>。</p>\n<p>实现多层循环神经网络所需的许多逻辑细节在高级 API 中都是现成的。简单起见，我们仅示范使用此类内置函数的实现方式。以长短期记忆网络模型为例，该代码与上一节中使用的代码非常相似，实际上唯一的区别是我们指定了层的数量，而不是使用单一层这个默认值。像往常一样，我们从加载数据集开始：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.tensorboard <span class=\"keyword\">import</span> SummaryWriter</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"><span class=\"keyword\">from</span> tqdm <span class=\"keyword\">import</span> tqdm</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size, num_steps = <span class=\"number\">32</span>, <span class=\"number\">35</span></span><br><span class=\"line\">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure>\n<p>像选择超参数这类架构决策也跟上一节中的决策非常相似。因为我们有不同的词元，所以输入和输出都选择相同数量，即 <code>vocab_size</code>。隐藏单元的数量仍然是256。唯一的区别是，我们现在通过 <code>num_layers</code> 的值来设定隐藏层数：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">device = torch.device(<span class=\"string\">&#x27;cuda&#x27;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\">num_inputs, num_hiddens, num_layers = <span class=\"built_in\">len</span>(vocab), <span class=\"number\">256</span>, <span class=\"number\">2</span></span><br><span class=\"line\">lstm_layer = nn.LSTM(input_size=num_inputs, hidden_size=num_hiddens, num_layers=num_layers)</span><br><span class=\"line\">net = d2l.RNNModel(lstm_layer, <span class=\"built_in\">len</span>(vocab))  <span class=\"comment\"># 同上节的RNNModel</span></span><br><span class=\"line\">net = net.to(device)</span><br></pre></td></tr></table></figure>\n<p>最后和上一节一样训练模型看看效果：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_epochs, lr = <span class=\"number\">500</span>, <span class=\"number\">2</span></span><br><span class=\"line\">train(net, train_iter, vocab, lr * <span class=\"number\">1.0</span>, num_epochs, device)  <span class=\"comment\"># 同上节的train</span></span><br><span class=\"line\"><span class=\"comment\"># Perplexity: 1.0</span></span><br><span class=\"line\"><span class=\"comment\"># time traveller for so it will be convenient to speak of himwas e</span></span><br><span class=\"line\"><span class=\"comment\"># traveller with a slight accession ofcheerfulness really thi</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"4-双向循环神经网络\">4. 双向循环神经网络</h2>\n<p>在双向循环神经网络中，每个时间步的隐状态由当前时间步的前后数据同时决定，通过反向更新的隐藏层来利用反向时间信息，通常用来对序列抽取特征、填空，而<strong>不是预测未来</strong>。</p>\n<p>双向循环神经网络的数学表达详见：<a href=\"https://zh-v2.d2l.ai/chapter_recurrent-modern/bi-rnn.html\">双向循环神经网络</a>。</p>\n<p>由于双向循环神经网络使用了过去的和未来的数据，所以我们不能盲目地将这一语言模型应用于任何预测任务。尽管模型产出的困惑度是合理的，该模型预测未来词元的能力却可能存在严重缺陷。我们用下面的示例代码引以为戒，以防在错误的环境中使用它们：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载数据</span></span><br><span class=\"line\">batch_size, num_steps = <span class=\"number\">32</span>, <span class=\"number\">35</span></span><br><span class=\"line\">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 通过设置“bidirective=True”来定义双向LSTM模型</span></span><br><span class=\"line\">device = torch.device(<span class=\"string\">&#x27;cuda&#x27;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\">num_inputs, num_hiddens, num_layers = <span class=\"built_in\">len</span>(vocab), <span class=\"number\">256</span>, <span class=\"number\">2</span></span><br><span class=\"line\">lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=<span class=\"literal\">True</span>)</span><br><span class=\"line\">net = d2l.RNNModel(lstm_layer, <span class=\"built_in\">len</span>(vocab))</span><br><span class=\"line\">net = net.to(device)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 训练模型</span></span><br><span class=\"line\">num_epochs, lr = <span class=\"number\">500</span>, <span class=\"number\">1</span></span><br><span class=\"line\">train(net, train_iter, vocab, lr, num_epochs, device)</span><br><span class=\"line\"><span class=\"comment\"># Perplexity: 1.1</span></span><br><span class=\"line\"><span class=\"comment\"># time travellerererererererererererererererererererererererererer</span></span><br><span class=\"line\"><span class=\"comment\"># travellerererererererererererererererererererererererererer</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"5-机器翻译与数据集\">5. 机器翻译与数据集</h2>\n<p>语言模型是自然语言处理的关键，而机器翻译是语言模型最成功的基准测试。因为机器翻译正是将输入序列转换成输出序列的<strong>序列转换模型</strong>（sequence transduction）的核心问题。</p>\n<p>与语言模型那一节中的语料库是单一语言的语言模型问题存在不同，机器翻译的数据集是由源语言和目标语言的文本序列对组成的。因此，我们需要一种完全不同的方法来预处理机器翻译数据集，而不是复用语言模型的预处理程序。</p>\n<p>首先，下载一个由双语句子对组成的“英-法”数据集，数据集中的每一行都是制表符分隔的文本序列对，序列对由英文文本序列和翻译后的法语文本序列组成。请注意，每个文本序列可以是一个句子，也可以是包含多个句子的一个段落。在这个将英语翻译成法语的机器翻译问题中，英语是源语言（source language），法语是目标语言（target language）。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">d2l.DATA_HUB[<span class=\"string\">&#x27;fra-eng&#x27;</span>] = (d2l.DATA_URL + <span class=\"string\">&#x27;fra-eng.zip&#x27;</span>, <span class=\"string\">&#x27;94646ad1522d915e7b0f9296181140edcf86a4f5&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">read_data_nmt</span>():</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;载入“英语-法语”数据集&quot;&quot;&quot;</span></span><br><span class=\"line\">    data_dir = d2l.download_extract(<span class=\"string\">&#x27;fra-eng&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(os.path.join(data_dir, <span class=\"string\">&#x27;fra.txt&#x27;</span>), <span class=\"string\">&#x27;r&#x27;</span>, encoding=<span class=\"string\">&#x27;utf-8&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> f.read()</span><br><span class=\"line\"></span><br><span class=\"line\">raw_text = read_data_nmt()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(raw_text[:<span class=\"number\">75</span>])</span><br><span class=\"line\"><span class=\"comment\"># Go.    Va !</span></span><br><span class=\"line\"><span class=\"comment\"># Hi.    Salut !</span></span><br><span class=\"line\"><span class=\"comment\"># Run!    Cours !</span></span><br><span class=\"line\"><span class=\"comment\"># Run!    Courez !</span></span><br><span class=\"line\"><span class=\"comment\"># Who?    Qui ?</span></span><br><span class=\"line\"><span class=\"comment\"># Wow!    Ça alors !</span></span><br></pre></td></tr></table></figure>\n<p>下载数据集后，原始文本数据需要经过几个预处理步骤。例如，我们用空格代替不间断空格（non-breaking space），使用小写字母替换大写字母，并在单词和标点符号之间插入空格：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">preprocess_nmt</span>(<span class=\"params\">text</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;预处理“英语-法语”数据集&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">no_space</span>(<span class=\"params\">char, prev_char</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> char <span class=\"keyword\">in</span> <span class=\"built_in\">set</span>(<span class=\"string\">&#x27;,.!?&#x27;</span>) <span class=\"keyword\">and</span> prev_char != <span class=\"string\">&#x27; &#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 使用空格替换不间断空格，使用小写字母替换大写字母</span></span><br><span class=\"line\">    text = text.replace(<span class=\"string\">&#x27;\\u202f&#x27;</span>, <span class=\"string\">&#x27; &#x27;</span>).replace(<span class=\"string\">&#x27;\\xa0&#x27;</span>, <span class=\"string\">&#x27; &#x27;</span>).lower()</span><br><span class=\"line\">    <span class=\"comment\"># 在单词和标点符号之间插入空格</span></span><br><span class=\"line\">    out = [<span class=\"string\">&#x27; &#x27;</span> + char <span class=\"keyword\">if</span> i &gt; <span class=\"number\">0</span> <span class=\"keyword\">and</span> no_space(char, text[i - <span class=\"number\">1</span>]) <span class=\"keyword\">else</span> char <span class=\"keyword\">for</span> i, char <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(text)]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">&#x27;&#x27;</span>.join(out)</span><br><span class=\"line\"></span><br><span class=\"line\">text = preprocess_nmt(raw_text)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(text[:<span class=\"number\">80</span>])</span><br><span class=\"line\"><span class=\"comment\"># go .    va !</span></span><br><span class=\"line\"><span class=\"comment\"># hi .    salut !</span></span><br><span class=\"line\"><span class=\"comment\"># run !    cours !</span></span><br><span class=\"line\"><span class=\"comment\"># run !    courez !</span></span><br><span class=\"line\"><span class=\"comment\"># who ?    qui ?</span></span><br><span class=\"line\"><span class=\"comment\"># wow !    ça alors !</span></span><br></pre></td></tr></table></figure>\n<p>与之前的字符级词元化不同，在机器翻译中，我们更喜欢单词级词元化（最先进的模型可能使用更高级的词元化技术）。下面的 <code>tokenize_nmt</code> 函数对前 <code>num_examples</code> 个文本序列对进行词元化，其中每个词元要么是一个词，要么是一个标点符号。此函数返回两个词元列表：<code>source</code> 和 <code>target</code>，<code>source[i]</code> 是源语言（这里是英语）第 <code>i</code> 个文本序列的词元列表，<code>target[i]</code> 是目标语言（这里是法语）第 <code>i</code> 个文本序列的词元列表。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">tokenize_nmt</span>(<span class=\"params\">text, num_examples=<span class=\"literal\">None</span></span>):  <span class=\"comment\"># num_examples限制数据集的数量</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;词元化“英语-法语”数据集&quot;&quot;&quot;</span></span><br><span class=\"line\">    source, target = [], []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, line <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(text.split(<span class=\"string\">&#x27;\\n&#x27;</span>)):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> num_examples <span class=\"keyword\">and</span> i &gt; num_examples:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">        parts = line.split(<span class=\"string\">&#x27;\\t&#x27;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(parts) == <span class=\"number\">2</span>:</span><br><span class=\"line\">            source.append(parts[<span class=\"number\">0</span>].split(<span class=\"string\">&#x27; &#x27;</span>))</span><br><span class=\"line\">            target.append(parts[<span class=\"number\">1</span>].split(<span class=\"string\">&#x27; &#x27;</span>))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> source, target</span><br><span class=\"line\"></span><br><span class=\"line\">source, target = tokenize_nmt(text)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(source[:<span class=\"number\">3</span>], target[:<span class=\"number\">3</span>])  <span class=\"comment\"># [[&#x27;go&#x27;, &#x27;.&#x27;], [&#x27;hi&#x27;, &#x27;.&#x27;], [&#x27;run&#x27;, &#x27;!&#x27;]] [[&#x27;va&#x27;, &#x27;!&#x27;], [&#x27;salut&#x27;, &#x27;!&#x27;], [&#x27;cours&#x27;, &#x27;!&#x27;]]</span></span><br></pre></td></tr></table></figure>\n<p>让我们绘制每个文本序列所包含的词元数量的直方图。在这个简单的“英-法”数据集中，大多数文本序列的词元数量少于20个：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">show_list_len_pair_hist</span>(<span class=\"params\">legend, title, xlabel, ylabel, xlist, ylist</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;绘制列表长度对的直方图&quot;&quot;&quot;</span></span><br><span class=\"line\">    plt.figure(figsize=(<span class=\"number\">8</span>, <span class=\"number\">6</span>), dpi=<span class=\"number\">150</span>)</span><br><span class=\"line\">    _, _, patches = plt.hist([[<span class=\"built_in\">len</span>(l) <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> xlist], [<span class=\"built_in\">len</span>(l) <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> ylist]], edgecolor=<span class=\"string\">&#x27;r&#x27;</span>, alpha=<span class=\"number\">0.5</span>)</span><br><span class=\"line\">    plt.title(title)</span><br><span class=\"line\">    plt.xlabel(xlabel)</span><br><span class=\"line\">    plt.ylabel(ylabel)</span><br><span class=\"line\">    plt.legend(legend)</span><br><span class=\"line\">    <span class=\"comment\"># for patch in patches[1].patches:</span></span><br><span class=\"line\">    <span class=\"comment\">#     patch.set_hatch(&#x27;/&#x27;)  # 给第二个list对的条形设置填充效果</span></span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\">show_list_len_pair_hist([<span class=\"string\">&#x27;source&#x27;</span>, <span class=\"string\">&#x27;target&#x27;</span>], <span class=\"string\">&#x27;The length of list&#x27;</span>, <span class=\"string\">&#x27;# tokens per sequence&#x27;</span>, <span class=\"string\">&#x27;count&#x27;</span>, source, target)</span><br></pre></td></tr></table></figure>\n<p>由于机器翻译数据集由语言对组成，因此我们可以分别为源语言和目标语言构建两个词表。使用单词级词元化时，词表大小将明显大于使用字符级词元化时的词表大小。为了缓解这一问题，这里我们将出现次数少于2次的低频率词元视为相同的未知（<code>&lt;unk&gt;</code>）词元。除此之外，我们还指定了额外的特定词元，例如在小批量时用于将序列填充到相同长度的填充词元（<code>&lt;pad&gt;</code>），以及序列的开始词元（<code>&lt;bos&gt;</code>）和结束词元（<code>&lt;eos&gt;</code>）。这些特殊词元在自然语言处理任务中比较常用。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">src_vocab = d2l.Vocab(source, min_freq=<span class=\"number\">2</span>, reserved_tokens=[<span class=\"string\">&#x27;&lt;pad&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;bos&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">len</span>(src_vocab))  <span class=\"comment\"># 10012</span></span><br></pre></td></tr></table></figure>\n<p>回想一下，语言模型中的序列样本都有一个固定的长度，无论这个样本是一个句子的一部分还是跨越了多个句子的一个片断。这个固定长度是由语言模型中的 <code>num_steps</code>（时间步数或词元数量）参数指定的。在机器翻译中，每个样本都是由源和目标组成的文本序列对，其中的每个文本序列可能具有不同的长度。</p>\n<p>为了提高计算效率，我们仍然可以通过<strong>截断</strong>（truncation）和<strong>填充</strong>（padding）方式实现一次只处理一个小批量的文本序列。假设同一个小批量中的每个序列都应该具有相同的长度 <code>num_steps</code>，那么如果文本序列的词元数目少于 <code>num_steps</code> 时，我们将继续在其末尾添加特定的 <code>&lt;pad&gt;</code> 词元，直到其长度达到 <code>num_steps</code>；反之，我们将截断文本序列时，只取其前 <code>num_steps</code> 个词元，并且丢弃剩余的词元。这样，每个文本序列将具有相同的长度，以便以相同形状的小批量进行加载。下面的 <code>truncate_pad</code> 函数将截断或填充文本序列：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">truncate_pad</span>(<span class=\"params\">line, num_steps, padding_token</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;截断或填充文本序列&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(line) &gt; num_steps:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> line[:num_steps]  <span class=\"comment\"># 截断</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> line + [padding_token] * (num_steps - <span class=\"built_in\">len</span>(line))  <span class=\"comment\"># 填充</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(truncate_pad(src_vocab[source[<span class=\"number\">0</span>]], <span class=\"number\">10</span>, src_vocab[<span class=\"string\">&#x27;&lt;pad&gt;&#x27;</span>]))  <span class=\"comment\"># [47, 4, 1, 1, 1, 1, 1, 1, 1, 1]</span></span><br></pre></td></tr></table></figure>\n<p>现在我们定义一个函数，可以将文本序列转换成小批量数据集用于训练。我们将特定的 <code>&lt;eos&gt;</code> 词元添加到所有序列的末尾，用于表示序列的结束。当模型通过一个词元接一个词元地生成序列进行预测时，生成的 <code>&lt;eos&gt;</code> 词元说明完成了序列输出工作。此外，我们还记录了每个文本序列的长度，统计长度时排除了填充词元，在稍后将要介绍的一些模型会需要这个长度信息。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">build_array_nmt</span>(<span class=\"params\">lines, vocab, num_steps</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;将机器翻译的文本序列转换成小批量&quot;&quot;&quot;</span></span><br><span class=\"line\">    lines = [vocab[l] <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> lines]</span><br><span class=\"line\">    lines = [l + [vocab[<span class=\"string\">&#x27;&lt;eos&gt;&#x27;</span>]] <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> lines]</span><br><span class=\"line\">    array = torch.tensor([truncate_pad(l, num_steps, vocab[<span class=\"string\">&#x27;&lt;pad&gt;&#x27;</span>]) <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> lines])</span><br><span class=\"line\">    valid_len = (array != vocab[<span class=\"string\">&#x27;&lt;pad&gt;&#x27;</span>]).<span class=\"built_in\">type</span>(torch.int32).<span class=\"built_in\">sum</span>(<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> array, valid_len</span><br></pre></td></tr></table></figure>\n<p>最后，我们定义 <code>load_data_nmt</code> 函数来返回数据迭代器，以及源语言和目标语言的两种词表：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">load_data_nmt</span>(<span class=\"params\">batch_size, num_steps, num_examples=<span class=\"number\">600</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;返回翻译数据集的迭代器和词表&quot;&quot;&quot;</span></span><br><span class=\"line\">    text = preprocess_nmt(read_data_nmt())</span><br><span class=\"line\">    source, target = tokenize_nmt(text, num_examples)</span><br><span class=\"line\">    src_vocab = d2l.Vocab(source, min_freq=<span class=\"number\">2</span>, reserved_tokens=[<span class=\"string\">&#x27;&lt;pad&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;bos&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class=\"line\">    tgt_vocab = d2l.Vocab(target, min_freq=<span class=\"number\">2</span>, reserved_tokens=[<span class=\"string\">&#x27;&lt;pad&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;bos&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class=\"line\">    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)</span><br><span class=\"line\">    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)</span><br><span class=\"line\">    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class=\"line\">    data_iter = d2l.load_array(data_arrays, batch_size)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> data_iter, src_vocab, tgt_vocab</span><br><span class=\"line\"></span><br><span class=\"line\">train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size=<span class=\"number\">2</span>, num_steps=<span class=\"number\">8</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span> X, X_valid_len, Y, Y_valid_len <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;X:&#x27;</span>, X.<span class=\"built_in\">type</span>(torch.int32))  <span class=\"comment\"># X: tensor([[ 6, 18, 43,  4,  3,  1,  1,  1], [78,  9,  4,  3,  1,  1,  1,  1]], dtype=torch.int32)</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;X的有效长度:&#x27;</span>, X_valid_len)  <span class=\"comment\"># X的有效长度: tensor([5, 4])</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Y:&#x27;</span>, Y.<span class=\"built_in\">type</span>(torch.int32))  <span class=\"comment\"># Y: tensor([[ 6,  7, 40,  4,  3,  1,  1,  1], [ 0,  4,  3,  1,  1,  1,  1,  1]], dtype=torch.int32)</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Y的有效长度:&#x27;</span>, Y_valid_len)  <span class=\"comment\"># Y的有效长度: tensor([5, 3])</span></span><br><span class=\"line\">    <span class=\"keyword\">break</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"6-编码器-解码器架构\">6. 编码器-解码器架构</h2>\n<p>正如我们在上一节中所讨论的，机器翻译是序列转换模型的一个核心问题，其输入和输出都是长度可变的序列。为了处理这种类型的输入和输出，我们可以设计一个包含两个主要组件的架构：第一个组件是一个<strong>编码器</strong>（encoder）：它接受一个<strong>长度可变</strong>的序列作为输入，并将其转换为具有<strong>固定形状</strong>的编码状态。第二个组件是<strong>解码器</strong>（decoder）：它将<strong>固定形状</strong>的编码状态映射到<strong>长度可变</strong>的序列。这被称为编码器-解码器（encoder-decoder）架构，示意图可见：<a href=\"https://zh-v2.d2l.ai/chapter_recurrent-modern/encoder-decoder.html\">编码器-解码器架构</a>。</p>\n<p>由于“编码器-解码器”架构是形成后续章节中不同序列转换模型的基础，因此本节将把这个架构转换为接口方便后面的代码实现。</p>\n<p>在编码器接口中，我们只指定长度可变的序列作为编码器的输入 <code>X</code>。任何继承这个 <code>Encoder</code> 基类的模型将完成代码实现：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Encoder</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;编码器-解码器架构的基本编码器接口&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Encoder, self).__init__(**kwargs)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X, *args</span>):</span><br><span class=\"line\">        <span class=\"keyword\">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>\n<p>在下面的解码器接口中，我们新增一个 <code>init_state</code> 函数，用于将编码器的输出（enc_outputs）转换为编码后的状态。注意，此步骤可能需要额外的输入，例如输入序列的有效长度。为了逐个地生成长度可变的词元序列，解码器在每个时间步都会将输入（例如在前一时间步生成的词元）和编码后的状态映射成当前时间步的输出词元：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Decoder</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;编码器-解码器架构的基本解码器接口&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Decoder, self).__init__(**kwargs)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">init_state</span>(<span class=\"params\">self, enc_outputs, *args</span>):</span><br><span class=\"line\">        <span class=\"keyword\">raise</span> NotImplementedError</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X, state</span>):</span><br><span class=\"line\">        <span class=\"keyword\">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>\n<p>总而言之，“编码器-解码器”架构包含了一个编码器和一个解码器，并且还拥有可选的额外的参数。在前向传播中，编码器的输出用于生成编码状态，这个状态又被解码器作为其输入的一部分：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">EncoderDecoder</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;编码器-解码器架构的基类&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, encoder, decoder, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(EncoderDecoder, self).__init__(**kwargs)</span><br><span class=\"line\">        self.encoder = encoder</span><br><span class=\"line\">        self.decoder = decoder</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, enc_X, dec_X, *args</span>):</span><br><span class=\"line\">        enc_outputs = self.encoder(enc_X, *args)</span><br><span class=\"line\">        dec_state = self.decoder.init_state(enc_outputs, *args)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.decoder(dec_X, dec_state)</span><br></pre></td></tr></table></figure>\n<h2 id=\"7-序列到序列学习（seq2seq）\">7. 序列到序列学习（seq2seq）</h2>\n<p>遵循编码器-解码器架构的设计原则，循环神经网络编码器使用长度可变的序列作为输入，将其转换为固定形状的隐状态。换言之，输入序列的信息被编码到循环神经网络编码器的隐状态中。为了连续生成输出序列的词元，独立的循环神经网络解码器是基于输入序列的编码信息和输出序列已经看见的或者生成的词元来预测下一个词元。在机器翻译中使用两个循环神经网络进行序列到序列学习的图示以及理论介绍可见：<a href=\"https://zh-v2.d2l.ai/chapter_recurrent-modern/seq2seq.html\">序列到序列学习（seq2seq）</a>。</p>\n<p>在序列到序列学习中，特定的 <code>&lt;eos&gt;</code> 表示序列结束词元。一旦输出序列生成此词元，模型就会停止预测。在循环神经网络解码器的初始化时间步，有两个特定的设计决定：首先，特定的 <code>&lt;bos&gt;</code> 表示序列开始词元，它是解码器的输入序列的第一个词元。其次，使用循环神经网络编码器最终的隐状态来初始化解码器的隐状态。</p>\n<p>从技术上讲，编码器将长度可变的输入序列转换成形状固定的上下文变量，并且将输入序列的信息在该上下文变量中进行编码。</p>\n<p>现在，让我们实现循环神经网络编码器。注意，我们使用了嵌入层（embedding layer）来获得输入序列中每个词元的特征向量。嵌入层的权重是一个矩阵，其行数等于输入词表的大小（vocab_size），其列数等于特征向量的维度（embed_size）。另外，本文选择了一个多层门控循环单元来实现编码器。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> collections</span><br><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Seq2SeqEncoder</span>(d2l.Encoder):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;用于序列到序列学习的循环神经网络编码器&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, vocab_size, embed_size, num_hiddens, num_layers, dropout=<span class=\"number\">0.</span>, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Seq2SeqEncoder, self).__init__(**kwargs)</span><br><span class=\"line\">        self.embedding = nn.Embedding(vocab_size, embed_size)  <span class=\"comment\"># 嵌入层</span></span><br><span class=\"line\">        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=dropout)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X, *args</span>):</span><br><span class=\"line\">        X = self.embedding(X)  <span class=\"comment\"># X.shape: (batch_size, num_steps, embed_size)</span></span><br><span class=\"line\">        X = X.permute(<span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">2</span>)  <span class=\"comment\"># 在循环神经网络模型中，第一个轴对应于时间步</span></span><br><span class=\"line\">        output, state = self.rnn(X)  <span class=\"comment\"># 如果未提及状态，则默认为0</span></span><br><span class=\"line\">        <span class=\"comment\"># output.shape: (num_steps, batch_size, num_hiddens)</span></span><br><span class=\"line\">        <span class=\"comment\"># state.shape: (num_layers, batch_size, num_hiddens)</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, state</span><br></pre></td></tr></table></figure>\n<p>下面，我们实例化上述编码器的实现：我们使用一个两层门控循环单元编码器，其隐藏单元数为16。给定一小批量的输入序列 <code>X</code>（批量大小为4，时间步为7）。在完成所有时间步后，最后一层的隐状态的输出是一个张量（<code>output</code> 由编码器的循环层返回），其形状为 <code>(时间步数, 批量大小, 隐藏单元数)</code>。</p>\n<p>由于这里使用的是门控循环单元，所以在最后一个时间步的多层隐状态的形状是 <code>(隐藏层的数量, 批量大小, 隐藏单元的数量)</code>。如果使用长短期记忆网络，<code>state</code> 中还将包含记忆单元信息。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">encoder = Seq2SeqEncoder(vocab_size=<span class=\"number\">10</span>, embed_size=<span class=\"number\">8</span>, num_hiddens=<span class=\"number\">16</span>, num_layers=<span class=\"number\">2</span>)</span><br><span class=\"line\">encoder.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\">X = torch.zeros((<span class=\"number\">4</span>, <span class=\"number\">7</span>), dtype=torch.long)</span><br><span class=\"line\">output, state = encoder(X)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(output.shape, state.shape)  <span class=\"comment\"># torch.Size([7, 4, 16]) torch.Size([2, 4, 16])</span></span><br></pre></td></tr></table></figure>\n<p>当实现解码器时，我们直接使用编码器最后一个时间步的隐状态来初始化解码器的隐状态。这就要求使用循环神经网络实现的编码器和解码器具有<strong>相同数量的层和隐藏单元</strong>。为了进一步包含经过编码的输入序列的信息，上下文变量在所有的时间步与解码器的输入进行拼接（concatenate）。为了预测输出词元的概率分布，在循环神经网络解码器的最后一层使用全连接层来变换隐状态。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Seq2SeqDecoder</span>(d2l.Decoder):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;用于序列到序列学习的循环神经网络解码器&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, vocab_size, embed_size, num_hiddens, num_layers, dropout=<span class=\"number\">0.</span>, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Seq2SeqDecoder, self).__init__(**kwargs)</span><br><span class=\"line\">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class=\"line\">        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout)</span><br><span class=\"line\">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">init_state</span>(<span class=\"params\">self, enc_outputs, *args</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> enc_outputs[<span class=\"number\">1</span>]  <span class=\"comment\"># enc_outputs[1]即为[output, state]中的state</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X, state</span>):</span><br><span class=\"line\">        X = self.embedding(X).permute(<span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">2</span>)  <span class=\"comment\"># X.shape: (num_steps, batch_size, embed_size)</span></span><br><span class=\"line\">        context = state[-<span class=\"number\">1</span>].repeat(X.shape[<span class=\"number\">0</span>], <span class=\"number\">1</span>, <span class=\"number\">1</span>)  <span class=\"comment\"># 广播context，使其具有与X相同的num_steps</span></span><br><span class=\"line\">        X_and_context = torch.cat((X, context), <span class=\"number\">2</span>)  <span class=\"comment\"># 因此RNN的输入维度为embed_size + num_hiddens</span></span><br><span class=\"line\">        output, state = self.rnn(X_and_context, state)</span><br><span class=\"line\">        output = self.dense(output).permute(<span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">        <span class=\"comment\"># output.shape: (batch_size, num_steps, vocab_size)</span></span><br><span class=\"line\">        <span class=\"comment\"># state.shape: (num_layers, batch_size, num_hiddens)</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, state</span><br></pre></td></tr></table></figure>\n<p>下面，我们用与前面提到的编码器中相同的超参数来实例化解码器。如我们所见，解码器的输出形状变为 <code>(批量大小, 时间步数, 词表大小)</code>，其中张量的最后一个维度存储预测的词元分布。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">decoder = Seq2SeqDecoder(vocab_size=<span class=\"number\">10</span>, embed_size=<span class=\"number\">8</span>, num_hiddens=<span class=\"number\">16</span>, num_layers=<span class=\"number\">2</span>)</span><br><span class=\"line\">decoder.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\">state = decoder.init_state(encoder(X))</span><br><span class=\"line\">output, state = decoder(X, state)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(output.shape, state.shape)  <span class=\"comment\"># torch.Size([4, 7, 10]) torch.Size([2, 4, 16])</span></span><br></pre></td></tr></table></figure>\n<p>在每个时间步，解码器预测了输出词元的概率分布。类似于语言模型，可以使用 Softmax 来获得分布，并通过计算交叉熵损失函数来进行优化。回想一下我们将特定的填充词元添加到序列的末尾，因此不同长度的序列可以以相同形状的小批量加载。但是，我们应该将填充词元的预测排除在损失函数的计算之外。</p>\n<p>为此，我们可以使用下面的 <code>sequence_mask</code> 函数通过<strong>零值化</strong>屏蔽不相关的项，以便后面任何不相关预测的计算都是与零的乘积，结果都等于零。例如，如果两个序列的有效长度（不包括填充词元）分别为1和2，则第一个序列的第一项和第二个序列的前两项之后的剩余项将被清除为零：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 解析[None, :]和[:, None]</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.arange(<span class=\"number\">3</span>)[<span class=\"literal\">None</span>, :])  <span class=\"comment\"># tensor([[0, 1, 2]])，在第1维增加一维，同理[:, None]在第2维增加一维</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.arange(<span class=\"number\">3</span>)[<span class=\"literal\">None</span>, :] &lt; torch.tensor([<span class=\"number\">1</span>, <span class=\"number\">2</span>])[:, <span class=\"literal\">None</span>])</span><br><span class=\"line\"><span class=\"comment\"># tensor([[ True, False, False],</span></span><br><span class=\"line\"><span class=\"comment\">#         [ True,  True, False]])</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sequence_mask</span>(<span class=\"params\">X, valid_len, value=<span class=\"number\">0</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;在序列中屏蔽不相关的项&quot;&quot;&quot;</span></span><br><span class=\"line\">    maxlen = X.size(<span class=\"number\">1</span>)</span><br><span class=\"line\">    mask = torch.arange((maxlen), dtype=torch.float32, device=X.device)[<span class=\"literal\">None</span>, :] &lt; valid_len[:, <span class=\"literal\">None</span>]</span><br><span class=\"line\">    X[~mask] = value</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X</span><br><span class=\"line\"></span><br><span class=\"line\">X = torch.tensor([[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>], [<span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(sequence_mask(X, torch.tensor([<span class=\"number\">1</span>, <span class=\"number\">2</span>])))</span><br><span class=\"line\"><span class=\"comment\"># tensor([[1, 0, 0],</span></span><br><span class=\"line\"><span class=\"comment\">#         [4, 5, 0]])</span></span><br></pre></td></tr></table></figure>\n<p>现在，我们可以通过扩展 Softmax 交叉熵损失函数来遮蔽不相关的预测。最初，所有预测词元的掩码都设置为1。一旦给定了有效长度，与填充词元对应的掩码将被设置为0。最后，将所有词元的损失乘以掩码，以过滤掉损失中填充词元产生的不相关预测。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MaskedSoftmaxCELoss</span>(nn.CrossEntropyLoss):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;带遮蔽的softmax交叉熵损失函数&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># pred.shape: (batch_size, num_steps, vocab_size)</span></span><br><span class=\"line\">    <span class=\"comment\"># label.shape: (batch_size, num_steps)</span></span><br><span class=\"line\">    <span class=\"comment\"># valid_len.shape: (batch_size,)</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, pred, label, valid_len</span>):</span><br><span class=\"line\">        weights = torch.ones_like(label)</span><br><span class=\"line\">        weights = sequence_mask(weights, valid_len)</span><br><span class=\"line\">        self.reduction=<span class=\"string\">&#x27;none&#x27;</span></span><br><span class=\"line\">        unweighted_loss = <span class=\"built_in\">super</span>(MaskedSoftmaxCELoss, self).forward(pred.permute(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>), label)</span><br><span class=\"line\">        weighted_loss = (unweighted_loss * weights).mean(dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> weighted_loss</span><br></pre></td></tr></table></figure>\n<p>我们可以创建三个相同的序列来进行代码健全性检查，然后分别指定这些序列的有效长度为4、2和0。结果就是，第一个序列的损失应为第二个序列的两倍，而第三个序列的损失应为零。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">loss = MaskedSoftmaxCELoss()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(loss(torch.ones(<span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">10</span>), torch.ones((<span class=\"number\">3</span>, <span class=\"number\">4</span>), dtype=torch.long), torch.tensor([<span class=\"number\">4</span>, <span class=\"number\">2</span>, <span class=\"number\">0</span>])))  <span class=\"comment\"># tensor([2.3026, 1.1513, 0.0000])</span></span><br></pre></td></tr></table></figure>\n<p>在下面的循环训练过程中，特定的序列开始词元（<code>&lt;bos&gt;</code>）和原始的输出序列（不包括序列结束词元 <code>&lt;eos&gt;</code>）拼接在一起作为解码器的输入。这被称为<strong>强制教学</strong>（teacher forcing），因为原始的输出序列（词元的标签）被送入解码器。或者，将来自上一个时间步的预测得到的词元作为解码器的当前输入。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_seq2seq</span>(<span class=\"params\">net, data_iter, lr, num_epochs, tgt_vocab, device</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;训练序列到序列模型&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">xavier_init_weights</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">            nn.init.xavier_uniform_(m.weight)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.GRU:</span><br><span class=\"line\">            <span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> m._flat_weights_names:</span><br><span class=\"line\">                <span class=\"keyword\">if</span> <span class=\"string\">&quot;weight&quot;</span> <span class=\"keyword\">in</span> param:</span><br><span class=\"line\">                    nn.init.xavier_uniform_(m._parameters[param])</span><br><span class=\"line\"></span><br><span class=\"line\">    net.apply(xavier_init_weights)</span><br><span class=\"line\">    net.to(device)</span><br><span class=\"line\">    optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class=\"line\">    loss_function = MaskedSoftmaxCELoss()</span><br><span class=\"line\"></span><br><span class=\"line\">    net.train()</span><br><span class=\"line\">    writer = SummaryWriter(<span class=\"string\">&#x27;../logs/seq2seq_train_log&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        timer = d2l.Timer()</span><br><span class=\"line\">        num_tokens = <span class=\"number\">0</span>  <span class=\"comment\"># 词元数量</span></span><br><span class=\"line\">        train_loss = []</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> batch <span class=\"keyword\">in</span> tqdm(data_iter):</span><br><span class=\"line\">            X, X_valid_len, Y, Y_valid_len = [x.to(device) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> batch]</span><br><span class=\"line\">            bos = torch.tensor([tgt_vocab[<span class=\"string\">&#x27;&lt;bos&gt;&#x27;</span>]] * Y.shape[<span class=\"number\">0</span>], device=device).reshape(-<span class=\"number\">1</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">            dec_input = torch.cat([bos, Y[:, :-<span class=\"number\">1</span>]], <span class=\"number\">1</span>)  <span class=\"comment\"># 强制教学</span></span><br><span class=\"line\">            Y_hat, _ = net(X, dec_input, X_valid_len)</span><br><span class=\"line\">            loss = loss_function(Y_hat, Y, Y_valid_len).mean()</span><br><span class=\"line\">            optimizer.zero_grad()</span><br><span class=\"line\">            loss.backward()  <span class=\"comment\"># 损失函数的标量进行“反向传播”</span></span><br><span class=\"line\">            d2l.grad_clipping(net, <span class=\"number\">1</span>)</span><br><span class=\"line\">            optimizer.step()</span><br><span class=\"line\">            train_loss.append(loss)</span><br><span class=\"line\">            num_tokens += Y_valid_len.<span class=\"built_in\">sum</span>()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (epoch + <span class=\"number\">1</span>) % <span class=\"number\">10</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            train_loss = <span class=\"built_in\">sum</span>(train_loss) / <span class=\"built_in\">len</span>(train_loss)</span><br><span class=\"line\">            writer.add_scalar(<span class=\"string\">&#x27;train_loss&#x27;</span>, train_loss, epoch + <span class=\"number\">1</span>)</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;loss <span class=\"subst\">&#123;train_loss:<span class=\"number\">.3</span>f&#125;</span>, <span class=\"subst\">&#123;num_tokens / timer.stop():<span class=\"number\">.1</span>f&#125;</span> tokens/sec on <span class=\"subst\">&#123;<span class=\"built_in\">str</span>(device)&#125;</span>&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    writer.close()</span><br><span class=\"line\"></span><br><span class=\"line\">embed_size, num_hiddens, num_layers, dropout = <span class=\"number\">32</span>, <span class=\"number\">32</span>, <span class=\"number\">2</span>, <span class=\"number\">0.1</span></span><br><span class=\"line\">batch_size, num_steps, lr, num_epochs = <span class=\"number\">64</span>, <span class=\"number\">10</span>, <span class=\"number\">0.005</span>, <span class=\"number\">300</span></span><br><span class=\"line\">device = torch.device(<span class=\"string\">&#x27;cuda&#x27;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class=\"line\">encoder = Seq2SeqEncoder(<span class=\"built_in\">len</span>(src_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class=\"line\">decoder = Seq2SeqDecoder(<span class=\"built_in\">len</span>(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class=\"line\">net = d2l.EncoderDecoder(encoder, decoder)</span><br><span class=\"line\">train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br></pre></td></tr></table></figure>\n<p>为了采用一个接着一个词元的方式预测输出序列，每个解码器当前时间步的输入都将来自于前一时间步的预测词元。与训练类似，序列开始词元（<code>&lt;bos&gt;</code>）在初始时间步被输入到解码器中，当输出序列的预测遇到序列结束词元（<code>&lt;eos&gt;</code>）时，预测就结束了。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">predict_seq2seq</span>(<span class=\"params\">net, src_sentence, src_vocab, tgt_vocab, num_steps, device, save_attention_weights=<span class=\"literal\">False</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;序列到序列模型的预测&quot;&quot;&quot;</span></span><br><span class=\"line\">    net.<span class=\"built_in\">eval</span>()  <span class=\"comment\"># 在预测时将net设置为评估模式</span></span><br><span class=\"line\">    src_tokens = src_vocab[src_sentence.lower().split(<span class=\"string\">&#x27; &#x27;</span>)] + [src_vocab[<span class=\"string\">&#x27;&lt;eos&gt;&#x27;</span>]]</span><br><span class=\"line\">    enc_valid_len = torch.tensor([<span class=\"built_in\">len</span>(src_tokens)], device=device)</span><br><span class=\"line\">    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab[<span class=\"string\">&#x27;&lt;pad&gt;&#x27;</span>])</span><br><span class=\"line\">    enc_X = torch.unsqueeze(torch.tensor(src_tokens, dtype=torch.long, device=device), dim=<span class=\"number\">0</span>)  <span class=\"comment\"># 添加批量维度</span></span><br><span class=\"line\">    enc_outputs = net.encoder(enc_X, enc_valid_len)</span><br><span class=\"line\">    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)</span><br><span class=\"line\">    dec_X = torch.unsqueeze(torch.tensor([tgt_vocab[<span class=\"string\">&#x27;&lt;bos&gt;&#x27;</span>]], dtype=torch.long, device=device), dim=<span class=\"number\">0</span>)  <span class=\"comment\"># 添加批量维度</span></span><br><span class=\"line\">    output_seq, attention_weight_seq = [], []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_steps):</span><br><span class=\"line\">        Y, dec_state = net.decoder(dec_X, dec_state)</span><br><span class=\"line\">        dec_X = Y.argmax(dim=<span class=\"number\">2</span>)  <span class=\"comment\"># 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入</span></span><br><span class=\"line\">        pred = dec_X.squeeze(dim=<span class=\"number\">0</span>).<span class=\"built_in\">type</span>(torch.int32).item()</span><br><span class=\"line\">        <span class=\"comment\"># 保存注意力权重（稍后讨论）</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> save_attention_weights:</span><br><span class=\"line\">            attention_weight_seq.append(net.decoder.attention_weights)</span><br><span class=\"line\">        <span class=\"comment\"># 一旦序列结束词元被预测，输出序列的生成就完成了</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> pred == tgt_vocab[<span class=\"string\">&#x27;&lt;eos&gt;&#x27;</span>]:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">        output_seq.append(pred)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">&#x27; &#x27;</span>.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq</span><br></pre></td></tr></table></figure>\n<p>我们可以通过与真实的标签序列进行比较来评估预测序列。虽然BLEU（bilingual evaluation understudy）最先是用于评估机器翻译的结果，但现在它已经被广泛用于测量许多应用的输出序列的质量。BLEU的详细介绍可见：<a href=\"https://zh-v2.d2l.ai/chapter_recurrent-modern/seq2seq.html\">序列到序列学习（seq2seq）</a>。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">bleu</span>(<span class=\"params\">pred_seq, label_seq, k</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;计算BLEU&quot;&quot;&quot;</span></span><br><span class=\"line\">    pred_tokens, label_tokens = pred_seq.split(<span class=\"string\">&#x27; &#x27;</span>), label_seq.split(<span class=\"string\">&#x27; &#x27;</span>)</span><br><span class=\"line\">    len_pred, len_label = <span class=\"built_in\">len</span>(pred_tokens), <span class=\"built_in\">len</span>(label_tokens)</span><br><span class=\"line\">    score = math.exp(<span class=\"built_in\">min</span>(<span class=\"number\">0</span>, <span class=\"number\">1</span> - len_label / len_pred))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> n <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, k + <span class=\"number\">1</span>):</span><br><span class=\"line\">        num_matches, label_subs = <span class=\"number\">0</span>, collections.defaultdict(<span class=\"built_in\">int</span>)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(len_label - n + <span class=\"number\">1</span>):</span><br><span class=\"line\">            label_subs[<span class=\"string\">&#x27; &#x27;</span>.join(label_tokens[i: i + n])] += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(len_pred - n + <span class=\"number\">1</span>):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> label_subs[<span class=\"string\">&#x27; &#x27;</span>.join(pred_tokens[i: i + n])] &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">                num_matches += <span class=\"number\">1</span></span><br><span class=\"line\">                label_subs[<span class=\"string\">&#x27; &#x27;</span>.join(pred_tokens[i: i + n])] -= <span class=\"number\">1</span></span><br><span class=\"line\">        score *= math.<span class=\"built_in\">pow</span>(num_matches / (len_pred - n + <span class=\"number\">1</span>), math.<span class=\"built_in\">pow</span>(<span class=\"number\">0.5</span>, n))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> score</span><br><span class=\"line\"></span><br><span class=\"line\">engs = [<span class=\"string\">&#x27;go .&#x27;</span>, <span class=\"string\">&quot;i lost .&quot;</span>, <span class=\"string\">&#x27;he\\&#x27;s calm .&#x27;</span>, <span class=\"string\">&#x27;i\\&#x27;m home .&#x27;</span>]</span><br><span class=\"line\">fras = [<span class=\"string\">&#x27;va !&#x27;</span>, <span class=\"string\">&#x27;j\\&#x27;ai perdu .&#x27;</span>, <span class=\"string\">&#x27;il est calme .&#x27;</span>, <span class=\"string\">&#x27;je suis chez moi .&#x27;</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> eng, fra <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(engs, fras):</span><br><span class=\"line\">    translation, attention_weight_seq = predict_seq2seq(net, eng, src_vocab, tgt_vocab, num_steps, device)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;<span class=\"subst\">&#123;eng&#125;</span> =&gt; <span class=\"subst\">&#123;translation&#125;</span>, bleu <span class=\"subst\">&#123;bleu(translation, fra, k=<span class=\"number\">2</span>):<span class=\"number\">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># go . =&gt; va !, bleu 1.000</span></span><br><span class=\"line\"><span class=\"comment\"># i lost . =&gt; j&#x27;ai perdu ., bleu 1.000</span></span><br><span class=\"line\"><span class=\"comment\"># he&#x27;s calm . =&gt; il est bon malade pas gagné pas en gagné pas, bleu 0.258</span></span><br><span class=\"line\"><span class=\"comment\"># i&#x27;m home . =&gt; je suis fainéante fainéante tomber ai ai homme paresseux ?, bleu 0.258</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"8-束搜索\">8. 束搜索</h2>\n<p>束搜索为预测输出序列的一个算法，详细介绍可见：<a href=\"https://zh-v2.d2l.ai/chapter_recurrent-modern/beam-search.html\">束搜索</a>。</p>\n",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/11559.html",
            "url": "https://asanosaki.github.io/posts/11559.html",
            "title": "动手学深度学习笔记(李沐)-循环神经网络",
            "date_published": "2023-04-05T06:04:00.000Z",
            "content_html": "<blockquote>\n<p>李沐动手学深度学习（PyTorch）课程学习笔记第八章：循环神经网络。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-序列模型\">1. 序列模型</h2>\n<p>由于涉及较多数学公式，序列模型的讲解可以转至：<a href=\"https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/sequence.html\">序列模型</a>。</p>\n<p>首先，我们生成一些数据：使用正弦函数和一些可加性噪声来生成序列数据，时间步为 <code>1, 2, ..., 1000</code>：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"><span class=\"keyword\">from</span> tqdm <span class=\"keyword\">import</span> tqdm</span><br><span class=\"line\"></span><br><span class=\"line\">T = <span class=\"number\">1000</span>  <span class=\"comment\"># 总共产生1000个点</span></span><br><span class=\"line\">time = torch.arange(<span class=\"number\">1</span>, T + <span class=\"number\">1</span>, dtype=torch.float32)</span><br><span class=\"line\">x = torch.sin(<span class=\"number\">0.01</span> * time) + torch.normal(<span class=\"number\">0</span>, <span class=\"number\">0.2</span>, (T,))  <span class=\"comment\"># 0~10大概为一个半周期(3*PI)，并加入噪声</span></span><br><span class=\"line\">d2l.plot(time, [x], <span class=\"string\">&#x27;time&#x27;</span>, <span class=\"string\">&#x27;x&#x27;</span>, xlim=[<span class=\"number\">1</span>, <span class=\"number\">1000</span>], figsize=(<span class=\"number\">6</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>接下来，我们将这个序列转换为模型的特征-标签（feature-label）对。基于嵌入维度 <code>𝜏</code>，我们将数据映射为数据对 <code>𝑦_𝑡 = 𝑥_𝑡</code> 和 <code>𝐱_𝑡 = [𝑥_&#123;𝑡 - 𝜏&#125;, ...,𝑥_&#123;𝑡 - 1&#125;]</code>，这比我们提供的数据样本少了 <code>𝜏</code> 个，因为我们没有足够的历史记录来描述前 <code>𝜏</code> 个数据样本。一个简单的解决办法是：如果拥有足够长的序列就丢弃这几项；另一个方法是用零填充序列。在这里，我们仅使用前600个特征-标签对进行训练：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tau = <span class=\"number\">4</span></span><br><span class=\"line\">features = torch.zeros((T - tau, tau))  <span class=\"comment\"># 一共996个样本，每个样本的特征长度为4</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(tau):</span><br><span class=\"line\">    features[:, i] = x[i:T - tau + i]  <span class=\"comment\"># 按列填充</span></span><br><span class=\"line\">labels = x[tau:].reshape((-<span class=\"number\">1</span>, <span class=\"number\">1</span>))  <span class=\"comment\"># labels的元素在x中的下标为[4, 5, 6, ...]，即0~3预测4，1~4预测5</span></span><br><span class=\"line\"><span class=\"comment\"># features的元素在x中的下标:</span></span><br><span class=\"line\"><span class=\"comment\"># [0, 1, 2, 3]</span></span><br><span class=\"line\"><span class=\"comment\"># [1, 2, 3, 4]</span></span><br><span class=\"line\"><span class=\"comment\"># ...</span></span><br><span class=\"line\"><span class=\"comment\"># [T - tau, T - tau + 1, T - tau + 2, T - tau + 3]</span></span><br><span class=\"line\"></span><br><span class=\"line\">batch_size, n_train = <span class=\"number\">16</span>, <span class=\"number\">600</span></span><br><span class=\"line\"><span class=\"comment\"># 只有前n_train个样本用于训练</span></span><br><span class=\"line\">train_iter = d2l.load_array((features[:n_train], labels[:n_train]), batch_size, is_train=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n<p>在这里，我们使用一个相当简单的架构训练模型：一个拥有两个全连接层的多层感知机，ReLU 激活函数和平方损失：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_weights</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.xavier_uniform_(m.weight)</span><br><span class=\"line\"></span><br><span class=\"line\">net = nn.Sequential(nn.Linear(<span class=\"number\">4</span>, <span class=\"number\">10</span>), nn.ReLU(), nn.Linear(<span class=\"number\">10</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">net.apply(init_weights)</span><br><span class=\"line\"></span><br><span class=\"line\">loss_function = nn.MSELoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)  <span class=\"comment\"># 注意：MSELoss计算平方误差时不带系数1/2</span></span><br></pre></td></tr></table></figure>\n<p>现在准备训练模型，实现下面的训练代码的方式与前面几章中的循环训练基本相同。因此，我们不会深入探讨太多细节：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">net, train_iter, loss_function, num_epochs, lr, device</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;training on&#x27;</span>, device)</span><br><span class=\"line\">    net.to(device)</span><br><span class=\"line\">    loss_function.to(device)</span><br><span class=\"line\">    optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        net.train()</span><br><span class=\"line\">        train_loss = []</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> tqdm(train_iter):</span><br><span class=\"line\">            X, y = X.to(device), y.to(device)</span><br><span class=\"line\">            optimizer.zero_grad()</span><br><span class=\"line\">            loss = loss_function(net(X), y)</span><br><span class=\"line\">            loss.mean().backward()</span><br><span class=\"line\">            optimizer.step()</span><br><span class=\"line\"></span><br><span class=\"line\">            train_loss.append(loss.mean())</span><br><span class=\"line\">        train_loss = <span class=\"built_in\">sum</span>(train_loss) / <span class=\"built_in\">len</span>(train_loss)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;[ Train | <span class=\"subst\">&#123;epoch + <span class=\"number\">1</span>:03d&#125;</span>/<span class=\"subst\">&#123;num_epochs:03d&#125;</span> ] loss = <span class=\"subst\">&#123;train_loss:<span class=\"number\">.5</span>f&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">device = torch.device(<span class=\"string\">&#x27;cuda&#x27;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\">lr, num_epochs = <span class=\"number\">0.01</span>, <span class=\"number\">5</span></span><br><span class=\"line\"></span><br><span class=\"line\">train(net, train_iter, loss_function, <span class=\"number\">5</span>, <span class=\"number\">0.01</span>, device)</span><br></pre></td></tr></table></figure>\n<p>由于训练损失很小，因此我们期望模型能有很好的工作效果。让我们看看这在实践中意味着什么。首先是检查模型预测下一个时间步的能力，也就是单步预测（one-step-ahead prediction）：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">onestep_preds = net(features.to(device))</span><br><span class=\"line\">d2l.plot([time, time[tau:]], [x.detach().numpy(), onestep_preds.cpu().detach().numpy()],</span><br><span class=\"line\">         <span class=\"string\">&#x27;time&#x27;</span>, <span class=\"string\">&#x27;x&#x27;</span>, legend=[<span class=\"string\">&#x27;data&#x27;</span>, <span class=\"string\">&#x27;1-step preds&#x27;</span>], xlim=[<span class=\"number\">1</span>, <span class=\"number\">1000</span>], figsize=(<span class=\"number\">6</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>正如我们所料，单步预测效果不错。即使这些预测的时间步超过了604（<code>n_train + tau</code>），其结果看起来仍然是可信的。然而有一个小问题：如果数据观察序列的时间步只到604，我们需要一步一步地向前迈进，换句话说，我们必须使用我们自己的预测（而不是原始数据）来进行多步预测。让我们看看效果如何：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">multistep_preds = torch.zeros(T)</span><br><span class=\"line\">multistep_preds[:n_train + tau] = x[:n_train + tau]</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n_train + tau, T):</span><br><span class=\"line\">    multistep_preds[i] = net(multistep_preds[i - tau:i].reshape((<span class=\"number\">1</span>, -<span class=\"number\">1</span>)).to(device))</span><br><span class=\"line\"></span><br><span class=\"line\">d2l.plot([time, time[tau:], time[n_train + tau:]],</span><br><span class=\"line\">         [x.detach().numpy(), onestep_preds.cpu().detach().numpy(),</span><br><span class=\"line\">          multistep_preds[n_train + tau:].cpu().detach().numpy()], <span class=\"string\">&#x27;time&#x27;</span>, <span class=\"string\">&#x27;x&#x27;</span>,</span><br><span class=\"line\">         legend=[<span class=\"string\">&#x27;data&#x27;</span>, <span class=\"string\">&#x27;1-step preds&#x27;</span>, <span class=\"string\">&#x27;multi-step preds&#x27;</span>], xlim=[<span class=\"number\">1</span>, <span class=\"number\">1000</span>], figsize=(<span class=\"number\">6</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>如上面的例子所示，绿线的预测显然并不理想。经过几个预测步骤之后，预测的结果很快就会衰减到一个常数。为什么这个算法效果这么差呢？事实是由于误差的累积：假设在步骤1之后，我们积累了一些误差，于是步骤2的输入被扰动了，后面的预测误差依此类推。因此误差可能会相当快地偏离真实的观测结果。例如，未来24小时的天气预报往往相当准确，但超过这一点，精度就会迅速下降。我们将在本章及后续章节中讨论如何改进这一点。</p>\n<p>基于 <code>k = 1, 4, 16, 64</code>，通过对整个序列预测的计算，让我们更仔细地看一下 <code>k</code> 步预测的困难：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">max_steps = <span class=\"number\">64</span></span><br><span class=\"line\">features = torch.zeros((T - tau - max_steps + <span class=\"number\">1</span>, tau + max_steps))</span><br><span class=\"line\"><span class=\"comment\"># 列i(i&lt;tau)是来自x的观测，其时间步从(i)到(i+T-tau-max_steps+1)</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(tau):</span><br><span class=\"line\">    features[:, i] = x[i:i + T - tau - max_steps + <span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 列i(i&gt;=tau)是来自(i-tau+1)步的预测，其时间步从(i)到(i+T-tau-max_steps+1)</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(tau, tau + max_steps):</span><br><span class=\"line\">    features[:, i] = net(features[:, i - tau:i].to(device)).reshape(-<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">steps = (<span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"number\">16</span>, <span class=\"number\">64</span>)</span><br><span class=\"line\">d2l.plot([time[tau + i - <span class=\"number\">1</span>:T - max_steps + i] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> steps],</span><br><span class=\"line\">         [features[:, tau + i - <span class=\"number\">1</span>].cpu().detach().numpy() <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> steps], <span class=\"string\">&#x27;time&#x27;</span>, <span class=\"string\">&#x27;x&#x27;</span>,</span><br><span class=\"line\">         legend=[<span class=\"string\">f&#x27;<span class=\"subst\">&#123;i&#125;</span>-step preds&#x27;</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> steps], xlim=[<span class=\"number\">5</span>, <span class=\"number\">1000</span>], figsize=(<span class=\"number\">6</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-文本预处理\">2. 文本预处理</h2>\n<p>对于序列数据处理问题，我们在上一节中评估了所需的统计工具和预测时面临的挑战。这样的数据存在许多种形式，文本是最常见例子之一。例如，一篇文章可以被简单地看作一串单词序列，甚至是一串字符序列。本节中，我们将解析文本的常见预处理步骤。这些步骤通常包括：</p>\n<ul>\n<li>将文本作为字符串加载到内存中。</li>\n<li>将字符串拆分为词元（如单词和字符）。</li>\n<li>建立一个词表，将拆分的词元映射到数字索引。</li>\n<li>将文本转换为数字索引序列，方便模型操作。</li>\n</ul>\n<p>首先，我们从 H.G.Well 的<a href=\"https://www.gutenberg.org/ebooks/35\">时光机器</a>中加载文本。这是一个相当小的语料库，只有30000多个单词，但足够我们小试牛刀，而现实中的文档集合可能会包含数十亿个单词。下面的函数将数据集读取到由多条文本行组成的列表中，其中每条文本行都是一个字符串。为简单起见，我们在这里忽略了标点符号和字母大写：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> collections</span><br><span class=\"line\"><span class=\"keyword\">import</span> re</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">d2l.DATA_HUB[<span class=\"string\">&#x27;time_machine&#x27;</span>] = (d2l.DATA_URL + <span class=\"string\">&#x27;timemachine.txt&#x27;</span>, <span class=\"string\">&#x27;090b5e7e70c295757f55df93cb0a180b9691891a&#x27;</span>)</span><br><span class=\"line\">d2l.download(<span class=\"string\">&#x27;time_machine&#x27;</span>)  <span class=\"comment\"># 默认路径在../data</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">read_time_machine</span>():</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;将时间机器数据集加载到文本行的列表中，同时将非大小写字母外的所有字符替换为空格&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;../data/timemachine.txt&#x27;</span>, <span class=\"string\">&#x27;r&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        lines = f.readlines()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> [re.sub(<span class=\"string\">&#x27;[^A-Za-z]+&#x27;</span>, <span class=\"string\">&#x27; &#x27;</span>, line).strip().lower() <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines]</span><br><span class=\"line\"></span><br><span class=\"line\">lines = read_time_machine()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;文本总行数: <span class=\"subst\">&#123;<span class=\"built_in\">len</span>(lines)&#125;</span>&#x27;</span>)  <span class=\"comment\"># 文本总行数: 3221</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(lines[<span class=\"number\">0</span>])  <span class=\"comment\"># the time machine by h g wells</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(lines[<span class=\"number\">10</span>])  <span class=\"comment\"># twinkled and his usually pale face was flushed and animated the</span></span><br></pre></td></tr></table></figure>\n<p>下面的 <code>tokenize</code> 函数将文本行列表（lines）作为输入，列表中的每个元素是一个文本序列（如一条文本行）。每个文本序列又被拆分成一个词元列表，词元（token）是文本的基本单位。最后，返回一个由词元列表组成的列表，其中的每个词元都是一个字符串（string）：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">tokenize</span>(<span class=\"params\">lines, token=<span class=\"string\">&#x27;word&#x27;</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;将文本行拆分为单词或字符词元&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> token == <span class=\"string\">&#x27;word&#x27;</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> [line.split() <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines]</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> token == <span class=\"string\">&#x27;char&#x27;</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> [<span class=\"built_in\">list</span>(line) <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines]  <span class=\"comment\"># list(str)能将字符串中的每个字符分隔开形成list</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;错误！未知词元类型:&#x27;</span> + token)</span><br><span class=\"line\"></span><br><span class=\"line\">tokens = tokenize(lines)</span><br><span class=\"line\"><span class=\"comment\"># tokens = tokenize(lines, token=&#x27;char&#x27;)</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(tokens[i])</span><br><span class=\"line\"><span class=\"comment\"># [&#x27;the&#x27;, &#x27;time&#x27;, &#x27;machine&#x27;, &#x27;by&#x27;, &#x27;h&#x27;, &#x27;g&#x27;, &#x27;wells&#x27;]</span></span><br><span class=\"line\"><span class=\"comment\"># []</span></span><br><span class=\"line\"><span class=\"comment\"># []</span></span><br><span class=\"line\"><span class=\"comment\"># []</span></span><br><span class=\"line\"><span class=\"comment\"># []</span></span><br><span class=\"line\"><span class=\"comment\"># [&#x27;i&#x27;]</span></span><br><span class=\"line\"><span class=\"comment\"># []</span></span><br><span class=\"line\"><span class=\"comment\"># []</span></span><br><span class=\"line\"><span class=\"comment\"># [&#x27;the&#x27;, &#x27;time&#x27;, &#x27;traveller&#x27;, &#x27;for&#x27;, &#x27;so&#x27;, &#x27;it&#x27;, &#x27;will&#x27;, &#x27;be&#x27;, &#x27;convenient&#x27;, &#x27;to&#x27;, &#x27;speak&#x27;, &#x27;of&#x27;, &#x27;him&#x27;]</span></span><br><span class=\"line\"><span class=\"comment\"># [&#x27;was&#x27;, &#x27;expounding&#x27;, &#x27;a&#x27;, &#x27;recondite&#x27;, &#x27;matter&#x27;, &#x27;to&#x27;, &#x27;us&#x27;, &#x27;his&#x27;, &#x27;grey&#x27;, &#x27;eyes&#x27;, &#x27;shone&#x27;, &#x27;and&#x27;]</span></span><br></pre></td></tr></table></figure>\n<p>词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。现在，让我们构建一个字典，通常也叫做词表（vocabulary），用来将字符串类型的词元映射到从0开始的数字索引中。我们先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计，得到的统计结果称之为语料（corpus）。然后根据每个唯一词元的出现频率，为其分配一个数字索引。很少出现的词元通常被移除，这可以降低复杂性。另外，语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元 <code>&lt;unk&gt;</code>。我们可以选择增加一个列表，用于保存那些被保留的词元，例如：填充词元（<code>&lt;pad&gt;</code>）、序列开始词元（<code>&lt;bos&gt;</code>）、序列结束词元（<code>&lt;eos&gt;</code>）：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Vocab</span>:</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;文本词表&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, tokens=<span class=\"literal\">None</span>, min_freq=<span class=\"number\">0</span>, reserved_tokens=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> tokens <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            tokens = []</span><br><span class=\"line\">        <span class=\"keyword\">if</span> reserved_tokens <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            reserved_tokens = []</span><br><span class=\"line\">        <span class=\"comment\"># 按出现频率从大到小排序</span></span><br><span class=\"line\">        counter = count_corpus(tokens)</span><br><span class=\"line\">        self._token_freqs = <span class=\"built_in\">sorted</span>(counter.items(), key=<span class=\"keyword\">lambda</span> x: x[<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 构建索引到词元与词元到索引的映射，未知词元的索引为0</span></span><br><span class=\"line\">        self.idx_to_token = [<span class=\"string\">&#x27;&lt;unk&gt;&#x27;</span>] + reserved_tokens</span><br><span class=\"line\">        self.token_to_idx = &#123;token: idx <span class=\"keyword\">for</span> idx, token <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(self.idx_to_token)&#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> token, freq <span class=\"keyword\">in</span> self._token_freqs:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> freq &lt; min_freq:  <span class=\"comment\"># 如果token出现的次数少于min_freq次则直接丢弃</span></span><br><span class=\"line\">                <span class=\"keyword\">break</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> token <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> self.token_to_idx:</span><br><span class=\"line\">                self.idx_to_token.append(token)</span><br><span class=\"line\">                self.token_to_idx[token] = <span class=\"built_in\">len</span>(self.idx_to_token) - <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__len__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">len</span>(self.idx_to_token)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__getitem__</span>(<span class=\"params\">self, tokens</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(tokens, (<span class=\"built_in\">list</span>, <span class=\"built_in\">tuple</span>)):</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.token_to_idx.get(tokens, self.unk)  <span class=\"comment\"># tokens不存在则返回0</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> [self.__getitem__(token) <span class=\"keyword\">for</span> token <span class=\"keyword\">in</span> tokens]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">to_tokens</span>(<span class=\"params\">self, indices</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(indices, (<span class=\"built_in\">list</span>, <span class=\"built_in\">tuple</span>)):</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.idx_to_token[indices]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> [self.idx_to_token[index] <span class=\"keyword\">for</span> index <span class=\"keyword\">in</span> indices]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">    @property</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">unk</span>(<span class=\"params\">self</span>):  <span class=\"comment\"># 未知词元的索引为0</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">    @property</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">token_freqs</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self._token_freqs</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">count_corpus</span>(<span class=\"params\">tokens</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;统计词元的频率&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 这里的tokens是1D列表或2D列表</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(tokens) == <span class=\"number\">0</span> <span class=\"keyword\">or</span> <span class=\"built_in\">isinstance</span>(tokens[<span class=\"number\">0</span>], <span class=\"built_in\">list</span>):</span><br><span class=\"line\">        tokens = [token <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> tokens <span class=\"keyword\">for</span> token <span class=\"keyword\">in</span> line]  <span class=\"comment\"># 将词元列表展平成一个列表</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> collections.Counter(tokens)</span><br></pre></td></tr></table></figure>\n<p>我们首先使用时光机器数据集作为语料库来构建词表，然后打印前几个高频词元及其索引：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vocab = Vocab(tokens)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">list</span>(vocab.token_to_idx.items())[:<span class=\"number\">10</span>])  <span class=\"comment\"># 注意不加item()的话只会将key转成list</span></span><br><span class=\"line\"><span class=\"comment\"># [(&#x27;&lt;unk&gt;&#x27;, 0), (&#x27;the&#x27;, 1), (&#x27;i&#x27;, 2), (&#x27;and&#x27;, 3), (&#x27;of&#x27;, 4), (&#x27;a&#x27;, 5), (&#x27;to&#x27;, 6), (&#x27;was&#x27;, 7), (&#x27;in&#x27;, 8), (&#x27;that&#x27;, 9)]</span></span><br></pre></td></tr></table></figure>\n<p>现在，我们可以将每一条文本行转换成一个数字索引列表：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;文本:&#x27;</span>, tokens[<span class=\"number\">0</span>])  <span class=\"comment\"># 文本: [&#x27;the&#x27;, &#x27;time&#x27;, &#x27;machine&#x27;, &#x27;by&#x27;, &#x27;h&#x27;, &#x27;g&#x27;, &#x27;wells&#x27;]</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;索引:&#x27;</span>, vocab[tokens[<span class=\"number\">0</span>]])  <span class=\"comment\"># 索引: [1, 19, 50, 40, 2183, 2184, 400]</span></span><br></pre></td></tr></table></figure>\n<p>在使用上述函数时，我们将所有功能打包到 <code>load_corpus_time_machine</code> 函数中，该函数返回 <code>corpus</code>（词元索引列表）和 <code>vocab</code>（时光机器语料库的词表）。我们在这里所做的改变是：</p>\n<ul>\n<li>为了简化后面章节中的训练，我们使用字符（而不是单词）实现文本词元化；</li>\n<li>时光机器数据集中的每个文本行不一定是一个句子或一个段落，还可能是一个单词，因此返回的 <code>corpus</code> 仅处理为单个列表，而不是使用多词元列表构成的一个列表。</li>\n</ul>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">load_corpus_time_machine</span>(<span class=\"params\">max_tokens=-<span class=\"number\">1</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;返回时光机器数据集的词元索引列表和词表&quot;&quot;&quot;</span></span><br><span class=\"line\">    lines = read_time_machine()</span><br><span class=\"line\">    tokens = tokenize(lines, <span class=\"string\">&#x27;char&#x27;</span>)</span><br><span class=\"line\">    vocab = Vocab(tokens)</span><br><span class=\"line\">    <span class=\"comment\"># 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，所以将所有文本行展平到一个列表中</span></span><br><span class=\"line\">    corpus = [vocab[token] <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> tokens <span class=\"keyword\">for</span> token <span class=\"keyword\">in</span> line]</span><br><span class=\"line\">    <span class=\"keyword\">if</span> max_tokens &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">        corpus = corpus[:max_tokens]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> corpus, vocab</span><br><span class=\"line\"></span><br><span class=\"line\">corpus, vocab = load_corpus_time_machine()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">len</span>(corpus), <span class=\"built_in\">len</span>(vocab))  <span class=\"comment\"># 170580 28</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;索引:&#x27;</span>, corpus[:<span class=\"number\">10</span>])  <span class=\"comment\"># 索引: [3, 9, 2, 1, 3, 5, 13, 2, 1, 13]</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;文本:&#x27;</span>, vocab.to_tokens(corpus[:<span class=\"number\">10</span>]))  <span class=\"comment\"># 文本: [&#x27;t&#x27;, &#x27;h&#x27;, &#x27;e&#x27;, &#x27; &#x27;, &#x27;t&#x27;, &#x27;i&#x27;, &#x27;m&#x27;, &#x27;e&#x27;, &#x27; &#x27;, &#x27;m&#x27;]</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"3-语言模型和数据集\">3. 语言模型和数据集</h2>\n<p>由于涉及较多数学公式，语言模型的讲解可以转至：<a href=\"https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html\">语言模型和数据集</a>。</p>\n<p>根据上一节中介绍的时光机器数据集构建词表，并打印前10个最常用的（频率最高的）单词：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> random</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">tokens = d2l.tokenize(d2l.read_time_machine())</span><br><span class=\"line\"><span class=\"comment\"># 因为每个文本行不一定是一个句子或一个段落，因此我们把所有文本行拼接到一起</span></span><br><span class=\"line\">corpus = [token <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> tokens <span class=\"keyword\">for</span> token <span class=\"keyword\">in</span> line]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(corpus[:<span class=\"number\">10</span>])  <span class=\"comment\"># [&#x27;the&#x27;, &#x27;time&#x27;, &#x27;machine&#x27;, &#x27;by&#x27;, &#x27;h&#x27;, &#x27;g&#x27;, &#x27;wells&#x27;, &#x27;i&#x27;, &#x27;the&#x27;, &#x27;time&#x27;]</span></span><br><span class=\"line\">vocab = d2l.Vocab(corpus)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(vocab.token_freqs[:<span class=\"number\">10</span>])  <span class=\"comment\"># [(&#x27;the&#x27;, 2261), (&#x27;i&#x27;, 1267), (&#x27;and&#x27;, 1245), (&#x27;of&#x27;, 1155), ...]</span></span><br></pre></td></tr></table></figure>\n<p>正如我们所看到的，最流行的词看起来很无聊，这些词通常被称为停用词（stop words），因此可以被过滤掉。尽管如此，它们本身仍然是有意义的，我们仍然会在模型中使用它们。此外，还有个明显的问题是词频衰减的速度相当地快。例如，最常用单词的词频对比，第10个还不到第1个的1/5。为了更好地理解，我们可以画出的词频图：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">freqs = [freq <span class=\"keyword\">for</span> token, freq <span class=\"keyword\">in</span> vocab.token_freqs]</span><br><span class=\"line\">d2l.plot(freqs, xlabel=<span class=\"string\">&#x27;token: x&#x27;</span>, ylabel=<span class=\"string\">&#x27;frequency: n(x)&#x27;</span>, xscale=<span class=\"string\">&#x27;log&#x27;</span>, yscale=<span class=\"string\">&#x27;log&#x27;</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>通过词频图我们可以发现：词频以一种明确的方式迅速衰减。将前几个单词作为例外消除后，剩余的所有单词大致遵循双对数坐标图上的一条直线。这意味着单词的频率满足齐普夫定律（Zipf’s law）。这告诉我们想要通过计数统计和平滑来建模单词是不可行的，因为这样建模的结果会大大高估尾部单词的频率，也就是所谓的不常用单词。那么其他的词元组合，比如二元语法、三元语法等等，又会如何呢？我们来看看二元语法的频率是否与一元语法的频率表现出相同的行为方式：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bigram_tokens = [pair <span class=\"keyword\">for</span> pair <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(corpus[:-<span class=\"number\">1</span>], corpus[<span class=\"number\">1</span>:])]  <span class=\"comment\"># 遍历所有连续的两个词元</span></span><br><span class=\"line\">bigram_vocab = d2l.Vocab(bigram_tokens)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(bigram_vocab.token_freqs[:<span class=\"number\">10</span>])  <span class=\"comment\"># [((&#x27;of&#x27;, &#x27;the&#x27;), 309), ((&#x27;in&#x27;, &#x27;the&#x27;), 169), ...]</span></span><br></pre></td></tr></table></figure>\n<p>这里值得注意：在十个最频繁的词对中，有九个是由两个停用词组成的，只有一个与 <code>the time</code> 有关。我们再进一步看看三元语法的频率是否表现出相同的行为方式：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">trigram_tokens = [triple <span class=\"keyword\">for</span> triple <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(corpus[:-<span class=\"number\">2</span>], corpus[<span class=\"number\">1</span>:-<span class=\"number\">1</span>], corpus[<span class=\"number\">2</span>:])]  <span class=\"comment\"># 遍历所有连续的三个词元</span></span><br><span class=\"line\">trigram_vocab = d2l.Vocab(trigram_tokens)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(trigram_vocab.token_freqs[:<span class=\"number\">10</span>])  <span class=\"comment\"># [((&#x27;the&#x27;, &#x27;time&#x27;, &#x27;traveller&#x27;), 59), ((&#x27;the&#x27;, &#x27;time&#x27;, &#x27;machine&#x27;), 30), ...]</span></span><br></pre></td></tr></table></figure>\n<p>最后，我们直观地对比三种模型中的词元频率：一元语法、二元语法和三元语法：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bigram_freqs = [freq <span class=\"keyword\">for</span> token, freq <span class=\"keyword\">in</span> bigram_vocab.token_freqs]</span><br><span class=\"line\">trigram_freqs = [freq <span class=\"keyword\">for</span> token, freq <span class=\"keyword\">in</span> trigram_vocab.token_freqs]</span><br><span class=\"line\">d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel=<span class=\"string\">&#x27;token: x&#x27;</span>, ylabel=<span class=\"string\">&#x27;frequency: n(x)&#x27;</span>,</span><br><span class=\"line\">         xscale=<span class=\"string\">&#x27;log&#x27;</span>, yscale=<span class=\"string\">&#x27;log&#x27;</span>, legend=[<span class=\"string\">&#x27;unigram&#x27;</span>, <span class=\"string\">&#x27;bigram&#x27;</span>, <span class=\"string\">&#x27;trigram&#x27;</span>])</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>由于序列数据本质上是<strong>连续的</strong>，因此我们在处理数据时需要解决这个问题。在第一节中我们以一种相当特别的方式做到了这一点：当序列变得太长而不能被模型一次性全部处理时，我们可能希望拆分这样的序列方便模型读取。</p>\n<p>在介绍该模型之前，我们看一下总体策略。假设我们将使用神经网络来训练语言模型，模型中的网络一次处理具有预定义长度（例如 𝑛 个时间步）的一个小批量序列。现在的问题是如何随机生成一个小批量数据的特征和标签以供读取。</p>\n<p>首先，由于文本序列可以是任意长的，例如整本《时光机器》（The Time Machine），于是任意长的序列可以被我们划分为具有相同时间步数的子序列。当训练我们的神经网络时，这样的小批量子序列将被输入到模型中。假设网络一次只处理具有 𝑛 个时间步的子序列，那么可以从指定的起始位置开始截取连续的长度为 𝑛 的子序列，因为我们可以选择任意偏移量来指示初始位置，所以我们有相当大的自由度。</p>\n<p>如果我们只选择一个偏移量，那么用于训练网络的、所有可能的子序列的覆盖范围将是有限的。因此，我们可以从<strong>随机偏移量</strong>开始划分序列，以同时获得覆盖性（coverage）和随机性（randomness）。下面，我们将描述如何实现<strong>随机采样</strong>（random sampling）和<strong>顺序分区</strong>（sequential partitioning）策略。</p>\n<p>在随机采样中，每个样本都是在原始的长序列上<strong>任意捕获</strong>的子序列。在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元，因此标签是移位了一个词元的原始序列。</p>\n<p>下面的代码每次可以从数据中随机生成一个小批量。在这里，参数 <code>batch_size</code> 指定了每个小批量中子序列样本的数目，参数 <code>num_steps</code> 是每个子序列中预定义的时间步数：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">seq_data_iter_random</span>(<span class=\"params\">corpus, batch_size, num_steps</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;使用随机抽样生成一个小批量子序列&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1</span></span><br><span class=\"line\">    corpus = corpus[random.randint(<span class=\"number\">0</span>, num_steps - <span class=\"number\">1</span>):]  <span class=\"comment\"># 截取随机起始位置之后的部分</span></span><br><span class=\"line\">    <span class=\"comment\"># 减去1，是因为我们需要考虑标签，要留至少一个数据作为最后一组预测的标签</span></span><br><span class=\"line\">    num_subseqs = (<span class=\"built_in\">len</span>(corpus) - <span class=\"number\">1</span>) // num_steps  <span class=\"comment\"># 分区数</span></span><br><span class=\"line\">    <span class=\"comment\"># 长度为num_steps的子序列的起始索引</span></span><br><span class=\"line\">    initial_indices = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(<span class=\"number\">0</span>, num_subseqs * num_steps, num_steps))</span><br><span class=\"line\">    <span class=\"comment\"># 在随机抽样的迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻</span></span><br><span class=\"line\">    random.shuffle(initial_indices)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">data</span>(<span class=\"params\">pos</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 返回从pos位置开始的长度为num_steps的序列</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> corpus[pos:pos + num_steps]</span><br><span class=\"line\"></span><br><span class=\"line\">    num_batches = num_subseqs // batch_size</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, batch_size * num_batches, batch_size):</span><br><span class=\"line\">        <span class=\"comment\"># 在这里，initial_indices包含子序列的随机起始索引</span></span><br><span class=\"line\">        initial_indices_per_batch = initial_indices[i:i + batch_size]</span><br><span class=\"line\">        X = [data(j) <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> initial_indices_per_batch]</span><br><span class=\"line\">        Y = [data(j + <span class=\"number\">1</span>) <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> initial_indices_per_batch]</span><br><span class=\"line\">        <span class=\"keyword\">yield</span> torch.tensor(X), torch.tensor(Y)</span><br></pre></td></tr></table></figure>\n<p>下面我们生成一个从0到34的序列。假设批量大小为2，时间步数为5，这意味着可以生成6个特征-标签子序列对。如果设置小批量大小为2，我们只能得到3个小批量：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">my_seq = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(<span class=\"number\">35</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> X, Y <span class=\"keyword\">in</span> seq_data_iter_random(my_seq, batch_size=<span class=\"number\">2</span>, num_steps=<span class=\"number\">5</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;X:&#x27;</span>, X, <span class=\"string\">&#x27;\\nY:&#x27;</span>, Y)</span><br><span class=\"line\"><span class=\"comment\"># X: tensor([[ 7,  8,  9, 10, 11], [17, 18, 19, 20, 21]])</span></span><br><span class=\"line\"><span class=\"comment\"># Y: tensor([[ 8,  9, 10, 11, 12], [18, 19, 20, 21, 22]])</span></span><br><span class=\"line\"><span class=\"comment\"># X: tensor([[22, 23, 24, 25, 26], [27, 28, 29, 30, 31]])</span></span><br><span class=\"line\"><span class=\"comment\"># Y: tensor([[23, 24, 25, 26, 27], [28, 29, 30, 31, 32]])</span></span><br><span class=\"line\"><span class=\"comment\"># X: tensor([[ 2,  3,  4,  5,  6], [12, 13, 14, 15, 16]])</span></span><br><span class=\"line\"><span class=\"comment\"># Y: tensor([[ 3,  4,  5,  6,  7], [13, 14, 15, 16, 17]])</span></span><br></pre></td></tr></table></figure>\n<p>在迭代过程中，除了对原始序列可以随机抽样外，我们还可以保证两个相邻的小批量中的子序列在原始序列上也是相邻的。这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为顺序分区：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">seq_data_iter_sequential</span>(<span class=\"params\">corpus, batch_size, num_steps</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;使用顺序分区生成一个小批量子序列&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 从随机偏移量开始划分序列</span></span><br><span class=\"line\">    offset = random.randint(<span class=\"number\">0</span>, num_steps)</span><br><span class=\"line\">    num_tokens = ((<span class=\"built_in\">len</span>(corpus) - offset - <span class=\"number\">1</span>) // batch_size) * batch_size</span><br><span class=\"line\">    Xs = torch.tensor(corpus[offset:offset + num_tokens])</span><br><span class=\"line\">    Ys = torch.tensor(corpus[offset + <span class=\"number\">1</span>:offset + <span class=\"number\">1</span> + num_tokens])</span><br><span class=\"line\">    Xs, Ys = Xs.reshape(batch_size, -<span class=\"number\">1</span>), Ys.reshape(batch_size, -<span class=\"number\">1</span>)</span><br><span class=\"line\">    num_batches = Xs.shape[<span class=\"number\">1</span>] // num_steps  <span class=\"comment\"># batch的数量</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, num_steps * num_batches, num_steps):</span><br><span class=\"line\">        X = Xs[:, i:i + num_steps]</span><br><span class=\"line\">        Y = Ys[:, i:i + num_steps]</span><br><span class=\"line\">        <span class=\"keyword\">yield</span> X, Y</span><br></pre></td></tr></table></figure>\n<p>基于相同的设置，通过顺序分区读取每个小批量的子序列的特征 <code>X</code> 和标签 <code>Y</code>。通过将它们打印出来可以发现：迭代期间来自两个相邻的小批量中的子序列在原始序列中确实是相邻的：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> X, Y <span class=\"keyword\">in</span> seq_data_iter_sequential(my_seq, batch_size=<span class=\"number\">2</span>, num_steps=<span class=\"number\">5</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;X:&#x27;</span>, X, <span class=\"string\">&#x27;\\nY:&#x27;</span>, Y)</span><br><span class=\"line\"><span class=\"comment\"># X: tensor([[ 2,  3,  4,  5,  6], [18, 19, 20, 21, 22]])</span></span><br><span class=\"line\"><span class=\"comment\"># Y: tensor([[ 3,  4,  5,  6,  7], [19, 20, 21, 22, 23]])</span></span><br><span class=\"line\"><span class=\"comment\"># X: tensor([[ 7,  8,  9, 10, 11], [23, 24, 25, 26, 27]])</span></span><br><span class=\"line\"><span class=\"comment\"># Y: tensor([[ 8,  9, 10, 11, 12], [24, 25, 26, 27, 28]])</span></span><br><span class=\"line\"><span class=\"comment\"># X: tensor([[12, 13, 14, 15, 16], [28, 29, 30, 31, 32]])</span></span><br><span class=\"line\"><span class=\"comment\"># Y: tensor([[13, 14, 15, 16, 17], [29, 30, 31, 32, 33]])</span></span><br></pre></td></tr></table></figure>\n<p>现在，我们将上面的两个采样函数包装到一个类中，以便稍后可以将其用作数据迭代器：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SeqDataLoader</span>:</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;加载序列数据的迭代器&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, batch_size, num_steps, use_random_iter, max_tokens</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> use_random_iter:</span><br><span class=\"line\">            self.data_iter_fn = d2l.seq_data_iter_random</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.data_iter_fn = d2l.seq_data_iter_sequential</span><br><span class=\"line\">        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)</span><br><span class=\"line\">        self.batch_size, self.num_steps = batch_size, num_steps</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__iter__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)</span><br></pre></td></tr></table></figure>\n<p>最后，我们定义了一个函数 <code>load_data_time_machine</code>，它同时返回数据迭代器和词表，因此可以与其他带有 <code>load_data</code> 前缀的函数（如 <code>d2l.load_data_fashion_mnist</code>）类似地使用：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">load_data_time_machine</span>(<span class=\"params\">batch_size, num_steps, use_random_iter=<span class=\"literal\">False</span>, max_tokens=<span class=\"number\">10000</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;返回时光机器数据集的迭代器和词表&quot;&quot;&quot;</span></span><br><span class=\"line\">    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> data_iter, data_iter.vocab</span><br></pre></td></tr></table></figure>\n<h2 id=\"4-循环神经网络\">4. 循环神经网络</h2>\n<p>由于涉及较多数学公式，循环神经网络的理论部分可以转至：<a href=\"https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/rnn.html\">循环神经网络</a>。</p>\n<h3 id=\"4-1-循环神经网络的从零开始实现\">4.1 循环神经网络的从零开始实现</h3>\n<p>本节将从头开始基于循环神经网络实现字符级语言模型。这样的模型将在 H.G.Wells 的时光机器数据集上训练。和前面上一节中介绍过的一样，我们先读取数据集：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.tensorboard <span class=\"keyword\">import</span> SummaryWriter</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"><span class=\"keyword\">from</span> tqdm <span class=\"keyword\">import</span> tqdm</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size, num_steps = <span class=\"number\">32</span>, <span class=\"number\">35</span></span><br><span class=\"line\">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps, max_tokens=<span class=\"number\">10000</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">len</span>(train_iter.corpus))  <span class=\"comment\"># 10000</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(X.shape, y.shape)  <span class=\"comment\"># torch.Size([32, 35]) torch.Size([32, 35])</span></span><br><span class=\"line\">    <span class=\"keyword\">break</span></span><br></pre></td></tr></table></figure>\n<p>回想一下，在 <code>train_iter</code> 中，每个词元都表示为一个数字索引，将这些索引直接输入神经网络可能会使学习变得困难。我们通常将每个词元表示为更具表现力的特征向量。最简单的表示称为独热编码（one-hot encoding）。</p>\n<p>简言之，将每个索引映射为相互不同的单位向量：假设词表中不同词元的数目为N（即 <code>len(vocab)</code>），词元索引的范围为0~N-1。如果词元的索引是整数 <code>i</code>，那么我们将创建一个长度为N的全0向量，并将第 <code>i</code> 处的元素设置为1。此向量是原始词元的一个独热向量。索引为0和2的独热向量如下所示：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(F.one_hot(torch.tensor([<span class=\"number\">0</span>, <span class=\"number\">2</span>]), <span class=\"built_in\">len</span>(vocab)))</span><br><span class=\"line\"><span class=\"comment\"># tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span></span><br><span class=\"line\"><span class=\"comment\">#         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])</span></span><br></pre></td></tr></table></figure>\n<p>我们每次采样的小批量数据形状是二维张量：<code>(批量大小, 时间步数)</code>。<code>one_hot</code> 函数将这样一个小批量数据转换成三维张量，张量的最后一个维度等于词表大小（<code>len(vocab)</code>）。我们经常转换输入的维度，以便获得形状为 <code>(时间步数, 批量大小, 词表大小)</code> 的输出。这将使我们能够更方便地通过最外层的维度，一步一步地更新小批量数据的隐状态：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.arange(<span class=\"number\">10</span>).reshape((<span class=\"number\">2</span>, <span class=\"number\">5</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(F.one_hot(X.T, <span class=\"number\">28</span>).shape)  <span class=\"comment\"># torch.Size([5, 2, 28])</span></span><br></pre></td></tr></table></figure>\n<p>接下来，我们初始化循环神经网络模型的模型参数。隐藏单元数 <code>num_hiddens</code> 是一个可调的超参数。当训练语言模型时，输入和输出来自相同的词表（输出可以看成多分类问题，即输出表示对每个词元的预测概率）。因此，它们具有相同的维度，即词表的大小：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_params</span>(<span class=\"params\">vocab_size, num_hiddens, device</span>):</span><br><span class=\"line\">    num_inputs = num_outputs = vocab_size</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">normal</span>(<span class=\"params\">shape</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.randn(size=shape, device=device) * <span class=\"number\">0.01</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 隐藏层参数</span></span><br><span class=\"line\">    W_xh = normal((num_inputs, num_hiddens))</span><br><span class=\"line\">    W_hh = normal((num_hiddens, num_hiddens))</span><br><span class=\"line\">    b_h = torch.zeros(num_hiddens, device=device)</span><br><span class=\"line\">    <span class=\"comment\"># 输出层参数</span></span><br><span class=\"line\">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class=\"line\">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class=\"line\">    <span class=\"comment\"># 附加梯度</span></span><br><span class=\"line\">    params = [W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> params:</span><br><span class=\"line\">        param.requires_grad_(<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> params</span><br></pre></td></tr></table></figure>\n<p>为了定义循环神经网络模型，我们首先需要一个 <code>init_rnn_state</code> 函数在初始化时返回隐状态。这个函数的返回是一个张量，张量全用0填充，形状为 <code>(批量大小, 隐藏单元数)</code>。在后面的章节中我们将会遇到隐状态包含多个变量的情况，而使用元组可以更容易地处理些：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_rnn_state</span>(<span class=\"params\">batch_size, num_hiddens, device</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (torch.zeros((batch_size, num_hiddens), device=device),)</span><br></pre></td></tr></table></figure>\n<p>下面的 <code>rnn</code> 函数定义了如何在一个时间步内计算隐状态和输出。循环神经网络模型通过 <code>inputs</code> 最外层的维度实现循环，以便逐时间步更新小批量数据的隐状态H。此外，这里使用 <code>tanh</code> 函数作为激活函数，当元素在实数上满足均匀分布时，<code>tanh</code> 函数的平均值为0：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">rnn</span>(<span class=\"params\">inputs, state, params</span>):</span><br><span class=\"line\">    <span class=\"comment\"># inputs.shape: (时间步数量, 批量大小, 词表大小)</span></span><br><span class=\"line\">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class=\"line\">    H, = state</span><br><span class=\"line\">    outputs = []</span><br><span class=\"line\">    <span class=\"comment\"># X.shape: (批量大小, 词表大小)</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> X <span class=\"keyword\">in</span> inputs:</span><br><span class=\"line\">        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)</span><br><span class=\"line\">        Y = torch.mm(H, W_hq) + b_q</span><br><span class=\"line\">        outputs.append(Y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.cat(outputs, dim=<span class=\"number\">0</span>), (H,)</span><br></pre></td></tr></table></figure>\n<p>定义了所有需要的函数之后，接下来我们创建一个类来包装这些函数，并存储从零开始实现的循环神经网络模型的参数：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">RNNModelScratch</span>:</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;从零开始实现的循环神经网络模型&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn</span>):</span><br><span class=\"line\">        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens</span><br><span class=\"line\">        self.params = get_params(vocab_size, num_hiddens, device)</span><br><span class=\"line\">        self.init_state, self.forward_fn = init_state, forward_fn</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__call__</span>(<span class=\"params\">self, X, state</span>):</span><br><span class=\"line\">        X = F.one_hot(X.T, self.vocab_size).<span class=\"built_in\">type</span>(torch.float32)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.forward_fn(X, state, self.params)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">begin_state</span>(<span class=\"params\">self, batch_size, device</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.init_state(batch_size, self.num_hiddens, device)</span><br></pre></td></tr></table></figure>\n<p>让我们检查输出是否具有正确的形状。例如，隐状态的维数是否保持不变：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">device = torch.device(<span class=\"string\">&#x27;cuda&#x27;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\">num_hiddens = <span class=\"number\">512</span></span><br><span class=\"line\">net = RNNModelScratch(<span class=\"built_in\">len</span>(vocab), num_hiddens, device, get_params, init_rnn_state, rnn)</span><br><span class=\"line\">state = net.begin_state(X.shape[<span class=\"number\">0</span>], device)</span><br><span class=\"line\">Y, new_state = net(X.to(device), state)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Y.shape, <span class=\"built_in\">len</span>(new_state), new_state[<span class=\"number\">0</span>].shape)  <span class=\"comment\"># torch.Size([10, 28]) 1 torch.Size([2, 512])</span></span><br></pre></td></tr></table></figure>\n<p>我们可以看到输出形状是 <code>(时间步数 * 批量大小, 词表大小)</code>，而隐状态形状保持不变，即 <code>(批量大小, 隐藏单元数)</code>。</p>\n<p>让我们首先定义预测函数来生成 <code>prefix</code> 之后的新字符，其中的 <code>prefix</code> 是一个用户提供的包含多个字符的字符串。在循环遍历 <code>prefix</code> 中的开始字符时，我们不断地将隐状态传递到下一个时间步，但是不生成任何输出。这被称为预热（warm-up）期，因为在此期间模型会自我更新（例如，更新隐状态），但不会进行预测。预热期结束后，隐状态的值通常比刚开始的初始值更适合预测，从而预测字符并输出它们：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">prefix, num_preds, net, vocab, device</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;在prefix后面生成新字符&quot;&quot;&quot;</span></span><br><span class=\"line\">    state = net.begin_state(batch_size=<span class=\"number\">1</span>, device=device)</span><br><span class=\"line\">    outputs = [vocab[prefix[<span class=\"number\">0</span>]]]</span><br><span class=\"line\">    get_input = <span class=\"keyword\">lambda</span>: torch.tensor([outputs[-<span class=\"number\">1</span>]], device=device).reshape((<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> y <span class=\"keyword\">in</span> prefix[<span class=\"number\">1</span>:]:  <span class=\"comment\"># 预热期</span></span><br><span class=\"line\">        _, state = net(get_input(), state)</span><br><span class=\"line\">        outputs.append(vocab[y])</span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_preds):  <span class=\"comment\"># 预测num_preds步</span></span><br><span class=\"line\">        y, state = net(get_input(), state)</span><br><span class=\"line\">        outputs.append(<span class=\"built_in\">int</span>(y.argmax(dim=<span class=\"number\">1</span>).reshape(<span class=\"number\">1</span>)))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">&#x27;&#x27;</span>.join([vocab.idx_to_token[i] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> outputs])</span><br></pre></td></tr></table></figure>\n<p>现在我们可以测试 <code>predict</code> 函数。我们将前缀指定为 <code>time traveller</code>，并基于这个前缀生成10个后续字符。鉴于我们还没有训练网络，它会生成荒谬的预测结果：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(predict(<span class=\"string\">&#x27;time traveller &#x27;</span>, <span class=\"number\">10</span>, net, vocab, device))  <span class=\"comment\"># time traveller gxgtlsryyy</span></span><br></pre></td></tr></table></figure>\n<p>梯度裁剪的理论可转至：<a href=\"https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html\">循环神经网络的从零开始实现</a>。</p>\n<p>下面我们定义一个函数来裁剪模型的梯度，模型是从零开始实现的模型或由高级 API 构建的模型。我们在此计算了所有模型参数的梯度的范数：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">grad_clipping</span>(<span class=\"params\">net, theta</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;裁剪梯度&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, nn.Module):</span><br><span class=\"line\">        params = [p <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> net.parameters() <span class=\"keyword\">if</span> p.requires_grad]</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        params = net.params</span><br><span class=\"line\">    norm = torch.sqrt(<span class=\"built_in\">sum</span>(torch.<span class=\"built_in\">sum</span>((p.grad ** <span class=\"number\">2</span>)) <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> params))</span><br><span class=\"line\">    <span class=\"keyword\">if</span> norm &gt; theta:</span><br><span class=\"line\">        <span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> params:</span><br><span class=\"line\">            param.grad[:] *= theta / norm</span><br></pre></td></tr></table></figure>\n<p>在训练模型之前，让我们定义一个函数在一个迭代周期内训练模型。它与我们训练 Softmax 模型的方式有三个不同之处：</p>\n<ul>\n<li>序列数据的不同采样方法（随机采样和顺序分区）将导致隐状态初始化的差异。</li>\n<li>我们在更新模型参数之前裁剪梯度。这样的操作的目的是，即使训练过程中某个点上发生了梯度爆炸，也能保证模型不会发散。</li>\n<li>我们用困惑度来评价模型。这样的度量确保了不同长度的序列具有可比性。</li>\n</ul>\n<p>具体来说，当使用顺序分区时，我们只在每个迭代周期的开始位置初始化隐状态。由于下一个小批量数据中的第 <code>i</code> 个子序列样本与当前第 <code>i</code> 个子序列样本相邻，因此当前小批量数据最后一个样本的隐状态，将用于初始化下一个小批量数据第一个样本的隐状态。这样，存储在隐状态中的序列的历史信息可以在一个迭代周期内流经相邻的子序列。然而，在任何一点隐状态的计算，都依赖于同一迭代周期中前面所有的小批量数据，这使得梯度计算变得复杂。为了降低计算量，在处理任何一个小批量数据之前，我们先<strong>分离梯度</strong>，使得隐状态的梯度计算总是限制在一个小批量数据的时间步内。</p>\n<p>当使用随机抽样时，因为每个样本都是在一个随机位置抽样的，因此需要<strong>为每个迭代周期重新初始化隐状态</strong>。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_epoch</span>(<span class=\"params\">net, train_iter, loss_function, optimizer, device, use_random_iter</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;训练网络一个迭代周期（定义见第8章）&quot;&quot;&quot;</span></span><br><span class=\"line\">    state = <span class=\"literal\">None</span></span><br><span class=\"line\">    train_loss = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X, Y <span class=\"keyword\">in</span> tqdm(train_iter):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> state <span class=\"keyword\">is</span> <span class=\"literal\">None</span> <span class=\"keyword\">or</span> use_random_iter:</span><br><span class=\"line\">            <span class=\"comment\"># 在第一次迭代或使用随机抽样时初始化state</span></span><br><span class=\"line\">            state = net.begin_state(batch_size=X.shape[<span class=\"number\">0</span>], device=device)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, nn.Module) <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(state, <span class=\"built_in\">tuple</span>):</span><br><span class=\"line\">                <span class=\"comment\"># state对于nn.GRU是个张量</span></span><br><span class=\"line\">                state.detach_()</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                <span class=\"comment\"># state对于nn.LSTM或对于我们从零开始实现的模型是个元组</span></span><br><span class=\"line\">                <span class=\"keyword\">for</span> s <span class=\"keyword\">in</span> state:</span><br><span class=\"line\">                    s.detach_()</span><br><span class=\"line\">        y = Y.T.reshape(-<span class=\"number\">1</span>)</span><br><span class=\"line\">        X, y = X.to(device), y.to(device)</span><br><span class=\"line\">        y_hat, state = net(X, state)</span><br><span class=\"line\">        loss = loss_function(y_hat, y.long()).mean()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(optimizer, torch.optim.Optimizer):</span><br><span class=\"line\">            optimizer.zero_grad()</span><br><span class=\"line\">            loss.backward()</span><br><span class=\"line\">            grad_clipping(net, <span class=\"number\">1</span>)</span><br><span class=\"line\">            optimizer.step()</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            loss.backward()</span><br><span class=\"line\">            grad_clipping(net, <span class=\"number\">1</span>)</span><br><span class=\"line\">            optimizer(batch_size=<span class=\"number\">1</span>)</span><br><span class=\"line\">        train_loss.append(loss)  <span class=\"comment\"># 因为已经调用了mean函数</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> math.exp(<span class=\"built_in\">sum</span>(train_loss) / <span class=\"built_in\">len</span>(train_loss))  <span class=\"comment\"># 返回困惑度</span></span><br></pre></td></tr></table></figure>\n<p>循环神经网络模型的训练函数既支持从零开始实现，也可以使用高级 API 来实现。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">net, train_iter, vocab, lr, num_epochs, device, use_random_iter=<span class=\"literal\">False</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;训练模型（定义见第8章）&quot;&quot;&quot;</span></span><br><span class=\"line\">    loss_function = nn.CrossEntropyLoss()</span><br><span class=\"line\">    <span class=\"comment\"># 初始化</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, nn.Module):</span><br><span class=\"line\">        optimizer = torch.optim.SGD(net.parameters(), lr)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        optimizer = <span class=\"keyword\">lambda</span> batch_size: d2l.sgd(net.params, lr, batch_size)</span><br><span class=\"line\">    pred = <span class=\"keyword\">lambda</span> prefix: predict(prefix, <span class=\"number\">50</span>, net, vocab, device)</span><br><span class=\"line\">    <span class=\"comment\"># 训练和预测</span></span><br><span class=\"line\">    writer = SummaryWriter(<span class=\"string\">&#x27;../logs/RNN_scratch_train_log&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        ppl = train_epoch(net, train_iter, loss_function, optimizer, device, use_random_iter)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (epoch + <span class=\"number\">1</span>) % <span class=\"number\">10</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(pred(<span class=\"string\">&#x27;time traveller&#x27;</span>))</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Perplexity: <span class=\"subst\">&#123;ppl:<span class=\"number\">.1</span>f&#125;</span>&#x27;</span>)</span><br><span class=\"line\">            writer.add_scalar(<span class=\"string\">&#x27;train_loss&#x27;</span>, ppl, epoch + <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(pred(<span class=\"string\">&#x27;time traveller&#x27;</span>))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(pred(<span class=\"string\">&#x27;traveller&#x27;</span>))</span><br><span class=\"line\">    writer.close()</span><br></pre></td></tr></table></figure>\n<p>现在，我们训练循环神经网络模型。因为我们在数据集中只使用了10000个词元，所以模型需要更多的迭代周期来更好地收敛：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_epochs, lr = <span class=\"number\">500</span>, <span class=\"number\">1</span></span><br><span class=\"line\">train(net, train_iter, vocab, lr, num_epochs, device)</span><br><span class=\"line\"><span class=\"comment\"># Perplexity: 1.0</span></span><br><span class=\"line\"><span class=\"comment\"># time travelleryou can show black is white by argument said filby</span></span><br><span class=\"line\"><span class=\"comment\"># travelleryou can show black is white by argument said filby</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-2-循环神经网络的简洁实现\">4.2 循环神经网络的简洁实现</h3>\n<p>虽然从零开始实现循环神经网络对了解网络的实现方式具有指导意义，但并不方便。本节将展示如何使用深度学习框架的高级 API 提供的函数更有效地实现相同的语言模型。我们仍然从读取时光机器数据集开始：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.tensorboard <span class=\"keyword\">import</span> SummaryWriter</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"><span class=\"keyword\">from</span> tqdm <span class=\"keyword\">import</span> tqdm</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size, num_steps = <span class=\"number\">32</span>, <span class=\"number\">35</span></span><br><span class=\"line\">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure>\n<p>高级 API 提供了循环神经网络的实现。我们构造一个具有256个隐藏单元的单隐藏层的循环神经网络层 <code>rnn_layer</code>。事实上，我们还没有讨论多层循环神经网络的意义（这将在深度循环神经网络中介绍）。现在仅需要将多层理解为一层循环神经网络的输出被用作下一层循环神经网络的输入就足够了：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_hiddens = <span class=\"number\">256</span></span><br><span class=\"line\">rnn_layer = nn.RNN(<span class=\"built_in\">len</span>(vocab), num_hiddens)</span><br></pre></td></tr></table></figure>\n<p>我们使用张量来初始化隐状态，它的形状是 <code>(隐藏层数, 批量大小, 隐藏单元数)</code>：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">state = torch.zeros((<span class=\"number\">1</span>, batch_size, num_hiddens))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(state.shape)  <span class=\"comment\"># torch.Size([1, 32, 256])</span></span><br></pre></td></tr></table></figure>\n<p>通过一个隐状态和一个输入，我们就可以用更新后的隐状态计算输出。需要强调的是，<code>rnn_layer</code> 的输出（<code>Y</code>）不涉及输出层的计算：它是指<strong>每个时间步的隐状态</strong>，这些隐状态可以用作后续输出层的输入：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.rand(size=(num_steps, batch_size, <span class=\"built_in\">len</span>(vocab)))</span><br><span class=\"line\">Y, state_new = rnn_layer(X, state)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Y.shape, state_new.shape)  <span class=\"comment\"># torch.Size([35, 32, 256]) torch.Size([1, 32, 256])</span></span><br></pre></td></tr></table></figure>\n<p>我们为一个完整的循环神经网络模型定义了一个 <code>RNNModel</code> 类。注意，<code>rnn_layer</code> 只包含隐藏的循环层，我们还需要创建一个单独的输出层：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">RNNModel</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;循环神经网络模型&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, rnn_layer, vocab_size, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(RNNModel, self).__init__(**kwargs)</span><br><span class=\"line\">        self.rnn = rnn_layer</span><br><span class=\"line\">        self.vocab_size = vocab_size</span><br><span class=\"line\">        self.num_hiddens = self.rnn.hidden_size</span><br><span class=\"line\">        <span class=\"comment\"># 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> self.rnn.bidirectional:</span><br><span class=\"line\">            self.num_directions = <span class=\"number\">1</span></span><br><span class=\"line\">            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.num_directions = <span class=\"number\">2</span></span><br><span class=\"line\">            self.linear = nn.Linear(self.num_hiddens * <span class=\"number\">2</span>, self.vocab_size)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, inputs, state</span>):</span><br><span class=\"line\">        X = F.one_hot(inputs.T.long(), self.vocab_size)</span><br><span class=\"line\">        X = X.to(torch.float32)</span><br><span class=\"line\">        Y, state = self.rnn(X, state)</span><br><span class=\"line\">        <span class=\"comment\"># 全连接层首先将Y的形状改为：(时间步数 * 批量大小, 隐藏单元数)</span></span><br><span class=\"line\">        <span class=\"comment\"># 它的输出形状是：(时间步数 * 批量大小, 词表大小)</span></span><br><span class=\"line\">        output = self.linear(Y.reshape((-<span class=\"number\">1</span>, Y.shape[-<span class=\"number\">1</span>])))  <span class=\"comment\"># (35 * 32, 256) -&gt; (35 * 32, 28)</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, state</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">begin_state</span>(<span class=\"params\">self, device, batch_size=<span class=\"number\">1</span></span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(self.rnn, nn.LSTM):</span><br><span class=\"line\">            <span class=\"comment\"># nn.GRU以张量作为隐状态</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"comment\"># nn.LSTM以元组作为隐状态</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> (torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device),</span><br><span class=\"line\">                    torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device))</span><br></pre></td></tr></table></figure>\n<p>在训练模型之前，让我们基于一个具有随机权重的模型进行预测，<code>d2l.predict_ch8</code> 函数与上一节中的 <code>predict</code> 函数相同：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">device = torch.device(<span class=\"string\">&#x27;cuda&#x27;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\">net = RNNModel(rnn_layer, vocab_size=<span class=\"built_in\">len</span>(vocab))</span><br><span class=\"line\">net = net.to(device)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(d2l.predict_ch8(<span class=\"string\">&#x27;time traveller&#x27;</span>, <span class=\"number\">10</span>, net, vocab, device))  <span class=\"comment\"># time travellerxhhhhhhhhh</span></span><br></pre></td></tr></table></figure>\n<p>很明显，这种模型根本不能输出好的结果。接下来，我们使用上一节中定义的超参数训练模型：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_epoch</span>(<span class=\"params\">net, train_iter, loss_function, optimizer, device, use_random_iter</span>):</span><br><span class=\"line\">    state = <span class=\"literal\">None</span></span><br><span class=\"line\">    train_loss = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X, Y <span class=\"keyword\">in</span> tqdm(train_iter):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> state <span class=\"keyword\">is</span> <span class=\"literal\">None</span> <span class=\"keyword\">or</span> use_random_iter:</span><br><span class=\"line\">            state = net.begin_state(batch_size=X.shape[<span class=\"number\">0</span>], device=device)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, nn.Module) <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(state, <span class=\"built_in\">tuple</span>):</span><br><span class=\"line\">                state.detach_()</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                <span class=\"keyword\">for</span> s <span class=\"keyword\">in</span> state:</span><br><span class=\"line\">                    s.detach_()</span><br><span class=\"line\">        y = Y.T.reshape(-<span class=\"number\">1</span>)</span><br><span class=\"line\">        X, y = X.to(device), y.to(device)</span><br><span class=\"line\">        loss_function.to(device)</span><br><span class=\"line\">        y_hat, state = net(X, state)</span><br><span class=\"line\">        loss = loss_function(y_hat, y.long()).mean()</span><br><span class=\"line\">        optimizer.zero_grad()</span><br><span class=\"line\">        loss.backward()</span><br><span class=\"line\">        d2l.grad_clipping(net, <span class=\"number\">1</span>)  <span class=\"comment\"># 与上一节中的grad_clipping函数相同</span></span><br><span class=\"line\">        optimizer.step()</span><br><span class=\"line\">        train_loss.append(loss)  <span class=\"comment\"># 因为已经调用了mean函数</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> math.exp(<span class=\"built_in\">sum</span>(train_loss) / <span class=\"built_in\">len</span>(train_loss))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">net, train_iter, vocab, lr, num_epochs, device, use_random_iter=<span class=\"literal\">False</span></span>):</span><br><span class=\"line\">    loss_function = nn.CrossEntropyLoss()</span><br><span class=\"line\">    optimizer = torch.optim.SGD(net.parameters(), lr)</span><br><span class=\"line\">    pred = <span class=\"keyword\">lambda</span> prefix: d2l.predict_ch8(prefix, <span class=\"number\">50</span>, net, vocab, device)</span><br><span class=\"line\"></span><br><span class=\"line\">    writer = SummaryWriter(<span class=\"string\">&#x27;../logs/RNN_scratch_train_log&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        ppl = train_epoch(net, train_iter, loss_function, optimizer, device, use_random_iter)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (epoch + <span class=\"number\">1</span>) % <span class=\"number\">10</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(pred(<span class=\"string\">&#x27;time traveller&#x27;</span>))</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Perplexity: <span class=\"subst\">&#123;ppl:<span class=\"number\">.1</span>f&#125;</span>&#x27;</span>)</span><br><span class=\"line\">            writer.add_scalar(<span class=\"string\">&#x27;train_loss&#x27;</span>, ppl, epoch + <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(pred(<span class=\"string\">&#x27;time traveller&#x27;</span>))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(pred(<span class=\"string\">&#x27;traveller&#x27;</span>))</span><br><span class=\"line\">    writer.close()</span><br><span class=\"line\"></span><br><span class=\"line\">num_epochs, lr = <span class=\"number\">500</span>, <span class=\"number\">1</span></span><br><span class=\"line\">train(net, train_iter, vocab, lr, num_epochs, device)</span><br><span class=\"line\"><span class=\"comment\"># Perplexity: 1.3</span></span><br><span class=\"line\"><span class=\"comment\"># time traveller for so ig will aboca thoursugli gpseknop how stac</span></span><br><span class=\"line\"><span class=\"comment\"># travelleryou can space of the simestiok satt or al and wisc</span></span><br></pre></td></tr></table></figure>\n",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/55.html",
            "url": "https://asanosaki.github.io/posts/55.html",
            "title": "Python路径操作模块pathlib教程",
            "date_published": "2023-03-31T02:47:00.000Z",
            "content_html": "<blockquote>\n<p>Python 路径操作新标准：<code>pathlib</code> 模块相较于老式的 <code>os.path</code> 更为简洁易用，本文为该模块的使用教程。</p>\n</blockquote>\n<span id=\"more\"></span>\n<p><code>pathlib</code> 库从 Python 3.4 开始，到 Python 3.6 已经比较成熟。如果你的新项目可以直接用 3.6 及以上，建议用 <code>pathlib</code>。相比于老式的 <code>os.path</code> 有几个优势：</p>\n<ul>\n<li>老的路径操作函数管理比较混乱，有的是导入 <code>os</code>，有的又是在 <code>os.path</code> 当中，而新的用法统一可以用 <code>pathlib</code> 管理。</li>\n<li>老用法在处理不同操作系统 Win、Mac 以及 Linux 之间很吃力。换了操作系统常常要改代码，还经常需要进行一些额外操作。</li>\n<li>老用法主要是函数形式，返回的数据类型通常是字符串。但是路径和字符串并不等价，所以在使用 <code>os</code> 操作路径的时候常常还要引入其他类库协助操作。新用法是面向对象，处理起来更灵活方便。</li>\n<li><code>pathlib</code> 简化了很多操作，用起来更轻松。</li>\n</ul>\n<h2 id=\"1-路径获取\">1. 路径获取</h2>\n<p><strong>（1）获取当前工作目录</strong></p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(pathlib.Path.cwd())  <span class=\"comment\"># D:\\Dive into Deep Learning\\src</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">str</span>(pathlib.Path.cwd()))  <span class=\"comment\"># D:\\Dive into Deep Learning\\src</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>注意：工作目录是在哪个目录下运行你的程序，不是项目目录。</p>\n</blockquote>\n<p>虽然在这里打印出来的很像一个字符串，但实际上得到的是一个 <code>WindowsPath('D:\\Dive into Deep Learning\\src')</code> 对象，如果只想得到字符串表示，不想要 <code>WindowsPath</code> 对象，可以用 <code>str()</code> 转化。</p>\n<p><strong>（2）获取用户 Home 目录</strong></p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(pathlib.Path.home())  <span class=\"comment\"># C:\\Users\\AsanoSaki</span></span><br></pre></td></tr></table></figure>\n<p><strong>（3）获取当前文件路径</strong></p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(pathlib.Path(__file__))  <span class=\"comment\"># D:\\Dive into Deep Learning\\src\\路径操作.py</span></span><br></pre></td></tr></table></figure>\n<p><strong>（4）获取任意字符串路径</strong></p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dir_path = pathlib.Path(<span class=\"string\">&#x27;../images&#x27;</span>)</span><br><span class=\"line\">file_path = pathlib.Path(<span class=\"string\">&#x27;../images/cat1.jpg&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dir_path)  <span class=\"comment\"># ..\\images</span></span><br></pre></td></tr></table></figure>\n<p><strong>（5）获取绝对路径</strong></p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(dir_path.resolve())  <span class=\"comment\"># D:\\Dive into Deep Learning\\images</span></span><br></pre></td></tr></table></figure>\n<p><strong>（6）获取文件属性</strong></p>\n<p>文件属性比如文件大小、创建时间、修改时间等。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(file_path.stat())  <span class=\"comment\"># os.stat_result(st_mode=33206, st_ino=281474978388098, st_dev=80873957, st_nlink=1, st_uid=0, st_gid=0, st_size=53081, st_atime=1680229254, st_mtime=1670460384, st_ctime=1680228818)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(file_path.stat().st_size)  <span class=\"comment\"># 53081</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"2-路径组成部分\">2. 路径组成部分</h2>\n<p>获取路径的组成部分非常方便：</p>\n<ul>\n<li><code>.name</code>：文件名，包含后缀名，如果是目录则获取目录名。</li>\n<li><code>.stem</code>：文件名，不包含后缀。</li>\n<li><code>.suffix</code>：后缀，比如 <code>.txt</code>、<code>.png</code>。</li>\n<li><code>.parent</code>：父级目录，相当于 <code>cd ..</code>。</li>\n<li><code>.anchor</code>：锚，目录前面的部分 <code>C:\\</code> 或者 <code>/</code>。</li>\n</ul>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(file_path.name)  <span class=\"comment\"># cat1.jpg</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(file_path.stem)  <span class=\"comment\"># cat1</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(file_path.suffix)  <span class=\"comment\"># .jpg</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(file_path.parent)  <span class=\"comment\"># ..\\images</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"3-子路径扫描\">3. 子路径扫描</h2>\n<p><strong>（1）扫描某个目录下的所有路径</strong></p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">all_path = [<span class=\"built_in\">str</span>(p) <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> dir_path.iterdir() <span class=\"keyword\">if</span> dir_path.is_dir()]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(all_path)  <span class=\"comment\"># [&#x27;..\\\\images\\\\cat1.jpg&#x27;, &#x27;..\\\\images\\\\cat2.jpg&#x27;, &#x27;..\\\\images\\\\cat3.jpg&#x27;, &#x27;..\\\\images\\\\catdog.jpg&#x27;]</span></span><br></pre></td></tr></table></figure>\n<p><strong>（2）使用模式匹配（正则表达式）查找目录下的指定文件</strong></p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">all_jpg = <span class=\"built_in\">list</span>(dir_path.glob(<span class=\"string\">&#x27;*.jpg&#x27;</span>))</span><br><span class=\"line\">all_jpg_strpath = [<span class=\"built_in\">str</span>(p) <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> all_jpg]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(all_jpg)  <span class=\"comment\"># [WindowsPath(&#x27;../images/cat1.jpg&#x27;), WindowsPath(&#x27;../images/cat2.jpg&#x27;), WindowsPath(&#x27;../images/cat3.jpg&#x27;), WindowsPath(&#x27;../images/catdog.jpg&#x27;)]</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(all_jpg_strpath)  <span class=\"comment\"># [&#x27;..\\\\images\\\\cat1.jpg&#x27;, &#x27;..\\\\images\\\\cat2.jpg&#x27;, &#x27;..\\\\images\\\\cat3.jpg&#x27;, &#x27;..\\\\images\\\\catdog.jpg&#x27;]</span></span><br></pre></td></tr></table></figure>\n<p><strong>（3）检查路径是否符合规则</strong></p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(file_path.<span class=\"keyword\">match</span>(<span class=\"string\">&#x27;*.jpg&#x27;</span>))  <span class=\"comment\"># True</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"4-路径拼接\">4. 路径拼接</h2>\n<p><code>pathlib</code> 支持用 <code>/</code> 拼接路径，如果用不惯 <code>/</code>，也可以用类似 <code>os.path.join</code> 的方法：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(dir_path / <span class=\"string\">&#x27;dir1&#x27;</span> / <span class=\"string\">&#x27;file1.txt&#x27;</span>)  <span class=\"comment\"># ..\\images\\dir1\\file1.txt</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(dir_path.joinpath(<span class=\"string\">&#x27;dir1&#x27;</span>, <span class=\"string\">&#x27;file1.txt&#x27;</span>))  <span class=\"comment\"># ..\\images\\dir1\\file1.txt</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"5-路径测试（判断）\">5. 路径测试（判断）</h2>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(dir_path.is_file())  <span class=\"comment\"># 是否为文件，False</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(dir_path.is_dir())  <span class=\"comment\"># 是否为文件夹，True</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(dir_path.exists())  <span class=\"comment\"># 是否存在，True</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"6-文件操作\">6. 文件操作</h2>\n<p><strong>（1）创建文件</strong></p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">new_file = pathlib.Path(<span class=\"string\">&#x27;../images/readme.txt&#x27;</span>)</span><br><span class=\"line\">new_file.touch(exist_ok=<span class=\"literal\">True</span>)</span><br><span class=\"line\">new_file.touch(exist_ok=<span class=\"literal\">False</span>)  <span class=\"comment\"># FileExistsError: [Errno 17] File exists: &#x27;..\\\\images\\\\readme.txt&#x27;</span></span><br></pre></td></tr></table></figure>\n<p><code>exist_ok</code> 表示当文件已经存在时，程序的反应。如果为 <code>True</code>，文件存在时，不进行任何操作；如果为 <code>False</code>，则会报 <code>FileExistsError</code> 错误。</p>\n<p><strong>（2）创建目录</strong></p>\n<p>用 <code>os</code> 创建目录分为两个函数：<code>mkdir()</code> 和 <code>makedirs()</code>。<code>mkdir()</code> 一次只能创建一级目录，<code>makedirs()</code> 可以同时创建多级目录。使用 <code>pathlib</code> 只需要用 <code>path.mkdir()</code> 函数就可以。它提供了 <code>parents</code> 参数，设置为 <code>True</code> 可以创建多级目录，不设置则只能创建一层：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">new_dir = pathlib.Path(<span class=\"string\">&#x27;../images/dir1/dir2&#x27;</span>)</span><br><span class=\"line\">new_dir.mkdir()  <span class=\"comment\"># FileNotFoundError: [WinError 3] The system cannot find the path specified: &#x27;..\\\\images\\\\dir1\\\\dir2&#x27;</span></span><br><span class=\"line\">new_dir.mkdir(parents=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n<p><strong>（3）删除目录</strong></p>\n<p>删除目录非常危险，并且没有提示，一定要谨慎操作。一次只删除一级目录，且当前目录必须为空：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">del_dir = pathlib.Path(<span class=\"string\">&#x27;../images/dir&#x27;</span>)</span><br><span class=\"line\">del_dir.rmdir()</span><br></pre></td></tr></table></figure>\n<p><strong>（4）删除文件</strong></p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">del_file = pathlib.Path(<span class=\"string\">&#x27;../images/readme.txt&#x27;</span>)</span><br><span class=\"line\">del_file.unlink()</span><br></pre></td></tr></table></figure>\n",
            "tags": [
                "Python"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/24840.html",
            "url": "https://asanosaki.github.io/posts/24840.html",
            "title": "动手学深度学习笔记(李沐)-计算机视觉",
            "date_published": "2023-03-10T02:24:00.000Z",
            "content_html": "<blockquote>\n<p>李沐动手学深度学习（PyTorch）课程学习笔记第七章：计算机视觉。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-图像增广\">1. 图像增广</h2>\n<p>图像增广在对训练图像进行一系列的随机变化之后，生成相似但不同的训练样本，从而扩大了训练集的规模。此外，应用图像增广的原因是，随机改变训练样本可以减少模型对某些属性的依赖，从而提高模型的泛化能力。例如，我们可以以不同的方式裁剪图像，使感兴趣的对象出现在不同的位置，减少模型对于对象出现位置的依赖。我们还可以调整亮度、颜色等因素来降低模型对颜色的敏感度。</p>\n<p>下面的代码有50%的几率使图像向左或向右翻转：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">trans = torchvision.transforms.RandomHorizontalFlip()</span><br></pre></td></tr></table></figure>\n<p>有50%的几率向上或向下翻转，注意，上下翻转图像不如左右图像翻转那样常用，需要根据数据集的特征考虑是否可以将图像上下翻转：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">trans = torchvision.transforms.RandomVerticalFlip()</span><br></pre></td></tr></table></figure>\n<p>随机裁剪一个面积为原始面积10%到100%的区域，该区域的宽高比从0.5~2之间随机取值。然后，区域的宽度和高度都被缩放到200像素：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">trans = torchvision.transforms.RandomResizedCrop((<span class=\"number\">200</span>, <span class=\"number\">200</span>), scale=(<span class=\"number\">0.1</span>, <span class=\"number\">1</span>), ratio=(<span class=\"number\">0.5</span>, <span class=\"number\">2</span>))</span><br></pre></td></tr></table></figure>\n<p>我们可以改变图像颜色的四个方面：亮度、对比度、饱和度和色调。在下面的示例中，我们随机更改图像的亮度，随机值为原始图像的50%(1 - 0.5)到150%(1 + 0.5)之间：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">trans = torchvision.transforms.ColorJitter(brightness=<span class=\"number\">0.5</span>, contrast=<span class=\"number\">0</span>, saturation=<span class=\"number\">0</span>, hue=<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>\n<p>在实践中，我们将结合多种图像增广方法。我们可以通过使用一个 <code>Compose</code> 实例来综合上面定义的不同的图像增广方法，并将它们应用到每个图像：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">trans = transforms.Compose([</span><br><span class=\"line\">    transforms.RandomHorizontalFlip(p=<span class=\"number\">0.5</span>),  <span class=\"comment\"># 50%的概率使图片水平翻转</span></span><br><span class=\"line\">    transforms.ColorJitter(brightness=<span class=\"number\">0.5</span>, contrast=<span class=\"number\">0.5</span>, saturation=<span class=\"number\">0.5</span>, hue=<span class=\"number\">0.5</span>),</span><br><span class=\"line\">    transforms.ToTensor()])</span><br></pre></td></tr></table></figure>\n<p>图像增广可以直接作用在图像数据上，也可以在使用 <code>torchvision.datasets</code> 导入数据集的时候通过 <code>transform</code> 参数指定：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = trans(X)</span><br><span class=\"line\"></span><br><span class=\"line\">cifar_train = torchvision.datasets.CIFAR10(root=<span class=\"string\">&quot;../data&quot;</span>, train=<span class=\"literal\">True</span>, transform=trans, download=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-微调\">2. 微调</h2>\n<p>微调（fine-tuning）是迁移学习（transfer learning）中的常见技巧，微调包括以下四个步骤：</p>\n<ul>\n<li>在源数据集（例如 ImageNet 数据集）上预训练神经网络模型，即源模型。</li>\n<li>创建一个新的神经网络模型，即目标模型。这将复制源模型上的所有模型设计及其参数（输出层除外）。我们假定这些模型参数包含从源数据集中学到的知识，这些知识也将适用于目标数据集。我们还假设源模型的输出层与源数据集的标签密切相关；因此不在目标模型中使用该层。</li>\n<li>向目标模型添加输出层，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。</li>\n<li>在目标数据集（如椅子数据集）上训练目标模型。输出层将从头开始进行训练，而所有其他层的参数将根据源模型的参数进行微调。</li>\n</ul>\n<p>当目标数据集比源数据集小得多时，微调有助于提高模型的泛化能力。</p>\n<p>我们将在一个 CIFAR10 数据集上微调 ResNet-18 模型。该模型已在 ImageNet 数据集上进行了预训练：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils <span class=\"keyword\">import</span> data</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> models</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.tensorboard <span class=\"keyword\">import</span> SummaryWriter</span><br><span class=\"line\"><span class=\"keyword\">from</span> tqdm <span class=\"keyword\">import</span> tqdm</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># ---------- Data ----------</span></span><br><span class=\"line\">train_trans = transforms.Compose([</span><br><span class=\"line\">    transforms.RandomResizedCrop(<span class=\"number\">224</span>),</span><br><span class=\"line\">    transforms.RandomHorizontalFlip(),</span><br><span class=\"line\">    transforms.ToTensor(),</span><br><span class=\"line\">    <span class=\"comment\"># 使用ImageNet的RGB通道的均值和标准差，以标准化每个通道</span></span><br><span class=\"line\">    transforms.Normalize([<span class=\"number\">0.485</span>, <span class=\"number\">0.456</span>, <span class=\"number\">0.406</span>],</span><br><span class=\"line\">                         [<span class=\"number\">0.229</span>, <span class=\"number\">0.224</span>, <span class=\"number\">0.225</span>])])</span><br><span class=\"line\"></span><br><span class=\"line\">test_trans = transforms.Compose([</span><br><span class=\"line\">    transforms.Resize([<span class=\"number\">256</span>, <span class=\"number\">256</span>]),</span><br><span class=\"line\">    transforms.CenterCrop(<span class=\"number\">224</span>),</span><br><span class=\"line\">    transforms.ToTensor(),</span><br><span class=\"line\">    transforms.Normalize([<span class=\"number\">0.485</span>, <span class=\"number\">0.456</span>, <span class=\"number\">0.406</span>],</span><br><span class=\"line\">                         [<span class=\"number\">0.229</span>, <span class=\"number\">0.224</span>, <span class=\"number\">0.225</span>])])</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">128</span></span><br><span class=\"line\">cifar_train = torchvision.datasets.CIFAR10(root=<span class=\"string\">&quot;../data&quot;</span>, train=<span class=\"literal\">True</span>, transform=train_trans, download=<span class=\"literal\">True</span>)</span><br><span class=\"line\">cifar_test = torchvision.datasets.CIFAR10(root=<span class=\"string\">&quot;../data&quot;</span>, train=<span class=\"literal\">False</span>, transform=test_trans, download=<span class=\"literal\">True</span>)</span><br><span class=\"line\">train_iter = data.DataLoader(cifar_train, batch_size, shuffle=<span class=\"literal\">True</span>, num_workers=<span class=\"number\">0</span>)</span><br><span class=\"line\">test_iter = data.DataLoader(cifar_test, batch_size, shuffle=<span class=\"literal\">False</span>, num_workers=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># ---------- ResNet-18 ----------</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">resnet_model</span>(<span class=\"params\">num_classes, use_pretrained=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">    net = models.resnet18(pretrained=use_pretrained)</span><br><span class=\"line\">    net.fc = nn.Linear(net.fc.in_features, num_classes)</span><br><span class=\"line\">    nn.init.xavier_uniform_(net.fc.weight)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> net</span><br><span class=\"line\"></span><br><span class=\"line\">net = resnet_model(<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># ---------- Train ----------</span></span><br><span class=\"line\"><span class=\"comment\"># 如果param_group=True，输出层中的模型参数将使用十倍的学习率</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">net, train_iter, test_iter, num_epochs, lr, device, wd, param_group=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;training on&#x27;</span>, device)</span><br><span class=\"line\">    net.to(device)</span><br><span class=\"line\">    loss_function = nn.CrossEntropyLoss()</span><br><span class=\"line\">    loss_function.to(device)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> param_group:</span><br><span class=\"line\">        params_1x = [param <span class=\"keyword\">for</span> name, param <span class=\"keyword\">in</span> net.named_parameters() <span class=\"keyword\">if</span> name <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> [<span class=\"string\">&quot;fc.weight&quot;</span>, <span class=\"string\">&quot;fc.bias&quot;</span>]]</span><br><span class=\"line\">        optimizer = torch.optim.SGD([&#123;<span class=\"string\">&#x27;params&#x27;</span>: params_1x&#125;,</span><br><span class=\"line\">                                     &#123;<span class=\"string\">&#x27;params&#x27;</span>: net.fc.parameters(), <span class=\"string\">&#x27;lr&#x27;</span>: lr * <span class=\"number\">10</span>&#125;], lr=lr, weight_decay=wd)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd)</span><br><span class=\"line\"></span><br><span class=\"line\">    writer = SummaryWriter(<span class=\"string\">&#x27;../logs/FineTune_CIFAR10_train_log&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    best_acc = <span class=\"number\">0.0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        net.train()</span><br><span class=\"line\">        train_loss = []</span><br><span class=\"line\">        train_acc = []</span><br><span class=\"line\">        <span class=\"keyword\">for</span> img, label <span class=\"keyword\">in</span> tqdm(train_iter):</span><br><span class=\"line\">            img, label = img.to(device), label.to(device)</span><br><span class=\"line\">            label_hat = net(img)</span><br><span class=\"line\"></span><br><span class=\"line\">            loss = loss_function(label_hat, label)</span><br><span class=\"line\">            optimizer.zero_grad()</span><br><span class=\"line\">            loss.backward()</span><br><span class=\"line\">            optimizer.step()</span><br><span class=\"line\"></span><br><span class=\"line\">            label_hat = label_hat.argmax(axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">            acc = (label_hat.<span class=\"built_in\">type</span>(label.dtype) == label).<span class=\"built_in\">float</span>().mean()</span><br><span class=\"line\">            train_loss.append(loss.item())</span><br><span class=\"line\">            train_acc.append(acc)</span><br><span class=\"line\"></span><br><span class=\"line\">        train_loss = <span class=\"built_in\">sum</span>(train_loss) / <span class=\"built_in\">len</span>(train_loss)</span><br><span class=\"line\">        train_acc = <span class=\"built_in\">sum</span>(train_acc) / <span class=\"built_in\">len</span>(train_acc)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;[ Train | <span class=\"subst\">&#123;epoch + <span class=\"number\">1</span>:03d&#125;</span>/<span class=\"subst\">&#123;num_epochs:03d&#125;</span> ] loss = <span class=\"subst\">&#123;train_loss:<span class=\"number\">.5</span>f&#125;</span>, acc = <span class=\"subst\">&#123;train_acc:<span class=\"number\">.5</span>f&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        net.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\">        valid_loss = []</span><br><span class=\"line\">        valid_acc = []</span><br><span class=\"line\">        <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">            <span class=\"keyword\">for</span> img, label <span class=\"keyword\">in</span> tqdm(test_iter):</span><br><span class=\"line\">                img, label = img.to(device), label.to(device)</span><br><span class=\"line\">                label_hat = net(img)</span><br><span class=\"line\">                loss = loss_function(label_hat, label)</span><br><span class=\"line\">                label_hat = label_hat.argmax(axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">                acc = (label_hat.<span class=\"built_in\">type</span>(label.dtype) == label).<span class=\"built_in\">float</span>().mean()</span><br><span class=\"line\">                valid_loss.append(loss.item())</span><br><span class=\"line\">                valid_acc.append(acc)</span><br><span class=\"line\"></span><br><span class=\"line\">        valid_loss = <span class=\"built_in\">sum</span>(valid_loss) / <span class=\"built_in\">len</span>(valid_loss)</span><br><span class=\"line\">        valid_acc = <span class=\"built_in\">sum</span>(valid_acc) / <span class=\"built_in\">len</span>(valid_acc)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;[ Valid | <span class=\"subst\">&#123;epoch + <span class=\"number\">1</span>:03d&#125;</span>/<span class=\"subst\">&#123;num_epochs:03d&#125;</span> ] loss = <span class=\"subst\">&#123;valid_loss:<span class=\"number\">.5</span>f&#125;</span>, acc = <span class=\"subst\">&#123;valid_acc:<span class=\"number\">.5</span>f&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        writer.add_scalar(<span class=\"string\">&#x27;train_loss&#x27;</span>, train_loss, epoch + <span class=\"number\">1</span>)</span><br><span class=\"line\">        writer.add_scalar(<span class=\"string\">&#x27;train_acc&#x27;</span>, train_acc, epoch + <span class=\"number\">1</span>)</span><br><span class=\"line\">        writer.add_scalar(<span class=\"string\">&#x27;valid_loss&#x27;</span>, valid_loss, epoch + <span class=\"number\">1</span>)</span><br><span class=\"line\">        writer.add_scalar(<span class=\"string\">&#x27;valid_acc&#x27;</span>, valid_acc, epoch + <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> valid_acc &gt; best_acc:</span><br><span class=\"line\">            best_acc = valid_acc</span><br><span class=\"line\">            torch.save(net.state_dict(), <span class=\"string\">&#x27;../save/FineTune_CIFAR10_train.params&#x27;</span>)</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;saving model with acc &#123;:.3f&#125;&#x27;</span>.<span class=\"built_in\">format</span>(best_acc))</span><br><span class=\"line\"></span><br><span class=\"line\">    writer.close()</span><br><span class=\"line\"></span><br><span class=\"line\">device = torch.device(<span class=\"string\">&#x27;cuda&#x27;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\">lr, wd, num_epochs = <span class=\"number\">0.0005</span>, <span class=\"number\">0.001</span>, <span class=\"number\">50</span></span><br><span class=\"line\"></span><br><span class=\"line\">train(net, train_iter, test_iter, num_epochs, lr, device, wd, param_group=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-目标检测和边界框\">3. 目标检测和边界框</h2>\n<p>在图像分类任务中，我们假设图像中只有一个主要物体对象，我们只关注如何识别其类别。然而，很多时候图像里有多个我们感兴趣的目标，我们不仅想知道它们的类别，还想得到它们在图像中的<strong>具体位置</strong>。在计算机视觉里，我们将这类任务称为<strong>目标检测</strong>（object detection）或<strong>目标识别</strong>（object recognition）。</p>\n<p>下面加载本节将使用的示例图像。图像左边是一只狗，右边是一只猫。它们是这张图像里的两个主要目标：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\">plt.rcParams[<span class=\"string\">&#x27;font.sans-serif&#x27;</span>] = [<span class=\"string\">&#x27;SimHei&#x27;</span>]</span><br><span class=\"line\">plt.rcParams[<span class=\"string\">&#x27;axes.unicode_minus&#x27;</span>] = <span class=\"literal\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\">plt.figure(dpi=<span class=\"number\">100</span>, figsize=(<span class=\"number\">8</span>, <span class=\"number\">6</span>))</span><br><span class=\"line\">img = plt.imread(<span class=\"string\">&#x27;../images/catdog.jpg&#x27;</span>)</span><br><span class=\"line\">plt.imshow(img)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>在目标检测中，我们通常使用边界框（bounding box）来描述对象的空间位置。边界框是矩形的，由矩形左上角的以及右下角的 <code>x</code> 和 <code>y</code> 坐标决定。另一种常用的边界框表示方法是边界框中心的 <code>(x, y)</code> 轴坐标以及框的宽度和高度。</p>\n<p>在这里，我们定义在这两种表示法之间进行转换的函数：<code>box_corner_to_center</code> 从两角表示法转换为中心宽度表示法，而 <code>box_center_to_corner</code> 反之亦然。输入参数 <code>boxes</code> 可以是长度为4的张量，也可以是形状为 <code>(N, 4)</code> 的二维张量，其中 N 是边界框的数量。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">box_corner_to_center</span>(<span class=\"params\">boxes</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;从（左上，右下）转换到（中间，宽度，高度）&quot;&quot;&quot;</span></span><br><span class=\"line\">    x1, y1, x2, y2 = boxes[:, <span class=\"number\">0</span>], boxes[:, <span class=\"number\">1</span>], boxes[:, <span class=\"number\">2</span>], boxes[:, <span class=\"number\">3</span>]</span><br><span class=\"line\">    cx = (x1 + x2) / <span class=\"number\">2</span></span><br><span class=\"line\">    cy = (y1 + y2) / <span class=\"number\">2</span></span><br><span class=\"line\">    w = x2 - x1</span><br><span class=\"line\">    h = y2 - y1</span><br><span class=\"line\">    boxes = torch.stack((cx, cy, w, h), dim=-<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> boxes</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">box_center_to_corner</span>(<span class=\"params\">boxes</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;从（中间，宽度，高度）转换到（左上，右下）&quot;&quot;&quot;</span></span><br><span class=\"line\">    cx, cy, w, h = boxes[:, <span class=\"number\">0</span>], boxes[:, <span class=\"number\">1</span>], boxes[:, <span class=\"number\">2</span>], boxes[:, <span class=\"number\">3</span>]</span><br><span class=\"line\">    x1 = cx - <span class=\"number\">0.5</span> * w</span><br><span class=\"line\">    y1 = cy - <span class=\"number\">0.5</span> * h</span><br><span class=\"line\">    x2 = cx + <span class=\"number\">0.5</span> * w</span><br><span class=\"line\">    y2 = cy + <span class=\"number\">0.5</span> * h</span><br><span class=\"line\">    boxes = torch.stack((x1, y1, x2, y2), dim=-<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> boxes</span><br></pre></td></tr></table></figure>\n<p>我们将根据坐标信息定义图像中狗和猫的边界框。图像中坐标的原点是图像的左上角，向右的方向为 <code>x</code> 轴的正方向，向下的方向为 <code>y</code> 轴的正方向：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># bbox是边界框的英文缩写</span></span><br><span class=\"line\">dog_bbox, cat_bbox = [<span class=\"number\">60.0</span>, <span class=\"number\">45.0</span>, <span class=\"number\">378.0</span>, <span class=\"number\">516.0</span>], [<span class=\"number\">400.0</span>, <span class=\"number\">112.0</span>, <span class=\"number\">655.0</span>, <span class=\"number\">493.0</span>]</span><br><span class=\"line\">boxes = torch.tensor((dog_bbox, cat_bbox))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(box_center_to_corner(box_corner_to_center(boxes)) == boxes)</span><br><span class=\"line\"><span class=\"comment\"># tensor([[True, True, True, True],</span></span><br><span class=\"line\"><span class=\"comment\">#         [True, True, True, True]])</span></span><br></pre></td></tr></table></figure>\n<p>我们可以将边界框在图中画出，以检查其是否准确。画之前，我们定义一个辅助函数 <code>bbox_to_rect</code>。它将边界框表示成 <code>matplotlib</code> 的边界框格式，在图像上添加边界框之后，我们可以看到两个物体的主要轮廓基本上在两个框内：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">bbox_to_rect</span>(<span class=\"params\">bbox, color</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 将边界框(左上x, 左上y, 右下x, 右下y)格式转换成matplotlib格式：(xy=(左上x, 左上y), width=宽, height=高)</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> plt.Rectangle(xy=(bbox[<span class=\"number\">0</span>], bbox[<span class=\"number\">1</span>]), width=bbox[<span class=\"number\">2</span>]-bbox[<span class=\"number\">0</span>], height=bbox[<span class=\"number\">3</span>]-bbox[<span class=\"number\">1</span>],</span><br><span class=\"line\">                         fill=<span class=\"literal\">False</span>, edgecolor=color, linewidth=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">fig = plt.imshow(img)</span><br><span class=\"line\">fig.axes.add_patch(bbox_to_rect(dog_bbox, <span class=\"string\">&#x27;blue&#x27;</span>))</span><br><span class=\"line\">fig.axes.add_patch(bbox_to_rect(cat_bbox, <span class=\"string\">&#x27;red&#x27;</span>))</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<h2 id=\"4-目标检测数据集\">4. 目标检测数据集</h2>\n<p>目标检测领域没有像 MNIST 和 Fashion-MNIST 那样的小数据集。为了快速测试目标检测模型，我们收集并标记了一个小型数据集。首先，我们拍摄了一组香蕉的照片，并生成了1000张不同角度和大小的香蕉图像。然后，我们在一些背景图片的随机位置上放一张香蕉的图像。最后，我们在图片上为这些香蕉标记了边界框。</p>\n<p>包含所有图像和 CSV 标签文件的香蕉检测数据集可以直接从互联网下载，通过 <code>read_data_bananas</code> 函数，我们读取香蕉检测数据集的图像和标签。该数据集的 CSV 文件内含目标类别标签和位于左上角和右下角的真实边界框坐标：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\">plt.rcParams[<span class=\"string\">&#x27;font.sans-serif&#x27;</span>] = [<span class=\"string\">&#x27;SimHei&#x27;</span>]</span><br><span class=\"line\">plt.rcParams[<span class=\"string\">&#x27;axes.unicode_minus&#x27;</span>] = <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> Dataset, DataLoader</span><br><span class=\"line\"></span><br><span class=\"line\">d2l.DATA_HUB[<span class=\"string\">&#x27;banana-detection&#x27;</span>] = (d2l.DATA_URL + <span class=\"string\">&#x27;banana-detection.zip&#x27;</span>, <span class=\"string\">&#x27;5de26c8fce5ccdea9f91267273464dc968d20d72&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">read_data_bananas</span>(<span class=\"params\">is_train=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;读取香蕉检测数据集中的图像和标签&quot;&quot;&quot;</span></span><br><span class=\"line\">    data_dir = d2l.download_extract(<span class=\"string\">&#x27;banana-detection&#x27;</span>)  <span class=\"comment\"># 路径为../data</span></span><br><span class=\"line\">    csv_fname = os.path.join(data_dir, <span class=\"string\">&#x27;bananas_train&#x27;</span> <span class=\"keyword\">if</span> is_train <span class=\"keyword\">else</span> <span class=\"string\">&#x27;bananas_val&#x27;</span>, <span class=\"string\">&#x27;label.csv&#x27;</span>)</span><br><span class=\"line\">    csv_data = pd.read_csv(csv_fname)</span><br><span class=\"line\">    csv_data = csv_data.set_index(<span class=\"string\">&#x27;img_name&#x27;</span>)</span><br><span class=\"line\">    images, targets = [], []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> img_name, target <span class=\"keyword\">in</span> csv_data.iterrows():</span><br><span class=\"line\">        images.append(torchvision.io.read_image(os.path.join(data_dir, <span class=\"string\">&#x27;bananas_train&#x27;</span> <span class=\"keyword\">if</span> is_train <span class=\"keyword\">else</span> <span class=\"string\">&#x27;bananas_val&#x27;</span>, <span class=\"string\">&#x27;images&#x27;</span>, <span class=\"string\">f&#x27;<span class=\"subst\">&#123;img_name&#125;</span>&#x27;</span>)))</span><br><span class=\"line\">        <span class=\"comment\"># 这里的target为：(类别, 左上角x, 左上角y, 右下角x, 右下角y)，其中所有图像都具有相同的香蕉类（索引为0）</span></span><br><span class=\"line\">        targets.append(<span class=\"built_in\">list</span>(target))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> images, torch.tensor(targets).unsqueeze(<span class=\"number\">1</span>) / <span class=\"number\">256</span></span><br></pre></td></tr></table></figure>\n<p>以下 <code>BananasDataset</code> 类别将允许我们创建一个自定义 <code>Dataset</code> 实例来加载香蕉检测数据集：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">BananasDataset</span>(<span class=\"title class_ inherited__\">Dataset</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;一个用于加载香蕉检测数据集的自定义数据集&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, is_train</span>):</span><br><span class=\"line\">        self.features, self.labels = read_data_bananas(is_train)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;read &#x27;</span> + <span class=\"built_in\">str</span>(<span class=\"built_in\">len</span>(self.features)) + (<span class=\"string\">f&#x27; training examples&#x27;</span> <span class=\"keyword\">if</span> is_train <span class=\"keyword\">else</span> <span class=\"string\">f&#x27; validation examples&#x27;</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__getitem__</span>(<span class=\"params\">self, idx</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (self.features[idx].<span class=\"built_in\">float</span>(), self.labels[idx])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__len__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">len</span>(self.features)</span><br></pre></td></tr></table></figure>\n<p>最后，我们定义 <code>load_data_bananas</code> 函数，来为训练集和测试集返回两个数据加载器实例。对于测试集，无须按随机顺序读取它：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">load_data_bananas</span>(<span class=\"params\">batch_size</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;加载香蕉检测数据集&quot;&quot;&quot;</span></span><br><span class=\"line\">    train_iter = DataLoader(BananasDataset(is_train=<span class=\"literal\">True</span>), batch_size, shuffle=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    val_iter = DataLoader(BananasDataset(is_train=<span class=\"literal\">False</span>), batch_size)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> train_iter, val_iter</span><br></pre></td></tr></table></figure>\n<p>让我们读取一个小批量，并打印其中的图像和标签的形状。图像的小批量的形状为：<code>(批量大小, 通道数, 高度, 宽度)</code>，它与我们之前图像分类任务中的相同。标签的小批量的形状为：<code>(批量大小, M, 5)</code>，其中 M 是数据集的任何图像中边界框可能出现的最大数量。</p>\n<p>小批量计算虽然高效，但它要求每张图像含有相同数量的边界框，以便放在同一个批量中。通常来说，图像可能拥有不同数量个边界框；因此，在达到 M 之前，边界框少于 M 的图像将被非法边界框填充。这样，每个边界框的标签将被长度为5的数组表示。数组中的第一个元素是边界框中对象的类别，其中-1表示用于填充的非法边界框。数组的其余四个元素是边界框左上角和右下角的 <code>(x, y)</code> 坐标值（值域在0~1之间）。对于香蕉数据集而言，由于每张图像上只有一个边界框，因此 <code>M = 1</code>。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">batch_size, edge_size = <span class=\"number\">32</span>, <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, val_iter = load_data_bananas(batch_size)</span><br><span class=\"line\">features, labels = <span class=\"built_in\">next</span>(<span class=\"built_in\">iter</span>(train_iter))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(features.shape, labels.shape)  <span class=\"comment\"># torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 5])</span></span><br></pre></td></tr></table></figure>\n<p>接下来让我们展示10幅带有真实边界框的图像。我们可以看到在所有这些图像中香蕉的旋转角度、大小和位置都有所不同。当然，这只是一个简单的人工数据集，实践中真实世界的数据集通常要复杂得多：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># d2l.show_images()函数的实现</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">show_images</span>(<span class=\"params\">imgs, num_rows, num_cols, titles=<span class=\"literal\">None</span>, scale=<span class=\"number\">1.5</span></span>):</span><br><span class=\"line\">    figsize = (num_cols * scale, num_rows * scale)</span><br><span class=\"line\">    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)</span><br><span class=\"line\">    axes = axes.flatten()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, (ax, img) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(<span class=\"built_in\">zip</span>(axes, imgs)):</span><br><span class=\"line\">        <span class=\"keyword\">try</span>:</span><br><span class=\"line\">            img = img.numpy()</span><br><span class=\"line\">        <span class=\"keyword\">except</span>:</span><br><span class=\"line\">            <span class=\"keyword\">pass</span></span><br><span class=\"line\">        ax.imshow(img)</span><br><span class=\"line\">        ax.axes.get_xaxis().set_visible(<span class=\"literal\">False</span>)</span><br><span class=\"line\">        ax.axes.get_yaxis().set_visible(<span class=\"literal\">False</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> titles:</span><br><span class=\"line\">            ax.set_title(titles[i])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> axes</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">bbox_to_rect</span>(<span class=\"params\">bbox, color</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> plt.Rectangle(xy=(bbox[<span class=\"number\">0</span>], bbox[<span class=\"number\">1</span>]), width=bbox[<span class=\"number\">2</span>]-bbox[<span class=\"number\">0</span>], height=bbox[<span class=\"number\">3</span>]-bbox[<span class=\"number\">1</span>],</span><br><span class=\"line\">                         fill=<span class=\"literal\">False</span>, edgecolor=color, linewidth=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">imgs = (features[<span class=\"number\">0</span>:<span class=\"number\">10</span>].permute(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>)) / <span class=\"number\">255</span></span><br><span class=\"line\">axes = show_images(imgs, <span class=\"number\">2</span>, <span class=\"number\">5</span>, scale=<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span> ax, label <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(axes, labels[<span class=\"number\">0</span>:<span class=\"number\">10</span>]):</span><br><span class=\"line\">    ax.add_patch(bbox_to_rect(label[<span class=\"number\">0</span>][<span class=\"number\">1</span>:<span class=\"number\">5</span>] * edge_size, color=<span class=\"string\">&#x27;white&#x27;</span>))</span><br><span class=\"line\">    <span class=\"comment\"># d2l.show_bboxes(ax, [label[0][1:5] * edge_size], colors=[&#x27;w&#x27;])  # 功能与上一行相同</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<h2 id=\"5-锚框\">5. 锚框</h2>\n<p>由于本节难度较大，因此详细分析见：<a href=\"https://zh-v2.d2l.ai/chapter_computer-vision/anchor.html\">D2L-计算机视觉-锚框</a>。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br><span class=\"line\">211</span><br><span class=\"line\">212</span><br><span class=\"line\">213</span><br><span class=\"line\">214</span><br><span class=\"line\">215</span><br><span class=\"line\">216</span><br><span class=\"line\">217</span><br><span class=\"line\">218</span><br><span class=\"line\">219</span><br><span class=\"line\">220</span><br><span class=\"line\">221</span><br><span class=\"line\">222</span><br><span class=\"line\">223</span><br><span class=\"line\">224</span><br><span class=\"line\">225</span><br><span class=\"line\">226</span><br><span class=\"line\">227</span><br><span class=\"line\">228</span><br><span class=\"line\">229</span><br><span class=\"line\">230</span><br><span class=\"line\">231</span><br><span class=\"line\">232</span><br><span class=\"line\">233</span><br><span class=\"line\">234</span><br><span class=\"line\">235</span><br><span class=\"line\">236</span><br><span class=\"line\">237</span><br><span class=\"line\">238</span><br><span class=\"line\">239</span><br><span class=\"line\">240</span><br><span class=\"line\">241</span><br><span class=\"line\">242</span><br><span class=\"line\">243</span><br><span class=\"line\">244</span><br><span class=\"line\">245</span><br><span class=\"line\">246</span><br><span class=\"line\">247</span><br><span class=\"line\">248</span><br><span class=\"line\">249</span><br><span class=\"line\">250</span><br><span class=\"line\">251</span><br><span class=\"line\">252</span><br><span class=\"line\">253</span><br><span class=\"line\">254</span><br><span class=\"line\">255</span><br><span class=\"line\">256</span><br><span class=\"line\">257</span><br><span class=\"line\">258</span><br><span class=\"line\">259</span><br><span class=\"line\">260</span><br><span class=\"line\">261</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">torch.set_printoptions(<span class=\"number\">2</span>)  <span class=\"comment\"># 精简输出精度</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">multibox_prior</span>(<span class=\"params\">data, sizes, ratios</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;生成以每个像素为中心具有不同形状的锚框&quot;&quot;&quot;</span></span><br><span class=\"line\">    in_height, in_width = data.shape[-<span class=\"number\">2</span>:]</span><br><span class=\"line\">    device, num_sizes, num_ratios = data.device, <span class=\"built_in\">len</span>(sizes), <span class=\"built_in\">len</span>(ratios)</span><br><span class=\"line\">    boxes_per_pixel = (num_sizes + num_ratios - <span class=\"number\">1</span>)</span><br><span class=\"line\">    size_tensor = torch.tensor(sizes, device=device)</span><br><span class=\"line\">    ratio_tensor = torch.tensor(ratios, device=device)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 为了将锚点移动到像素的中心，需要设置偏移量。</span></span><br><span class=\"line\">    <span class=\"comment\"># 因为一个像素的高为1且宽为1，我们选择偏移我们的中心0.5</span></span><br><span class=\"line\">    offset_h, offset_w = <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span></span><br><span class=\"line\">    steps_h = <span class=\"number\">1.0</span> / in_height  <span class=\"comment\"># 在y轴上缩放步长</span></span><br><span class=\"line\">    steps_w = <span class=\"number\">1.0</span> / in_width  <span class=\"comment\"># 在x轴上缩放步长</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 生成锚框的所有中心点</span></span><br><span class=\"line\">    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h</span><br><span class=\"line\">    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w</span><br><span class=\"line\">    shift_y, shift_x = torch.meshgrid(center_h, center_w, indexing=<span class=\"string\">&#x27;ij&#x27;</span>)</span><br><span class=\"line\">    shift_y, shift_x = shift_y.reshape(-<span class=\"number\">1</span>), shift_x.reshape(-<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(center_h.shape)  <span class=\"comment\"># torch.Size([561])</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(shift_y.shape)  <span class=\"comment\"># torch.Size([408408])</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 生成“boxes_per_pixel”个高和宽，之后用于创建锚框的四角坐标(xmin, xmax, ymin, ymax)</span></span><br><span class=\"line\">    w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[<span class=\"number\">0</span>]),</span><br><span class=\"line\">                   sizes[<span class=\"number\">0</span>] * torch.sqrt(ratio_tensor[<span class=\"number\">1</span>:]))) * in_height / in_width  <span class=\"comment\"># 处理矩形输入</span></span><br><span class=\"line\">    h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[<span class=\"number\">0</span>]),</span><br><span class=\"line\">                   sizes[<span class=\"number\">0</span>] / torch.sqrt(ratio_tensor[<span class=\"number\">1</span>:])))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(w.shape)  <span class=\"comment\"># torch.Size([5])</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 除以2来获得半高和半宽作为中心点到左上和右下的偏移量，repeat(a, b)表示在行上复制a倍，在列上复制b倍</span></span><br><span class=\"line\">    anchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(in_height * in_width, <span class=\"number\">1</span>) / <span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(anchor_manipulations.shape)  <span class=\"comment\"># torch.Size([2042040, 4])</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 每个中心点都将有“boxes_per_pixel”个锚框，所以生成含所有锚框中心的网格，重复了“boxes_per_pixel”次</span></span><br><span class=\"line\">    out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y], dim=<span class=\"number\">1</span>).repeat_interleave(boxes_per_pixel, dim=<span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(out_grid.shape)  <span class=\"comment\"># torch.Size([2042040, 4])</span></span><br><span class=\"line\">    output = out_grid + anchor_manipulations</span><br><span class=\"line\">    <span class=\"keyword\">return</span> output.unsqueeze(<span class=\"number\">0</span>)  <span class=\"comment\"># 增加batch维度</span></span><br><span class=\"line\"></span><br><span class=\"line\">img = plt.imread(<span class=\"string\">&#x27;../images/catdog.jpg&#x27;</span>)</span><br><span class=\"line\">h, w = img.shape[:<span class=\"number\">2</span>]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(h, w)  <span class=\"comment\"># 561 728</span></span><br><span class=\"line\">X = torch.rand(size=(<span class=\"number\">1</span>, <span class=\"number\">3</span>, h, w))</span><br><span class=\"line\">Y = multibox_prior(X, sizes=[<span class=\"number\">0.75</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.25</span>], ratios=[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">0.5</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Y.shape)  <span class=\"comment\"># torch.Size([1, 2042040, 4])</span></span><br><span class=\"line\"></span><br><span class=\"line\">boxes = Y.reshape(h, w, <span class=\"number\">5</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(boxes[<span class=\"number\">250</span>, <span class=\"number\">250</span>, <span class=\"number\">0</span>, :])  <span class=\"comment\"># tensor([0.06, 0.07, 0.63, 0.82])</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">show_bboxes</span>(<span class=\"params\">axes, bboxes, labels=<span class=\"literal\">None</span>, colors=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;显示所有边界框&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_make_list</span>(<span class=\"params\">obj, default_values=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> obj <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            obj = default_values</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(obj, (<span class=\"built_in\">list</span>, <span class=\"built_in\">tuple</span>)):</span><br><span class=\"line\">            obj = [obj]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> obj</span><br><span class=\"line\"></span><br><span class=\"line\">    labels = _make_list(labels)</span><br><span class=\"line\">    colors = _make_list(colors, [<span class=\"string\">&#x27;b&#x27;</span>, <span class=\"string\">&#x27;g&#x27;</span>, <span class=\"string\">&#x27;r&#x27;</span>, <span class=\"string\">&#x27;m&#x27;</span>, <span class=\"string\">&#x27;c&#x27;</span>])</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, bbox <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(bboxes):</span><br><span class=\"line\">        color = colors[i % <span class=\"built_in\">len</span>(colors)]</span><br><span class=\"line\">        rect = d2l.bbox_to_rect(bbox.detach().numpy(), color)</span><br><span class=\"line\">        axes.add_patch(rect)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> labels <span class=\"keyword\">and</span> <span class=\"built_in\">len</span>(labels) &gt; i:</span><br><span class=\"line\">            text_color = <span class=\"string\">&#x27;k&#x27;</span> <span class=\"keyword\">if</span> color == <span class=\"string\">&#x27;w&#x27;</span> <span class=\"keyword\">else</span> <span class=\"string\">&#x27;w&#x27;</span></span><br><span class=\"line\">            axes.text(rect.xy[<span class=\"number\">0</span>], rect.xy[<span class=\"number\">1</span>], labels[i], va=<span class=\"string\">&#x27;center&#x27;</span>, ha=<span class=\"string\">&#x27;center&#x27;</span>,</span><br><span class=\"line\">                      fontsize=<span class=\"number\">9</span>, color=text_color, bbox=<span class=\"built_in\">dict</span>(facecolor=color, lw=<span class=\"number\">0</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">plt.figure(dpi=<span class=\"number\">100</span>)</span><br><span class=\"line\">bbox_scale = torch.tensor((w, h, w, h))  <span class=\"comment\"># 用于将坐标值从0~1复原为0~w(h)</span></span><br><span class=\"line\"><span class=\"comment\"># fig = plt.imshow(img)</span></span><br><span class=\"line\"><span class=\"comment\"># show_bboxes(fig.axes, boxes[250, 250, :, :] * bbox_scale,</span></span><br><span class=\"line\"><span class=\"comment\">#             [&#x27;s=0.75, r=1&#x27;, &#x27;s=0.5, r=1&#x27;, &#x27;s=0.25, r=1&#x27;, &#x27;s=0.75, r=2&#x27;, &#x27;s=0.75, r=0.5&#x27;])</span></span><br><span class=\"line\"><span class=\"comment\"># plt.show()</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">box_iou</span>(<span class=\"params\">boxes1, boxes2</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;计算两个锚框或边界框列表中成对的交并比&quot;&quot;&quot;</span></span><br><span class=\"line\">    box_area = <span class=\"keyword\">lambda</span> boxes: ((boxes[:, <span class=\"number\">2</span>] - boxes[:, <span class=\"number\">0</span>]) * (boxes[:, <span class=\"number\">3</span>] - boxes[:, <span class=\"number\">1</span>]))</span><br><span class=\"line\">    <span class=\"comment\"># boxes1.shape: (boxes1的数量, 4)</span></span><br><span class=\"line\">    <span class=\"comment\"># boxes2.shape: (boxes2的数量, 4)</span></span><br><span class=\"line\">    <span class=\"comment\"># areas1.shape: (boxes1的数量,)</span></span><br><span class=\"line\">    <span class=\"comment\"># areas2.shape: (boxes2的数量,)</span></span><br><span class=\"line\">    areas1 = box_area(boxes1)</span><br><span class=\"line\">    areas2 = box_area(boxes2)</span><br><span class=\"line\">    <span class=\"comment\"># inter_upperlefts.shape, inter_lowerrights.shape, inters.shape: (boxes1的数量, boxes2的数量, 2)</span></span><br><span class=\"line\">    inter_upperlefts = torch.<span class=\"built_in\">max</span>(boxes1[:, <span class=\"literal\">None</span>, :<span class=\"number\">2</span>], boxes2[:, :<span class=\"number\">2</span>])</span><br><span class=\"line\">    inter_lowerrights = torch.<span class=\"built_in\">min</span>(boxes1[:, <span class=\"literal\">None</span>, <span class=\"number\">2</span>:], boxes2[:, <span class=\"number\">2</span>:])</span><br><span class=\"line\">    inters = (inter_lowerrights - inter_upperlefts).clamp(<span class=\"built_in\">min</span>=<span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"comment\"># inter_areas.shape, union_areas.shape: (boxes1的数量, boxes2的数量)</span></span><br><span class=\"line\">    inter_areas = inters[:, :, <span class=\"number\">0</span>] * inters[:, :, <span class=\"number\">1</span>]</span><br><span class=\"line\">    union_areas = areas1[:, <span class=\"literal\">None</span>] + areas2 - inter_areas</span><br><span class=\"line\">    <span class=\"keyword\">return</span> inter_areas / union_areas</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">assign_anchor_to_bbox</span>(<span class=\"params\">ground_truth, anchors, device, iou_threshold=<span class=\"number\">0.5</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;将最接近的真实边界框分配给锚框&quot;&quot;&quot;</span></span><br><span class=\"line\">    num_anchors, num_gt_boxes = anchors.shape[<span class=\"number\">0</span>], ground_truth.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    <span class=\"comment\"># 位于第i行和第j列的元素x_ij是锚框i和真实边界框j的IoU</span></span><br><span class=\"line\">    jaccard = box_iou(anchors, ground_truth)</span><br><span class=\"line\">    <span class=\"comment\"># 对于每个锚框，分配的真实边界框的张量</span></span><br><span class=\"line\">    anchors_bbox_map = torch.full((num_anchors,), -<span class=\"number\">1</span>, dtype=torch.long, device=device)</span><br><span class=\"line\">    <span class=\"comment\"># 根据阈值，决定是否分配真实边界框</span></span><br><span class=\"line\">    max_ious, indices = torch.<span class=\"built_in\">max</span>(jaccard, dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">    anc_i = torch.nonzero(max_ious &gt;= iou_threshold).reshape(-<span class=\"number\">1</span>)</span><br><span class=\"line\">    box_j = indices[max_ious &gt;= iou_threshold]</span><br><span class=\"line\">    anchors_bbox_map[anc_i] = box_j</span><br><span class=\"line\">    col_discard = torch.full((num_anchors,), -<span class=\"number\">1</span>)</span><br><span class=\"line\">    row_discard = torch.full((num_gt_boxes,), -<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_gt_boxes):</span><br><span class=\"line\">        max_idx = torch.argmax(jaccard)</span><br><span class=\"line\">        box_idx = (max_idx % num_gt_boxes).long()</span><br><span class=\"line\">        anc_idx = (max_idx / num_gt_boxes).long()</span><br><span class=\"line\">        anchors_bbox_map[anc_idx] = box_idx</span><br><span class=\"line\">        jaccard[:, box_idx] = col_discard</span><br><span class=\"line\">        jaccard[anc_idx, :] = row_discard</span><br><span class=\"line\">    <span class=\"keyword\">return</span> anchors_bbox_map</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">offset_boxes</span>(<span class=\"params\">anchors, assigned_bb, eps=<span class=\"number\">1e-6</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;对锚框偏移量的转换&quot;&quot;&quot;</span></span><br><span class=\"line\">    c_anc = d2l.box_corner_to_center(anchors)</span><br><span class=\"line\">    c_assigned_bb = d2l.box_corner_to_center(assigned_bb)</span><br><span class=\"line\">    offset_xy = <span class=\"number\">10</span> * (c_assigned_bb[:, :<span class=\"number\">2</span>] - c_anc[:, :<span class=\"number\">2</span>]) / c_anc[:, <span class=\"number\">2</span>:]</span><br><span class=\"line\">    offset_wh = <span class=\"number\">5</span> * torch.log(eps + c_assigned_bb[:, <span class=\"number\">2</span>:] / c_anc[:, <span class=\"number\">2</span>:])</span><br><span class=\"line\">    offset = torch.cat([offset_xy, offset_wh], dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> offset</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">multibox_target</span>(<span class=\"params\">anchors, labels</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;使用真实边界框标记锚框&quot;&quot;&quot;</span></span><br><span class=\"line\">    batch_size, anchors = labels.shape[<span class=\"number\">0</span>], anchors.squeeze(<span class=\"number\">0</span>)</span><br><span class=\"line\">    batch_offset, batch_mask, batch_class_labels = [], [], []</span><br><span class=\"line\">    device, num_anchors = anchors.device, anchors.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(batch_size):</span><br><span class=\"line\">        label = labels[i, :, :]</span><br><span class=\"line\">        anchors_bbox_map = assign_anchor_to_bbox(label[:, <span class=\"number\">1</span>:], anchors, device)</span><br><span class=\"line\">        bbox_mask = ((anchors_bbox_map &gt;= <span class=\"number\">0</span>).<span class=\"built_in\">float</span>().unsqueeze(-<span class=\"number\">1</span>)).repeat(<span class=\"number\">1</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 将类标签和分配的边界框坐标初始化为零</span></span><br><span class=\"line\">        class_labels = torch.zeros(num_anchors, dtype=torch.long, device=device)</span><br><span class=\"line\">        assigned_bb = torch.zeros((num_anchors, <span class=\"number\">4</span>), dtype=torch.float32, device=device)</span><br><span class=\"line\">        <span class=\"comment\"># 使用真实边界框来标记锚框的类别</span></span><br><span class=\"line\">        <span class=\"comment\"># 如果一个锚框没有被分配，标记其为背景（值为零）</span></span><br><span class=\"line\">        indices_true = torch.nonzero(anchors_bbox_map &gt;= <span class=\"number\">0</span>)</span><br><span class=\"line\">        bb_idx = anchors_bbox_map[indices_true]</span><br><span class=\"line\">        class_labels[indices_true] = label[bb_idx, <span class=\"number\">0</span>].long() + <span class=\"number\">1</span></span><br><span class=\"line\">        assigned_bb[indices_true] = label[bb_idx, <span class=\"number\">1</span>:]</span><br><span class=\"line\">        <span class=\"comment\"># 偏移量转换</span></span><br><span class=\"line\">        offset = offset_boxes(anchors, assigned_bb) * bbox_mask</span><br><span class=\"line\">        batch_offset.append(offset.reshape(-<span class=\"number\">1</span>))</span><br><span class=\"line\">        batch_mask.append(bbox_mask.reshape(-<span class=\"number\">1</span>))</span><br><span class=\"line\">        batch_class_labels.append(class_labels)</span><br><span class=\"line\">    bbox_offset = torch.stack(batch_offset)</span><br><span class=\"line\">    bbox_mask = torch.stack(batch_mask)</span><br><span class=\"line\">    class_labels = torch.stack(batch_class_labels)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (bbox_offset, bbox_mask, class_labels)</span><br><span class=\"line\"></span><br><span class=\"line\">ground_truth = torch.tensor([[<span class=\"number\">0</span>, <span class=\"number\">0.1</span>, <span class=\"number\">0.08</span>, <span class=\"number\">0.52</span>, <span class=\"number\">0.92</span>],</span><br><span class=\"line\">                             [<span class=\"number\">1</span>, <span class=\"number\">0.55</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.9</span>, <span class=\"number\">0.88</span>]])</span><br><span class=\"line\">anchors = torch.tensor([[<span class=\"number\">0</span>, <span class=\"number\">0.1</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.3</span>], [<span class=\"number\">0.15</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.4</span>, <span class=\"number\">0.4</span>],</span><br><span class=\"line\">                        [<span class=\"number\">0.63</span>, <span class=\"number\">0.05</span>, <span class=\"number\">0.88</span>, <span class=\"number\">0.98</span>], [<span class=\"number\">0.66</span>, <span class=\"number\">0.45</span>, <span class=\"number\">0.8</span>, <span class=\"number\">0.8</span>],</span><br><span class=\"line\">                        [<span class=\"number\">0.57</span>, <span class=\"number\">0.3</span>, <span class=\"number\">0.92</span>, <span class=\"number\">0.9</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># fig = plt.imshow(img)</span></span><br><span class=\"line\"><span class=\"comment\"># show_bboxes(fig.axes, ground_truth[:, 1:] * bbox_scale, [&#x27;dog&#x27;, &#x27;cat&#x27;], &#x27;k&#x27;)</span></span><br><span class=\"line\"><span class=\"comment\"># show_bboxes(fig.axes, anchors * bbox_scale, [&#x27;0&#x27;, &#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;, &#x27;4&#x27;])</span></span><br><span class=\"line\"><span class=\"comment\"># plt.show()</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 返回的结果中有三个元素，都是张量格式。第一个元素包含了为每个锚框标记的四个偏移值。注意负类锚框的偏移量被标记为零</span></span><br><span class=\"line\"><span class=\"comment\"># 第二个元素是掩码（mask）变量，形状为（批量大小，锚框数的四倍）</span></span><br><span class=\"line\"><span class=\"comment\"># 第三个元素包含标记的输入锚框的类别</span></span><br><span class=\"line\">labels = multibox_target(anchors.unsqueeze(<span class=\"number\">0</span>), ground_truth.unsqueeze(<span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(labels[<span class=\"number\">2</span>])  <span class=\"comment\"># tensor([[0, 1, 2, 0, 2]])</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">offset_inverse</span>(<span class=\"params\">anchors, offset_preds</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;根据带有预测偏移量的锚框来预测边界框&quot;&quot;&quot;</span></span><br><span class=\"line\">    anc = d2l.box_corner_to_center(anchors)</span><br><span class=\"line\">    pred_bbox_xy = (offset_preds[:, :<span class=\"number\">2</span>] * anc[:, <span class=\"number\">2</span>:] / <span class=\"number\">10</span>) + anc[:, :<span class=\"number\">2</span>]</span><br><span class=\"line\">    pred_bbox_wh = torch.exp(offset_preds[:, <span class=\"number\">2</span>:] / <span class=\"number\">5</span>) * anc[:, <span class=\"number\">2</span>:]</span><br><span class=\"line\">    pred_bbox = torch.cat((pred_bbox_xy, pred_bbox_wh), dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">    predicted_bbox = d2l.box_center_to_corner(pred_bbox)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> predicted_bbox</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">nms</span>(<span class=\"params\">boxes, scores, iou_threshold</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;对预测边界框的置信度进行排序&quot;&quot;&quot;</span></span><br><span class=\"line\">    B = torch.argsort(scores, dim=-<span class=\"number\">1</span>, descending=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    keep = []  <span class=\"comment\"># 保留预测边界框的指标</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> B.numel() &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">        i = B[<span class=\"number\">0</span>]</span><br><span class=\"line\">        keep.append(i)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> B.numel() == <span class=\"number\">1</span>: <span class=\"keyword\">break</span></span><br><span class=\"line\">        iou = box_iou(boxes[i, :].reshape(-<span class=\"number\">1</span>, <span class=\"number\">4</span>), boxes[B[<span class=\"number\">1</span>:], :].reshape(-<span class=\"number\">1</span>, <span class=\"number\">4</span>)).reshape(-<span class=\"number\">1</span>)</span><br><span class=\"line\">        inds = torch.nonzero(torch.as_tensor(iou &lt;= iou_threshold, dtype=torch.float32)).reshape(-<span class=\"number\">1</span>)</span><br><span class=\"line\">        B = B[inds + <span class=\"number\">1</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.tensor(keep, device=boxes.device)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">multibox_detection</span>(<span class=\"params\">cls_probs, offset_preds, anchors, nms_threshold=<span class=\"number\">0.5</span>, pos_threshold=<span class=\"number\">0.009999999</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;使用非极大值抑制来预测边界框&quot;&quot;&quot;</span></span><br><span class=\"line\">    device, batch_size = cls_probs.device, cls_probs.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    anchors = anchors.squeeze(<span class=\"number\">0</span>)</span><br><span class=\"line\">    num_classes, num_anchors = cls_probs.shape[<span class=\"number\">1</span>], cls_probs.shape[<span class=\"number\">2</span>]</span><br><span class=\"line\">    out = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(batch_size):</span><br><span class=\"line\">        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-<span class=\"number\">1</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\">        conf, class_id = torch.<span class=\"built_in\">max</span>(cls_prob[<span class=\"number\">1</span>:], <span class=\"number\">0</span>)</span><br><span class=\"line\">        predicted_bb = offset_inverse(anchors, offset_pred)</span><br><span class=\"line\">        keep = nms(predicted_bb, conf, nms_threshold)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 找到所有的non_keep索引，并将类设置为背景</span></span><br><span class=\"line\">        all_idx = torch.arange(num_anchors, dtype=torch.long, device=device)</span><br><span class=\"line\">        combined = torch.cat((keep, all_idx))</span><br><span class=\"line\">        uniques, counts = combined.unique(return_counts=<span class=\"literal\">True</span>)</span><br><span class=\"line\">        non_keep = uniques[counts == <span class=\"number\">1</span>]</span><br><span class=\"line\">        all_id_sorted = torch.cat((keep, non_keep))</span><br><span class=\"line\">        class_id[non_keep] = -<span class=\"number\">1</span></span><br><span class=\"line\">        class_id = class_id[all_id_sorted]</span><br><span class=\"line\">        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]</span><br><span class=\"line\">        <span class=\"comment\"># pos_threshold是一个用于非背景预测的阈值</span></span><br><span class=\"line\">        below_min_idx = (conf &lt; pos_threshold)</span><br><span class=\"line\">        class_id[below_min_idx] = -<span class=\"number\">1</span></span><br><span class=\"line\">        conf[below_min_idx] = <span class=\"number\">1</span> - conf[below_min_idx]</span><br><span class=\"line\">        pred_info = torch.cat((class_id.unsqueeze(<span class=\"number\">1</span>), conf.unsqueeze(<span class=\"number\">1</span>), predicted_bb), dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        out.append(pred_info)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.stack(out)</span><br><span class=\"line\"></span><br><span class=\"line\">anchors = torch.tensor([[<span class=\"number\">0.1</span>, <span class=\"number\">0.08</span>, <span class=\"number\">0.52</span>, <span class=\"number\">0.92</span>], [<span class=\"number\">0.08</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.56</span>, <span class=\"number\">0.95</span>],</span><br><span class=\"line\">                        [<span class=\"number\">0.15</span>, <span class=\"number\">0.3</span>, <span class=\"number\">0.62</span>, <span class=\"number\">0.91</span>], [<span class=\"number\">0.55</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.9</span>, <span class=\"number\">0.88</span>]])</span><br><span class=\"line\">offset_preds = torch.tensor([<span class=\"number\">0</span>] * anchors.numel())</span><br><span class=\"line\">cls_probs = torch.tensor([[<span class=\"number\">0</span>] * <span class=\"number\">4</span>,  <span class=\"comment\"># 背景的预测概率</span></span><br><span class=\"line\">                          [<span class=\"number\">0.9</span>, <span class=\"number\">0.8</span>, <span class=\"number\">0.7</span>, <span class=\"number\">0.1</span>],  <span class=\"comment\"># 狗的预测概率</span></span><br><span class=\"line\">                          [<span class=\"number\">0.1</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.3</span>, <span class=\"number\">0.9</span>]])  <span class=\"comment\"># 猫的预测概率</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># fig = plt.imshow(img)</span></span><br><span class=\"line\"><span class=\"comment\"># show_bboxes(fig.axes, anchors * bbox_scale, [&#x27;dog=0.9&#x27;, &#x27;dog=0.8&#x27;, &#x27;dog=0.7&#x27;, &#x27;cat=0.9&#x27;])</span></span><br><span class=\"line\"><span class=\"comment\"># plt.show()</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 返回结果的形状是（批量大小，锚框的数量，6）</span></span><br><span class=\"line\"><span class=\"comment\"># 第一个元素是预测的类索引，从0开始（0代表狗，1代表猫），值-1表示背景或在非极大值抑制中被移除了</span></span><br><span class=\"line\"><span class=\"comment\"># 第二个元素是预测的边界框的置信度</span></span><br><span class=\"line\"><span class=\"comment\"># 其余四个元素分别是预测边界框左上角和右下角的坐标（范围介于0~1之间）</span></span><br><span class=\"line\">output = multibox_detection(cls_probs.unsqueeze(<span class=\"number\">0</span>),</span><br><span class=\"line\">                            offset_preds.unsqueeze(<span class=\"number\">0</span>),</span><br><span class=\"line\">                            anchors.unsqueeze(<span class=\"number\">0</span>),</span><br><span class=\"line\">                            nms_threshold=<span class=\"number\">0.5</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(output)</span><br><span class=\"line\"><span class=\"comment\"># tensor([[[ 0.00,  0.90,  0.10,  0.08,  0.52,  0.92],</span></span><br><span class=\"line\"><span class=\"comment\">#          [ 1.00,  0.90,  0.55,  0.20,  0.90,  0.88],</span></span><br><span class=\"line\"><span class=\"comment\">#          [-1.00,  0.80,  0.08,  0.20,  0.56,  0.95],</span></span><br><span class=\"line\"><span class=\"comment\">#          [-1.00,  0.70,  0.15,  0.30,  0.62,  0.91]]])</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 删除-1类别（背景）的预测边界框后，我们可以输出由非极大值抑制保存的最终预测边界框</span></span><br><span class=\"line\">fig = plt.imshow(img)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> output[<span class=\"number\">0</span>].detach().numpy():</span><br><span class=\"line\">    <span class=\"keyword\">if</span> i[<span class=\"number\">0</span>] == -<span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"keyword\">continue</span></span><br><span class=\"line\">    label = (<span class=\"string\">&#x27;dog=&#x27;</span>, <span class=\"string\">&#x27;cat=&#x27;</span>)[<span class=\"built_in\">int</span>(i[<span class=\"number\">0</span>])] + <span class=\"built_in\">str</span>(i[<span class=\"number\">1</span>])</span><br><span class=\"line\">    show_bboxes(fig.axes, [torch.tensor(i[<span class=\"number\">2</span>:]) * bbox_scale], label)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<h2 id=\"6-多尺度目标检测\">6. 多尺度目标检测</h2>\n<p>在上一节中，我们以输入图像的每个像素为中心，生成了多个锚框。基本而言，这些锚框代表了图像不同区域的样本。然而，如果为每个像素都生成的锚框，我们最终可能会得到太多需要计算的锚框。想象一个561*728的输入图像，如果以每个像素为中心生成五个形状不同的锚框，就需要在图像上标记和预测超过200万个锚框（561*728*5）。</p>\n<h3 id=\"6-1-多尺度锚框\">6.1 多尺度锚框</h3>\n<p>减少图像上的锚框数量并不困难。比如，我们可以在输入图像中均匀采样一小部分像素，并以它们为中心生成锚框。此外，在不同尺度下，我们可以生成不同数量和不同大小的锚框。直观地说，比起较大的目标，较小的目标在图像上出现的可能性更多样。例如，1*1、1*2和2*2的目标可以分别以4、2和1种可能的方式出现在2*2的图像上。因此，当使用较小的锚框检测较小的物体时，我们可以采样更多的区域，而对于较大的物体，我们可以采样较少的区域。</p>\n<p>为了演示如何在多个尺度下生成锚框，让我们先读取一张图像。它的高度和宽度分别为561和728像素：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">img = plt.imread(<span class=\"string\">&#x27;../images/catdog.jpg&#x27;</span>)</span><br><span class=\"line\">h, w = img.shape[:<span class=\"number\">2</span>]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(h, w)  <span class=\"comment\"># 561 728</span></span><br></pre></td></tr></table></figure>\n<p><code>display_anchors</code> 函数定义如下。我们在<strong>特征图</strong>（fmap）上生成锚框（anchors），每个单位（像素）作为锚框的中心。由于锚框中的 <code>(x, y)</code> 轴坐标值（anchors）已经被除以特征图（fmap）的宽度和高度，因此这些值介于0和1之间，表示特征图中锚框的相对位置。</p>\n<p>由于锚框（anchors）的中心分布于特征图（fmap）上的所有单位，因此这些中心必须根据其相对空间位置在任何输入图像上均匀分布。更具体地说，给定特征图的宽度和高度 <code>fmap_w</code> 和 <code>fmap_h</code>，以下函数将均匀地对任何输入图像中 <code>fmap_h</code> 行和 <code>fmap_w</code> 列中的像素进行采样。以这些均匀采样的像素为中心，将会生成大小为 <code>s</code>（假设列表 <code>s</code> 的长度为1）且宽高比（ratios）不同的锚框：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">display_anchors</span>(<span class=\"params\">fmap_w, fmap_h, s</span>):</span><br><span class=\"line\">    plt.figure(dpi=<span class=\"number\">100</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 前两个维度上的值不影响输出</span></span><br><span class=\"line\">    fmap = torch.zeros((<span class=\"number\">1</span>, <span class=\"number\">10</span>, fmap_h, fmap_w))</span><br><span class=\"line\">    anchors = d2l.multibox_prior(fmap, sizes=s, ratios=[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">0.5</span>])</span><br><span class=\"line\">    bbox_scale = torch.tensor((w, h, w, h))</span><br><span class=\"line\">    d2l.show_bboxes(plt.imshow(img).axes, anchors[<span class=\"number\">0</span>] * bbox_scale)</span><br><span class=\"line\">    plt.show()</span><br></pre></td></tr></table></figure>\n<p>首先，让我们考虑探测小目标。为了在显示时更容易分辨，在这里具有不同中心的锚框不会重叠：锚框的尺度设置为0.15，特征图的高度和宽度设置为4。我们可以看到，图像上4行和4列的锚框的中心是均匀分布的：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">display_anchors(fmap_w=<span class=\"number\">4</span>, fmap_h=<span class=\"number\">4</span>, s=[<span class=\"number\">0.15</span>])</span><br></pre></td></tr></table></figure>\n<p>然后，我们将特征图的高度和宽度减小一半，然后使用较大的锚框来检测较大的目标。当尺度设置为0.4时，一些锚框将彼此重叠：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">display_anchors(fmap_w=<span class=\"number\">2</span>, fmap_h=<span class=\"number\">2</span>, s=[<span class=\"number\">0.4</span>])</span><br></pre></td></tr></table></figure>\n<p>最后，我们进一步将特征图的高度和宽度减小一半，然后将锚框的尺度增加到0.8。此时，锚框的中心即是图像的中心：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">display_anchors(fmap_w=<span class=\"number\">1</span>, fmap_h=<span class=\"number\">1</span>, s=[<span class=\"number\">0.8</span>])</span><br></pre></td></tr></table></figure>\n<h3 id=\"6-2-多尺度检测\">6.2 多尺度检测</h3>\n<p>既然我们已经生成了多尺度的锚框，我们就将使用它们来检测不同尺度下各种大小的目标。下面，我们介绍一种基于 CNN 的多尺度目标检测方法，将在第8节（SSD）中实现。</p>\n<p>在某种规模上，假设我们有 <code>c</code> 张形状为 <code>h * w</code> 的特征图。使用上一小节中的方法，我们生成了 <code>hw</code> 组锚框，其中每组都有 <code>a</code> 个中心相同的锚框。例如，在上一小节实验的第一个尺度上，给定10个（通道数量）<code>4 * 4</code> 的特征图，我们生成了16组锚框，每组包含3个中心相同的锚框。接下来，每个锚框都根据真实值边界框来标记了类和偏移量。在当前尺度下，目标检测模型需要预测输入图像上 <code>hw</code> 组锚框类别和偏移量，其中不同组锚框具有不同的中心。</p>\n<p>假设此处的 <code>c</code> 张特征图是 CNN 基于输入图像的正向传播算法获得的中间输出。既然每张特征图上都有 <code>hw</code> 个不同的空间位置，那么相同空间位置可以看作含有 <code>c</code> 个单元。根据感受野的定义，特征图在相同空间位置的 <code>c</code> 个单元在输入图像上的感受野相同：它们表征了同一感受野内的输入图像信息。因此，我们可以将特征图在同一空间位置的 <code>c</code> 个单元变换为使用此空间位置生成的 <code>a</code> 个锚框类别和偏移量。本质上，我们用输入图像在某个感受野区域内的信息，来预测输入图像上与该区域位置相近的锚框类别和偏移量。</p>\n<p>当不同层的特征图在输入图像上分别拥有不同大小的感受野时，它们可以用于检测不同大小的目标。例如，我们可以设计一个神经网络，其中靠近输出层的特征图单元具有更宽的感受野，这样它们就可以从输入图像中检测到较大的目标。</p>\n<p>简言之，我们可以利用深层神经网络在多个层次上对图像进行分层表示，从而实现多尺度目标检测。在第8节我们将通过一个具体的例子来说明它是如何工作的。</p>\n<h2 id=\"7-区域卷积神经网络（R-CNN）系列\">7. 区域卷积神经网络（R-CNN）系列</h2>\n<h3 id=\"7-1-R-CNN\">7.1 R-CNN</h3>\n<p>R-CNN 首先从输入图像中选取若干（例如2000个）提议区域（如锚框也是一种选取方法），并标注它们的类别和边界框（如偏移量）。然后，用卷积神经网络对每个提议区域进行前向传播以抽取其特征。接下来，我们用每个提议区域的特征来预测类别和边界框。具体来说，R-CNN 包括以下四个步骤：</p>\n<ol>\n<li>对输入图像使用选择性搜索来选取多个高质量的提议区域。这些提议区域通常是在多个尺度下选取的，并具有不同的形状和大小。每个提议区域都将被标注类别和真实边界框；</li>\n<li>选择一个预训练的卷积神经网络，并将其在输出层之前截断。将每个提议区域变形为网络需要的输入尺寸，并通过前向传播输出抽取的提议区域特征；</li>\n<li>将每个提议区域的特征连同其标注的类别作为一个样本。训练多个支持向量机对目标分类，其中每个支持向量机用来判断样本是否属于某一个类别；</li>\n<li>将每个提议区域的特征连同其标注的边界框作为一个样本，训练线性回归模型来预测真实边界框。</li>\n</ol>\n<p>尽管 R-CNN 模型通过预训练的卷积神经网络有效地抽取了图像特征，但它的速度很慢。想象一下，我们可能从一张图像中选出上千个提议区域，这需要上千次的卷积神经网络的前向传播来执行目标检测。这种庞大的计算量使得 R-CNN 在现实世界中难以被广泛应用。</p>\n<h3 id=\"7-2-Fast-R-CNN\">7.2 Fast R-CNN</h3>\n<p>R-CNN 的主要性能瓶颈在于，对每个提议区域，卷积神经网络的前向传播是独立的，而没有共享计算。由于这些区域通常有重叠，独立的特征抽取会导致重复的计算。Fast R-CNN 对 R-CNN 的主要改进之一，是仅在<strong>整张图像</strong>上执行卷积神经网络的前向传播。Fast R-CNN 的主要计算如下：</p>\n<ol>\n<li>与 R-CNN 相比，Fast R-CNN 用来提取特征的入卷积神经网络的输入是整个图像，而不是各个提议区域。此外，这个网络通常会参与训练。设输入为一张图像，将卷积神经网络的输出的形状记为 <code>1 * c * h1 * w1</code>；</li>\n<li>假设选择性搜索生成了 <code>n</code> 个提议区域。这些形状各异的提议区域在卷积神经网络的输出上分别标出了形状各异的兴趣区域。然后，这些感兴趣的区域需要进一步抽取出<strong>形状相同</strong>的特征（比如指定高度 <code>h2</code> 和宽度 <code>w2</code>），以便于连结后输出。为了实现这一目标，Fast R-CNN 引入了<strong>兴趣区域汇聚层</strong>（RoI pooling）：将卷积神经网络的输出和提议区域作为输入，输出连结后的各个提议区域抽取的特征，形状为 <code>n * c * h2 * w2</code>；</li>\n<li>通过全连接层将输出形状变换为 <code>n * d</code>，其中超参数 <code>d</code> 取决于模型设计；</li>\n<li>预测 <code>n</code> 个提议区域中每个区域的类别和边界框。更具体地说，在预测类别和边界框时，将全连接层的输出分别转换为形状为 <code>n * q</code>（<code>q</code> 是类别的数量）的输出和形状为 <code>n * 4</code> 的输出。其中预测类别时使用 Softmax 回归。</li>\n</ol>\n<p>下面，我们演示了兴趣区域汇聚层的计算方法。假设卷积神经网络抽取的特征 <code>X</code> 的高度和宽度都是4，且只有单通道：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision</span><br><span class=\"line\"></span><br><span class=\"line\">X = torch.arange(<span class=\"number\">16.</span>).reshape(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(X)</span><br><span class=\"line\"><span class=\"comment\"># tensor([[[[ 0.,  1.,  2.,  3.],</span></span><br><span class=\"line\"><span class=\"comment\">#           [ 4.,  5.,  6.,  7.],</span></span><br><span class=\"line\"><span class=\"comment\">#           [ 8.,  9., 10., 11.],</span></span><br><span class=\"line\"><span class=\"comment\">#           [12., 13., 14., 15.]]]])</span></span><br></pre></td></tr></table></figure>\n<p>让我们进一步假设输入图像的高度和宽度都是40像素，且选择性搜索在此图像上生成了两个提议区域。每个区域由5个元素表示：区域目标类别、左上角和右下角的 <code>(x, y)</code> 坐标：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rois = torch.Tensor([[<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">20</span>, <span class=\"number\">20</span>], [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">10</span>, <span class=\"number\">30</span>, <span class=\"number\">30</span>]])</span><br></pre></td></tr></table></figure>\n<p>由于 <code>X</code> 的高和宽是输入图像高和宽的1/10，因此，两个提议区域的坐标先按 <code>spatial_scale</code> 乘以0.1。然后，在 <code>X</code> 上分别标出这两个兴趣区域 <code>X[:, :, 0:3, 0:3]</code> 和 <code>X[:, :, 1:4, 0:4]</code>。最后，在 <code>2 * 2</code> 的兴趣区域汇聚层中，每个兴趣区域被划分为子窗口网格，并进一步抽取相同形状 <code>2 * 2</code> 的特征：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(torchvision.ops.roi_pool(X, rois, output_size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>), spatial_scale=<span class=\"number\">0.1</span>))</span><br><span class=\"line\"><span class=\"comment\"># tensor([[[[ 5.,  6.],</span></span><br><span class=\"line\"><span class=\"comment\">#           [ 9., 10.]]],</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\">#         [[[ 9., 11.],</span></span><br><span class=\"line\"><span class=\"comment\">#           [13., 15.]]]])</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"7-3-Faster-R-CNN\">7.3 Faster R-CNN</h3>\n<p>为了较精确地检测目标结果，Fast R-CNN 模型通常需要在选择性搜索中生成大量的提议区域。Faster R-CNN 提出将选择性搜索替换为<strong>区域提议网络</strong>（region proposal network），从而减少提议区域的生成数量，并保证目标检测的精度。具体来说，区域提议网络的计算步骤如下：</p>\n<ol>\n<li>使用填充为1的 <code>3 * 3</code> 的卷积层变换卷积神经网络的输出，并将输出通道数记为 <code>c</code>。这样，卷积神经网络为图像抽取的特征图中的每个单元均得到一个长度为 <code>c</code> 的新特征；</li>\n<li>以特征图的每个像素为中心，生成多个不同大小和宽高比的锚框并标注它们；</li>\n<li>使用锚框中心单元长度为 <code>c</code> 的特征，分别预测该锚框的二元类别（含目标还是背景）和边界框；</li>\n<li>使用非极大值抑制，从预测类别为目标的预测边界框中移除相似的结果。最终输出的预测边界框即是兴趣区域汇聚层所需的提议区域。</li>\n</ol>\n<p>值得一提的是，区域提议网络作为 Faster R-CNN 模型的一部分，是和整个模型一起训练得到的。换句话说，Faster R-CNN 的目标函数不仅包括目标检测中的类别和边界框预测，还包括区域提议网络中锚框的二元类别和边界框预测。作为端到端训练的结果，区域提议网络能够学习到如何生成高质量的提议区域，从而在减少了从数据中学习的提议区域的数量的情况下，仍保持目标检测的精度。</p>\n<h3 id=\"7-4-Mask-R-CNN\">7.4 Mask R-CNN</h3>\n<p>如果在训练集中还标注了每个目标在图像上的<strong>像素级位置</strong>，那么 Mask R-CNN 能够有效地利用这些详尽的标注信息进一步提升目标检测的精度。</p>\n<p>Mask R-CNN 是基于 Faster R-CNN 修改而来的。具体来说，Mask R-CNN 将兴趣区域汇聚层替换为了兴趣区域对齐层（RoI Align），使用双线性插值（bilinear interpolation）来保留特征图上的空间信息，从而更适于像素级预测。兴趣区域对齐层的输出包含了所有与兴趣区域的形状相同的特征图。它们不仅被用于预测每个兴趣区域的类别和边界框，还通过额外的全卷积网络预测目标的像素级位置。本章的后续章节将更详细地介绍如何使用全卷积网络预测图像中像素级的语义。</p>\n<h2 id=\"8-单发多框检测（SSD）\">8. 单发多框检测（SSD）</h2>\n<p>SSD 模型主要由基础网络组成，其后是几个多尺度特征块。基本网络用于从输入图像中提取特征，因此它可以使用深度卷积神经网络。单发多框检测论文中选用了在分类层之前截断的 VGG，现在也常用 ResNet 替代。我们可以设计基础网络，使它输出的高和宽较大。这样一来，基于该特征图生成的锚框数量较多，可以用来检测尺寸较小的目标。接下来的每个多尺度特征块将上一层提供的特征图的高和宽缩小（如减半），并使特征图中每个单元在输入图像上的感受野变得更广阔。</p>\n<p>回想一下在第6节中，通过深度神经网络分层表示图像的多尺度目标检测的设计。由于接近顶部的多尺度特征图较小，但具有较大的感受野，它们适合检测较少但较大的物体。简而言之，通过多尺度特征块，单发多框检测生成不同大小的锚框，并通过预测边界框的类别和偏移量来检测大小不同的目标，因此这是一个多尺度目标检测模型。</p>\n<h3 id=\"8-1-类别预测层与边界框预测层\">8.1 类别预测层与边界框预测层</h3>\n<p>设目标类别的数量为 <code>q</code>。这样一来，锚框有 <code>q + 1</code> 个类别，其中第0类是背景。在某个尺度下，设特征图的高和宽分别为 <code>h</code> 和 <code>w</code>。如果以其中每个单元为中心生成 <code>a</code> 个锚框，那么我们需要对 <code>hwa</code> 个锚框进行分类。如果使用全连接层作为输出，很容易导致模型参数过多。回忆 NiN 一节介绍的使用卷积层的通道来输出类别预测的方法，单发多框检测采用同样的方法来降低模型复杂度。</p>\n<p>具体来说，类别预测层使用一个保持输入高和宽的卷积层。这样一来，输出和输入在特征图宽和高上的空间坐标一一对应。考虑输出和输入同一空间坐标 <code>(x, y)</code>：输出特征图上 <code>(x, y)</code> 坐标的通道里包含了以输入特征图 <code>(x, y)</code> 坐标为中心生成的所有锚框的类别预测。因此输出通道数为 <code>a * (q + 1)</code>。</p>\n<p>类别预测层的定义如下：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.tensorboard <span class=\"keyword\">import</span> SummaryWriter</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"><span class=\"keyword\">from</span> tqdm <span class=\"keyword\">import</span> tqdm</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># num_inputs为输入通道数，(num_classes + 1)表示还有一个背景类，因为需要预测每个锚框是哪个类因此输出通道要乘以num_anchors</span></span><br><span class=\"line\"><span class=\"comment\"># 即对于输入的每一个像素，它的输出通道数就是以该像素为中心的num_anchors个锚框的预测值</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">cls_predictor</span>(<span class=\"params\">num_inputs, num_anchors, num_classes</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Conv2d(num_inputs, num_anchors * (num_classes + <span class=\"number\">1</span>), kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n<p>边界框预测层的设计与类别预测层的设计类似。唯一不同的是，这里需要为每个锚框预测4个偏移量，而不是 <code>q + 1</code> 个类别：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 预测锚框和真实边界框的offset，对每一个锚框有4个预测值</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">bbox_predictor</span>(<span class=\"params\">num_inputs, num_anchors</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Conv2d(num_inputs, num_anchors * <span class=\"number\">4</span>, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"8-2-连结多尺度的预测\">8.2 连结多尺度的预测</h3>\n<p>单发多框检测使用多尺度特征图来生成锚框并预测其类别和偏移量。在不同的尺度下，特征图的形状或以同一单元为中心的锚框的数量可能会有所不同。因此，不同尺度下预测输出的形状可能会有所不同。</p>\n<p>在以下示例中，我们为同一个小批量构建两个不同比例（<code>Y1</code> 和 <code>Y2</code>）的特征图，其中 <code>Y2</code> 的高度和宽度是 <code>Y1</code> 的一半。以类别预测为例，假设 <code>Y1</code> 和 <code>Y2</code> 的每个单元分别生成了5个和3个锚框。进一步假设目标类别的数量为10，对于特征图 <code>Y1</code> 和 <code>Y2</code>，类别预测输出中的通道数分别为 <code>5 * (10 + 1) = 55</code> 和 <code>3 * (10 + 1) = 33</code>，其中任一输出的形状是 <code>(批量大小, 通道数, 高度, 宽度)</code>：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">x, block</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> block(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Y1为对输入的400(20*20)个像素都会做55(5*(10+1))个预测</span></span><br><span class=\"line\"><span class=\"comment\"># 因此在不同尺度下的预测除了batch维之外另外三个维度都会发生变化</span></span><br><span class=\"line\">Y1 = forward(torch.zeros((<span class=\"number\">2</span>, <span class=\"number\">8</span>, <span class=\"number\">20</span>, <span class=\"number\">20</span>)), cls_predictor(<span class=\"number\">8</span>, <span class=\"number\">5</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\">Y2 = forward(torch.zeros((<span class=\"number\">2</span>, <span class=\"number\">16</span>, <span class=\"number\">10</span>, <span class=\"number\">10</span>)), cls_predictor(<span class=\"number\">16</span>, <span class=\"number\">3</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Y1.shape, Y2.shape)  <span class=\"comment\"># torch.Size([2, 55, 20, 20]) torch.Size([2, 33, 10, 10])</span></span><br></pre></td></tr></table></figure>\n<p>除了批量大小这一维度外，其他三个维度都具有不同的尺寸。为了将这两个预测输出链接起来以提高计算效率，我们将把这些张量转换为更一致的格式。</p>\n<p>通道维包含中心相同的锚框的预测结果。我们首先将通道维移到最后一维。因为不同尺度下批量大小仍保持不变，我们可以将预测结果转成二维的 <code>(批量大小, 高 * 宽 * 通道数)</code> 的格式，以方便之后在维度1上的连结。这样一来，尽管 <code>Y1</code> 和 <code>Y2</code> 在通道数、高度和宽度方面具有不同的大小，我们仍然可以在同一个小批量的两个不同尺度上连接这两个预测输出：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># start_dim=1表示将后面三个维度展平成一维</span></span><br><span class=\"line\"><span class=\"comment\"># 把通道放最后表示对于每个像素的预测是连续值，否则展平后每个像素的预测就不是连续的</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">flatten_pred</span>(<span class=\"params\">pred</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.flatten(pred.permute(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>), start_dim=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">concat_preds</span>(<span class=\"params\">preds</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.cat([flatten_pred(p) <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> preds], dim=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(concat_preds([Y1, Y2]).shape)  <span class=\"comment\"># torch.Size([2, 25300])</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"8-3-高和宽减半块\">8.3 高和宽减半块</h3>\n<p>高和宽减半块将输入特征图的高度和宽度减半，会扩大每个单元在其输出特征图中的感受野，该模块此前已在 VGG 中使用过：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 高宽减半块，该模块将输入特征图的高度和宽度减半</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">down_sample_blk</span>(<span class=\"params\">in_channels, out_channels</span>):</span><br><span class=\"line\">    blk = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">2</span>):</span><br><span class=\"line\">        blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>))</span><br><span class=\"line\">        blk.append(nn.BatchNorm2d(out_channels))</span><br><span class=\"line\">        blk.append(nn.ReLU())</span><br><span class=\"line\">        in_channels = out_channels</span><br><span class=\"line\">    blk.append(nn.MaxPool2d(<span class=\"number\">2</span>))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(*blk)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(forward(torch.zeros((<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">20</span>, <span class=\"number\">20</span>)), down_sample_blk(<span class=\"number\">3</span>, <span class=\"number\">10</span>)).shape)  <span class=\"comment\"># torch.Size([2, 10, 10, 10])</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"8-4-基本网络块\">8.4 基本网络块</h3>\n<p>基本网络块用于从输入图像中抽取特征。为了计算简洁，我们构造了一个小的基础网络，该网络串联3个高和宽减半块，并逐步将通道数翻倍：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">base_net</span>():</span><br><span class=\"line\">    blk = []</span><br><span class=\"line\">    num_filters = [<span class=\"number\">3</span>, <span class=\"number\">16</span>, <span class=\"number\">32</span>, <span class=\"number\">64</span>]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(num_filters) - <span class=\"number\">1</span>):</span><br><span class=\"line\">        blk.append(down_sample_blk(num_filters[i], num_filters[i + <span class=\"number\">1</span>]))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(*blk)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(forward(torch.zeros((<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">256</span>, <span class=\"number\">256</span>)), base_net()).shape)  <span class=\"comment\"># torch.Size([2, 64, 32, 32])</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"8-5-完整的模型\">8.5 完整的模型</h3>\n<p>完整的单发多框检测模型由<strong>五个模块</strong>组成，每个块生成的特征图既用于生成锚框，又用于预测这些锚框的类别和偏移量。在这五个模块中，第一个是<strong>基本网络块</strong>，第二个到第四个是<strong>高和宽减半块</strong>，最后一个模块使用<strong>全局最大池化层</strong>将高度和宽度都降到1。从技术上讲，第二到第五个区块都是 SSD 中的<strong>多尺度特征块</strong>：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_blk</span>(<span class=\"params\">i</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> i == <span class=\"number\">0</span>:</span><br><span class=\"line\">        blk = base_net()</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> i == <span class=\"number\">1</span>:</span><br><span class=\"line\">        blk = down_sample_blk(<span class=\"number\">64</span>, <span class=\"number\">128</span>)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> i == <span class=\"number\">4</span>:</span><br><span class=\"line\">        blk = nn.AdaptiveMaxPool2d((<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        blk = down_sample_blk(<span class=\"number\">128</span>, <span class=\"number\">128</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> blk</span><br></pre></td></tr></table></figure>\n<p>现在我们为每个块定义前向传播。与图像分类任务不同，此处的输出包括：CNN 特征图 <code>Y</code>、在当前尺度下根据 <code>Y</code> 生成的锚框、预测的这些锚框的类别和偏移量（基于 <code>Y</code>）：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 为每个块定义前向传播，此处的cls_predictor和bbox_predictor为已经构造好的卷积层</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">blk_forward</span>(<span class=\"params\">X, blk, size, ratio, cls_predictor, bbox_predictor</span>):</span><br><span class=\"line\">    Y = blk(X)  <span class=\"comment\"># feature map</span></span><br><span class=\"line\">    anchors = d2l.multibox_prior(Y, sizes=size, ratios=ratio)</span><br><span class=\"line\">    cls_preds = cls_predictor(Y)</span><br><span class=\"line\">    bbox_preds = bbox_predictor(Y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (Y, anchors, cls_preds, bbox_preds)</span><br></pre></td></tr></table></figure>\n<p>超参数的设置过程可以看：<a href=\"https://zh-v2.d2l.ai/chapter_computer-vision/ssd.html\">单发多框检测（SSD）</a>。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sizes = [[<span class=\"number\">0.2</span>, <span class=\"number\">0.272</span>], [<span class=\"number\">0.37</span>, <span class=\"number\">0.447</span>], [<span class=\"number\">0.54</span>, <span class=\"number\">0.619</span>], [<span class=\"number\">0.71</span>, <span class=\"number\">0.79</span>], [<span class=\"number\">0.88</span>, <span class=\"number\">0.961</span>]]</span><br><span class=\"line\">ratios = [[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">0.5</span>]] * <span class=\"number\">5</span></span><br><span class=\"line\">num_anchors = <span class=\"built_in\">len</span>(sizes[<span class=\"number\">0</span>]) + <span class=\"built_in\">len</span>(ratios[<span class=\"number\">0</span>]) - <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n<p>现在，我们就可以按如下方式定义完整的模型 <code>TinySSD</code> 了：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">TinySSD</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, num_classes, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(TinySSD, self).__init__(**kwargs)</span><br><span class=\"line\">        self.num_classes = num_classes</span><br><span class=\"line\">        idx_to_in_channels = [<span class=\"number\">64</span>, <span class=\"number\">128</span>, <span class=\"number\">128</span>, <span class=\"number\">128</span>, <span class=\"number\">128</span>]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">5</span>):</span><br><span class=\"line\">            <span class=\"comment\"># 即赋值语句self.blk_i=get_blk(i)</span></span><br><span class=\"line\">            <span class=\"built_in\">setattr</span>(self, <span class=\"string\">f&#x27;blk_<span class=\"subst\">&#123;i&#125;</span>&#x27;</span>, get_blk(i))</span><br><span class=\"line\">            <span class=\"built_in\">setattr</span>(self, <span class=\"string\">f&#x27;cls_<span class=\"subst\">&#123;i&#125;</span>&#x27;</span>, cls_predictor(idx_to_in_channels[i], num_anchors, num_classes))</span><br><span class=\"line\">            <span class=\"built_in\">setattr</span>(self, <span class=\"string\">f&#x27;bbox_<span class=\"subst\">&#123;i&#125;</span>&#x27;</span>, bbox_predictor(idx_to_in_channels[i], num_anchors))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        anchors, cls_preds, bbox_preds = [<span class=\"literal\">None</span>] * <span class=\"number\">5</span>, [<span class=\"literal\">None</span>] * <span class=\"number\">5</span>, [<span class=\"literal\">None</span>] * <span class=\"number\">5</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">5</span>):</span><br><span class=\"line\">            <span class=\"comment\"># getattr(self, &#x27;blk_%d&#x27;%i)即访问self.blk_i</span></span><br><span class=\"line\">            X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(</span><br><span class=\"line\">                X, <span class=\"built_in\">getattr</span>(self, <span class=\"string\">f&#x27;blk_<span class=\"subst\">&#123;i&#125;</span>&#x27;</span>), sizes[i], ratios[i],</span><br><span class=\"line\">                <span class=\"built_in\">getattr</span>(self, <span class=\"string\">f&#x27;cls_<span class=\"subst\">&#123;i&#125;</span>&#x27;</span>), <span class=\"built_in\">getattr</span>(self, <span class=\"string\">f&#x27;bbox_<span class=\"subst\">&#123;i&#125;</span>&#x27;</span>))</span><br><span class=\"line\">            <span class=\"comment\"># print(f&#x27;anchors[&#123;i&#125;], cls_preds[&#123;i&#125;], bbox_preds[&#123;i&#125;]:&#x27;, anchors[i].shape, cls_preds[i].shape, bbox_preds[i].shape)</span></span><br><span class=\"line\">        anchors = torch.cat(anchors, dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        cls_preds = concat_preds(cls_preds)</span><br><span class=\"line\">        cls_preds = cls_preds.reshape(cls_preds.shape[<span class=\"number\">0</span>], -<span class=\"number\">1</span>, self.num_classes + <span class=\"number\">1</span>)</span><br><span class=\"line\">        bbox_preds = concat_preds(bbox_preds)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> anchors, cls_preds, bbox_preds</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># f_map: (32, 3, 256, 256)-&gt;(32, 64, 32, 32)-&gt;(32, 128, 16, 16)-&gt;(32, 128, 8, 8)-&gt;(32, 128, 4, 4)-&gt;(32, 128, 1, 1)</span></span><br><span class=\"line\"><span class=\"comment\"># anchors[0], cls_preds[0], bbox_preds[0]: torch.Size([1, 4096, 4]) torch.Size([32, 8, 32, 32]) torch.Size([32, 16, 32, 32])</span></span><br><span class=\"line\"><span class=\"comment\"># anchors[1], cls_preds[1], bbox_preds[1]: torch.Size([1, 1024, 4]) torch.Size([32, 8, 16, 16]) torch.Size([32, 16, 16, 16])</span></span><br><span class=\"line\"><span class=\"comment\"># anchors[2], cls_preds[2], bbox_preds[2]: torch.Size([1, 256, 4]) torch.Size([32, 8, 8, 8]) torch.Size([32, 16, 8, 8])</span></span><br><span class=\"line\"><span class=\"comment\"># anchors[3], cls_preds[3], bbox_preds[3]: torch.Size([1, 64, 4]) torch.Size([32, 8, 4, 4]) torch.Size([32, 16, 4, 4])</span></span><br><span class=\"line\"><span class=\"comment\"># anchors[4], cls_preds[4], bbox_preds[4]: torch.Size([1, 4, 4]) torch.Size([32, 8, 1, 1]) torch.Size([32, 16, 1, 1])</span></span><br><span class=\"line\">net = TinySSD(num_classes=<span class=\"number\">1</span>)</span><br><span class=\"line\">X = torch.zeros((<span class=\"number\">32</span>, <span class=\"number\">3</span>, <span class=\"number\">256</span>, <span class=\"number\">256</span>))</span><br><span class=\"line\">anchors, cls_preds, bbox_preds = net(X)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;output anchors:&#x27;</span>, anchors.shape)  <span class=\"comment\"># output anchors: torch.Size([1, 5444, 4])</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;output class preds:&#x27;</span>, cls_preds.shape)  <span class=\"comment\"># output class preds: torch.Size([32, 5444, 2])</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;output bbox preds:&#x27;</span>, bbox_preds.shape)  <span class=\"comment\"># output bbox preds: torch.Size([32, 21776])</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"8-6-训练模型\">8.6 训练模型</h3>\n<p>首先读取数据集和设置超参数：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">device = torch.device(<span class=\"string\">&#x27;cuda&#x27;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\">lr, num_epochs, batch_size = <span class=\"number\">0.2</span>, <span class=\"number\">20</span>, <span class=\"number\">32</span></span><br><span class=\"line\">train_iter, valid_iter = d2l.load_data_bananas(batch_size)</span><br></pre></td></tr></table></figure>\n<p>然后定义损失函数和评价函数，目标检测有两种类型的损失。第一种有关<strong>锚框类别</strong>的损失：我们可以简单地复用之前图像分类问题里一直使用的交叉熵损失函数来计算；第二种有关<strong>正类锚框偏移量</strong>的损失：预测偏移量是一个回归问题。但是，对于这个回归问题，我们在这里不使用平方损失，而是使用 L1 范数损失，即预测值和真实值之差的绝对值。掩码变量 <code>bbox_masks</code> 令负类锚框和填充锚框不参与损失的计算。最后，我们将锚框类别和偏移量的损失相加，以获得模型的最终损失函数：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cls_loss = nn.CrossEntropyLoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">bbox_loss = nn.L1Loss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">calc_loss</span>(<span class=\"params\">cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks</span>):</span><br><span class=\"line\">    batch_size, num_classes = cls_preds.shape[<span class=\"number\">0</span>], cls_preds.shape[<span class=\"number\">2</span>]</span><br><span class=\"line\">    cls = cls_loss(cls_preds.reshape(-<span class=\"number\">1</span>, num_classes), cls_labels.reshape(-<span class=\"number\">1</span>)).reshape(batch_size, -<span class=\"number\">1</span>).mean(dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">    bbox = bbox_loss(bbox_preds * bbox_masks, bbox_labels * bbox_masks).mean(dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> cls + bbox</span><br></pre></td></tr></table></figure>\n<p>我们可以沿用准确率评价分类结果。由于偏移量使用了 L1 范数损失，我们使用平均绝对误差来（MAE）评价边界框的预测结果。这些预测结果是从生成的锚框及其预测偏移量中获得的：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">cls_eval</span>(<span class=\"params\">cls_preds, cls_labels</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 由于类别预测结果放在最后一维，argmax需要指定最后一维</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">float</span>((cls_preds.argmax(dim=-<span class=\"number\">1</span>).<span class=\"built_in\">type</span>(cls_labels.dtype) == cls_labels).<span class=\"built_in\">sum</span>())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">bbox_eval</span>(<span class=\"params\">bbox_preds, bbox_labels, bbox_masks</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">float</span>((torch.<span class=\"built_in\">abs</span>((bbox_labels - bbox_preds) * bbox_masks)).<span class=\"built_in\">sum</span>())</span><br></pre></td></tr></table></figure>\n<p>最后是训练模型，在训练模型时，我们需要在模型的前向传播过程中生成多尺度锚框 <code>anchors</code>，并预测其类别 <code>cls_preds</code> 和偏移量 <code>bbox_preds</code>。然后，我们根据标签信息 <code>label</code> 为生成的锚框标记类别 <code>cls_labels</code> 和偏移量 <code>bbox_labels</code>。最后，我们根据类别和偏移量的预测和标注值计算损失函数：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">net, train_iter, valid_iter, num_epochs, lr, device</span>):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">init_weights</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear <span class=\"keyword\">or</span> <span class=\"built_in\">type</span>(m) == nn.Conv2d:</span><br><span class=\"line\">            nn.init.xavier_uniform_(m.weight)</span><br><span class=\"line\">    net.apply(init_weights)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;training on&#x27;</span>, device)</span><br><span class=\"line\">    net.to(device)</span><br><span class=\"line\">    optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=<span class=\"number\">5e-4</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    writer = SummaryWriter(<span class=\"string\">&#x27;../logs/SSD_train_log&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    best_acc = <span class=\"number\">0.0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        net.train()</span><br><span class=\"line\">        train_loss, train_acc, train_bbox_err = [], [], []</span><br><span class=\"line\">        <span class=\"keyword\">for</span> feature, label <span class=\"keyword\">in</span> tqdm(train_iter):</span><br><span class=\"line\">            feature, label = feature.to(device), label.to(device)</span><br><span class=\"line\">            <span class=\"comment\"># 生成多尺度的锚框，为每个锚框预测类别和偏移量</span></span><br><span class=\"line\">            anchors, cls_preds, bbox_preds = net(feature)</span><br><span class=\"line\">            <span class=\"comment\"># 为每个锚框标注类别和偏移量</span></span><br><span class=\"line\">            bbox_labels, bbox_masks, cls_labels = d2l.multibox_target(anchors, label)</span><br><span class=\"line\">            <span class=\"comment\"># 根据类别和偏移量的预测和标注值计算损失函数</span></span><br><span class=\"line\">            loss = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks)</span><br><span class=\"line\"></span><br><span class=\"line\">            optimizer.zero_grad()</span><br><span class=\"line\">            loss.mean().backward()</span><br><span class=\"line\">            optimizer.step()</span><br><span class=\"line\"></span><br><span class=\"line\">            acc = cls_eval(cls_preds, cls_labels) / cls_labels.numel()</span><br><span class=\"line\">            bbox_mae = bbox_eval(bbox_preds, bbox_labels, bbox_masks) / bbox_labels.numel()</span><br><span class=\"line\"></span><br><span class=\"line\">            train_loss.append(loss.mean())</span><br><span class=\"line\">            train_acc.append(acc)</span><br><span class=\"line\">            train_bbox_err.append(bbox_mae)</span><br><span class=\"line\"></span><br><span class=\"line\">        train_loss = <span class=\"built_in\">sum</span>(train_loss) / <span class=\"built_in\">len</span>(train_loss)</span><br><span class=\"line\">        train_acc = <span class=\"built_in\">sum</span>(train_acc) / <span class=\"built_in\">len</span>(train_acc)</span><br><span class=\"line\">        train_bbox_err = <span class=\"built_in\">sum</span>(train_bbox_err) / <span class=\"built_in\">len</span>(train_bbox_err)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;[ Train | <span class=\"subst\">&#123;epoch + <span class=\"number\">1</span>:03d&#125;</span>/<span class=\"subst\">&#123;num_epochs:03d&#125;</span> ] loss = <span class=\"subst\">&#123;train_loss:<span class=\"number\">.5</span>f&#125;</span>, acc = <span class=\"subst\">&#123;train_acc:<span class=\"number\">.5</span>f&#125;</span>, bbox_err = <span class=\"subst\">&#123;train_bbox_err:<span class=\"number\">.5</span>f&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        net.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\">        valid_loss, valid_acc, valid_bbox_err = [], [], []</span><br><span class=\"line\">        <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">            <span class=\"keyword\">for</span> feature, label <span class=\"keyword\">in</span> tqdm(valid_iter):</span><br><span class=\"line\">                feature, label = feature.to(device), label.to(device)</span><br><span class=\"line\">                anchors, cls_preds, bbox_preds = net(feature)</span><br><span class=\"line\">                bbox_labels, bbox_masks, cls_labels = d2l.multibox_target(anchors, label)</span><br><span class=\"line\"></span><br><span class=\"line\">                loss = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks)</span><br><span class=\"line\">                acc = cls_eval(cls_preds, cls_labels) / cls_labels.numel()</span><br><span class=\"line\">                bbox_mae = bbox_eval(bbox_preds, bbox_labels, bbox_masks) / bbox_labels.numel()</span><br><span class=\"line\"></span><br><span class=\"line\">                valid_loss.append(loss.mean())</span><br><span class=\"line\">                valid_acc.append(acc)</span><br><span class=\"line\">                valid_bbox_err.append(bbox_mae)</span><br><span class=\"line\"></span><br><span class=\"line\">        valid_loss = <span class=\"built_in\">sum</span>(valid_loss) / <span class=\"built_in\">len</span>(valid_loss)</span><br><span class=\"line\">        valid_acc = <span class=\"built_in\">sum</span>(valid_acc) / <span class=\"built_in\">len</span>(valid_acc)</span><br><span class=\"line\">        valid_bbox_err = <span class=\"built_in\">sum</span>(valid_bbox_err) / <span class=\"built_in\">len</span>(valid_bbox_err)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;[ Valid | <span class=\"subst\">&#123;epoch + <span class=\"number\">1</span>:03d&#125;</span>/<span class=\"subst\">&#123;num_epochs:03d&#125;</span> ] loss = <span class=\"subst\">&#123;valid_loss:<span class=\"number\">.5</span>f&#125;</span>, acc = <span class=\"subst\">&#123;valid_acc:<span class=\"number\">.5</span>f&#125;</span>, bbox_err = <span class=\"subst\">&#123;valid_bbox_err:<span class=\"number\">.5</span>f&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        writer.add_scalars(<span class=\"string\">&#x27;train&#x27;</span>, &#123;<span class=\"string\">&#x27;loss&#x27;</span>: train_loss,</span><br><span class=\"line\">                                     <span class=\"string\">&#x27;acc&#x27;</span>: train_acc,</span><br><span class=\"line\">                                     <span class=\"string\">&#x27;bbox_err&#x27;</span>: train_bbox_err&#125;, epoch + <span class=\"number\">1</span>)</span><br><span class=\"line\">        writer.add_scalars(<span class=\"string\">&#x27;valid&#x27;</span>, &#123;<span class=\"string\">&#x27;loss&#x27;</span>: valid_loss,</span><br><span class=\"line\">                                     <span class=\"string\">&#x27;acc&#x27;</span>: valid_acc,</span><br><span class=\"line\">                                     <span class=\"string\">&#x27;bbox_err&#x27;</span>: valid_bbox_err&#125;, epoch + <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> valid_acc &gt; best_acc:</span><br><span class=\"line\">            best_acc = valid_acc</span><br><span class=\"line\">            torch.save(net.state_dict(), <span class=\"string\">&#x27;../save/SSD_train.params&#x27;</span>)</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;saving model with acc &#123;:.3f&#125;&#x27;</span>.<span class=\"built_in\">format</span>(best_acc))</span><br><span class=\"line\"></span><br><span class=\"line\">    writer.close()</span><br><span class=\"line\"></span><br><span class=\"line\">train(net, train_iter, valid_iter, num_epochs, lr, device)</span><br></pre></td></tr></table></figure>\n<h3 id=\"8-7-预测目标\">8.7 预测目标</h3>\n<p>在预测阶段，我们希望能把图像里面所有我们感兴趣的目标检测出来。在下面，我们读取并调整测试图像的大小，然后将其转成卷积层需要的四维格式。使用 <code>multibox_detection</code> 函数，我们可以根据锚框及其预测偏移量得到预测边界框，然后通过非极大值抑制来移除相似的预测边界框。最后，我们筛选所有置信度不低于0.9的边界框，做为最终输出：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torchvision.io.read_image(<span class=\"string\">&#x27;../images/banana.jpg&#x27;</span>).unsqueeze(<span class=\"number\">0</span>).<span class=\"built_in\">float</span>()  <span class=\"comment\"># 将其转成卷积层需要的四维格式</span></span><br><span class=\"line\">img = X.squeeze(<span class=\"number\">0</span>).permute(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">0</span>).long()  <span class=\"comment\"># (h, w, c)</span></span><br><span class=\"line\">net.to(device)</span><br><span class=\"line\">net.load_state_dict(torch.load(<span class=\"string\">&#x27;../save/SSD_train.params&#x27;</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">x, net, device</span>):</span><br><span class=\"line\">    net.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\">    anchors, cls_preds, bbox_preds = net(X.to(device))</span><br><span class=\"line\">    cls_probs = F.softmax(cls_preds, dim=<span class=\"number\">2</span>).permute(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">    output = d2l.multibox_detection(cls_probs, bbox_preds, anchors)</span><br><span class=\"line\">    idx = [i <span class=\"keyword\">for</span> i, row <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(output[<span class=\"number\">0</span>]) <span class=\"keyword\">if</span> row[<span class=\"number\">0</span>] != -<span class=\"number\">1</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> output[<span class=\"number\">0</span>, idx]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">display</span>(<span class=\"params\">img, output, threshold</span>):</span><br><span class=\"line\">    plt.figure(dpi=<span class=\"number\">100</span>)</span><br><span class=\"line\">    fig = plt.imshow(img)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> output:</span><br><span class=\"line\">        score = <span class=\"built_in\">float</span>(row[<span class=\"number\">1</span>])</span><br><span class=\"line\">        <span class=\"keyword\">if</span> score &lt; threshold:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        h, w = img.shape[<span class=\"number\">0</span>:<span class=\"number\">2</span>]</span><br><span class=\"line\">        bbox = [row[<span class=\"number\">2</span>:<span class=\"number\">6</span>] * torch.tensor((w, h, w, h), device=row.device)]</span><br><span class=\"line\">        d2l.show_bboxes(fig.axes, bbox, <span class=\"string\">&#x27;%.2f&#x27;</span> % score, <span class=\"string\">&#x27;w&#x27;</span>)</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\">output = predict(X, net, device)</span><br><span class=\"line\">display(img, output.cpu(), threshold=<span class=\"number\">0.9</span>)</span><br></pre></td></tr></table></figure>\n<h2 id=\"9-语义分割和数据集\">9. 语义分割和数据集</h2>\n<p>在前几节中讨论的目标检测问题中，我们一直使用方形边界框来标注和预测图像中的目标。本节将探讨<strong>语义分割</strong>（semantic segmentation）问题，它重点关注于如何将图像分割成属于不同语义类别的区域。与目标检测不同，语义分割可以识别并理解图像中<strong>每一个像素</strong>的内容：其语义区域的标注和预测是像素级的。与目标检测相比，语义分割标注的像素级的边框显然更加精细。</p>\n<h3 id=\"9-1-图像分割和实例分割\">9.1 图像分割和实例分割</h3>\n<p>计算机视觉领域还有2个与语义分割相似的重要问题，即<strong>图像分割</strong>（image segmentation）和<strong>实例分割</strong>（instance segmentation）。我们在这里将它们同语义分割简单区分一下：</p>\n<ul>\n<li>图像分割将图像划分为若干组成区域，这类问题的方法通常利用图像中像素之间的相关性。它在训练时不需要有关图像像素的标签信息，在预测时也无法保证分割出的区域具有我们希望得到的语义。以图像 <code>catdog.jpg</code> 作为输入，图像分割可能会将狗分为两个区域：一个覆盖以黑色为主的嘴和眼睛，另一个覆盖以黄色为主的其余部分身体。</li>\n<li>实例分割也叫同时检测并分割（simultaneous detection and segmentation），它研究如何识别图像中各个目标实例的像素级区域。与语义分割不同，实例分割不仅需要区分语义，还要<strong>区分不同的目标实例</strong>。例如，如果图像中有两条狗，则实例分割需要区分像素属于的两条狗中的哪一条。</li>\n</ul>\n<h3 id=\"9-2-Pascal-VOC2012-语义分割数据集\">9.2 Pascal VOC2012 语义分割数据集</h3>\n<p>最重要的语义分割数据集之一是 Pascal VOC2012，下面我们深入了解一下这个数据集：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> Dataset, DataLoader</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">d2l.DATA_HUB[<span class=\"string\">&#x27;voc2012&#x27;</span>] = (d2l.DATA_URL + <span class=\"string\">&#x27;VOCtrainval_11-May-2012.tar&#x27;</span>, <span class=\"string\">&#x27;4e443f8a2eca6b1dac8a6c57641b67dd40621a49&#x27;</span>)</span><br><span class=\"line\">voc_dir = d2l.download_extract(<span class=\"string\">&#x27;voc2012&#x27;</span>, <span class=\"string\">&#x27;VOCdevkit/VOC2012&#x27;</span>)  <span class=\"comment\"># 提取出的数据集位于../data/VOCdevkit/VOC2012</span></span><br></pre></td></tr></table></figure>\n<p>进入路径 <code>../data/VOCdevkit/VOC2012</code> 之后，我们可以看到数据集的不同组件。<code>ImageSets/Segmentation</code> 路径包含用于训练和测试样本的文本文件，而 <code>JPEGImages</code> 和 <code>SegmentationClass</code> 路径分别存储着每个示例的输入图像和标签。此处的标签也采用图像格式，其尺寸和它所标注的输入图像的尺寸相同。此外，标签中颜色相同的像素属于同一个语义类别。下面将 <code>read_voc_images</code> 函数定义为将所有输入的图像和标签读入内存：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">read_voc_images</span>(<span class=\"params\">voc_dir, is_train=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;读取所有VOC图像并标注&quot;&quot;&quot;</span></span><br><span class=\"line\">    txt_fname = os.path.join(voc_dir, <span class=\"string\">&#x27;ImageSets&#x27;</span>, <span class=\"string\">&#x27;Segmentation&#x27;</span>, <span class=\"string\">&#x27;train.txt&#x27;</span> <span class=\"keyword\">if</span> is_train <span class=\"keyword\">else</span> <span class=\"string\">&#x27;val.txt&#x27;</span>)</span><br><span class=\"line\">    mode = torchvision.io.image.ImageReadMode.RGB</span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(txt_fname, <span class=\"string\">&#x27;r&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        images = f.read().split()</span><br><span class=\"line\">    features, labels = [], []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, fname <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(images):</span><br><span class=\"line\">        features.append(torchvision.io.read_image(os.path.join(voc_dir, <span class=\"string\">&#x27;JPEGImages&#x27;</span>, <span class=\"string\">f&#x27;<span class=\"subst\">&#123;fname&#125;</span>.jpg&#x27;</span>)))</span><br><span class=\"line\">        labels.append(torchvision.io.read_image(os.path.join(voc_dir, <span class=\"string\">&#x27;SegmentationClass&#x27;</span> , <span class=\"string\">f&#x27;<span class=\"subst\">&#123;fname&#125;</span>.png&#x27;</span>), mode))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> features, labels</span><br><span class=\"line\"></span><br><span class=\"line\">train_features, train_labels = read_voc_images(voc_dir, <span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">len</span>(train_features))  <span class=\"comment\"># 1464</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(train_features[<span class=\"number\">0</span>].shape, train_labels[<span class=\"number\">0</span>].shape)  <span class=\"comment\"># torch.Size([3, 281, 500]) torch.Size([3, 281, 500])</span></span><br></pre></td></tr></table></figure>\n<p>下面我们绘制前5个输入图像及其标签。在标签图像中，白色和黑色分别表示边框和背景，而其他颜色则对应不同的类别：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">imgs = train_features[<span class=\"number\">0</span>:<span class=\"number\">5</span>] + train_labels[<span class=\"number\">0</span>:<span class=\"number\">5</span>]</span><br><span class=\"line\">imgs = [img.permute(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">0</span>) <span class=\"keyword\">for</span> img <span class=\"keyword\">in</span> imgs]  <span class=\"comment\"># 将通道放到最后一维</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">len</span>(imgs), imgs[<span class=\"number\">0</span>].shape)  <span class=\"comment\"># 10 torch.Size([281, 500, 3])</span></span><br><span class=\"line\">d2l.show_images(imgs, <span class=\"number\">2</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>接下来，我们列举 RGB 颜色值和类名：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">VOC_COLORMAP = [[<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>], [<span class=\"number\">128</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>], [<span class=\"number\">0</span>, <span class=\"number\">128</span>, <span class=\"number\">0</span>], [<span class=\"number\">128</span>, <span class=\"number\">128</span>, <span class=\"number\">0</span>], [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">128</span>], [<span class=\"number\">128</span>, <span class=\"number\">0</span>, <span class=\"number\">128</span>],</span><br><span class=\"line\">                [<span class=\"number\">0</span>, <span class=\"number\">128</span>, <span class=\"number\">128</span>], [<span class=\"number\">128</span>, <span class=\"number\">128</span>, <span class=\"number\">128</span>], [<span class=\"number\">64</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>], [<span class=\"number\">192</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>], [<span class=\"number\">64</span>, <span class=\"number\">128</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">                [<span class=\"number\">192</span>, <span class=\"number\">128</span>, <span class=\"number\">0</span>], [<span class=\"number\">64</span>, <span class=\"number\">0</span>, <span class=\"number\">128</span>], [<span class=\"number\">192</span>, <span class=\"number\">0</span>, <span class=\"number\">128</span>], [<span class=\"number\">64</span>, <span class=\"number\">128</span>, <span class=\"number\">128</span>], [<span class=\"number\">192</span>, <span class=\"number\">128</span>, <span class=\"number\">128</span>],</span><br><span class=\"line\">                [<span class=\"number\">0</span>, <span class=\"number\">64</span>, <span class=\"number\">0</span>], [<span class=\"number\">128</span>, <span class=\"number\">64</span>, <span class=\"number\">0</span>], [<span class=\"number\">0</span>, <span class=\"number\">192</span>, <span class=\"number\">0</span>], [<span class=\"number\">128</span>, <span class=\"number\">192</span>, <span class=\"number\">0</span>], [<span class=\"number\">0</span>, <span class=\"number\">64</span>, <span class=\"number\">128</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\">VOC_CLASSES = [<span class=\"string\">&#x27;background&#x27;</span>, <span class=\"string\">&#x27;aeroplane&#x27;</span>, <span class=\"string\">&#x27;bicycle&#x27;</span>, <span class=\"string\">&#x27;bird&#x27;</span>, <span class=\"string\">&#x27;boat&#x27;</span>, <span class=\"string\">&#x27;bottle&#x27;</span>, <span class=\"string\">&#x27;bus&#x27;</span>,</span><br><span class=\"line\">               <span class=\"string\">&#x27;car&#x27;</span>, <span class=\"string\">&#x27;cat&#x27;</span>, <span class=\"string\">&#x27;chair&#x27;</span>, <span class=\"string\">&#x27;cow&#x27;</span>, <span class=\"string\">&#x27;diningtable&#x27;</span>, <span class=\"string\">&#x27;dog&#x27;</span>, <span class=\"string\">&#x27;horse&#x27;</span>, <span class=\"string\">&#x27;motorbike&#x27;</span>,</span><br><span class=\"line\">               <span class=\"string\">&#x27;person&#x27;</span>, <span class=\"string\">&#x27;potted plant&#x27;</span>, <span class=\"string\">&#x27;sheep&#x27;</span>, <span class=\"string\">&#x27;sofa&#x27;</span>, <span class=\"string\">&#x27;train&#x27;</span>, <span class=\"string\">&#x27;tv/monitor&#x27;</span>]</span><br></pre></td></tr></table></figure>\n<p>通过上面定义的两个常量，我们可以方便地查找标签中每个像素的类索引。我们定义了 <code>voc_colormap2label</code> 函数来构建从上述 RGB 颜色值到类别索引的映射，而 <code>voc_label_indices</code> 函数将 RGB 值映射到在 Pascal VOC2012 数据集中的类别索引：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">voc_colormap2label</span>():</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;构建从RGB到VOC类别索引的映射&quot;&quot;&quot;</span></span><br><span class=\"line\">    colormap2label = torch.zeros(<span class=\"number\">256</span> ** <span class=\"number\">3</span>, dtype=torch.long)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, colormap <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(VOC_COLORMAP):</span><br><span class=\"line\">        colormap2label[(colormap[<span class=\"number\">0</span>] * <span class=\"number\">256</span> + colormap[<span class=\"number\">1</span>]) * <span class=\"number\">256</span> + colormap[<span class=\"number\">2</span>]] = i  <span class=\"comment\"># 哈希映射</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> colormap2label</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">voc_label_indices</span>(<span class=\"params\">colormap, colormap2label</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;将VOC标签中的RGB值映射到它们的类别索引&quot;&quot;&quot;</span></span><br><span class=\"line\">    colormap = colormap.permute(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">0</span>).numpy().astype(<span class=\"string\">&#x27;int32&#x27;</span>)</span><br><span class=\"line\">    idx = ((colormap[:, :, <span class=\"number\">0</span>] * <span class=\"number\">256</span> + colormap[:, :, <span class=\"number\">1</span>]) * <span class=\"number\">256</span> + colormap[:, :, <span class=\"number\">2</span>])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> colormap2label[idx]</span><br></pre></td></tr></table></figure>\n<p>例如，在第一张样本图像中，飞机头部区域的类别索引为1，而背景索引为0：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">colormap2label = voc_colormap2label()</span><br><span class=\"line\">y = voc_label_indices(train_labels[<span class=\"number\">0</span>], colormap2label)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(y[<span class=\"number\">105</span>:<span class=\"number\">115</span>, <span class=\"number\">130</span>:<span class=\"number\">140</span>], VOC_CLASSES[<span class=\"number\">1</span>])</span><br><span class=\"line\"><span class=\"comment\"># tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],</span></span><br><span class=\"line\"><span class=\"comment\">#         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],</span></span><br><span class=\"line\"><span class=\"comment\">#         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],</span></span><br><span class=\"line\"><span class=\"comment\">#         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span></span><br><span class=\"line\"><span class=\"comment\">#         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span></span><br><span class=\"line\"><span class=\"comment\">#         [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],</span></span><br><span class=\"line\"><span class=\"comment\">#         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span></span><br><span class=\"line\"><span class=\"comment\">#         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span></span><br><span class=\"line\"><span class=\"comment\">#         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],</span></span><br><span class=\"line\"><span class=\"comment\">#         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]) aeroplane</span></span><br></pre></td></tr></table></figure>\n<p>之前的实验我们通过再缩放图像使其符合模型的输入形状。然而在语义分割中，这样做需要将预测的像素类别重新映射回原始尺寸的输入图像。这样的映射可能不够精确，尤其在不同语义的分割区域。为了避免这个问题，我们将图像<strong>裁剪为固定尺寸</strong>，而不是再缩放。具体来说，我们使用图像增广中的随机裁剪，裁剪输入图像和标签的相同区域：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">voc_rand_crop</span>(<span class=\"params\">feature, label, height, width</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;随机裁剪特征和标签图像&quot;&quot;&quot;</span></span><br><span class=\"line\">    rect = torchvision.transforms.RandomCrop.get_params(feature, (height, width))</span><br><span class=\"line\">    feature = torchvision.transforms.functional.crop(feature, *rect)</span><br><span class=\"line\">    label = torchvision.transforms.functional.crop(label, *rect)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> feature, label</span><br><span class=\"line\"></span><br><span class=\"line\">imgs = []</span><br><span class=\"line\"><span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">5</span>):</span><br><span class=\"line\">    imgs += voc_rand_crop(train_features[<span class=\"number\">0</span>], train_labels[<span class=\"number\">0</span>], <span class=\"number\">200</span>, <span class=\"number\">300</span>)</span><br><span class=\"line\">imgs = [img.permute(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">0</span>) <span class=\"keyword\">for</span> img <span class=\"keyword\">in</span> imgs]</span><br><span class=\"line\">d2l.show_images(imgs[::<span class=\"number\">2</span>] + imgs[<span class=\"number\">1</span>::<span class=\"number\">2</span>], <span class=\"number\">2</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>我们通过继承高级 API 提供的 <code>Dataset</code> 类，自定义了一个语义分割数据集类 <code>VOCSegDataset</code>。通过实现 <code>__getitem__</code> 函数，我们可以任意访问数据集中索引为 <code>idx</code> 的输入图像及其每个像素的类别索引。由于数据集中有些图像的尺寸可能小于随机裁剪所指定的输出尺寸，这些样本可以通过自定义的 <code>filter</code> 函数移除掉。此外，我们还定义了 <code>normalize_image</code> 函数，从而对输入图像的 RGB 三个通道的值分别做标准化：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">VOCSegDataset</span>(<span class=\"title class_ inherited__\">Dataset</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;一个用于加载VOC数据集的自定义数据集&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, is_train, crop_size, voc_dir</span>):</span><br><span class=\"line\">        self.transform = torchvision.transforms.Normalize(mean=[<span class=\"number\">0.485</span>, <span class=\"number\">0.456</span>, <span class=\"number\">0.406</span>], std=[<span class=\"number\">0.229</span>, <span class=\"number\">0.224</span>, <span class=\"number\">0.225</span>])</span><br><span class=\"line\">        self.crop_size = crop_size</span><br><span class=\"line\">        features, labels = read_voc_images(voc_dir, is_train=is_train)</span><br><span class=\"line\">        self.features = [self.normalize_image(feature) <span class=\"keyword\">for</span> feature <span class=\"keyword\">in</span> self.<span class=\"built_in\">filter</span>(features)]</span><br><span class=\"line\">        self.labels = self.<span class=\"built_in\">filter</span>(labels)</span><br><span class=\"line\">        self.colormap2label = voc_colormap2label()</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;read &#x27;</span> + <span class=\"built_in\">str</span>(<span class=\"built_in\">len</span>(self.features)) + <span class=\"string\">&#x27; examples&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">normalize_image</span>(<span class=\"params\">self, img</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.transform(img.<span class=\"built_in\">float</span>() / <span class=\"number\">255</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 过滤掉比裁切大小还小的图像</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">filter</span>(<span class=\"params\">self, imgs</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> [img <span class=\"keyword\">for</span> img <span class=\"keyword\">in</span> imgs <span class=\"keyword\">if</span> (img.shape[<span class=\"number\">1</span>] &gt;= self.crop_size[<span class=\"number\">0</span>] <span class=\"keyword\">and</span></span><br><span class=\"line\">                                        img.shape[<span class=\"number\">2</span>] &gt;= self.crop_size[<span class=\"number\">1</span>])]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__getitem__</span>(<span class=\"params\">self, idx</span>):</span><br><span class=\"line\">        feature, label = voc_rand_crop(self.features[idx], self.labels[idx], *self.crop_size)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (feature, voc_label_indices(label, self.colormap2label))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__len__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">len</span>(self.features)</span><br><span class=\"line\"></span><br><span class=\"line\">crop_size = (<span class=\"number\">320</span>, <span class=\"number\">480</span>)</span><br><span class=\"line\">voc_train, voc_valid = VOCSegDataset(<span class=\"literal\">True</span>, crop_size, voc_dir), VOCSegDataset(<span class=\"literal\">False</span>, crop_size, voc_dir)</span><br></pre></td></tr></table></figure>\n<p>最后，我们定义以下 <code>load_data_voc</code> 函数来下载并读取 Pascal VOC2012 语义分割数据集。它返回训练集和测试集的数据迭代器：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">load_data_voc</span>(<span class=\"params\">batch_size</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;加载VOC语义分割数据集&quot;&quot;&quot;</span></span><br><span class=\"line\">    train_iter = DataLoader(voc_train, batch_size, shuffle=<span class=\"literal\">True</span>, drop_last=<span class=\"literal\">True</span>, num_workers=<span class=\"number\">0</span>)</span><br><span class=\"line\">    valid_iter = DataLoader(voc_valid, batch_size, drop_last=<span class=\"literal\">True</span>, num_workers=<span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> train_iter, valid_iter</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">64</span></span><br><span class=\"line\">train_iter, valid_iter = load_data_voc(batch_size)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> features, labels <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(features.shape)  <span class=\"comment\"># torch.Size([64, 3, 320, 480])</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(labels.shape)  <span class=\"comment\"># torch.Size([64, 320, 480])</span></span><br><span class=\"line\">    <span class=\"keyword\">break</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"10-转置卷积\">10. 转置卷积</h2>\n",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/21165.html",
            "url": "https://asanosaki.github.io/posts/21165.html",
            "title": "动手学深度学习笔记(李沐)-现代卷积神经网络",
            "date_published": "2023-03-03T01:57:00.000Z",
            "content_html": "<blockquote>\n<p>李沐动手学深度学习（PyTorch）课程学习笔记第六章：现代卷积神经网络。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-深度卷积神经网络（AlexNet）\">1. 深度卷积神经网络（AlexNet）</h2>\n<p>AlexNet 和 LeNet 的设计理念非常相似，但也存在显著差异：</p>\n<ul>\n<li>AlexNet 比相对较小的 LeNet5 要深得多。AlexNet 由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。</li>\n<li>AlexNet 使用 ReLU 而不是 Sigmoid 作为其激活函数。</li>\n</ul>\n<p>此外，AlexNet 将 Sigmoid 激活函数改为更简单的 ReLU 激活函数。一方面，ReLU 激活函数的计算更简单，它不需要如 Sigmoid 激活函数那般复杂的求幂运算。另一方面，当使用不同的参数初始化方法时，ReLU 激活函数使训练模型更加容易。当 Sigmoid 激活函数的输出非常接近于0或1时，这些区域的梯度几乎为0，因此反向传播无法继续更新一些模型参数。相反，ReLU 激活函数在正区间的梯度总是1。因此，如果模型参数没有正确初始化，Sigmoid 函数可能在正区间内得到几乎为0的梯度，从而使模型无法得到有效的训练。</p>\n<p>尽管原文中 AlexNet 是在 ImageNet 上进行训练的，但本文在这里使用的是 Fashion-MNIST 数据集。因为即使在现代 GPU 上，训练 ImageNet 模型，同时使其收敛可能需要数小时或数天的时间。将 AlexNet 直接应用于 Fashion-MNIST 的一个问题是 Fashion-MNIST 图像的分辨率（28×28像素）低于 ImageNet 图像。为了解决这个问题，我们将它们增加到224×224像素（通常来讲这不是一个明智的做法，但在这里这样做是为了有效使用 AlexNet 架构）。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils <span class=\"keyword\">import</span> data</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms</span><br><span class=\"line\"><span class=\"keyword\">from</span> util.functions <span class=\"keyword\">import</span> train_classifier</span><br><span class=\"line\"></span><br><span class=\"line\">net = nn.Sequential(</span><br><span class=\"line\">    <span class=\"comment\"># 这里使用11*11的更大窗口来捕捉对象，同时步幅为4，以减少输出的高度和宽度，此外输出通道的数目远大于LeNet</span></span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">96</span>, kernel_size=<span class=\"number\">11</span>, stride=<span class=\"number\">4</span>, padding=<span class=\"number\">1</span>), nn.ReLU(),  <span class=\"comment\"># (96, 54, 54)</span></span><br><span class=\"line\">    nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>),  <span class=\"comment\"># (96, 26, 26)</span></span><br><span class=\"line\">    <span class=\"comment\"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">96</span>, <span class=\"number\">256</span>, kernel_size=<span class=\"number\">5</span>, padding=<span class=\"number\">2</span>), nn.ReLU(),  <span class=\"comment\"># (256, 26, 26)</span></span><br><span class=\"line\">    nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>),  <span class=\"comment\"># (256, 12, 12)</span></span><br><span class=\"line\">    <span class=\"comment\"># 使用三个连续的卷积层和较小的卷积窗口，除了最后的卷积层，输出通道的数量进一步增加</span></span><br><span class=\"line\">    <span class=\"comment\"># 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度</span></span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">256</span>, <span class=\"number\">384</span>, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>), nn.ReLU(),  <span class=\"comment\"># (384, 12, 12)</span></span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">384</span>, <span class=\"number\">384</span>, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>), nn.ReLU(),  <span class=\"comment\"># (384, 12, 12)</span></span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">384</span>, <span class=\"number\">256</span>, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>), nn.ReLU(),  <span class=\"comment\"># (256, 12, 12)</span></span><br><span class=\"line\">    nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>),  <span class=\"comment\"># (256, 5, 5)</span></span><br><span class=\"line\">    nn.Flatten(),</span><br><span class=\"line\">    <span class=\"comment\"># 这里全连接层的输出数量是LeNet中的好几倍，因此使用Dropout层来减轻过拟合</span></span><br><span class=\"line\">    nn.Linear(<span class=\"number\">6400</span>, <span class=\"number\">4096</span>), nn.ReLU(),</span><br><span class=\"line\">    nn.Dropout(p=<span class=\"number\">0.5</span>),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">4096</span>, <span class=\"number\">4096</span>), nn.ReLU(),</span><br><span class=\"line\">    nn.Dropout(p=<span class=\"number\">0.5</span>),</span><br><span class=\"line\">    <span class=\"comment\"># 最后是输出层，由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class=\"line\">    nn.Linear(<span class=\"number\">4096</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">128</span></span><br><span class=\"line\">trans = transforms.Compose([transforms.Resize((<span class=\"number\">224</span>, <span class=\"number\">224</span>)), transforms.ToTensor()])</span><br><span class=\"line\">mnist_train = torchvision.datasets.FashionMNIST(root=<span class=\"string\">&quot;../data&quot;</span>, train=<span class=\"literal\">True</span>, transform=trans, download=<span class=\"literal\">True</span>)</span><br><span class=\"line\">mnist_test = torchvision.datasets.FashionMNIST(root=<span class=\"string\">&quot;../data&quot;</span>, train=<span class=\"literal\">False</span>, transform=trans, download=<span class=\"literal\">True</span>)</span><br><span class=\"line\">train_iter = data.DataLoader(mnist_train, batch_size, shuffle=<span class=\"literal\">True</span>, num_workers=<span class=\"number\">0</span>)</span><br><span class=\"line\">test_iter = data.DataLoader(mnist_test, batch_size, shuffle=<span class=\"literal\">True</span>, num_workers=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">device = torch.device(<span class=\"string\">&#x27;cuda&#x27;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\">lr, num_epochs = <span class=\"number\">0.01</span>, <span class=\"number\">50</span></span><br><span class=\"line\"></span><br><span class=\"line\">train_classifier(net, train_iter, test_iter, num_epochs, lr, device, <span class=\"string\">&#x27;../logs/AlexNet_train_log&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-使用块的网络（VGG）\">2. 使用块的网络（VGG）</h2>\n<p>虽然 AlexNet 证明深层神经网络卓有成效，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络。</p>\n<p>经典卷积神经网络的基本组成部分是下面的这个序列：</p>\n<ul>\n<li>带填充以保持分辨率的卷积层。</li>\n<li>非线性激活函数，如 ReLU。</li>\n<li>汇聚层，如最大汇聚层。</li>\n</ul>\n<p>而一个 VGG 块与之类似，由一系列卷积层组成，后面再加上用于空间下采样的最大汇聚层。</p>\n<p>VGG 使用<strong>可重复使用</strong>的卷积块来构建深度卷积神经网络，不同的卷积块个数和超参数可以得到不同复杂度的变种。</p>\n<p>下面的代码中，我们定义了一个名为 <code>vgg_block</code> 的函数来实现一个 VGG 块，该函数有三个参数，分别对应于卷积层的数量 <code>num_convs</code>、输入通道的数量 <code>in_channels</code> 和输出通道的数量 <code>out_channels</code>：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">vgg_block</span>(<span class=\"params\">num_convs, in_channels, out_channels</span>):</span><br><span class=\"line\">    layers = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_convs):</span><br><span class=\"line\">        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>))</span><br><span class=\"line\">        layers.append(nn.ReLU())</span><br><span class=\"line\">        in_channels = out_channels</span><br><span class=\"line\">    layers.append(nn.MaxPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure>\n<p>原始 VGG 网络有5个卷积块，其中前两个块各有一个卷积层，后三个块各包含两个卷积层。第一个模块有64个输出通道，每个后续模块将输出通道数量翻倍，直到达到512。由于该网络使用8个卷积层和3个全连接层，因此它通常被称为 VGG-11。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">vgg</span>(<span class=\"params\">conv_arch</span>):</span><br><span class=\"line\">    conv_blks = []</span><br><span class=\"line\">    in_channels = <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"comment\"># 卷积层部分</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (num_convs, out_channels) <span class=\"keyword\">in</span> conv_arch:</span><br><span class=\"line\">        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))</span><br><span class=\"line\">        in_channels = out_channels</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(</span><br><span class=\"line\">        *conv_blks, nn.Flatten(),</span><br><span class=\"line\">        <span class=\"comment\"># 全连接层部分</span></span><br><span class=\"line\">        nn.Linear(out_channels * <span class=\"number\">7</span> * <span class=\"number\">7</span>, <span class=\"number\">4096</span>), nn.ReLU(), nn.Dropout(<span class=\"number\">0.5</span>),</span><br><span class=\"line\">        nn.Linear(<span class=\"number\">4096</span>, <span class=\"number\">4096</span>), nn.ReLU(), nn.Dropout(<span class=\"number\">0.5</span>),</span><br><span class=\"line\">        nn.Linear(<span class=\"number\">4096</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">conv_arch = ((<span class=\"number\">1</span>, <span class=\"number\">64</span>), (<span class=\"number\">1</span>, <span class=\"number\">128</span>), (<span class=\"number\">2</span>, <span class=\"number\">256</span>), (<span class=\"number\">2</span>, <span class=\"number\">512</span>), (<span class=\"number\">2</span>, <span class=\"number\">512</span>))</span><br><span class=\"line\">net = vgg(conv_arch)</span><br></pre></td></tr></table></figure>\n<p>由于 VGG-11 比 AlexNet 计算量更大，因此我们构建了一个通道数较少的网络，足够用于训练 Fashion-MNIST 数据集：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conv_arch = ((<span class=\"number\">1</span>, <span class=\"number\">16</span>), (<span class=\"number\">1</span>, <span class=\"number\">32</span>), (<span class=\"number\">2</span>, <span class=\"number\">64</span>), (<span class=\"number\">2</span>, <span class=\"number\">128</span>), (<span class=\"number\">2</span>, <span class=\"number\">128</span>))</span><br><span class=\"line\">net = vgg(conv_arch)</span><br></pre></td></tr></table></figure>\n<p>最后我们读取数据集并进行训练，超参数设置：<code>lr, num_epochs = 0.02, 15</code>，训练过程与第一节内容一样，因此不再放出代码。</p>\n<p>PS：如果显存不够可以减小 <code>batch_size</code>，从128改为64或32。</p>\n<h2 id=\"3-网络中的网络（NiN）\">3. 网络中的网络（NiN）</h2>\n<p>回想一下，卷积层的输入和输出由四维张量组成，张量的每个轴分别对应样本、通道、高度和宽度。另外，全连接层的输入和输出通常是分别对应于样本和特征的二维张量。NiN 的想法是<strong>在每个像素位置（针对每个高度和宽度）应用一个全连接层</strong>。如果我们将权重连接到每个空间位置，我们可以将其视为1×1卷积层（如第五章第四节 PS 中所述），或作为在每个像素位置上独立作用的全连接层。从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征（feature）。</p>\n<p>NiN 块以一个普通卷积层开始，后面是两个1×1的卷积层。这两个1×1卷积层充当带有 ReLU 激活函数的逐像素全连接层，对每个像素增加了非线性特性：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">nin_block</span>(<span class=\"params\">in_channels, out_channels, kernel_size, strides, padding</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(</span><br><span class=\"line\">        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding), nn.ReLU(),</span><br><span class=\"line\">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class=\"number\">1</span>), nn.ReLU(),</span><br><span class=\"line\">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class=\"number\">1</span>), nn.ReLU())</span><br></pre></td></tr></table></figure>\n<p>NiN 和 AlexNet 之间的一个显著区别是 NiN 完全取消了全连接层。相反，NiN 使用一个 NiN 块，其输出通道数等于标签类别的数量。最后放一个全局平均汇聚层（global average pooling layer），生成一个对数几率（logits）。NiN 设计的一个优点是，它显著减少了模型所需参数的数量。然而，在实践中，这种设计有时会增加训练模型的时间。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(</span><br><span class=\"line\">    nin_block(<span class=\"number\">1</span>, <span class=\"number\">96</span>, kernel_size=<span class=\"number\">11</span>, strides=<span class=\"number\">4</span>, padding=<span class=\"number\">0</span>),</span><br><span class=\"line\">    nn.MaxPool2d(<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">    nin_block(<span class=\"number\">96</span>, <span class=\"number\">256</span>, kernel_size=<span class=\"number\">5</span>, strides=<span class=\"number\">1</span>, padding=<span class=\"number\">2</span>),</span><br><span class=\"line\">    nn.MaxPool2d(<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">    nin_block(<span class=\"number\">256</span>, <span class=\"number\">384</span>, kernel_size=<span class=\"number\">3</span>, strides=<span class=\"number\">1</span>, padding=<span class=\"number\">1</span>),</span><br><span class=\"line\">    nn.MaxPool2d(<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">    nn.Dropout(<span class=\"number\">0.5</span>),</span><br><span class=\"line\">    nin_block(<span class=\"number\">384</span>, <span class=\"number\">10</span>, kernel_size=<span class=\"number\">3</span>, strides=<span class=\"number\">1</span>, padding=<span class=\"number\">1</span>),  <span class=\"comment\"># 标签类别数是10</span></span><br><span class=\"line\">    nn.AdaptiveAvgPool2d((<span class=\"number\">1</span>, <span class=\"number\">1</span>)),  <span class=\"comment\"># 将四维的输出转成二维的输出，其形状为(批量大小,10)</span></span><br><span class=\"line\">    nn.Flatten())</span><br></pre></td></tr></table></figure>\n<p>最后我们读取数据集并进行训练，超参数设置：<code>lr, num_epochs = 0.1, 15</code>，训练过程与第一节内容一样，因此不再放出代码。</p>\n<h2 id=\"4-含并行连结的网络（GoogLeNet）\">4. 含并行连结的网络（GoogLeNet）</h2>\n<p>在 GoogLeNet 中，基本的卷积块被称为 Inception 块（Inception block）。Inception 块由四条并行路径组成。前三条路径使用窗口大小为1×1、3×3和5×5的卷积层，从不同空间大小中提取信息。中间的两条路径在输入上执行1×1卷积，以减少通道数，从而降低模型的复杂性。第四条路径使用3×3最大汇聚层，然后使用1×1卷积层来改变通道数。这四条路径都使用合适的填充来使<strong>输入与输出的高和宽一致</strong>，最后我们将每条线路的输出在<strong>通道维度</strong>上连结，并构成 Inception 块的输出。在 Inception 块中，通常调整的超参数是每层输出通道数。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Inception</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"comment\"># c1-c4是每条路径的输出通道数</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, in_channels, c1, c2, c3, c4, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Inception, self).__init__(**kwargs)</span><br><span class=\"line\">        <span class=\"comment\"># 线路1，单1x1卷积层</span></span><br><span class=\"line\">        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 线路2，1x1卷积层后接3x3卷积层</span></span><br><span class=\"line\">        self.p2_1 = nn.Conv2d(in_channels, c2[<span class=\"number\">0</span>], kernel_size=<span class=\"number\">1</span>)</span><br><span class=\"line\">        self.p2_2 = nn.Conv2d(c2[<span class=\"number\">0</span>], c2[<span class=\"number\">1</span>], kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 线路3，1x1卷积层后接5x5卷积层</span></span><br><span class=\"line\">        self.p3_1 = nn.Conv2d(in_channels, c3[<span class=\"number\">0</span>], kernel_size=<span class=\"number\">1</span>)</span><br><span class=\"line\">        self.p3_2 = nn.Conv2d(c3[<span class=\"number\">0</span>], c3[<span class=\"number\">1</span>], kernel_size=<span class=\"number\">5</span>, padding=<span class=\"number\">2</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 线路4，3x3最大汇聚层后接1x1卷积层</span></span><br><span class=\"line\">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        p1 = F.relu(self.p1_1(x))</span><br><span class=\"line\">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class=\"line\">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class=\"line\">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class=\"line\">        <span class=\"comment\"># 在通道维度（第一维）上连结输出</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.cat((p1, p2, p3, p4), dim=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n<p>GoogLeNet 一共使用9个 Inception 块和全局平均汇聚层的堆叠来生成其估计值。Inception 块之间的最大汇聚层可降低维度。第一个模块类似于 AlexNet 和 LeNet，Inception 块的组合从 VGG 继承，全局平均汇聚层避免了在最后使用全连接层。</p>\n<p>现在，我们逐一实现 GoogLeNet 的每个模块。第一个模块使用64个通道、7×7卷积层：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b1 = nn.Sequential(nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">64</span>, kernel_size=<span class=\"number\">7</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">3</span>),</span><br><span class=\"line\">                   nn.ReLU(),</span><br><span class=\"line\">                   nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<p>第二个模块使用两个卷积层：第一个卷积层是64个通道、1×1卷积层；第二个卷积层使用将通道数量增加三倍的3×3卷积层。这对应于 Inception 块中的第二条路径：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b2 = nn.Sequential(nn.Conv2d(<span class=\"number\">64</span>, <span class=\"number\">64</span>, kernel_size=<span class=\"number\">1</span>),</span><br><span class=\"line\">                   nn.ReLU(),</span><br><span class=\"line\">                   nn.Conv2d(<span class=\"number\">64</span>, <span class=\"number\">192</span>, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>),</span><br><span class=\"line\">                   nn.ReLU(),</span><br><span class=\"line\">                   nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<p>第三个模块串联两个完整的 Inception 块。第一个 Inception 块的输出通道数为 <code>64 + 128 + 32 + 32 = 256</code>，四个路径之间的输出通道数量比为 <code>2 : 4 : 1 : 1</code>，第二个和第三个路径首先将输入通道的数量分别减少到96和16，然后连接第二个卷积层。第二个 Inception 块的输出通道数增加到 <code>128 + 192 + 96 + 64 = 480</code>，四个路径之间的输出通道数量比为 <code>4 : 6 : 3 : 2</code>，第二条和第三条路径首先将输入通道的数量分别减少到128和32：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b3 = nn.Sequential(Inception(<span class=\"number\">192</span>, <span class=\"number\">64</span>, (<span class=\"number\">96</span>, <span class=\"number\">128</span>), (<span class=\"number\">16</span>, <span class=\"number\">32</span>), <span class=\"number\">32</span>),</span><br><span class=\"line\">                   Inception(<span class=\"number\">256</span>, <span class=\"number\">128</span>, (<span class=\"number\">128</span>, <span class=\"number\">192</span>), (<span class=\"number\">32</span>, <span class=\"number\">96</span>), <span class=\"number\">64</span>),</span><br><span class=\"line\">                   nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<p>第四模块更加复杂，它串联了5个 Inception 块，其输出通道数分别是 <code>192 + 208 + 48 + 64 = 512</code>、<code>160 + 224 + 64 + 64 = 512</code>、<code>128 + 256 + 64 + 64 = 512</code>、<code>112 + 288 + 64 + 64 = 528</code> 和 <code>256 + 320 + 128 + 128 = 832</code>：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b4 = nn.Sequential(Inception(<span class=\"number\">480</span>, <span class=\"number\">192</span>, (<span class=\"number\">96</span>, <span class=\"number\">208</span>), (<span class=\"number\">16</span>, <span class=\"number\">48</span>), <span class=\"number\">64</span>),</span><br><span class=\"line\">                   Inception(<span class=\"number\">512</span>, <span class=\"number\">160</span>, (<span class=\"number\">112</span>, <span class=\"number\">224</span>), (<span class=\"number\">24</span>, <span class=\"number\">64</span>), <span class=\"number\">64</span>),</span><br><span class=\"line\">                   Inception(<span class=\"number\">512</span>, <span class=\"number\">128</span>, (<span class=\"number\">128</span>, <span class=\"number\">256</span>), (<span class=\"number\">24</span>, <span class=\"number\">64</span>), <span class=\"number\">64</span>),</span><br><span class=\"line\">                   Inception(<span class=\"number\">512</span>, <span class=\"number\">112</span>, (<span class=\"number\">144</span>, <span class=\"number\">288</span>), (<span class=\"number\">32</span>, <span class=\"number\">64</span>), <span class=\"number\">64</span>),</span><br><span class=\"line\">                   Inception(<span class=\"number\">528</span>, <span class=\"number\">256</span>, (<span class=\"number\">160</span>, <span class=\"number\">320</span>), (<span class=\"number\">32</span>, <span class=\"number\">128</span>), <span class=\"number\">128</span>),</span><br><span class=\"line\">                   nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<p>第五模块包含输出通道数为 <code>256 + 320 + 128 + 128 = 832</code> 和 <code>384 + 384 + 128 + 128 = 1024</code> 的两个 Inception 块。需要注意的是，第五模块的后面紧跟输出层，该模块同 NiN 一样使用全局平均汇聚层，将每个通道的高和宽变成1。最后我们将输出变成二维数组，再接上一个输出个数为标签类别数的全连接层：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b5 = nn.Sequential(Inception(<span class=\"number\">832</span>, <span class=\"number\">256</span>, (<span class=\"number\">160</span>, <span class=\"number\">320</span>), (<span class=\"number\">32</span>, <span class=\"number\">128</span>), <span class=\"number\">128</span>),</span><br><span class=\"line\">                   Inception(<span class=\"number\">832</span>, <span class=\"number\">384</span>, (<span class=\"number\">192</span>, <span class=\"number\">384</span>), (<span class=\"number\">48</span>, <span class=\"number\">128</span>), <span class=\"number\">128</span>),</span><br><span class=\"line\">                   nn.AdaptiveAvgPool2d((<span class=\"number\">1</span>, <span class=\"number\">1</span>)),</span><br><span class=\"line\">                   nn.Flatten())</span><br><span class=\"line\"></span><br><span class=\"line\">net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class=\"number\">1024</span>, <span class=\"number\">10</span>))</span><br></pre></td></tr></table></figure>\n<p>最后我们读取数据集并进行训练，超参数设置：<code>lr, num_epochs = 0.1, 15</code>，训练过程与第一节内容一样，因此不再放出代码。</p>\n<h2 id=\"5-批量规范化（BN）\">5. 批量规范化（BN）</h2>\n<p>批量规范化（Batch Normalization）是一种流行且有效的技术，可持续加速深层网络的收敛速度。</p>\n<p>使用真实数据时，我们的第一步是标准化输入特征，使其平均值为0，方差为1。直观地说，这种标准化可以很好地与我们的优化器配合使用，因为它可以将参数的量级进行统一。</p>\n<p>第二，对于典型的多层感知机或卷积神经网络。当我们训练时，中间层中的变量（例如，多层感知机中的仿射变换输出）可能具有更广的变化范围：不论是沿着从输入到输出的层，跨同一层中的单元，或是随着时间的推移，模型参数随着训练更新的变幻莫测。批量规范化的发明者非正式地假设，这些变量分布中的这种偏移可能会阻碍网络的收敛。直观地说，我们可能会猜想，如果一个层的可变值是另一层的100倍，这可能需要对学习率进行补偿调整。</p>\n<p>第三，更深层的网络很复杂，容易过拟合。这意味着正则化变得更加重要。</p>\n<p>批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，我们首先规范化输入，即通过<strong>减去其均值并除以其标准差</strong>，其中两者均基于当前小批量处理。接下来，我们应用<strong>比例系数和比例偏移</strong>。正是由于这个基于批量统计的标准化，才有了批量规范化的名称。</p>\n<p>请注意，如果我们尝试使用大小为1的小批量应用批量规范化，我们将无法学到任何东西。这是因为在减去均值之后，每个隐藏单元将为0。所以，只有使用足够大的小批量，批量规范化这种方法才是有效且稳定的。请注意，在应用批量规范化时，批量大小的选择可能比没有批量规范化时更重要。</p>\n<p>通常，我们将批量规范化层置于全连接层中的仿射变换和激活函数之间。同样，对于卷积层，我们可以在卷积层之后和非线性激活函数之前应用批量规范化。</p>\n<p>下面，我们从头开始实现一个具有张量的批量规范化层：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">batch_norm</span>(<span class=\"params\">X, gamma, beta, moving_mean, moving_var, eps, momentum</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 通过is_grad_enabled来判断当前模式是训练模式还是预测模式</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> torch.is_grad_enabled():</span><br><span class=\"line\">        <span class=\"comment\"># 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差</span></span><br><span class=\"line\">        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"comment\"># 假设只考虑全连接和二维卷积</span></span><br><span class=\"line\">        <span class=\"keyword\">assert</span> <span class=\"built_in\">len</span>(X.shape) <span class=\"keyword\">in</span> (<span class=\"number\">2</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(X.shape) == <span class=\"number\">2</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 使用全连接层的情况，计算特征维上的均值和方差</span></span><br><span class=\"line\">            mean = X.mean(dim=<span class=\"number\">0</span>)</span><br><span class=\"line\">            var = ((X - mean) ** <span class=\"number\">2</span>).mean(dim=<span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。</span></span><br><span class=\"line\">            <span class=\"comment\"># 这里我们需要保持X的形状以便后面可以做广播运算</span></span><br><span class=\"line\">            mean = X.mean(dim=(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>), keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\">            var = ((X - mean) ** <span class=\"number\">2</span>).mean(dim=(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>), keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 训练模式下，用当前的均值和方差做标准化</span></span><br><span class=\"line\">        X_hat = (X - mean) / torch.sqrt(var + eps)</span><br><span class=\"line\">        <span class=\"comment\"># 更新移动平均的均值和方差</span></span><br><span class=\"line\">        moving_mean = momentum * moving_mean + (<span class=\"number\">1.0</span> - momentum) * mean</span><br><span class=\"line\">        moving_var = momentum * moving_var + (<span class=\"number\">1.0</span> - momentum) * var</span><br><span class=\"line\">    Y = gamma * X_hat + beta  <span class=\"comment\"># 缩放和移位</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> Y, moving_mean.data, moving_var.data</span><br></pre></td></tr></table></figure>\n<p>我们现在可以创建一个正确的 BatchNorm 层。这个层将保持适当的参数：拉伸 <code>gamma</code> 和偏移 <code>beta</code>，这两个参数将在训练过程中更新。此外，我们的层将保存均值和方差的移动平均值，以便在模型预测期间随后使用。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">BatchNorm</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"comment\"># num_features：全连接层的输出数量或卷积层的输出通道数</span></span><br><span class=\"line\">    <span class=\"comment\"># num_dims：2表示全连接层，4表示卷积层</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, num_features, num_dims</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> num_dims == <span class=\"number\">2</span>:</span><br><span class=\"line\">            shape = (<span class=\"number\">1</span>, num_features)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            shape = (<span class=\"number\">1</span>, num_features, <span class=\"number\">1</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0</span></span><br><span class=\"line\">        self.gamma = nn.Parameter(torch.ones(shape))</span><br><span class=\"line\">        self.beta = nn.Parameter(torch.zeros(shape))</span><br><span class=\"line\">        <span class=\"comment\"># 非模型参数的变量初始化为0和1</span></span><br><span class=\"line\">        self.moving_mean = torch.zeros(shape)</span><br><span class=\"line\">        self.moving_var = torch.ones(shape)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 如果X不在内存上，将moving_mean和moving_var，复制到X所在显存上</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.moving_mean.device != X.device:</span><br><span class=\"line\">            self.moving_mean = self.moving_mean.to(X.device)</span><br><span class=\"line\">            self.moving_var = self.moving_var.to(X.device)</span><br><span class=\"line\">        <span class=\"comment\"># 保存更新过的moving_mean和moving_var</span></span><br><span class=\"line\">        Y, self.moving_mean, self.moving_var = batch_norm(X, self.gamma,</span><br><span class=\"line\">            self.beta, self.moving_mean, self.moving_var, eps=<span class=\"number\">1e-5</span>, momentum=<span class=\"number\">0.9</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> Y</span><br></pre></td></tr></table></figure>\n<p>为了更好理解如何应用 BatchNorm，下面我们将其应用于 LeNet 模型：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">6</span>, kernel_size=<span class=\"number\">5</span>, padding=<span class=\"number\">2</span>), BatchNorm(<span class=\"number\">6</span>, num_dims=<span class=\"number\">4</span>), nn.Sigmoid(),</span><br><span class=\"line\">    nn.AvgPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, kernel_size=<span class=\"number\">5</span>), BatchNorm(<span class=\"number\">16</span>, num_dims=<span class=\"number\">4</span>), nn.Sigmoid(),</span><br><span class=\"line\">    nn.AvgPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>), nn.Flatten(),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">16</span> * <span class=\"number\">5</span> * <span class=\"number\">5</span>, <span class=\"number\">120</span>), BatchNorm(<span class=\"number\">120</span>, num_dims=<span class=\"number\">2</span>), nn.Sigmoid(),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>), BatchNorm(<span class=\"number\">84</span>, num_dims=<span class=\"number\">2</span>), nn.Sigmoid(),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">84</span>, <span class=\"number\">10</span>))</span><br></pre></td></tr></table></figure>\n<p>最后我们读取数据集并进行训练，超参数设置：<code>lr, num_epochs = 1, 15</code>，由于网络模型类似 LeNet，因此无需对输入图像进行 Resize 操作，训练过程与第一节内容一样，因此不再放出代码。</p>\n<h2 id=\"6-残差网络（ResNet）\">6. 残差网络（ResNet）</h2>\n<p>只有当较复杂的函数类包含较小的函数类时，我们才能确保提高它们的性能。对于深度神经网络，如果我们能将新添加的层训练成恒等映射（identity function）：<code>f(x) = x</code>，新模型和原模型将同样有效。同时，由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。</p>\n<p>残差网络的核心思想是：每个附加层都应该更容易地<strong>包含原始函数</strong>作为其元素之一。</p>\n<p>ResNet 沿用了 VGG 完整的卷积层设计。残差块里首先有2个有相同输出通道数的卷积层。每个卷积层后接一个批量规范化层和 ReLU 激活函数。然后我们通过跨层数据通路，跳过这2个卷积运算，将输入直接加在最后的 ReLU 激活函数前。这样的设计要求2个卷积层的输出与输入形状一样，从而使它们可以相加。如果想改变通道数，就需要引入一个额外的1×1卷积层来将输入变换成需要的形状后再做相加运算。残差块的实现如下：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Residual</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, input_channels, num_channels, use_1x1conv=<span class=\"literal\">False</span>, strides=<span class=\"number\">1</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>, stride=strides)</span><br><span class=\"line\">        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> use_1x1conv:</span><br><span class=\"line\">            self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=<span class=\"number\">1</span>, stride=strides)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.conv3 = <span class=\"literal\">None</span></span><br><span class=\"line\">        self.bn1 = nn.BatchNorm2d(num_channels)</span><br><span class=\"line\">        self.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class=\"line\">        Y = self.bn2(self.conv2(Y))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.conv3:</span><br><span class=\"line\">            X = self.conv3(X)</span><br><span class=\"line\">        Y += X</span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.relu(Y)</span><br></pre></td></tr></table></figure>\n<p>此代码生成两种类型的网络：一种是当 <code>use_1x1conv = False</code> 时，应用 ReLU 非线性函数之前，将输入添加到输出。另一种是当 <code>use_1x1conv = True</code> 时，添加通过1×1卷积调整通道和分辨率。</p>\n<p>ResNet 的前两层跟之前介绍的 GoogLeNet 中的一样：在输出通道数为64、步幅为2的7×7卷积层后，接步幅为2的3×3最大汇聚层。不同之处在于 ResNet 每个卷积层后增加了批量规范化层。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b1 = nn.Sequential(nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">64</span>, kernel_size=<span class=\"number\">7</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">3</span>),</span><br><span class=\"line\">                   nn.BatchNorm2d(<span class=\"number\">64</span>), nn.ReLU(),</span><br><span class=\"line\">                   nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<p>GoogLeNet 在后面接了4个由 Inception 块组成的模块。ResNet 则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。第一个模块的通道数同输入通道数一致。由于之前已经使用了步幅为2的最大汇聚层，所以无须减小高和宽。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。</p>\n<p>下面我们来实现这个模块。注意，我们对第一个模块做了特别处理：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">resnet_block</span>(<span class=\"params\">input_channels, num_channels, num_residuals, first_block=<span class=\"literal\">False</span></span>):</span><br><span class=\"line\">    blk = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_residuals):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> i == <span class=\"number\">0</span> <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> first_block:</span><br><span class=\"line\">            blk.append(Residual(input_channels, num_channels, use_1x1conv=<span class=\"literal\">True</span>, strides=<span class=\"number\">2</span>))</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            blk.append(Residual(num_channels, num_channels))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> blk</span><br></pre></td></tr></table></figure>\n<p>接着在 ResNet 加入所有残差块，这里每个模块使用2个残差块：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b2 = nn.Sequential(*resnet_block(<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">2</span>, first_block=<span class=\"literal\">True</span>))</span><br><span class=\"line\">b3 = nn.Sequential(*resnet_block(<span class=\"number\">64</span>, <span class=\"number\">128</span>, <span class=\"number\">2</span>))</span><br><span class=\"line\">b4 = nn.Sequential(*resnet_block(<span class=\"number\">128</span>, <span class=\"number\">256</span>, <span class=\"number\">2</span>))</span><br><span class=\"line\">b5 = nn.Sequential(*resnet_block(<span class=\"number\">256</span>, <span class=\"number\">512</span>, <span class=\"number\">2</span>))</span><br></pre></td></tr></table></figure>\n<p>最后，与 GoogLeNet 一样，在 ResNet 中加入全局平均汇聚层，以及全连接层输出：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(b1, b2, b3, b4, b5,</span><br><span class=\"line\">                    nn.AdaptiveAvgPool2d((<span class=\"number\">1</span>, <span class=\"number\">1</span>)),</span><br><span class=\"line\">                    nn.Flatten(), nn.Linear(<span class=\"number\">512</span>, <span class=\"number\">10</span>))</span><br></pre></td></tr></table></figure>\n<p>每个模块有4个卷积层（不包括恒等映射的卷积层）。加上第一个7×7卷积层和最后一个全连接层，共有18层。因此，这种模型通常被称为 ResNet-18。通过配置不同的通道数和模块里的残差块数可以得到不同的 ResNet 模型，例如更深的含152层的 ResNet-152。</p>\n<p>最后我们读取数据集并进行训练，超参数设置：<code>lr, num_epochs = 0.02, 15</code>，由于 ResNet 性能很强，对于 FashionMNIST 数据集很容易就过拟合了，因此可以将输入图像 Resize 为 <code>(96, 96)</code>，训练过程与第一节内容一样，因此不再放出代码。</p>\n<p>ResNet 其它版本的模型结构可以参考：</p>\n<ul>\n<li><a href=\"https://zhuanlan.zhihu.com/p/54289848\">ResNet 及其变种的结构梳理、有效性分析与代码解读</a>。</li>\n<li><a href=\"https://www.jianshu.com/p/085f4c8256f1\">ResNet18、50网络结构以及 PyTorch 实现代码</a>。</li>\n<li><a href=\"https://blog.csdn.net/New_WR/article/details/121777644\">ResNet50网络结构搭建（PyTorch）</a>。</li>\n</ul>\n<h2 id=\"7-稠密连接网络（DenseNet）\">7. 稠密连接网络（DenseNet）</h2>\n<p>ResNet 和 DenseNet 的关键区别在于，DenseNet 输出是连接（用 <code>[.]</code> 表示）而不是如 ResNet 的简单相加。</p>\n<p>稠密网络主要由2部分构成：稠密块（dense block）和过渡层（transition layer）。前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。</p>\n<p>DenseNet 使用了 ResNet 改良版的“批量规范化、激活和卷积”架构。我们首先实现一下这个架构：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">conv_block</span>(<span class=\"params\">input_channels, num_channels</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(</span><br><span class=\"line\">        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class=\"line\">        nn.Conv2d(input_channels, num_channels, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<p>一个稠密块由多个卷积块组成，每个卷积块使用相同数量的输出通道。然而，在前向传播中，我们将每个卷积块的输入和输出在通道维上连结：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">DenseBlock</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, num_convs, input_channels, num_channels</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(DenseBlock, self).__init__()</span><br><span class=\"line\">        layer = []</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_convs):</span><br><span class=\"line\">            layer.append(conv_block(num_channels * i + input_channels, num_channels))</span><br><span class=\"line\">        self.net = nn.Sequential(*layer)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> blk <span class=\"keyword\">in</span> self.net:</span><br><span class=\"line\">            Y = blk(X)</span><br><span class=\"line\">            <span class=\"comment\"># 连接通道维度上每个块的输入和输出</span></span><br><span class=\"line\">            X = torch.cat((X, Y), dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> X</span><br></pre></td></tr></table></figure>\n<p>例如我们构建一个 <code>DenseBlock(2, 3, 10)</code>，那么两层卷积层分别为 <code>conv_block(3, 10)</code>、<code>conv_block(13, 10)</code>。第一层卷积输出的通道维是10，与输入 <code>X</code> 在通道维上连结后通道维是13，因此第二层卷积输入的通道维是13，第二层卷积输出的通道维是10，与输入 <code>X</code> 在通道维上连结后通道维是23：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">blk = DenseBlock(<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">X = torch.randn(<span class=\"number\">4</span>, <span class=\"number\">3</span>, <span class=\"number\">8</span>, <span class=\"number\">8</span>)</span><br><span class=\"line\">Y = blk(X)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Y.shape)  <span class=\"comment\"># torch.Size([4, 23, 8, 8])</span></span><br></pre></td></tr></table></figure>\n<p>由于每个稠密块都会带来通道数的增加，使用过多则会过于复杂化模型。而过渡层可以用来控制模型复杂度。它通过1×1卷积层来<strong>减小通道数</strong>，并使用步幅为2的平均汇聚层减半高和宽，从而进一步降低模型复杂度：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">transition_block</span>(<span class=\"params\">input_channels, num_channels</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(</span><br><span class=\"line\">        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class=\"line\">        nn.Conv2d(input_channels, num_channels, kernel_size=<span class=\"number\">1</span>),</span><br><span class=\"line\">        nn.AvgPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>))</span><br></pre></td></tr></table></figure>\n<p>对上一个例子中稠密块的输出使用通道数为10的过渡层。此时输出的通道数减为10，高和宽均减半：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">blk = transition_block(<span class=\"number\">23</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(blk(Y).shape)  <span class=\"comment\"># torch.Size([4, 10, 4, 4])</span></span><br></pre></td></tr></table></figure>\n<p>我们来构造 DenseNet 模型。DenseNet 首先使用同 ResNet 一样的单卷积层和最大汇聚层：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b1 = nn.Sequential(</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">64</span>, kernel_size=<span class=\"number\">7</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">3</span>),</span><br><span class=\"line\">    nn.BatchNorm2d(<span class=\"number\">64</span>), nn.ReLU(),</span><br><span class=\"line\">    nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<p>接下来，类似于 ResNet 使用的4个残差块，DenseNet 使用的是4个稠密块。与 ResNet 类似，我们可以设置每个稠密块使用多少个卷积层。这里我们设成4，从而与之前的 ResNet-18 保持一致。稠密块里的卷积层通道数（即增长率）设为32，所以每个稠密块将增加128个通道。</p>\n<p>在每个模块之间，ResNet 通过步幅为2的残差块减小高和宽，DenseNet 则使用过渡层来减半高和宽，并减半通道数：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># num_channels为当前的通道数</span></span><br><span class=\"line\">num_channels, growth_rate = <span class=\"number\">64</span>, <span class=\"number\">32</span></span><br><span class=\"line\">num_convs_in_dense_blocks = [<span class=\"number\">4</span>, <span class=\"number\">4</span>, <span class=\"number\">4</span>, <span class=\"number\">4</span>]</span><br><span class=\"line\">blks = []</span><br><span class=\"line\"><span class=\"keyword\">for</span> i, num_convs <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(num_convs_in_dense_blocks):</span><br><span class=\"line\">    blks.append(DenseBlock(num_convs, num_channels, growth_rate))</span><br><span class=\"line\">    <span class=\"comment\"># 上一个稠密块的输出通道数</span></span><br><span class=\"line\">    num_channels += num_convs * growth_rate</span><br><span class=\"line\">    <span class=\"comment\"># 在稠密块之间添加一个转换层，使通道数量减半</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> i != <span class=\"built_in\">len</span>(num_convs_in_dense_blocks) - <span class=\"number\">1</span>:</span><br><span class=\"line\">        blks.append(transition_block(num_channels, num_channels // <span class=\"number\">2</span>))</span><br><span class=\"line\">        num_channels = num_channels // <span class=\"number\">2</span></span><br></pre></td></tr></table></figure>\n<p>与 ResNet 类似，最后接上全局汇聚层和全连接层来输出结果：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(</span><br><span class=\"line\">    b1, *blks,</span><br><span class=\"line\">    nn.BatchNorm2d(num_channels), nn.ReLU(),</span><br><span class=\"line\">    nn.AdaptiveAvgPool2d((<span class=\"number\">1</span>, <span class=\"number\">1</span>)),</span><br><span class=\"line\">    nn.Flatten(),</span><br><span class=\"line\">    nn.Linear(num_channels, <span class=\"number\">10</span>))</span><br></pre></td></tr></table></figure>\n<p>最后我们读取数据集并进行训练，超参数设置：<code>lr, num_epochs = 0.1, 15</code>，将输入图像 Resize 为 <code>(96, 96)</code>，训练过程与第一节内容一样，因此不再放出代码。</p>\n<p>下一章：<a href=\"/posts/24840.html\">计算机视觉</a>。</p>\n",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/25122.html",
            "url": "https://asanosaki.github.io/posts/25122.html",
            "title": "动手学深度学习笔记(李沐)-卷积神经网络",
            "date_published": "2023-03-01T01:20:00.000Z",
            "content_html": "<blockquote>\n<p>李沐动手学深度学习（PyTorch）课程学习笔记第五章：卷积神经网络。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-从全连接层到卷积\">1. 从全连接层到卷积</h2>\n<p>假设我们有一个足够充分的照片数据集，数据集中是拥有标注的照片，每张照片具有百万级像素，这意味着网络的每次输入都有一百万个维度。即使将隐藏层维度降低到1000，这个全连接层也将有十亿个参数。</p>\n<p>假设我们想从一张图片中找到某个物体。合理的假设是：无论哪种方法找到这个物体，都应该和物体的位置无关。卷积神经网络正是将<strong>空间不变性</strong>（spatial invariance）的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示：</p>\n<ul>\n<li>平移不变性（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。</li>\n<li>局部性（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</li>\n</ul>\n<h2 id=\"2-图像卷积\">2. 图像卷积</h2>\n<p>严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是<strong>互相关运算</strong>（cross-correlation），而不是卷积运算。在卷积层中，输入张量和核张量通过互相关运算产生输出张量。</p>\n<p>在二维互相关运算中，卷积窗口从输入张量的左上角开始，从左到右、从上到下滑动。当卷积窗口滑动到新一个位置时，包含在该窗口中的部分张量与卷积核张量进行按元素相乘，得到的张量再求和得到一个单一的标量值，由此我们得出了这一位置的输出张量值。</p>\n<p>我们可以自己实现如上过程：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">corr2d</span>(<span class=\"params\">X, K</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;计算二维互相关运算&quot;&quot;&quot;</span></span><br><span class=\"line\">    h, w = K.shape</span><br><span class=\"line\">    Y = torch.zeros((X.shape[<span class=\"number\">0</span>] - h + <span class=\"number\">1</span>, X.shape[<span class=\"number\">1</span>] - w + <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(Y.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(Y.shape[<span class=\"number\">1</span>]):</span><br><span class=\"line\">            Y[i, j] = (X[i:i + h, j:j + w] * K).<span class=\"built_in\">sum</span>()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Y</span><br><span class=\"line\"></span><br><span class=\"line\">X = torch.tensor([[<span class=\"number\">0.0</span>, <span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>], [<span class=\"number\">3.0</span>, <span class=\"number\">4.0</span>, <span class=\"number\">5.0</span>], [<span class=\"number\">6.0</span>, <span class=\"number\">7.0</span>, <span class=\"number\">8.0</span>]])</span><br><span class=\"line\">K = torch.tensor([[<span class=\"number\">0.0</span>, <span class=\"number\">1.0</span>], [<span class=\"number\">2.0</span>, <span class=\"number\">3.0</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(corr2d(X, K))  <span class=\"comment\"># tensor([[19., 25.], [37., 43.]])</span></span><br></pre></td></tr></table></figure>\n<p>卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。</p>\n<p>我们可以基于上面定义的 <code>corr2d</code> 函数实现二维卷积层：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Conv2D</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, kernel_size</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.weight = nn.Parameter(torch.rand(kernel_size))</span><br><span class=\"line\">        self.bias = nn.Parameter(torch.zeros(<span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> corr2d(x, self.weight) + self.bias</span><br></pre></td></tr></table></figure>\n<p>现在来看一下卷积层的一个简单应用：通过找到像素变化的位置，来检测图像中不同颜色的边缘。我们假设0为黑色像素，1为白色像素：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.ones((<span class=\"number\">6</span>, <span class=\"number\">8</span>))</span><br><span class=\"line\">X[:, <span class=\"number\">2</span>:<span class=\"number\">6</span>] = <span class=\"number\">0</span></span><br></pre></td></tr></table></figure>\n<p><code>X</code> 的内容如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class=\"line\">        [1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class=\"line\">        [1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class=\"line\">        [1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class=\"line\">        [1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class=\"line\">        [1., 1., 0., 0., 0., 0., 1., 1.]])</span><br></pre></td></tr></table></figure>\n<p>接下来，我们构造一个高度为1、宽度为2的卷积核K。当进行互相关运算时，如果水平相邻的两元素相同，则输出为零，否则输出为非零：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">K = torch.tensor([[<span class=\"number\">1.0</span>, -<span class=\"number\">1.0</span>]])</span><br><span class=\"line\">Y = corr2d(X, K)</span><br></pre></td></tr></table></figure>\n<p><code>Y</code> 的内容如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span><br><span class=\"line\">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span><br><span class=\"line\">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span><br><span class=\"line\">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span><br><span class=\"line\">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span><br><span class=\"line\">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])</span><br></pre></td></tr></table></figure>\n<p>现在我们使用 PyTorch 的卷积层尝试通过正确结果 <code>Y</code> 是否能学习出我们之前自己构造出的卷积核参数：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 构造一个二维卷积层，它具有1个输入通道和1个输出通道，卷积核形状为(1, 2)</span></span><br><span class=\"line\">conv2d = nn.Conv2d(in_channels=<span class=\"number\">1</span>, out_channels=<span class=\"number\">1</span>, kernel_size=(<span class=\"number\">1</span>, <span class=\"number\">2</span>), bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 这个二维卷积层使用四维输入和输出格式(批量大小, 通道, 高度, 宽度)</span></span><br><span class=\"line\"><span class=\"comment\"># 在本例中批量大小和通道数都为1</span></span><br><span class=\"line\">X = X.reshape((<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">6</span>, <span class=\"number\">8</span>))</span><br><span class=\"line\">Y = Y.reshape((<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>))</span><br><span class=\"line\">lr = <span class=\"number\">3e-2</span>  <span class=\"comment\"># 学习率</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>):</span><br><span class=\"line\">    Y_hat = conv2d(X)</span><br><span class=\"line\">    loss = (Y_hat - Y) ** <span class=\"number\">2</span>  <span class=\"comment\"># 均方误差</span></span><br><span class=\"line\">    conv2d.zero_grad()</span><br><span class=\"line\">    loss.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">    <span class=\"comment\"># 迭代卷积核，手动实现梯度下降</span></span><br><span class=\"line\">    conv2d.weight.data[:] -= lr * conv2d.weight.grad</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (i + <span class=\"number\">1</span>) % <span class=\"number\">2</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;epoch <span class=\"subst\">&#123;i + <span class=\"number\">1</span>&#125;</span>, loss <span class=\"subst\">&#123;loss.<span class=\"built_in\">sum</span>():<span class=\"number\">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(conv2d.weight.data.reshape((<span class=\"number\">1</span>, <span class=\"number\">2</span>)))  <span class=\"comment\"># tensor([[ 0.9756, -1.0059]])</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"3-填充和步幅\">3. 填充和步幅</h2>\n<p>在应用多层卷积时，我们常常丢失边缘像素。由于我们通常使用小卷积核，因此对于任何单个卷积，我们可能只会丢失几个像素。但随着我们应用许多连续卷积层，累积丢失的像素数就多了。解决这个问题的简单方法即为<strong>填充</strong>（padding）：在输入图像的边界填充元素（通常填充元素是0）。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 请注意，这里每边都填充了1行或1列，因此总共添加了2行或2列</span></span><br><span class=\"line\">conv2d = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">1</span>, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">X = torch.rand(size=(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">8</span>, <span class=\"number\">8</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(conv2d(X).shape)  <span class=\"comment\"># torch.Size([1, 1, 8, 8])</span></span><br></pre></td></tr></table></figure>\n<p>在计算互相关时，卷积窗口从输入张量的左上角开始，向下、向右滑动。在前面的例子中，我们默认每次滑动一个元素。但是，有时候为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素。我们将每次滑动元素的数量称为<strong>步幅</strong>（stride）。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conv2d = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">1</span>, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>, stride=<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(conv2d(X).shape)  <span class=\"comment\"># torch.Size([1, 1, 4, 4])</span></span><br><span class=\"line\"></span><br><span class=\"line\">conv2d = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">1</span>, kernel_size=(<span class=\"number\">3</span>, <span class=\"number\">5</span>), padding=(<span class=\"number\">0</span>, <span class=\"number\">1</span>), stride=(<span class=\"number\">3</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(conv2d(X).shape)  <span class=\"comment\"># torch.Size([1, 1, 2, 2])</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"4-多输入多输出通道\">4. 多输入多输出通道</h2>\n<p>当输入包含多个通道时，需要构造一个与输入数据具有相同输入通道数的卷积核，以便与输入数据进行互相关运算。</p>\n<p>为了加深理解，我们实现一下多输入通道互相关运算。简而言之，我们所做的就是对每个通道执行互相关操作，然后将结果相加：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">corr2d_multi_in</span>(<span class=\"params\">X, K</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 先遍历“X”和“K”的第0个维度（通道维度），再把它们加在一起</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">sum</span>(d2l.corr2d(x, k) <span class=\"keyword\">for</span> x, k <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(X, K))</span><br><span class=\"line\"></span><br><span class=\"line\">X = torch.tensor([[[<span class=\"number\">0.0</span>, <span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>], [<span class=\"number\">3.0</span>, <span class=\"number\">4.0</span>, <span class=\"number\">5.0</span>], [<span class=\"number\">6.0</span>, <span class=\"number\">7.0</span>, <span class=\"number\">8.0</span>]],</span><br><span class=\"line\">                  [[<span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>, <span class=\"number\">3.0</span>], [<span class=\"number\">4.0</span>, <span class=\"number\">5.0</span>, <span class=\"number\">6.0</span>], [<span class=\"number\">7.0</span>, <span class=\"number\">8.0</span>, <span class=\"number\">9.0</span>]]])</span><br><span class=\"line\">K = torch.tensor([[[<span class=\"number\">0.0</span>, <span class=\"number\">1.0</span>], [<span class=\"number\">2.0</span>, <span class=\"number\">3.0</span>]], [[<span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>], [<span class=\"number\">3.0</span>, <span class=\"number\">4.0</span>]]])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(corr2d_multi_in(X, K))  <span class=\"comment\"># tensor([[ 56.,  72.], [104., 120.]])</span></span><br></pre></td></tr></table></figure>\n<p>到目前为止，不论有多少输入通道，我们还只有一个输出通道。在最流行的神经网络架构中，随着神经网络层数的加深，我们常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。直观地说，我们可以<strong>将每个通道看作对不同特征的响应</strong>。而现实可能更为复杂一些，因为每个通道不是独立学习的，而是为了共同使用而优化的。因此，多输出通道并不仅是学习多个单通道的检测器。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">corr2d_multi_in_out</span>(<span class=\"params\">X, K</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 迭代“K”的第0个维度，每次都对输入“X”执行互相关运算，最后将所有结果都叠加在一起</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.stack([corr2d_multi_in(X, k) <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> K], <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 通过将核张量K与K+1（K中每个元素加1）和K+2连接起来，构造了一个具有3个输出通道的卷积核</span></span><br><span class=\"line\">K = torch.stack((K, K + <span class=\"number\">1</span>, K + <span class=\"number\">2</span>), <span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(K.shape)  <span class=\"comment\"># torch.Size([3, 2, 2, 2])</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(corr2d_multi_in_out(X, K))</span><br><span class=\"line\"><span class=\"comment\"># tensor([[[ 56.,  72.],</span></span><br><span class=\"line\"><span class=\"comment\">#          [104., 120.]],</span></span><br><span class=\"line\"><span class=\"comment\">#         [[ 76., 100.],</span></span><br><span class=\"line\"><span class=\"comment\">#          [148., 172.]],</span></span><br><span class=\"line\"><span class=\"comment\">#         [[ 96., 128.],</span></span><br><span class=\"line\"><span class=\"comment\">#          [192., 224.]]])</span></span><br></pre></td></tr></table></figure>\n<p>PS：1*1卷积层通常用于调整网络层的通道数量和控制模型复杂性，其失去了卷积层的特有能力：在高度和宽度维度上，识别相邻元素间相互作用的能力。</p>\n<h2 id=\"5-汇聚层（池化层）\">5. 汇聚层（池化层）</h2>\n<p>汇聚层（pooling layer）具有双重目的：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。</p>\n<p>与卷积层类似，汇聚层运算符由一个固定形状的窗口组成，该窗口根据其步幅大小在输入的所有区域上滑动（从左至右、从上至下），为固定形状窗口（有时称为汇聚窗口）遍历的每个位置计算一个输出。然而，不同于卷积层中的输入与卷积核之间的互相关计算，汇聚层<strong>不包含参数</strong>。相反，池运算是确定性的，我们通常计算汇聚窗口中所有元素的<strong>最大值或平均值</strong>。这些操作分别称为最大汇聚层（maximum pooling）和平均汇聚层（average pooling）。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">pool2d</span>(<span class=\"params\">X, pool_size, mode=<span class=\"string\">&#x27;max&#x27;</span></span>):</span><br><span class=\"line\">    p_h, p_w = pool_size</span><br><span class=\"line\">    Y = torch.zeros((X.shape[<span class=\"number\">0</span>] - p_h + <span class=\"number\">1</span>, X.shape[<span class=\"number\">1</span>] - p_w + <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(Y.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(Y.shape[<span class=\"number\">1</span>]):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> mode == <span class=\"string\">&#x27;max&#x27;</span>:</span><br><span class=\"line\">                Y[i, j] = X[i:i + p_h, j:j + p_w].<span class=\"built_in\">max</span>()</span><br><span class=\"line\">            <span class=\"keyword\">elif</span> mode == <span class=\"string\">&#x27;avg&#x27;</span>:</span><br><span class=\"line\">                Y[i, j] = X[i:i + p_h, j:j + p_w].mean()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Y</span><br><span class=\"line\"></span><br><span class=\"line\">X = torch.tensor([[<span class=\"number\">0.0</span>, <span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>], [<span class=\"number\">3.0</span>, <span class=\"number\">4.0</span>, <span class=\"number\">5.0</span>], [<span class=\"number\">6.0</span>, <span class=\"number\">7.0</span>, <span class=\"number\">8.0</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(pool2d(X, (<span class=\"number\">2</span>, <span class=\"number\">2</span>)))  <span class=\"comment\"># tensor([[4., 5.], [7., 8.]])</span></span><br></pre></td></tr></table></figure>\n<p>与卷积层一样，汇聚层也可以改变输出形状。和以前一样，我们可以通过填充和步幅以获得所需的输出形状。下面，我们用深度学习框架中内置的二维最大汇聚层，来演示汇聚层中填充和步幅的使用。</p>\n<p>默认情况下，深度学习框架中的<strong>步幅与汇聚窗口的大小相同</strong>。因此，如果我们使用形状为 <code>(3, 3)</code> 的汇聚窗口，那么默认情况下，我们得到的步幅形状为 <code>(3, 3)</code>。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.arange(<span class=\"number\">16</span>, dtype=torch.float32).reshape((<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">pool2d = nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(pool2d(X))  <span class=\"comment\"># tensor([[[[10.]]]])</span></span><br><span class=\"line\"></span><br><span class=\"line\">pool2d = nn.MaxPool2d(<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>, stride=<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(pool2d(X))  <span class=\"comment\"># tensor([[[[ 5.,  7.], [13., 15.]]]])</span></span><br></pre></td></tr></table></figure>\n<p>在处理多通道输入数据时，汇聚层<strong>在每个输入通道上单独运算</strong>，而不是像卷积层一样在通道上对输入进行汇总。这意味着汇聚层的输出通道数与输入通道数相同。下面，我们将在通道维度上连结张量 <code>X</code> 和 <code>X + 1</code>，以构建具有2个通道的输入：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.cat((X, X + <span class=\"number\">1</span>), dim=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(pool2d(X))</span><br><span class=\"line\"><span class=\"comment\"># tensor([[[[ 5.,  7.],</span></span><br><span class=\"line\"><span class=\"comment\">#           [13., 15.]],</span></span><br><span class=\"line\"><span class=\"comment\">#          [[ 6.,  8.],</span></span><br><span class=\"line\"><span class=\"comment\">#           [14., 16.]]]])</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"6-LeNet\">6. LeNet</h2>\n<p>本节将介绍 LeNet，它是最早发布的卷积神经网络之一。总体来看，LeNet（LeNet-5）由两个部分组成：</p>\n<ul>\n<li>卷积编码器：由两个卷积层组成。</li>\n<li>全连接层密集块：由三个全连接层组成。</li>\n</ul>\n<p>每个卷积块中的基本单元是一个卷积层、一个 Sigmoid 激活函数和平均汇聚层。请注意，虽然 ReLU 和最大汇聚层更有效，但它们在20世纪90年代还没有出现。每个卷积层使用5×5卷积核和一个 Sigmoid 激活函数。这些层将输入映射到多个二维特征输出，通常同时增加通道的数量。第一卷积层有6个输出通道，而第二个卷积层有16个输出通道。每个2×2池化操作（步幅2）通过空间下采样将维数减少4倍。卷积的输出形状由批量大小、通道数、高度、宽度决定。</p>\n<p>虽然卷积神经网络的参数较少，但与深度的多层感知机相比，它们的计算成本仍然很高，因为每个参数都参与更多的乘法。通过使用 GPU，可以用它加快训练。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils <span class=\"keyword\">import</span> data</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms</span><br><span class=\"line\"><span class=\"keyword\">from</span> util.functions <span class=\"keyword\">import</span> train_classifier</span><br><span class=\"line\"></span><br><span class=\"line\">net = nn.Sequential(</span><br><span class=\"line\">    nn.Conv2d(in_channels=<span class=\"number\">1</span>, out_channels=<span class=\"number\">6</span>, kernel_size=<span class=\"number\">5</span>, padding=<span class=\"number\">2</span>), nn.Sigmoid(),  <span class=\"comment\"># (6, 28, 28)</span></span><br><span class=\"line\">    nn.AvgPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>),  <span class=\"comment\"># (6, 14, 14)</span></span><br><span class=\"line\">    nn.Conv2d(in_channels=<span class=\"number\">6</span>, out_channels=<span class=\"number\">16</span>, kernel_size=<span class=\"number\">5</span>), nn.Sigmoid(),  <span class=\"comment\"># (16, 10, 10)</span></span><br><span class=\"line\">    nn.AvgPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>),  <span class=\"comment\"># (16, 5, 5)</span></span><br><span class=\"line\">    nn.Flatten(),  <span class=\"comment\"># (16 * 5 * 5,)</span></span><br><span class=\"line\">    nn.Linear(<span class=\"number\">16</span> * <span class=\"number\">5</span> * <span class=\"number\">5</span>, <span class=\"number\">120</span>), nn.Sigmoid(),  <span class=\"comment\"># (120,)</span></span><br><span class=\"line\">    nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>), nn.Sigmoid(),  <span class=\"comment\"># (84,)</span></span><br><span class=\"line\">    nn.Linear(<span class=\"number\">84</span>, <span class=\"number\">10</span>))  <span class=\"comment\"># (10,)</span></span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">64</span></span><br><span class=\"line\">trans = transforms.ToTensor()</span><br><span class=\"line\">mnist_train = torchvision.datasets.MNIST(root=<span class=\"string\">&quot;../data&quot;</span>, train=<span class=\"literal\">True</span>, transform=trans, download=<span class=\"literal\">True</span>)</span><br><span class=\"line\">mnist_test = torchvision.datasets.MNIST(root=<span class=\"string\">&quot;../data&quot;</span>, train=<span class=\"literal\">False</span>, transform=trans, download=<span class=\"literal\">True</span>)</span><br><span class=\"line\">train_iter = data.DataLoader(mnist_train, batch_size, shuffle=<span class=\"literal\">True</span>, num_workers=<span class=\"number\">0</span>)</span><br><span class=\"line\">test_iter = data.DataLoader(mnist_test, batch_size, shuffle=<span class=\"literal\">True</span>, num_workers=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">device = torch.device(<span class=\"string\">&#x27;cuda&#x27;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\">lr, num_epochs = <span class=\"number\">0.2</span>, <span class=\"number\">30</span></span><br><span class=\"line\"></span><br><span class=\"line\">train_classifier(net, train_iter, test_iter, num_epochs, lr, device, <span class=\"string\">&#x27;../logs/LeNet_train_log&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>下一章：<a href=\"/posts/21165.html\">现代卷积神经网络</a>。</p>\n",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/23526.html",
            "url": "https://asanosaki.github.io/posts/23526.html",
            "title": "动手学深度学习笔记(李沐)-PyTorch神经网络基础",
            "date_published": "2023-02-28T02:34:00.000Z",
            "content_html": "<blockquote>\n<p>李沐动手学深度学习（PyTorch）课程学习笔记第四章：PyTorch 神经网络基础。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-层和块\">1. 层和块</h2>\n<p>在构造自定义块之前，我们先回顾一下多层感知机（第三章第二节）的代码，下面的代码生成一个网络，其中包含一个具有256个单元和 ReLU 激活函数的全连接隐藏层，然后是一个具有10个隐藏单元且不带激活函数的全连接输出层：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\">net = nn.Sequential(nn.Linear(<span class=\"number\">20</span>, <span class=\"number\">256</span>), nn.ReLU(), nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">X = torch.rand(<span class=\"number\">2</span>, <span class=\"number\">20</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net(X).shape)  <span class=\"comment\"># torch.Size([2, 10])</span></span><br></pre></td></tr></table></figure>\n<p>在这个例子中，我们通过实例化 <code>nn.Sequential</code> 来构建我们的模型，层的执行顺序是作为参数传递的。简而言之，<code>nn.Sequential</code> 定义了一种特殊的 <code>Module</code>，即在 PyTorch 中表示一个块的类，它维护了一个由 <code>Module</code> 组成的<strong>有序列表</strong>。注意，两个全连接层都是 <code>Linear</code> 类的实例，<code>Linear</code> 类本身就是 <code>Module</code> 的子类。另外，到目前为止，我们一直在通过 <code>net(X)</code> 调用我们的模型来获得模型的输出。这实际上是 <code>net.__call__(X)</code> 的简写。这个前向传播函数非常简单：它将列表中的每个块连接在一起，将每个块的输出作为下一个块的输入。</p>\n<p>在下面的代码片段中，我们从零开始编写一个块。它包含一个多层感知机，其具有256个隐藏单元的隐藏层和一个10维输出层。注意，下面的 <code>MLP</code> 类继承了表示块的类。我们的实现只需要提供我们自己的构造函数（Python 中的 <code>__init__</code> 函数）和前向传播函数：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MLP</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"comment\"># 模型参数声明层。这里，我们声明两个全连接的层</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 调用MLP的父类Module的构造函数来执行必要的初始化。</span></span><br><span class=\"line\">        <span class=\"comment\"># 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.hidden = nn.Linear(<span class=\"number\">20</span>, <span class=\"number\">256</span>)  <span class=\"comment\"># 隐藏层</span></span><br><span class=\"line\">        self.out = nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">10</span>)  <span class=\"comment\"># 输出层</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 定义模型的前向传播，即如何根据输入X返回所需的模型输出</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.out(F.relu(self.hidden(X)))</span><br></pre></td></tr></table></figure>\n<p>接着我们实例化多层感知机的层，然后在每次调用前向传播函数时调用这些层：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = MLP()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net(X).shape)  <span class=\"comment\"># torch.Size([2, 10])</span></span><br></pre></td></tr></table></figure>\n<p>现在我们可以更仔细地看看 <code>Sequential</code> 类是如何工作的，回想一下 <code>Sequential</code> 的设计是为了把其他模块串起来。为了构建我们自己的简化的 <code>MySequential</code>，我们只需要定义两个关键函数：</p>\n<ul>\n<li>一种将块逐个追加到列表中的函数；</li>\n<li>一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。</li>\n</ul>\n<p>下面的 <code>MySequential</code> 类提供了与默认 <code>Sequential</code> 类相同的功能：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MySequential</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, *args</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> idx, module <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(args):</span><br><span class=\"line\">            <span class=\"comment\"># 这里，module是Module子类的一个实例，我们把它保存在&#x27;Module&#x27;类的成员</span></span><br><span class=\"line\">            <span class=\"comment\"># 变量_modules中，_module的类型是OrderedDict</span></span><br><span class=\"line\">            self._modules[<span class=\"built_in\">str</span>(idx)] = module</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        <span class=\"comment\"># OrderedDict保证了按照成员添加的顺序遍历它们</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> block <span class=\"keyword\">in</span> self._modules.values():</span><br><span class=\"line\">            X = block(X)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> X</span><br><span class=\"line\"></span><br><span class=\"line\">net = MySequential(nn.Linear(<span class=\"number\">20</span>, <span class=\"number\">256</span>), nn.ReLU(), nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net(X).shape)  <span class=\"comment\"># torch.Size([2, 10])</span></span><br></pre></td></tr></table></figure>\n<p><code>Sequential</code> 类使模型构造变得简单，允许我们组合新的架构，而不必定义自己的类。然而，并不是所有的架构都是简单的顺序架构。当需要更强的灵活性时，我们需要定义自己的块。例如，我们可能希望在前向传播函数中执行 Python 的控制流。此外，我们可能希望执行任意的数学运算，而不是简单地依赖预定义的神经网络层：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">FixedHiddenMLP</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"comment\"># 不计算梯度的随机权重参数，因此其在训练期间保持不变</span></span><br><span class=\"line\">        self.rand_weight = torch.rand((<span class=\"number\">20</span>, <span class=\"number\">20</span>), requires_grad=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        self.linear = nn.Linear(<span class=\"number\">20</span>, <span class=\"number\">20</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        X = self.linear(X)</span><br><span class=\"line\">        <span class=\"comment\"># 使用创建的常量参数以及relu和mm函数</span></span><br><span class=\"line\">        X = F.relu(torch.mm(X, self.rand_weight) + <span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 复用全连接层，这相当于两个全连接层共享参数</span></span><br><span class=\"line\">        X = self.linear(X)</span><br><span class=\"line\">        <span class=\"comment\"># 控制流</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> X.<span class=\"built_in\">abs</span>().<span class=\"built_in\">sum</span>() &gt; <span class=\"number\">1</span>:</span><br><span class=\"line\">            X /= <span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> X.<span class=\"built_in\">sum</span>()</span><br></pre></td></tr></table></figure>\n<p>我们可以混合搭配各种组合块的方法。在下面的例子中，我们以一些想到的方法嵌套块：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">NestMLP</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.net = nn.Sequential(nn.Linear(<span class=\"number\">20</span>, <span class=\"number\">64</span>), nn.ReLU(), nn.Linear(<span class=\"number\">64</span>, <span class=\"number\">32</span>), nn.ReLU())</span><br><span class=\"line\">        self.linear = nn.Linear(<span class=\"number\">32</span>, <span class=\"number\">16</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.linear(self.net(X))</span><br><span class=\"line\"></span><br><span class=\"line\">chimera = nn.Sequential(NestMLP(), nn.Linear(<span class=\"number\">16</span>, <span class=\"number\">20</span>), FixedHiddenMLP())</span><br><span class=\"line\"><span class=\"built_in\">print</span>(chimera(X))  <span class=\"comment\"># tensor(-0.2911, grad_fn=&lt;SumBackward0&gt;)</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"2-参数管理\">2. 参数管理</h2>\n<p>我们首先看一下具有单隐藏层的多层感知机：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\">net = nn.Sequential(nn.Linear(<span class=\"number\">4</span>, <span class=\"number\">8</span>), nn.ReLU(), nn.Linear(<span class=\"number\">8</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">X = torch.rand(size=(<span class=\"number\">2</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net(X).shape)  <span class=\"comment\"># torch.Size([2, 1])</span></span><br></pre></td></tr></table></figure>\n<p>我们从已有模型中访问参数。当通过 <code>Sequential</code> 类定义模型时，我们可以通过索引来访问模型的任意层。这就像模型是一个列表一样，每层的参数都在其属性中。如下所示，我们可以检查第二个全连接层的参数：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">2</span>].state_dict())</span><br><span class=\"line\"><span class=\"comment\"># OrderedDict([(&#x27;weight&#x27;, tensor([[-0.1326,  0.1692, -0.0925, -0.1721,  0.0828, -0.0890, -0.0742, -0.2730]])), (&#x27;bias&#x27;, tensor([0.2011]))])</span></span><br></pre></td></tr></table></figure>\n<p>这个全连接层包含两个参数，分别是该层的权重和偏置，每个参数都表示为参数类的一个实例。要对参数执行任何操作，首先我们需要访问底层的数值：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">type</span>(net[<span class=\"number\">2</span>].bias))  <span class=\"comment\"># &lt;class &#x27;torch.nn.parameter.Parameter&#x27;&gt;</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">2</span>].bias)  <span class=\"comment\"># Parameter containing: tensor([-0.2786], requires_grad=True)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">2</span>].bias.data)  <span class=\"comment\"># tensor([-0.1571])</span></span><br></pre></td></tr></table></figure>\n<p>参数是复合的对象，包含值、梯度和额外信息。这就是我们需要显式参数值的原因。除了值之外，我们还可以访问每个参数的梯度。在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">2</span>].weight.grad == <span class=\"literal\">None</span>)  <span class=\"comment\"># True</span></span><br></pre></td></tr></table></figure>\n<p>当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。下面，我们将通过演示来比较访问第一个全连接层的参数和访问所有层：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(*[(name, param.shape) <span class=\"keyword\">for</span> name, param <span class=\"keyword\">in</span> net[<span class=\"number\">0</span>].named_parameters()])</span><br><span class=\"line\"><span class=\"comment\"># (&#x27;weight&#x27;, torch.Size([8, 4])) (&#x27;bias&#x27;, torch.Size([8]))</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(*[(name, param.shape) <span class=\"keyword\">for</span> name, param <span class=\"keyword\">in</span> net.named_parameters()])</span><br><span class=\"line\"><span class=\"comment\"># (&#x27;0.weight&#x27;, torch.Size([8, 4])) (&#x27;0.bias&#x27;, torch.Size([8])) (&#x27;2.weight&#x27;, torch.Size([1, 8])) (&#x27;2.bias&#x27;, torch.Size([1]))</span></span><br></pre></td></tr></table></figure>\n<p>这为我们提供了另一种访问网络参数的方式，如下所示：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(net.state_dict()[<span class=\"string\">&#x27;2.bias&#x27;</span>].data)  <span class=\"comment\"># tensor([0.0992])</span></span><br></pre></td></tr></table></figure>\n<p>现在让我们看看如果我们将多个块相互嵌套，参数命名约定是如何工作的：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">block1</span>():</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(nn.Linear(<span class=\"number\">4</span>, <span class=\"number\">8</span>), nn.ReLU(), nn.Linear(<span class=\"number\">8</span>, <span class=\"number\">4</span>), nn.ReLU())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">block2</span>():</span><br><span class=\"line\">    net = nn.Sequential()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">4</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 在这里嵌套</span></span><br><span class=\"line\">        net.add_module(<span class=\"string\">f&#x27;block <span class=\"subst\">&#123;i&#125;</span>&#x27;</span>, block1())</span><br><span class=\"line\">    <span class=\"keyword\">return</span> net</span><br><span class=\"line\"></span><br><span class=\"line\">rgnet = nn.Sequential(block2(), nn.Linear(<span class=\"number\">4</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(rgnet)</span><br><span class=\"line\"><span class=\"comment\"># Sequential(</span></span><br><span class=\"line\"><span class=\"comment\">#   (0): Sequential(</span></span><br><span class=\"line\"><span class=\"comment\">#     (block 0): Sequential(</span></span><br><span class=\"line\"><span class=\"comment\">#       (0): Linear(in_features=4, out_features=8, bias=True)</span></span><br><span class=\"line\"><span class=\"comment\">#       (1): ReLU()</span></span><br><span class=\"line\"><span class=\"comment\">#       ...</span></span><br></pre></td></tr></table></figure>\n<p>因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(rgnet[<span class=\"number\">0</span>][<span class=\"number\">1</span>][<span class=\"number\">0</span>].bias.data)  <span class=\"comment\"># tensor([ 0.4102,  0.1565,  0.1458,  0.0826,  0.2460, -0.0115, -0.4241,  0.1192])</span></span><br></pre></td></tr></table></figure>\n<p>知道了如何访问参数后，现在我们看看如何正确地初始化参数。深度学习框架提供默认随机初始化，也允许我们创建自定义初始化方法。</p>\n<p>让我们首先调用内置的初始化器。下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量，且将偏置参数设置为0：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_normal</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.normal_(m.weight, mean=<span class=\"number\">0</span>, std=<span class=\"number\">0.01</span>)</span><br><span class=\"line\">        nn.init.zeros_(m.bias)</span><br><span class=\"line\"></span><br><span class=\"line\">net.apply(init_normal)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">0</span>].weight.data[<span class=\"number\">0</span>], net[<span class=\"number\">0</span>].bias.data[<span class=\"number\">0</span>])  <span class=\"comment\"># tensor([0.0037, 0.0052, 0.0020, 0.0028]) tensor(0.)</span></span><br></pre></td></tr></table></figure>\n<p>我们还可以将所有参数初始化为给定的常数，比如初始化为1：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nn.init.constant_(m.weight, <span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n<p>我们还可以对某些块应用不同的初始化方法。例如，下面我们使用 Xavier 初始化方法初始化第一个神经网络层，然后将第三个神经网络层初始化为常量值42：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_xavier</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.xavier_uniform_(m.weight)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_42</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.constant_(m.weight, <span class=\"number\">42</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">net[<span class=\"number\">0</span>].apply(init_xavier)</span><br><span class=\"line\">net[<span class=\"number\">2</span>].apply(init_42)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">0</span>].weight.data[<span class=\"number\">0</span>])  <span class=\"comment\"># tensor([-0.7014,  0.1061,  0.2061,  0.2125])</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">2</span>].weight.data)  <span class=\"comment\"># tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])</span></span><br></pre></td></tr></table></figure>\n<p>有时，深度学习框架没有提供我们需要的初始化方法。在下面的例子中，我们先初始化为-10~10的均匀分布，然后将绝对值小于5的参数置零：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">my_init</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Init&quot;</span>, *[(name, param.shape) <span class=\"keyword\">for</span> name, param <span class=\"keyword\">in</span> m.named_parameters()][<span class=\"number\">0</span>])</span><br><span class=\"line\">        nn.init.uniform_(m.weight, -<span class=\"number\">10</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">        m.weight.data *= m.weight.data.<span class=\"built_in\">abs</span>() &gt;= <span class=\"number\">5</span></span><br><span class=\"line\"></span><br><span class=\"line\">net.apply(my_init)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">0</span>].weight[:<span class=\"number\">2</span>])</span><br><span class=\"line\"><span class=\"comment\"># tensor([[-0.0000,  8.9053,  0.0000,  8.9382],</span></span><br><span class=\"line\"><span class=\"comment\">#         [-9.5017, -0.0000,  5.4470, -0.0000]], grad_fn=&lt;SliceBackward0&gt;)</span></span><br></pre></td></tr></table></figure>\n<p>有时我们希望在多个层间共享参数：我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">shared = nn.Linear(<span class=\"number\">8</span>, <span class=\"number\">8</span>)  <span class=\"comment\"># 我们需要给共享层一个名称，以便可以引用它的参数</span></span><br><span class=\"line\">net = nn.Sequential(nn.Linear(<span class=\"number\">4</span>, <span class=\"number\">8</span>), nn.ReLU(), shared, nn.ReLU(),</span><br><span class=\"line\">                    shared, nn.ReLU(), nn.Linear(<span class=\"number\">8</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"comment\"># 检查参数是否相同</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">2</span>].weight.data[<span class=\"number\">0</span>] == net[<span class=\"number\">4</span>].weight.data[<span class=\"number\">0</span>])  <span class=\"comment\"># tensor([True, True, True, True, True, True, True, True])</span></span><br><span class=\"line\">net[<span class=\"number\">2</span>].weight.data[<span class=\"number\">0</span>, <span class=\"number\">0</span>] = <span class=\"number\">100</span></span><br><span class=\"line\"><span class=\"comment\"># 确保它们实际上是同一个对象，而不只是有相同的值</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">2</span>].weight.data[<span class=\"number\">0</span>] == net[<span class=\"number\">4</span>].weight.data[<span class=\"number\">0</span>])  <span class=\"comment\"># tensor([True, True, True, True, True, True, True, True])</span></span><br></pre></td></tr></table></figure>\n<p>这个例子表明第三个和第五个神经网络层的参数是绑定的。它们不仅值相等，而且由相同的张量表示。因此，如果我们改变其中一个参数，另一个参数也会改变。这里有一个问题：当参数绑定时，梯度会发生什么情况？答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层（即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。</p>\n<h2 id=\"3-自定义层\">3. 自定义层</h2>\n<p>我们构造一个没有任何参数的自定义层，下面的 <code>CenteredLayer</code> 类要从其输入中减去均值。要构建它，我们只需继承基础层类并实现前向传播功能：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">CenteredLayer</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> X - X.mean()</span><br><span class=\"line\"></span><br><span class=\"line\">layer = CenteredLayer()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(layer(torch.FloatTensor([<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>])))  <span class=\"comment\"># tensor([-2., -1.,  0.,  1.,  2.])</span></span><br></pre></td></tr></table></figure>\n<p>下面我们继续定义具有参数的层，这些参数可以通过训练进行调整。让我们实现自定义版本的全连接层。回想一下，该层需要两个参数，一个用于表示权重，另一个用于表示偏置项。在此实现中，我们使用 ReLU 作为激活函数。该层需要输入参数：<code>in_units</code> 和 <code>units</code>，分别表示输入数和输出数：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MyLinear</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, in_units, units</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.weight = nn.Parameter(torch.randn(in_units, units))</span><br><span class=\"line\">        self.bias = nn.Parameter(torch.randn(units,))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        linear = torch.matmul(X, self.weight.data) + self.bias.data</span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.relu(linear)</span><br><span class=\"line\"></span><br><span class=\"line\">linear = MyLinear(<span class=\"number\">5</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(linear.weight.shape)  <span class=\"comment\"># torch.Size([5, 3])</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"4-读写文件\">4. 读写文件</h2>\n<p>加载和保存张量：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\">x = torch.arange(<span class=\"number\">4</span>)</span><br><span class=\"line\">torch.save(x, <span class=\"string\">&#x27;../save/x-file&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>将存储在文件中的数据读回内存：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x2 = torch.load(<span class=\"string\">&#x27;../save/x-file&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x2)  <span class=\"comment\"># tensor([0, 1, 2, 3])</span></span><br></pre></td></tr></table></figure>\n<p>存储一个张量列表，然后把它们读回内存：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y = torch.zeros(<span class=\"number\">4</span>)</span><br><span class=\"line\">torch.save([x, y],<span class=\"string\">&#x27;../save/x-files&#x27;</span>)</span><br><span class=\"line\">x2, y2 = torch.load(<span class=\"string\">&#x27;../save/x-files&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>((x2, y2))  <span class=\"comment\"># (tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))</span></span><br></pre></td></tr></table></figure>\n<p>我们可以写入或读取从字符串映射到张量的字典。当我们要读取或写入模型中的所有权重时，这很方便：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mydict = &#123;<span class=\"string\">&#x27;x&#x27;</span>: x, <span class=\"string\">&#x27;y&#x27;</span>: y&#125;</span><br><span class=\"line\">torch.save(mydict, <span class=\"string\">&#x27;../save/mydict&#x27;</span>)</span><br><span class=\"line\">mydict2 = torch.load(<span class=\"string\">&#x27;../save/mydict&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(mydict2)  <span class=\"comment\"># &#123;&#x27;x&#x27;: tensor([0, 1, 2, 3]), &#x27;y&#x27;: tensor([0., 0., 0., 0.])&#125;</span></span><br></pre></td></tr></table></figure>\n<p>深度学习框架提供了内置函数来保存和加载整个网络。需要注意的一个重要细节是，这将<strong>保存模型的参数</strong>而不是保存整个模型：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MLP</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.hidden = nn.Linear(<span class=\"number\">20</span>, <span class=\"number\">256</span>)</span><br><span class=\"line\">        self.output = nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.output(F.relu(self.hidden(x)))</span><br><span class=\"line\"></span><br><span class=\"line\">net = MLP()</span><br><span class=\"line\">X = torch.randn(size=(<span class=\"number\">2</span>, <span class=\"number\">20</span>))</span><br><span class=\"line\">Y = net(X)</span><br><span class=\"line\"></span><br><span class=\"line\">torch.save(net.state_dict(), <span class=\"string\">&#x27;../save/mlp.params&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 为了恢复模型，我们实例化了原始多层感知机模型的一个备份。这里我们不需要随机初始化模型参数，而是直接读取文件中存储的参数</span></span><br><span class=\"line\">clone = MLP()</span><br><span class=\"line\">clone.load_state_dict(torch.load(<span class=\"string\">&#x27;../save/mlp.params&#x27;</span>))</span><br><span class=\"line\">clone.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\">Y_clone = clone(X)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Y_clone == Y)</span><br><span class=\"line\"><span class=\"comment\"># tensor([[True, True, True, True, True, True, True, True, True, True],</span></span><br><span class=\"line\"><span class=\"comment\">#         [True, True, True, True, True, True, True, True, True, True]])</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"5-GPU\">5. GPU</h2>\n<p>CUDA 的安装配置教程：<a href=\"/posts/15428.html\">Anaconda与PyTorch安装教程</a></p>\n<p>在 PyTorch 中，CPU 和 GPU 可以用 <code>torch.device('cpu')</code> 和 <code>torch.device('cuda')</code> 表示：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.device(<span class=\"string\">&#x27;cpu&#x27;</span>), torch.device(<span class=\"string\">&#x27;cuda&#x27;</span>), torch.device(<span class=\"string\">&#x27;cuda:0&#x27;</span>))  <span class=\"comment\"># cpu cuda cuda:0</span></span><br></pre></td></tr></table></figure>\n<p>我们可以查询可用 GPU 的数量：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.device_count())  <span class=\"comment\"># 1</span></span><br></pre></td></tr></table></figure>\n<p>我们可以查询张量所在的设备。默认情况下，张量是在 CPU 上创建的：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.tensor([<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.device)  <span class=\"comment\"># cpu</span></span><br></pre></td></tr></table></figure>\n<p>需要注意的是，无论何时我们要对多个项进行操作，它们都必须在同一个设备上。例如，如果我们对两个张量求和，我们需要确保两个张量都位于同一个设备上，否则框架将不知道在哪里存储结果，甚至不知道在哪里执行计算。</p>\n<p>有几种方法可以在 GPU 上存储张量。例如，我们可以在创建张量时指定存储设备：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.ones(<span class=\"number\">2</span>, <span class=\"number\">3</span>, device=<span class=\"string\">&#x27;cuda&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(X.device)  <span class=\"comment\"># cuda:0</span></span><br></pre></td></tr></table></figure>\n<p>数据在同一个 GPU 上我们才可以将它们相加：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Y = X.cuda(<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Y.device)  <span class=\"comment\"># cuda:0</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>((X + Y).device)  <span class=\"comment\"># cuda:0</span></span><br></pre></td></tr></table></figure>\n<p>类似地，神经网络模型可以指定设备。下面的代码将模型参数放在 GPU 上：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(nn.Linear(<span class=\"number\">3</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">net = net.to(device=<span class=\"string\">&#x27;cuda&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(net(X))  <span class=\"comment\"># tensor([[-0.0370], [-0.0370]], device=&#x27;cuda:0&#x27;, grad_fn=&lt;AddmmBackward0&gt;)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">0</span>].weight.data.device)  <span class=\"comment\"># cuda:0</span></span><br></pre></td></tr></table></figure>\n<p>总之，只要所有的数据和参数都在同一个设备上，我们就可以有效地学习模型。</p>\n<p>下一章：<a href=\"/posts/25122.html\">卷积神经网络</a>。</p>\n",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/46068.html",
            "url": "https://asanosaki.github.io/posts/46068.html",
            "title": "动手学深度学习笔记(李沐)-多层感知机",
            "date_published": "2023-02-18T08:36:00.000Z",
            "content_html": "<blockquote>\n<p>李沐动手学深度学习（PyTorch）课程学习笔记第三章：多层感知机。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-多层感知机的从零实现\">1. 多层感知机的从零实现</h2>\n<p>FashionMNIST 数据集的读取与第二章第四节一样，此处不再放上代码。</p>\n<p>初始化模型参数，我们将实现一个具有单隐藏层的多层感知机，它包含256个隐藏单元。注意，我们可以将这两个变量都视为超参数。通常，我们选择2的若干次幂作为层的宽度。因为内存在硬件中的分配和寻址方式，这么做往往可以在计算上更高效。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_inputs, num_outputs, num_hiddens = <span class=\"number\">784</span>, <span class=\"number\">10</span>, <span class=\"number\">256</span></span><br><span class=\"line\"></span><br><span class=\"line\">W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=<span class=\"literal\">True</span>) * <span class=\"number\">0.01</span>)</span><br><span class=\"line\">b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=<span class=\"literal\">True</span>))</span><br><span class=\"line\">W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad=<span class=\"literal\">True</span>) * <span class=\"number\">0.01</span>)</span><br><span class=\"line\">b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=<span class=\"literal\">True</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">params = [W1, b1, W2, b2]</span><br></pre></td></tr></table></figure>\n<p>为了确保我们对模型的细节了如指掌，我们将实现 ReLU 激活函数，而不是直接调用内置的 <code>relu</code> 函数：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">relu</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">    a = torch.zeros_like(X)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.<span class=\"built_in\">max</span>(X, a)</span><br></pre></td></tr></table></figure>\n<p>接下来定义模型：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">net</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">    H = relu(torch.matmul(X.reshape((-<span class=\"number\">1</span>, num_inputs)), W1) + b1)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.matmul(H, W2) + b2</span><br></pre></td></tr></table></figure>\n<p>训练函数的代码也与2.4节基本一样，只需将 <code>net.to(device)</code> 与 <code>net.train()</code> 等与 <code>nn.Module</code> 相关的代码去掉即可，因此不放出完整代码：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_classifier</span>(<span class=\"params\">net, train_iter, test_iter, num_epochs, lr</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;---------- Training on cpu ----------&#x27;</span>)</span><br><span class=\"line\">    loss_function = nn.CrossEntropyLoss()  <span class=\"comment\"># reduction默认为&#x27;mean&#x27;</span></span><br><span class=\"line\">    optimizer = torch.optim.SGD(params, lr=lr)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        <span class=\"comment\"># train</span></span><br><span class=\"line\">        <span class=\"comment\"># valid</span></span><br><span class=\"line\"></span><br><span class=\"line\">lr, num_epochs = <span class=\"number\">0.1</span>, <span class=\"number\">10</span></span><br><span class=\"line\">train_classifier(net, train_iter, train_iter, num_epochs, lr)</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-多层感知机的简洁实现\">2. 多层感知机的简洁实现</h2>\n<p>与 Softmax 回归的简洁实现（第二章第四节）相比，唯一的区别是我们添加了2个全连接层（之前我们只添加了1个全连接层）。第一层是隐藏层，它包含256个隐藏单元，并使用了 ReLU 激活函数。第二层是输出层，因此我们只需要重点看一下模型即可：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> util.functions <span class=\"keyword\">import</span> train_classifier</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 读取数据</span></span><br><span class=\"line\"></span><br><span class=\"line\">num_inputs, num_outputs, num_hiddens = <span class=\"number\">784</span>, <span class=\"number\">10</span>, <span class=\"number\">256</span></span><br><span class=\"line\">net = nn.Sequential(nn.Flatten(),</span><br><span class=\"line\">                    nn.Linear(num_inputs, num_hiddens),</span><br><span class=\"line\">                    nn.ReLU(),</span><br><span class=\"line\">                    nn.Linear(num_hiddens, num_outputs))</span><br><span class=\"line\"></span><br><span class=\"line\">device = torch.device(<span class=\"string\">&#x27;cuda&#x27;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\">lr, num_epochs = <span class=\"number\">0.1</span>, <span class=\"number\">10</span></span><br><span class=\"line\">train_classifier(net, train_iter, test_iter, num_epochs, lr, device)</span><br><span class=\"line\"><span class=\"comment\"># [ Train | epoch: 010/010 ] loss = 0.35276, acc = 0.87549</span></span><br><span class=\"line\"><span class=\"comment\"># [ Valid | epoch: 010/010 ] loss = 0.38964, acc = 0.85898</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"3-模型选择、欠拟合和过拟合\">3. 模型选择、欠拟合和过拟合</h2>\n<p>首先我们需要了解<strong>训练误差</strong>和<strong>泛化误差</strong>：</p>\n<ul>\n<li>训练误差（training error）：模型在训练数据集上计算得到的误差。</li>\n<li>泛化误差（generalization error）：模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。</li>\n</ul>\n<p>问题是，我们永远不能准确地计算出泛化误差。这是因为无限多的数据样本是一个虚构的对象。在实际中，我们只能通过将模型应用于一个<strong>独立</strong>的测试集来估计泛化误差，该测试集由随机选取的、未曾在训练集中出现的数据样本构成。</p>\n<p>在机器学习中，我们通常在评估几个候选模型后选择最终的模型。这个过程叫做模型选择。有时，需要进行比较的模型在本质上是完全不同的（比如，决策树与线性模型）。又有时，我们需要比较<strong>不同的超参数设置</strong>下的同一类模型。</p>\n<p>例如，训练多层感知机模型时，我们可能希望比较具有不同数量的隐藏层、不同数量的隐藏单元以及不同的激活函数组合的模型。为了确定候选模型中的最佳模型，我们通常会使用<strong>验证集</strong>。</p>\n<ul>\n<li>验证数据集：一个用来评估模型好坏的数据集，训练数据集用来训练模型参数，验证数据集用来选择模型超参数。\n<ul>\n<li>例如在数据集中拿出50%的训练数据作为验证数据集。</li>\n<li>不要跟训练数据混在一起（常犯错误）。</li>\n</ul>\n</li>\n<li>测试数据集：只用一次的数据集。\n<ul>\n<li>例如未来的考试。</li>\n<li>例如我出价的房子的实际成交价。</li>\n</ul>\n</li>\n</ul>\n<p>接下来我们需要了解一下<strong>模型容量</strong>的概念：</p>\n<ul>\n<li>模型容量表示模型拟合各种函数的能力。</li>\n<li>低容量的模型难以拟合复杂的训练数据。</li>\n<li>高容量的模型可以记住所有的训练数据。</li>\n</ul>\n<p>是否过拟合或欠拟合主要取决于<strong>模型复杂性</strong>和<strong>可用训练数据集的大小</strong>。当我们比较训练和验证误差时，我们要注意两种常见的情况：</p>\n<ul>\n<li>训练误差和验证误差都很差，但它们之间仅有一点差距。如果模型不能降低训练误差，这可能意味着模型过于简单（即表达能力不足），无法捕获试图学习的模式。此外，由于我们的训练和验证误差之间的泛化误差很小，我们有理由相信可以用一个更复杂的模型降低训练误差。这种现象被称为欠拟合（underfitting）。</li>\n<li>当我们的训练误差<strong>明显低于</strong>验证误差时要小心，这表明严重的过拟合（overfitting）。注意，过拟合并不总是一件坏事。特别是在深度学习领域，众所周知，最好的预测模型在训练数据上的表现往往比在保留（验证）数据上好得多。最终，我们通常更关心验证误差，而不是训练误差和验证误差之间的差距。</li>\n</ul>\n<h2 id=\"4-权重衰减\">4. 权重衰减</h2>\n<p>在训练参数化机器学习模型时，权重衰减（weight decay）是最广泛使用的正则化的技术之一，它通常也被称为L2正则化。</p>\n<p>通过限制参数值的选择范围来控制模型容量，通常不限制偏移 <code>b</code>。</p>\n<p>首先生成一些数据：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils <span class=\"keyword\">import</span> data</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.tensorboard <span class=\"keyword\">import</span> SummaryWriter</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">n_train, n_test, num_inputs, batch_size = <span class=\"number\">20</span>, <span class=\"number\">100</span>, <span class=\"number\">200</span>, <span class=\"number\">5</span>  <span class=\"comment\"># 训练数据少，特征维度大，因此容易过拟合</span></span><br><span class=\"line\">true_w, true_b = torch.ones((num_inputs, <span class=\"number\">1</span>)) * <span class=\"number\">0.01</span>, <span class=\"number\">0.05</span></span><br><span class=\"line\"></span><br><span class=\"line\">train_data = d2l.synthetic_data(true_w, true_b, n_train)</span><br><span class=\"line\">train_iter = d2l.load_array(train_data, batch_size)</span><br><span class=\"line\">test_data = d2l.synthetic_data(true_w, true_b, n_test)</span><br><span class=\"line\">test_iter = d2l.load_array(test_data, batch_size, is_train=<span class=\"literal\">False</span>)</span><br></pre></td></tr></table></figure>\n<p>定义网络模型与损失函数：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(nn.Linear(num_inputs, <span class=\"number\">1</span>))</span><br><span class=\"line\">loss_function = nn.MSELoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>定义优化算法，我们在实例化优化器时直接通过 <code>weight_decay</code> 指定 <code>weight decay</code> 超参数。默认情况下，PyTorch 同时衰减权重和偏移。这里我们只为权重设置了 <code>weight_decay</code>，所以偏置参数不会衰减：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_epochs, lr, wd = <span class=\"number\">100</span>, <span class=\"number\">0.003</span>, <span class=\"number\">0</span>  <span class=\"comment\"># wd为0表示不使用权重衰减</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 偏置参数没有衰减，net[0]即为nn.Linear(num_inputs, 1)</span></span><br><span class=\"line\">optimizer = torch.optim.SGD([&#123;<span class=\"string\">&#x27;params&#x27;</span>:net[<span class=\"number\">0</span>].weight, <span class=\"string\">&#x27;weight_decay&#x27;</span>:wd&#125;, &#123;<span class=\"string\">&#x27;params&#x27;</span>:net[<span class=\"number\">0</span>].bias&#125;], lr=lr)</span><br></pre></td></tr></table></figure>\n<p>然后进行训练：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">writer = SummaryWriter(<span class=\"string\">&#x27;../logs/WeightDecay_train_log&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> net.parameters():</span><br><span class=\"line\">    param.data.normal_()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">    train_loss = <span class=\"number\">0.0</span></span><br><span class=\"line\">    test_loss = <span class=\"number\">0.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    net.train()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">        y_hat = net(X)</span><br><span class=\"line\">        loss = loss_function(y_hat, y)</span><br><span class=\"line\">        optimizer.zero_grad()</span><br><span class=\"line\">        loss.mean().backward()</span><br><span class=\"line\">        optimizer.step()</span><br><span class=\"line\">        train_loss += loss.<span class=\"built_in\">sum</span>()</span><br><span class=\"line\">    train_loss /= n_train</span><br><span class=\"line\"></span><br><span class=\"line\">    net.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> test_iter:</span><br><span class=\"line\">            y_hat = net(X)</span><br><span class=\"line\">            loss = loss_function(y_hat, y)</span><br><span class=\"line\">            test_loss += loss.<span class=\"built_in\">sum</span>()</span><br><span class=\"line\">    test_loss /= n_test</span><br><span class=\"line\"></span><br><span class=\"line\">    writer.add_scalar(<span class=\"string\">&#x27;train_loss&#x27;</span>, train_loss, epoch + <span class=\"number\">1</span>)</span><br><span class=\"line\">    writer.add_scalar(<span class=\"string\">&#x27;test_loss&#x27;</span>, test_loss, epoch + <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;w的L2范数：&#x27;</span>, net[<span class=\"number\">0</span>].weight.norm().item())</span><br></pre></td></tr></table></figure>\n<p>通过结果可以看到模型很快就过拟合了，可以通过修改超参数 <code>wd</code> 的值应用权重衰减的方式来缓解过拟合的现象。</p>\n<h2 id=\"5-暂退法（Dropout）\">5. 暂退法（Dropout）</h2>\n<p>一个好的模型需要对输入数据的扰动鲁棒。在训练过程中，建议在计算后续层之前<strong>向网络的每一层注入噪声</strong>，因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性。</p>\n<p>这个想法被称为暂退法（dropout）。暂退法在前向传播过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。这种方法之所以被称为暂退法，因为我们从表面上看是<strong>在训练过程中丢弃（drop out）一些神经元</strong>。在整个训练过程的每一次迭代中，标准暂退法包括<strong>在计算下一层之前将当前层中的一些节点置零</strong>。</p>\n<p>在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。换言之，每个中间活性值 <code>h</code> 以暂退概率 <code>p</code> 由随机变量替换。</p>\n<p>Dropout 说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率 <code>p</code> 停止工作（将一些输出项随机置0），这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征。</p>\n<p>我们可以将暂退法应用于每个隐藏层的输出（在激活函数之后），并且可以为每一层分别设置暂退概率：常见的技巧是在靠近输入层的地方设置较低的暂退概率。下面的模型将第一个和第二个隐藏层的暂退概率分别设置为0.2和0.5，并且暂退法<strong>只在训练期间有效</strong>。</p>\n<p>Dropout 的从零实现核心代码如下：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">dropout_layer</span>(<span class=\"params\">X, p</span>):</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"number\">0</span> &lt;= p &lt;= <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"comment\"># 在本情况中，所有元素都被丢弃</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> p == <span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.zeros_like(X)</span><br><span class=\"line\">    <span class=\"comment\"># 在本情况中，所有元素都被保留</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> p == <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> X</span><br><span class=\"line\">    mask = (torch.rand(X.shape) &gt; p).<span class=\"built_in\">float</span>()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> mask * X / (<span class=\"number\">1.0</span> - p)</span><br><span class=\"line\"></span><br><span class=\"line\">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class=\"number\">784</span>, <span class=\"number\">10</span>, <span class=\"number\">256</span>, <span class=\"number\">256</span></span><br><span class=\"line\">p1, p2 = <span class=\"number\">0.2</span>, <span class=\"number\">0.5</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Net</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, num_inputs, num_outputs, num_hiddens1, num_hiddens2, is_training = <span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Net, self).__init__()</span><br><span class=\"line\">        self.num_inputs = num_inputs</span><br><span class=\"line\">        self.training = is_training</span><br><span class=\"line\">        self.lin1 = nn.Linear(num_inputs, num_hiddens1)</span><br><span class=\"line\">        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)</span><br><span class=\"line\">        self.lin3 = nn.Linear(num_hiddens2, num_outputs)</span><br><span class=\"line\">        self.relu = nn.ReLU()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        H1 = self.relu(self.lin1(X.reshape((-<span class=\"number\">1</span>, self.num_inputs))))</span><br><span class=\"line\">        <span class=\"comment\"># 只有在训练模型时才使用dropout</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.training == <span class=\"literal\">True</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class=\"line\">            H1 = dropout_layer(H1, p1)</span><br><span class=\"line\">        H2 = self.relu(self.lin2(H1))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.training == <span class=\"literal\">True</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class=\"line\">            H2 = dropout_layer(H2, p2)</span><br><span class=\"line\">        out = self.lin3(H2)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out</span><br><span class=\"line\"></span><br><span class=\"line\">net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)</span><br></pre></td></tr></table></figure>\n<p>Dropout 的简洁实现及完整训练代码如下：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils <span class=\"keyword\">import</span> data</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms</span><br><span class=\"line\"><span class=\"keyword\">from</span> util.functions <span class=\"keyword\">import</span> train_classifier</span><br><span class=\"line\"></span><br><span class=\"line\">mnist_train = torchvision.datasets.FashionMNIST(root=<span class=\"string\">&quot;../data&quot;</span>, train=<span class=\"literal\">True</span>, transform=transforms.ToTensor(), download=<span class=\"literal\">True</span>)</span><br><span class=\"line\">mnist_test = torchvision.datasets.FashionMNIST(root=<span class=\"string\">&quot;../data&quot;</span>, train=<span class=\"literal\">False</span>, transform=transforms.ToTensor(), download=<span class=\"literal\">True</span>)</span><br><span class=\"line\">train_iter = data.DataLoader(mnist_train, batch_size=<span class=\"number\">256</span>, shuffle=<span class=\"literal\">True</span>, num_workers=<span class=\"number\">0</span>)</span><br><span class=\"line\">test_iter = data.DataLoader(mnist_test, batch_size=<span class=\"number\">256</span>, shuffle=<span class=\"literal\">False</span>, num_workers=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">net = nn.Sequential(nn.Flatten(),</span><br><span class=\"line\">                    nn.Linear(<span class=\"number\">784</span>, <span class=\"number\">256</span>),</span><br><span class=\"line\">                    nn.ReLU(),</span><br><span class=\"line\">                    nn.Dropout(<span class=\"number\">0.2</span>),</span><br><span class=\"line\">                    nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">256</span>),</span><br><span class=\"line\">                    nn.ReLU(),</span><br><span class=\"line\">                    nn.Dropout(<span class=\"number\">0.5</span>),</span><br><span class=\"line\">                    nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">device = torch.device(<span class=\"string\">&#x27;cuda&#x27;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\">lr, num_epochs = <span class=\"number\">0.1</span>, <span class=\"number\">10</span></span><br><span class=\"line\">writer_path = <span class=\"string\">&#x27;../logs/Dropout_train_log&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">train_classifier(net, train_iter, test_iter, num_epochs, lr, device, writer_path)</span><br><span class=\"line\"><span class=\"comment\"># [ Train | epoch: 010/010 ] loss = 0.37249, acc = 0.86702</span></span><br><span class=\"line\"><span class=\"comment\"># [ Valid | epoch: 010/010 ] loss = 0.37481, acc = 0.86348</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"6-数值稳定性和模型初始化\">6. 数值稳定性和模型初始化</h2>\n<p>我们可能面临一些问题，要么是<strong>梯度爆炸</strong>（gradient exploding）问题：参数更新过大，破坏了模型的稳定收敛；要么是<strong>梯度消失</strong>（gradient vanishing）问题：参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。</p>\n<p>例如当 Sigmoid 函数的输入很大或是很小时，它的梯度都会消失。此外，当反向传播通过许多层时，除非我们在刚刚好的地方，这些地方 Sigmoid 函数的输入接近于零，否则整个乘积的梯度可能会消失。</p>\n<p>Xavier 初始化：从均值为零，方差为 <code>2 / (n_in + n_out)</code> 的高斯分布中采样权重（<code>n_in</code> 为输入的数量，<code>n_out</code> 为输出的数量）。</p>\n<p>下一章：<a href=\"/posts/23526.html\">PyTorch 神经网络基础</a>。</p>\n",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/19931.html",
            "url": "https://asanosaki.github.io/posts/19931.html",
            "title": "动手学深度学习笔记(李沐)-线性神经网络",
            "date_published": "2023-02-02T11:25:00.000Z",
            "content_html": "<blockquote>\n<p>李沐动手学深度学习（PyTorch）课程学习笔记第二章：线性神经网络。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-线性回归的从零实现\">1. 线性回归的从零实现</h2>\n<p>为了简单起见，我们将根据带有噪声的线性模型构造一个人造数据集。我们的任务是使用这个有限样本的数据集来恢复这个模型的参数。</p>\n<p>首先我们生成数据集：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> random</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 生成数据集</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">synthetic_data</span>(<span class=\"params\">w, b, num_examples</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;生成y = Xw + b + 噪声&quot;&quot;&quot;</span></span><br><span class=\"line\">    X = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">1</span>, (num_examples, <span class=\"built_in\">len</span>(w)))  <span class=\"comment\"># 均值为0，方差为1的随机数，大小为(num_examples, len(w))</span></span><br><span class=\"line\">    y = torch.matmul(X, w) + b  <span class=\"comment\"># y = Xw + b</span></span><br><span class=\"line\">    y += torch.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, y.shape)  <span class=\"comment\"># 加入一个随机噪音</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> X, y.reshape((-<span class=\"number\">1</span>, <span class=\"number\">1</span>))  <span class=\"comment\"># y为列向量</span></span><br><span class=\"line\"></span><br><span class=\"line\">true_w = torch.tensor([<span class=\"number\">2</span>, -<span class=\"number\">3.4</span>])</span><br><span class=\"line\">true_b = <span class=\"number\">4.2</span></span><br><span class=\"line\">features, labels = synthetic_data(true_w, true_b, <span class=\"number\">1000</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;features:&#x27;</span>, features[<span class=\"number\">0</span>],<span class=\"string\">&#x27;\\nlabel:&#x27;</span>, labels[<span class=\"number\">0</span>])</span><br><span class=\"line\"><span class=\"comment\"># features: tensor([ 0.7764, -1.2998])</span></span><br><span class=\"line\"><span class=\"comment\"># label: tensor([10.1756])</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 通过生成第二个特征features[:, 1]和labels的散点图，可以直观观察到两者之间的线性关系</span></span><br><span class=\"line\">plt.plot(features[:, <span class=\"number\">1</span>].detach().numpy(), labels.detach().numpy(), <span class=\"string\">&#x27;ro&#x27;</span>, ms=<span class=\"number\">3</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>训练模型时要对数据集进行遍历，每次抽取一小批量样本，并使用它们来更新我们的模型。由于这个过程是训练机器学习算法的基础，所以有必要定义一个函数，该函数能打乱数据集中的样本并以小批量方式获取数据。</p>\n<p>在下面的代码中，我们定义一个 <code>data_iter</code> 函数，该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为 <code>batch_size</code> 的小批量。每个小批量包含一组特征和标签：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">data_iter</span>(<span class=\"params\">batch_size, features, labels</span>):</span><br><span class=\"line\">    num_examples = <span class=\"built_in\">len</span>(features)</span><br><span class=\"line\">    indices = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(num_examples))</span><br><span class=\"line\">    <span class=\"comment\"># 这些样本是随机读取的，没有特定的顺序，因此要打乱下标</span></span><br><span class=\"line\">    random.shuffle(indices)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, num_examples, batch_size):</span><br><span class=\"line\">        batch_indices = torch.tensor(indices[i:<span class=\"built_in\">min</span>(i + batch_size, num_examples)])</span><br><span class=\"line\">        <span class=\"keyword\">yield</span> features[batch_indices], labels[batch_indices]</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> data_iter(batch_size, features, labels):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(X, <span class=\"string\">&#x27;\\n&#x27;</span>, y)</span><br><span class=\"line\">    <span class=\"keyword\">break</span></span><br></pre></td></tr></table></figure>\n<p>在我们开始用小批量随机梯度下降优化我们的模型参数之前，我们需要先有一些参数。在下面的代码中，我们通过从均值为0、标准差为0.01的正态分布中采样随机数来<strong>初始化权重</strong>，并将偏置初始化为0：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">w = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, size=(<span class=\"number\">2</span>, <span class=\"number\">1</span>), requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">b = torch.zeros(<span class=\"number\">1</span>, requires_grad=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n<p>定义线性回归模型：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">linreg</span>(<span class=\"params\">X, w, b</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;线性回归模型&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.matmul(X, w) + b</span><br></pre></td></tr></table></figure>\n<p>定义损失函数：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">squared_loss</span>(<span class=\"params\">y_hat, y</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;均方损失&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (y_hat - y.reshape(y_hat.shape)) ** <span class=\"number\">2</span> / <span class=\"number\">2</span></span><br></pre></td></tr></table></figure>\n<p>定义优化算法：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sgd</span>(<span class=\"params\">params, lr, batch_size</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;小批量随机梯度下降&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():  <span class=\"comment\"># 更新参数的时候不需要计算梯度</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> params:</span><br><span class=\"line\">            param -= lr * param.grad / batch_size</span><br><span class=\"line\">            param.grad.zero_()  <span class=\"comment\"># 将梯度清零</span></span><br></pre></td></tr></table></figure>\n<p>现在我们已经准备好了模型训练所有需要的要素，可以实现主要的训练过程部分了。理解这段代码至关重要，因为从事深度学习后，相同的训练过程几乎一遍又一遍地出现。在每次迭代中，我们读取一小批量训练样本，并通过我们的模型来获得一组预测。计算完损失后，我们开始反向传播，存储每个参数的梯度。最后，我们调用优化算法（随机梯度下降法SGD）来更新模型参数：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 超参数</span></span><br><span class=\"line\">lr, num_epochs = <span class=\"number\">0.03</span>, <span class=\"number\">3</span></span><br><span class=\"line\">net = linreg</span><br><span class=\"line\">loss_function = squared_loss</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> data_iter(batch_size, features, labels):</span><br><span class=\"line\">        loss = loss_function(net(X, w, b), y)  <span class=\"comment\"># X和y的小批量损失</span></span><br><span class=\"line\">        <span class=\"comment\"># loss的形状是(batch_size, 1)，而不是标量，将loss中的所有元素加到一起，并以此计算关于[w, b]的梯度</span></span><br><span class=\"line\">        loss.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">        sgd([w, b], lr, batch_size)  <span class=\"comment\"># 使用参数的梯度更新参数</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">        train_loss = loss_function(net(features, w, b), labels)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;epoch <span class=\"subst\">&#123;epoch + <span class=\"number\">1</span>&#125;</span>, loss <span class=\"subst\">&#123;<span class=\"built_in\">float</span>(train_loss.mean()):f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-线性回归的简洁实现\">2. 线性回归的简洁实现</h2>\n<p>首先生成数据集：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils <span class=\"keyword\">import</span> data</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">true_w = torch.tensor([<span class=\"number\">2</span>, -<span class=\"number\">3.4</span>])</span><br><span class=\"line\">true_b = <span class=\"number\">4.2</span></span><br><span class=\"line\">features, labels = d2l.synthetic_data(true_w, true_b, <span class=\"number\">1000</span>)</span><br></pre></td></tr></table></figure>\n<p>读取数据集：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">load_array</span>(<span class=\"params\">data_arrays, batch_size, is_train=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;构造一个PyTorch数据迭代器&quot;&quot;&quot;</span></span><br><span class=\"line\">    dataset = data.TensorDataset(*data_arrays)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">10</span></span><br><span class=\"line\">data_iter = load_array((features, labels), batch_size)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 这里我们使用iter构造Python迭代器，并使用next从迭代器中获取第一项</span></span><br><span class=\"line\">feature, label = <span class=\"built_in\">next</span>(<span class=\"built_in\">iter</span>(data_iter))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(feature)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(label)</span><br></pre></td></tr></table></figure>\n<p>接下来我们定义模型，在 PyTorch 中，全连接层在 <code>Linear</code> 类中定义。值得注意的是，我们将两个参数传递到 <code>nn.Linear</code> 中。第一个指定输入特征形状，即2，第二个指定输出特征形状，输出特征形状为单个标量，因此为1：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\">net = nn.Sequential(nn.Linear(<span class=\"number\">2</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 初始化模型参数</span></span><br><span class=\"line\">net[<span class=\"number\">0</span>].weight.data.normal_(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>)</span><br><span class=\"line\">net[<span class=\"number\">0</span>].bias.data.fill_(<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>\n<p>定义损失函数，计算均方误差使用的是 <code>MSELoss</code> 类，也称平方 L2 范数，默认情况下，它返回所有样本损失的平均值：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">loss_function = nn.MSELoss()</span><br></pre></td></tr></table></figure>\n<p>定义优化算法：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">optimizer = torch.optim.SGD(net.parameters(), lr=<span class=\"number\">0.03</span>)</span><br></pre></td></tr></table></figure>\n<p>训练过程代码与我们从零开始实现时所做的非常相似：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_epochs = <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> data_iter:</span><br><span class=\"line\">        loss = loss_function(net(X), y)</span><br><span class=\"line\">        optimizer.zero_grad()</span><br><span class=\"line\">        loss.backward()</span><br><span class=\"line\">        optimizer.step()</span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">        train_loss = loss_function(net(features), labels)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;epoch <span class=\"subst\">&#123;epoch + <span class=\"number\">1</span>&#125;</span>, loss <span class=\"subst\">&#123;train_loss:f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-Softmax回归的从零实现\">3. Softmax回归的从零实现</h2>\n<p>首先读入 Fashion-MNIST 数据集，原始数据集中的每个样本都是28*28的图像。本节将展平每个图像，把它们看作长度为784的向量。在后面的章节中，我们将讨论能够利用<strong>图像空间结构</strong>的特征，但现在我们暂时只把每个像素位置看作一个特征。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils <span class=\"keyword\">import</span> data</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.tensorboard <span class=\"keyword\">import</span> SummaryWriter</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">load_data_fashion_mnist</span>(<span class=\"params\">batch_size, resize=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中&quot;&quot;&quot;</span></span><br><span class=\"line\">    trans = [transforms.ToTensor()]</span><br><span class=\"line\">    <span class=\"keyword\">if</span> resize:</span><br><span class=\"line\">        trans.insert(<span class=\"number\">0</span>, transforms.Resize(resize))</span><br><span class=\"line\">    trans = transforms.Compose(trans)</span><br><span class=\"line\">    mnist_train = torchvision.datasets.FashionMNIST(root=<span class=\"string\">&quot;../data&quot;</span>, train=<span class=\"literal\">True</span>, transform=trans, download=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    mnist_test = torchvision.datasets.FashionMNIST(root=<span class=\"string\">&quot;../data&quot;</span>, train=<span class=\"literal\">False</span>, transform=trans, download=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class=\"literal\">True</span>, num_workers=<span class=\"number\">0</span>),</span><br><span class=\"line\">            data.DataLoader(mnist_test, batch_size, shuffle=<span class=\"literal\">False</span>, num_workers=<span class=\"number\">0</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\"></span><br><span class=\"line\">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>\n<p>初始化模型参数，在 Softmax 回归中，我们的输出与类别一样多。因为我们的数据集有10个类别，所以网络输出维度为10。因此，权重将构成一个784*10的矩阵，偏置将构成一个1*10的行向量：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_inputs = <span class=\"number\">784</span></span><br><span class=\"line\">num_outputs = <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\">W = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, size=(num_inputs, num_outputs), requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">b = torch.zeros(num_outputs, requires_grad=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n<p>定义 Softmax 操作，注意，虽然这在数学上看起来是正确的，但我们在代码实现中有点草率。矩阵中的非常大或非常小的元素可能造成数值上溢或下溢，但我们没有采取措施来防止这点：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">softmax</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">    X_exp = torch.exp(X)</span><br><span class=\"line\">    partition = X_exp.<span class=\"built_in\">sum</span>(<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X_exp / partition  <span class=\"comment\"># 这里应用了广播机制</span></span><br></pre></td></tr></table></figure>\n<p>定义 Softmax 操作后，我们可以实现 Softmax 回归模型。下面的代码定义了输入如何通过网络映射到输出。注意，将数据传递到模型之前，我们使用 <code>reshape</code> 函数将每张原始图像展平为向量：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">net</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> softmax(torch.matmul(X.reshape((-<span class=\"number\">1</span>, W.shape[<span class=\"number\">0</span>])), W) + b)</span><br></pre></td></tr></table></figure>\n<p>接下来我们定义损失函数，交叉熵采用真实标签的预测概率的<strong>负对数似然</strong>。这里我们不使用 Python 的 <code>for</code> 循环迭代预测（这往往是低效的），而是通过一个运算符选择所有元素。下面我们创建一个数据样本 <code>y_hat</code>，其中包含2个样本在3个类别的预测概率，以及它们对应的标签 <code>y</code>。有了 <code>y</code>，我们知道在第一个样本中，第一类是正确的预测；而在第二个样本中，第三类是正确的预测。然后使用 <code>y</code> 作为 <code>y_hat</code> 中概率的索引，我们选择第一个样本中第一个类的概率和第二个样本中第三个类的概率：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y = torch.tensor([<span class=\"number\">0</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\">y_hat = torch.tensor([[<span class=\"number\">0.1</span>, <span class=\"number\">0.3</span>, <span class=\"number\">0.6</span>], [<span class=\"number\">0.3</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.5</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(y_hat[[<span class=\"number\">0</span>, <span class=\"number\">1</span>], y])  <span class=\"comment\"># tensor([0.1000, 0.5000])</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">cross_entropy</span>(<span class=\"params\">y_hat, y</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> -torch.log(y_hat[<span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(y_hat)), y])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(cross_entropy(y_hat, y))  <span class=\"comment\"># tensor([2.3026, 0.6931])</span></span><br></pre></td></tr></table></figure>\n<p>为了计算精度，我们执行以下操作。首先，如果 <code>y_hat</code> 是矩阵，那么假定第二个维度存储每个类的预测分数。我们使用 <code>argmax</code> 获得每行中<strong>最大</strong>元素的索引来获得预测类别。然后我们将预测类别与真实 <code>y</code> 元素进行比较。由于等式运算符 <code>==</code> 对数据类型很敏感，因此我们将 <code>y_hat</code> 的数据类型转换为与 <code>y</code> 的数据类型一致。结果是一个包含0（错）和1（对）的张量。最后，我们求和会得到正确预测的数量：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">accuracy</span>(<span class=\"params\">y_hat, y</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;计算预测正确的数量&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(y_hat.shape) &gt; <span class=\"number\">1</span> <span class=\"keyword\">and</span> y_hat.shape[<span class=\"number\">1</span>] &gt; <span class=\"number\">1</span>:</span><br><span class=\"line\">        y_hat = y_hat.argmax(axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">    cmp = y_hat.<span class=\"built_in\">type</span>(y.dtype) == y  <span class=\"comment\"># tensor([False,  True])</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">float</span>(cmp.<span class=\"built_in\">type</span>(y.dtype).<span class=\"built_in\">sum</span>())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(accuracy(y_hat, y) / <span class=\"built_in\">len</span>(y))  <span class=\"comment\"># 0.5</span></span><br></pre></td></tr></table></figure>\n<p>同样，对于任意数据迭代器 <code>data_iter</code> 可访问的数据集，我们可以评估在任意模型 <code>net</code> 的精度，这里定义一个实用程序类 <code>Accumulator</code>，用于对多个变量进行累加。在 <code>evaluate_accuracy</code> 函数中，我们在 <code>Accumulator</code> 实例中创建了2个变量，分别用于存储正确预测的数量和预测的总数量。当我们遍历数据集时，两者都将随着时间的推移而累加：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">evaluate_accuracy</span>(<span class=\"params\">net, data_iter</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;计算在指定数据集上模型的精度&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, torch.nn.Module):  <span class=\"comment\"># 如果是用torch.nn实现的模型</span></span><br><span class=\"line\">        net.<span class=\"built_in\">eval</span>()  <span class=\"comment\"># 将模型先设置为评估模式</span></span><br><span class=\"line\">    metric = Accumulator(<span class=\"number\">2</span>)  <span class=\"comment\"># 正确预测数、预测总数</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> data_iter:</span><br><span class=\"line\">            metric.add(accuracy(net(X), y), y.numel())</span><br><span class=\"line\">    <span class=\"keyword\">return</span> metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Accumulator</span>:</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;在n个变量上累加&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, n</span>):</span><br><span class=\"line\">        self.data = [<span class=\"number\">0.0</span>] * n</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">add</span>(<span class=\"params\">self, *args</span>):</span><br><span class=\"line\">        self.data = [a + <span class=\"built_in\">float</span>(b) <span class=\"keyword\">for</span> a, b <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(self.data, args)]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">reset</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        self.data = [<span class=\"number\">0.0</span>] * <span class=\"built_in\">len</span>(self.data)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__getitem__</span>(<span class=\"params\">self, idx</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.data[idx]</span><br></pre></td></tr></table></figure>\n<p>接下来可以开始进行训练：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_epoch_ch3</span>(<span class=\"params\">net, train_iter, loss_function, updater</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;训练模型一个迭代周期（定义见第3章）&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 如果net是用torch.nn实现的话先将模型设置为训练模式</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, torch.nn.Module):</span><br><span class=\"line\">        net.train()</span><br><span class=\"line\">    <span class=\"comment\"># 训练损失总和、训练准确度总和、样本数</span></span><br><span class=\"line\">    metric = Accumulator(<span class=\"number\">3</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">        <span class=\"comment\"># 计算梯度并更新参数</span></span><br><span class=\"line\">        y_hat = net(X)</span><br><span class=\"line\">        loss = loss_function(y_hat, y)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class=\"line\">            <span class=\"comment\"># 使用PyTorch内置的优化器和损失函数</span></span><br><span class=\"line\">            updater.zero_grad()</span><br><span class=\"line\">            loss.mean().backward()</span><br><span class=\"line\">            updater.step()</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 使用定制的优化器和损失函数</span></span><br><span class=\"line\">            loss.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">            updater(X.shape[<span class=\"number\">0</span>])</span><br><span class=\"line\">        metric.add(<span class=\"built_in\">float</span>(loss.<span class=\"built_in\">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class=\"line\">    <span class=\"comment\"># 返回训练损失和训练精度</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">2</span>], metric[<span class=\"number\">1</span>] / metric[<span class=\"number\">2</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_ch3</span>(<span class=\"params\">net, train_iter, test_iter, loss_function, num_epochs, updater</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;训练模型&quot;&quot;&quot;</span></span><br><span class=\"line\">    writer = SummaryWriter(<span class=\"string\">&#x27;../logs/FashionMNIST_train_log&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        train_metrics = train_epoch_ch3(net, train_iter, loss_function, updater)</span><br><span class=\"line\">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class=\"line\">        writer.add_scalar(<span class=\"string\">&#x27;train_loss&#x27;</span>, train_metrics[<span class=\"number\">0</span>], epoch + <span class=\"number\">1</span>)</span><br><span class=\"line\">        writer.add_scalar(<span class=\"string\">&#x27;train_acc&#x27;</span>, train_metrics[<span class=\"number\">1</span>], epoch + <span class=\"number\">1</span>)</span><br><span class=\"line\">        writer.add_scalar(<span class=\"string\">&#x27;test_acc&#x27;</span>, test_acc, epoch + <span class=\"number\">1</span>)</span><br><span class=\"line\">    train_loss, train_acc = train_metrics</span><br><span class=\"line\">    writer.close()</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> train_loss &lt; <span class=\"number\">0.5</span>, train_loss</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> train_acc &lt;= <span class=\"number\">1</span> <span class=\"keyword\">and</span> train_acc &gt; <span class=\"number\">0.7</span>, train_acc</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> test_acc &lt;= <span class=\"number\">1</span> <span class=\"keyword\">and</span> test_acc &gt; <span class=\"number\">0.7</span>, test_acc</span><br><span class=\"line\"></span><br><span class=\"line\">lr = <span class=\"number\">0.1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">updater</span>(<span class=\"params\">batch_size</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> d2l.sgd([W, b], lr, batch_size)</span><br><span class=\"line\"></span><br><span class=\"line\">num_epochs = <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)</span><br></pre></td></tr></table></figure>\n<p>可以在项目路径下打开 Anaconda 的 PyTorch 环境，然后使用 TensorBoard 查看训练曲线：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensorboard --logdir logs\\FashionMNIST_train_log</span><br></pre></td></tr></table></figure>\n<h2 id=\"4-Softmax回归的简洁实现\">4. Softmax回归的简洁实现</h2>\n<p>首先读取数据集：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils <span class=\"keyword\">import</span> data</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.tensorboard <span class=\"keyword\">import</span> SummaryWriter</span><br><span class=\"line\"><span class=\"keyword\">from</span> tqdm <span class=\"keyword\">import</span> tqdm</span><br><span class=\"line\"></span><br><span class=\"line\">mnist_train = torchvision.datasets.FashionMNIST(root=<span class=\"string\">&quot;../data&quot;</span>, train=<span class=\"literal\">True</span>, transform=transforms.ToTensor(), download=<span class=\"literal\">True</span>)</span><br><span class=\"line\">mnist_test = torchvision.datasets.FashionMNIST(root=<span class=\"string\">&quot;../data&quot;</span>, train=<span class=\"literal\">False</span>, transform=transforms.ToTensor(), download=<span class=\"literal\">True</span>)</span><br><span class=\"line\">train_iter = data.DataLoader(mnist_train, batch_size=<span class=\"number\">256</span>, shuffle=<span class=\"literal\">True</span>, num_workers=<span class=\"number\">0</span>)</span><br><span class=\"line\">test_iter = data.DataLoader(mnist_test, batch_size=<span class=\"number\">256</span>, shuffle=<span class=\"literal\">False</span>, num_workers=<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>\n<p>定义模型，我们只需在 <code>Sequential</code> 中添加一个带有10个输出的全连接层，这10个输出分别表示对10个类别的预测概率：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># PyTorch不会隐式地调整输入的形状。因此，我们在线性层前定义了展平层（Flatten），来调整网络输入的形状</span></span><br><span class=\"line\">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class=\"number\">784</span>, <span class=\"number\">10</span>))</span><br></pre></td></tr></table></figure>\n<p>定义训练函数，由于之后很多模型的训练过程也是相似的，因此该函数可以复用：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 以后较为通用的函数将定义到util.functions.py中</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_classifier</span>(<span class=\"params\">net, train_iter, test_iter, num_epochs, lr, device, writer_path=<span class=\"literal\">None</span>, save_path=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">init_weights</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear <span class=\"keyword\">or</span> <span class=\"built_in\">type</span>(m) == nn.Conv2d:</span><br><span class=\"line\">            <span class=\"comment\"># nn.init.normal_(m.weight, mean=0, std=0.01)  # 以均值0和标准差0.01随机初始化权重</span></span><br><span class=\"line\">            nn.init.xavier_uniform_(m.weight)  <span class=\"comment\"># Xavier初始化</span></span><br><span class=\"line\"></span><br><span class=\"line\">    net.apply(init_weights)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;---------- Training on <span class=\"subst\">&#123;device&#125;</span> ----------&#x27;</span>)</span><br><span class=\"line\">    net.to(device)</span><br><span class=\"line\">    loss_function = nn.CrossEntropyLoss()  <span class=\"comment\"># reduction默认为&#x27;mean&#x27;</span></span><br><span class=\"line\">    loss_function.to(device)</span><br><span class=\"line\">    optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> writer_path <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        writer = SummaryWriter(writer_path)</span><br><span class=\"line\"></span><br><span class=\"line\">    best_acc = <span class=\"number\">0.0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        net.train()</span><br><span class=\"line\">        train_loss, train_acc = [], []</span><br><span class=\"line\">        <span class=\"keyword\">for</span> x, y <span class=\"keyword\">in</span> tqdm(train_iter):</span><br><span class=\"line\">            x, y = x.to(device), y.to(device)</span><br><span class=\"line\">            y_hat = net(x)</span><br><span class=\"line\"></span><br><span class=\"line\">            loss = loss_function(y_hat, y)</span><br><span class=\"line\">            optimizer.zero_grad()</span><br><span class=\"line\">            loss.backward()</span><br><span class=\"line\">            optimizer.step()</span><br><span class=\"line\"></span><br><span class=\"line\">            y_hat = y_hat.argmax(axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">            acc = (y_hat.<span class=\"built_in\">type</span>(y.dtype) == y).<span class=\"built_in\">float</span>().mean()</span><br><span class=\"line\">            train_loss.append(loss.item())</span><br><span class=\"line\">            train_acc.append(acc)</span><br><span class=\"line\"></span><br><span class=\"line\">        train_loss = <span class=\"built_in\">sum</span>(train_loss) / <span class=\"built_in\">len</span>(train_loss)</span><br><span class=\"line\">        train_acc = <span class=\"built_in\">sum</span>(train_acc) / <span class=\"built_in\">len</span>(train_acc)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;[ Train | epoch: <span class=\"subst\">&#123;epoch + <span class=\"number\">1</span>:03d&#125;</span>/<span class=\"subst\">&#123;num_epochs:03d&#125;</span> ] loss = <span class=\"subst\">&#123;train_loss:<span class=\"number\">.5</span>f&#125;</span>, acc = <span class=\"subst\">&#123;train_acc:<span class=\"number\">.5</span>f&#125;</span>&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        net.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\">        valid_loss, valid_acc = [], []</span><br><span class=\"line\">        <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">            <span class=\"keyword\">for</span> x, y <span class=\"keyword\">in</span> tqdm(test_iter):</span><br><span class=\"line\">                x, y = x.to(device), y.to(device)</span><br><span class=\"line\">                y_hat = net(x)</span><br><span class=\"line\">                loss = loss_function(y_hat, y)</span><br><span class=\"line\">                y_hat = y_hat.argmax(axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">                acc = (y_hat.<span class=\"built_in\">type</span>(y.dtype) == y).<span class=\"built_in\">float</span>().mean()</span><br><span class=\"line\">                valid_loss.append(loss.item())</span><br><span class=\"line\">                valid_acc.append(acc)</span><br><span class=\"line\"></span><br><span class=\"line\">        valid_loss = <span class=\"built_in\">sum</span>(valid_loss) / <span class=\"built_in\">len</span>(valid_loss)</span><br><span class=\"line\">        valid_acc = <span class=\"built_in\">sum</span>(valid_acc) / <span class=\"built_in\">len</span>(valid_acc)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;[ Valid | epoch: <span class=\"subst\">&#123;epoch + <span class=\"number\">1</span>:03d&#125;</span>/<span class=\"subst\">&#123;num_epochs:03d&#125;</span> ] loss = <span class=\"subst\">&#123;valid_loss:<span class=\"number\">.5</span>f&#125;</span>, acc = <span class=\"subst\">&#123;valid_acc:<span class=\"number\">.5</span>f&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> writer_path <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            writer.add_scalars(<span class=\"string\">&#x27;loss&#x27;</span>, &#123;<span class=\"string\">&#x27;train&#x27;</span>: train_loss,</span><br><span class=\"line\">                                        <span class=\"string\">&#x27;valid&#x27;</span>: valid_loss&#125;, epoch + <span class=\"number\">1</span>)</span><br><span class=\"line\">            writer.add_scalars(<span class=\"string\">&#x27;acc&#x27;</span>, &#123;<span class=\"string\">&#x27;train&#x27;</span>: train_acc,</span><br><span class=\"line\">                                       <span class=\"string\">&#x27;valid&#x27;</span>: valid_acc&#125;, epoch + <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> save_path <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span> <span class=\"keyword\">and</span> valid_acc &gt; best_acc:</span><br><span class=\"line\">            best_acc = valid_acc</span><br><span class=\"line\">            torch.save(net.state_dict(), save_path)</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Saving model with acc &#123;:.3f&#125;&#x27;</span>.<span class=\"built_in\">format</span>(best_acc))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> writer_path <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        writer.close()</span><br></pre></td></tr></table></figure>\n<p>最后设定超参数训练模型：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">writer_path = <span class=\"string\">&#x27;../logs/FashionMNIST_train_log&#x27;</span></span><br><span class=\"line\">device = torch.device(<span class=\"string\">&#x27;cuda&#x27;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\">lr, num_epochs = <span class=\"number\">0.01</span>, <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\">train_classifier(net, train_iter, test_iter, num_epochs, lr, device, writer_path)</span><br><span class=\"line\"><span class=\"comment\"># [ Train | epoch: 010/010 ] loss = 0.60980, acc = 0.80254</span></span><br><span class=\"line\"><span class=\"comment\"># [ Valid | epoch: 010/010 ] loss = 0.62191, acc = 0.79395</span></span><br></pre></td></tr></table></figure>\n<p>下一章：<a href=\"/posts/46068.html\">多层感知机</a>。</p>\n",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/15604.html",
            "url": "https://asanosaki.github.io/posts/15604.html",
            "title": "动手学深度学习笔记(李沐)-预备知识",
            "date_published": "2023-01-18T08:37:00.000Z",
            "content_html": "<blockquote>\n<p>李沐动手学深度学习（PyTorch）课程学习笔记第一章：预备知识。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-环境安装\">1. 环境安装</h2>\n<p>首先安装好 PyTorch 环境：<a href=\"/posts/15428.html\">Anaconda与PyTorch安装教程</a>。</p>\n<p>安装需要的包：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install d2l</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-数据操作与数据预处理\">2. 数据操作与数据预处理</h2>\n<p>张量表示一个数值组成的数组，这个数组可能有多个维度：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\">x = torch.arange(<span class=\"number\">12</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x)  <span class=\"comment\"># tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])</span></span><br></pre></td></tr></table></figure>\n<p>我们可以通过张量的 <code>shape</code> 属性来访问张量的形状和张量中元素的总数：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(x.shape)  <span class=\"comment\"># torch.Size([12])</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.numel())  <span class=\"comment\"># 12</span></span><br></pre></td></tr></table></figure>\n<p>要改变一个张量的形状而不改变元素数量和元素值，我们可以调用 <code>reshape</code> 函数：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = x.reshape(<span class=\"number\">3</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x)</span><br><span class=\"line\"><span class=\"comment\"># tensor([[ 0,  1,  2,  3],</span></span><br><span class=\"line\"><span class=\"comment\">#         [ 4,  5,  6,  7],</span></span><br><span class=\"line\"><span class=\"comment\">#         [ 8,  9, 10, 11]])</span></span><br></pre></td></tr></table></figure>\n<p>使用全0、全1、其他常量或者从特定分布中随机采样的数字：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(torch.zeros((<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>)))</span><br><span class=\"line\"><span class=\"comment\"># tensor([[[0., 0., 0., 0.],</span></span><br><span class=\"line\"><span class=\"comment\">#          [0., 0., 0., 0.],</span></span><br><span class=\"line\"><span class=\"comment\">#          [0., 0., 0., 0.]],</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\">#         [[0., 0., 0., 0.],</span></span><br><span class=\"line\"><span class=\"comment\">#          [0., 0., 0., 0.],</span></span><br><span class=\"line\"><span class=\"comment\">#          [0., 0., 0., 0.]]])</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.ones((<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>)))</span><br><span class=\"line\"><span class=\"comment\"># tensor([[[1., 1., 1., 1.],</span></span><br><span class=\"line\"><span class=\"comment\">#          [1., 1., 1., 1.],</span></span><br><span class=\"line\"><span class=\"comment\">#          [1., 1., 1., 1.]],</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\">#         [[1., 1., 1., 1.],</span></span><br><span class=\"line\"><span class=\"comment\">#          [1., 1., 1., 1.],</span></span><br><span class=\"line\"><span class=\"comment\">#          [1., 1., 1., 1.]]])</span></span><br></pre></td></tr></table></figure>\n<p>通过提供包含数值的 Python 列表（或嵌套列表）来为所需张量中的每个元素赋予确定值：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.tensor([<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x)  <span class=\"comment\"># tensor([2, 3, 4, 5])</span></span><br></pre></td></tr></table></figure>\n<p>常见的标准算术运算符（<code>+</code>、<code>-</code>、<code>*</code>、<code>/</code> 和 <code>**</code>）都可以被升级为按元素运算：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.tensor([<span class=\"number\">1.0</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>])</span><br><span class=\"line\">y = torch.tensor([<span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;x + y:&#x27;</span>, x + y, <span class=\"string\">&#x27;x - y:&#x27;</span>, x - y)  <span class=\"comment\"># x + y: tensor([3., 4., 5.]) x - y: tensor([-1., 0., 1.])</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.exp(x))  <span class=\"comment\"># tensor([ 2.7183,  7.3891, 20.0855])</span></span><br></pre></td></tr></table></figure>\n<p>我们也可以把多个张量拼接在一起：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.arange(<span class=\"number\">6</span>, dtype=torch.float32).reshape(<span class=\"number\">2</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\">y = torch.tensor([[<span class=\"number\">2.0</span>, <span class=\"number\">1</span>, <span class=\"number\">4</span>], [<span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>]])</span><br><span class=\"line\"><span class=\"comment\"># dim表示在哪个维度上拼接</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cat((x, y), dim=<span class=\"number\">0</span>).shape)  <span class=\"comment\"># torch.Size([4, 3])</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cat((x, y), dim=<span class=\"number\">1</span>).shape)  <span class=\"comment\"># torch.Size([2, 6])</span></span><br></pre></td></tr></table></figure>\n<p>通过逻辑运算符构建二元张量：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.tensor([<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>])</span><br><span class=\"line\">y = torch.tensor([<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x == y)  <span class=\"comment\"># tensor([True, False, True])</span></span><br></pre></td></tr></table></figure>\n<p>对张量中的所有元素进行求和会产生一个只有一个元素的张量，或者指定在某一维度上求和：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.tensor([[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>], [<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.<span class=\"built_in\">sum</span>())  <span class=\"comment\"># tensor(9)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.<span class=\"built_in\">sum</span>(<span class=\"number\">0</span>))  <span class=\"comment\"># tensor([2, 3, 4])</span></span><br></pre></td></tr></table></figure>\n<p>即使形状不同，我们仍然可以通过调用广播机制（broadcasting mechanism）来执行按元素操作：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.arange(<span class=\"number\">3</span>).reshape((<span class=\"number\">3</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">y = torch.arange(<span class=\"number\">2</span>).reshape((<span class=\"number\">1</span>, <span class=\"number\">2</span>))</span><br><span class=\"line\"><span class=\"comment\"># 必须维度一样才能广播，先将x复制成3行2列，再将y复制成3行2列，然后运算</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x + y)</span><br><span class=\"line\"><span class=\"comment\"># tensor([[0, 1],</span></span><br><span class=\"line\"><span class=\"comment\">#         [1, 2],</span></span><br><span class=\"line\"><span class=\"comment\">#         [2, 3]])</span></span><br></pre></td></tr></table></figure>\n<p>与 NumPy 张量相互转化：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = x.numpy()</span><br><span class=\"line\">b = torch.tensor(a)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">type</span>(a), <span class=\"built_in\">type</span>(b))  <span class=\"comment\"># &lt;class &#x27;numpy.ndarray&#x27;&gt; &lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br></pre></td></tr></table></figure>\n<p>将大小为1的张量转换为 Python 标量：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.tensor([<span class=\"number\">3.5</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x, x.item(), <span class=\"built_in\">float</span>(x))  <span class=\"comment\"># tensor([3.5000]) 3.5 3.5</span></span><br></pre></td></tr></table></figure>\n<p>创建一个人工数据集，并存储在 CSV（逗号分隔值）文件中：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"></span><br><span class=\"line\">os.makedirs(os.path.join(<span class=\"string\">&#x27;..&#x27;</span>, <span class=\"string\">&#x27;data&#x27;</span>), exist_ok=<span class=\"literal\">True</span>)</span><br><span class=\"line\">data_file = os.path.join(<span class=\"string\">&#x27;..&#x27;</span>, <span class=\"string\">&#x27;data&#x27;</span>, <span class=\"string\">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(data_file, <span class=\"string\">&#x27;w&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;NumRooms,Alley,Price\\n&#x27;</span>)  <span class=\"comment\"># 列名</span></span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;NA,Pave,127500\\n&#x27;</span>)  <span class=\"comment\"># 每行表示一个数据样本</span></span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;2,NA,106000\\n&#x27;</span>)</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;4,NA,178100\\n&#x27;</span>)</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;NA,NA,140000\\n&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>从创建的 CSV 文件中加载原始数据集：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"></span><br><span class=\"line\">data = pd.read_csv(data_file)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(data)</span><br><span class=\"line\"><span class=\"comment\">#    NumRooms Alley   Price</span></span><br><span class=\"line\"><span class=\"comment\"># 0       NaN  Pave  127500</span></span><br><span class=\"line\"><span class=\"comment\"># 1       2.0   NaN  106000</span></span><br><span class=\"line\"><span class=\"comment\"># 2       4.0   NaN  178100</span></span><br><span class=\"line\"><span class=\"comment\"># 3       NaN   NaN  140000</span></span><br></pre></td></tr></table></figure>\n<p>为了处理缺失的数据，典型的方法包括<strong>插值</strong>和<strong>删除</strong>，这里，我们将考虑插值：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inputs, outputs = data.iloc[:, <span class=\"number\">0</span>:<span class=\"number\">2</span>], data.iloc[:, <span class=\"number\">2</span>]  <span class=\"comment\"># 将data的第0、1列作为input，第2列作为output</span></span><br><span class=\"line\">inputs = inputs.fillna(inputs.mean(numeric_only=<span class=\"literal\">True</span>))  <span class=\"comment\"># 将input中为NaN的数据用平均值填充</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(inputs)</span><br><span class=\"line\"><span class=\"comment\">#    NumRooms Alley</span></span><br><span class=\"line\"><span class=\"comment\"># 0       3.0  Pave</span></span><br><span class=\"line\"><span class=\"comment\"># 1       2.0   NaN</span></span><br><span class=\"line\"><span class=\"comment\"># 2       4.0   NaN</span></span><br><span class=\"line\"><span class=\"comment\"># 3       3.0   NaN</span></span><br></pre></td></tr></table></figure>\n<p>对于 <code>inputs</code> 中的类别值或离散值，我们可以将 <code>NaN</code> 视为一个类别：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inputs = pd.get_dummies(inputs, dummy_na=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(inputs)</span><br><span class=\"line\"><span class=\"comment\">#    NumRooms  Alley_Pave  Alley_nan</span></span><br><span class=\"line\"><span class=\"comment\"># 0       3.0           1          0</span></span><br><span class=\"line\"><span class=\"comment\"># 1       2.0           0          1</span></span><br><span class=\"line\"><span class=\"comment\"># 2       4.0           0          1</span></span><br><span class=\"line\"><span class=\"comment\"># 3       3.0           0          1</span></span><br></pre></td></tr></table></figure>\n<p>现在 <code>inputs</code> 和 <code>outputs</code> 中的所有条目都是数值类型，它们可以转换为张量格式：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\">x, y = torch.tensor(inputs.values), torch.tensor(outputs.values)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x, y)</span><br><span class=\"line\"><span class=\"comment\"># tensor([[3., 1., 0.],</span></span><br><span class=\"line\"><span class=\"comment\">#         [2., 0., 1.],</span></span><br><span class=\"line\"><span class=\"comment\">#         [4., 0., 1.],</span></span><br><span class=\"line\"><span class=\"comment\">#         [3., 0., 1.]], dtype=torch.float64) tensor([127500, 106000, 178100, 140000])</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"3-线性代数\">3. 线性代数</h2>\n<p>标量由只有一个元素的张量表示，你可以将向量视为标量值组成的列表：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\">x = torch.tensor(<span class=\"number\">3.0</span>)</span><br><span class=\"line\">y = torch.tensor(<span class=\"number\">2.0</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x + y, x * y)  <span class=\"comment\"># tensor(5.) tensor(6.)</span></span><br><span class=\"line\"></span><br><span class=\"line\">x = torch.arange(<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x)  <span class=\"comment\"># tensor([0, 1, 2, 3])</span></span><br></pre></td></tr></table></figure>\n<p>访问张量的长度和形状：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">len</span>(x))  <span class=\"comment\"># 4</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.shape)  <span class=\"comment\"># torch.Size([4])</span></span><br></pre></td></tr></table></figure>\n<p>矩阵和矩阵的转置：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A = torch.arange(<span class=\"number\">9</span>).reshape(<span class=\"number\">3</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(A.T)</span><br><span class=\"line\"><span class=\"comment\"># tensor([[0, 3, 6],</span></span><br><span class=\"line\"><span class=\"comment\">#         [1, 4, 7],</span></span><br><span class=\"line\"><span class=\"comment\">#         [2, 5, 8]])</span></span><br></pre></td></tr></table></figure>\n<p>给定具有相同形状的任何两个张量，任何按元素二元运算的结果都将是相同形状的张量：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A = torch.arange(<span class=\"number\">12</span>, dtype=torch.float32).reshape(<span class=\"number\">3</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\">B = A.clone()  <span class=\"comment\"># 通过分配新内存，将A的一个副本分配给B</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A + B)</span><br><span class=\"line\"><span class=\"comment\"># tensor([[ 0.,  2.,  4.,  6.],</span></span><br><span class=\"line\"><span class=\"comment\">#         [ 8., 10., 12., 14.],</span></span><br><span class=\"line\"><span class=\"comment\">#         [16., 18., 20., 22.]])</span></span><br></pre></td></tr></table></figure>\n<p>求和与求平均值：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(A.<span class=\"built_in\">sum</span>())  <span class=\"comment\"># tensor(66.)，所有元素求和</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A.<span class=\"built_in\">sum</span>(axis=<span class=\"number\">0</span>))  <span class=\"comment\"># tensor([12., 15., 18., 21.])，在第0维上求和</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A.<span class=\"built_in\">sum</span>(axis=<span class=\"number\">1</span>))  <span class=\"comment\"># tensor([ 6., 22., 38.])，在第1维上求和</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A.mean())  <span class=\"comment\"># tensor(5.5000)，所有元素均值</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A.mean(axis=<span class=\"number\">1</span>))  <span class=\"comment\"># tensor([1.5000, 5.5000, 9.5000])，在第1维上求均值</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A.<span class=\"built_in\">sum</span>(axis=<span class=\"number\">1</span>, keepdims=<span class=\"literal\">True</span>))  <span class=\"comment\"># 计算总和或均值时保持维度不变</span></span><br><span class=\"line\"><span class=\"comment\"># tensor([[ 6.],</span></span><br><span class=\"line\"><span class=\"comment\">#         [22.],</span></span><br><span class=\"line\"><span class=\"comment\">#         [38.]])</span></span><br></pre></td></tr></table></figure>\n<p>点积是相同位置的按元素乘积的和，<code>torch.dot</code> 只能对一维向量做点积。注意 NumPy 中的 <code>np.dot</code> 函数计算的是两个矩阵的矩阵乘法，而非对应元素相乘求和：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.arange(<span class=\"number\">4</span>, dtype=torch.float32)  <span class=\"comment\"># tensor([0., 1., 2., 3.])</span></span><br><span class=\"line\">y = torch.ones(<span class=\"number\">4</span>, dtype=torch.float32)  <span class=\"comment\"># tensor([1., 1., 1., 1.])</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.dot(x, y))  <span class=\"comment\"># tensor(6.)</span></span><br></pre></td></tr></table></figure>\n<p>矩阵向量积：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.arange(<span class=\"number\">6</span>, dtype=torch.float32).reshape(<span class=\"number\">2</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"comment\"># tensor([[0., 1., 2.],</span></span><br><span class=\"line\"><span class=\"comment\">#         [3., 4., 5.]])</span></span><br><span class=\"line\">y = torch.tensor([<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>], dtype=torch.float32)  <span class=\"comment\"># tensor([1., 2., 3.])</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.mv(x, y))  <span class=\"comment\"># tensor([ 8., 26.])</span></span><br></pre></td></tr></table></figure>\n<p>矩阵乘法，<code>torch.mm</code> 与 <code>np.dot</code> 类似：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.arange(<span class=\"number\">6</span>, dtype=torch.float32).reshape(<span class=\"number\">2</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"comment\"># tensor([[0., 1., 2.],</span></span><br><span class=\"line\"><span class=\"comment\">#         [3., 4., 5.]])</span></span><br><span class=\"line\">y = torch.ones(<span class=\"number\">6</span>, dtype=torch.float32).reshape(<span class=\"number\">3</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"comment\"># tensor([[1., 1.],</span></span><br><span class=\"line\"><span class=\"comment\">#         [1., 1.],</span></span><br><span class=\"line\"><span class=\"comment\">#         [1., 1.]])</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.mm(x, y))</span><br><span class=\"line\"><span class=\"comment\"># tensor([[ 3.,  3.],</span></span><br><span class=\"line\"><span class=\"comment\">#         [12., 12.]])</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"built_in\">print</span>(np.dot(x, y))</span><br><span class=\"line\"><span class=\"comment\"># [[ 3.  3.]</span></span><br><span class=\"line\"><span class=\"comment\">#  [12. 12.]]</span></span><br></pre></td></tr></table></figure>\n<p>L2 范数是向量所有元素的平方和的平方根：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.tensor([<span class=\"number\">3</span>, -<span class=\"number\">4</span>], dtype=torch.float32)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.norm(x))  <span class=\"comment\"># tensor(5.)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.norm(x, <span class=\"number\">2</span>))  <span class=\"comment\"># tensor(5.)</span></span><br></pre></td></tr></table></figure>\n<p>L1 范数为向量所有元素的绝对值之和：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(torch.<span class=\"built_in\">abs</span>(x).<span class=\"built_in\">sum</span>())  <span class=\"comment\"># tensor(7.)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.norm(x, <span class=\"number\">1</span>))  <span class=\"comment\"># tensor(7.)</span></span><br></pre></td></tr></table></figure>\n<p>F 范数（弗罗贝尼乌斯范数）是矩阵所有元素的平方和的平方根：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(torch.norm(torch.ones((<span class=\"number\">4</span>, <span class=\"number\">9</span>))))  <span class=\"comment\"># tensor(6.)</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"4-自动微分\">4. 自动微分</h2>\n<p>先举一个简单的例子：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\">x = torch.arange(<span class=\"number\">4.0</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x)  <span class=\"comment\"># tensor([0., 1., 2., 3.])</span></span><br><span class=\"line\"></span><br><span class=\"line\">x.requires_grad_(<span class=\"literal\">True</span>)  <span class=\"comment\"># 等价于x = torch.arange(4.0, requires_grad=True)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad)  <span class=\"comment\"># 默认值是None</span></span><br><span class=\"line\"></span><br><span class=\"line\">y = <span class=\"number\">2</span> * torch.dot(x, x)  <span class=\"comment\"># 计算y，注意一个标量函数关于向量x的梯度是向量，并且与x具有相同的形状</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(y)  <span class=\"comment\"># tensor(28., grad_fn=&lt;MulBackward0&gt;)，隐式构造了计算图，所以有一个求梯度的函数</span></span><br><span class=\"line\"></span><br><span class=\"line\">y.backward()  <span class=\"comment\"># 通过调用反向传播函数来自动计算y关于x每个分量的梯度</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad)  <span class=\"comment\"># tensor([ 0.,  4.,  8., 12.])，y=2*x*x的导数为4x</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad == <span class=\"number\">4</span> * x)  <span class=\"comment\"># tensor([True, True, True, True])</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 现在计算x的另一个函数</span></span><br><span class=\"line\"><span class=\"comment\"># 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值</span></span><br><span class=\"line\">x.grad.zero_()</span><br><span class=\"line\">y = x.<span class=\"built_in\">sum</span>()</span><br><span class=\"line\">y.backward()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad)  <span class=\"comment\"># tensor([1., 1., 1., 1.])</span></span><br></pre></td></tr></table></figure>\n<p>当 <code>y</code> 不是标量时，向量 <code>y</code> 关于向量 <code>x</code> 的导数的最自然解释是一个矩阵。对于高阶和高维的 <code>y</code> 和 <code>x</code>，求导的结果可以是一个高阶张量。</p>\n<p>然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括深度学习中），但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。这里，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 对非标量调用backward()需要传入一个gradient参数，该参数指定微分函数关于self的梯度。</span></span><br><span class=\"line\"><span class=\"comment\"># 本例只想求偏导数的和，所以传递一个1的梯度是合适的</span></span><br><span class=\"line\">x.grad.zero_()</span><br><span class=\"line\">y = x * x</span><br><span class=\"line\"><span class=\"built_in\">print</span>(y)  <span class=\"comment\"># tensor([0., 1., 4., 9.], grad_fn=&lt;MulBackward0&gt;)</span></span><br><span class=\"line\"><span class=\"comment\"># 等价于y.backward(torch.ones(len(x)))</span></span><br><span class=\"line\">y.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad)  <span class=\"comment\"># tensor([0., 2., 4., 6.])</span></span><br></pre></td></tr></table></figure>\n<p>有时，我们希望将某些计算移动到记录的计算图之外。例如，假设 <code>y</code> 是作为 <code>x</code> 的函数计算的，而 <code>z</code> 则是作为 <code>y</code> 和 <code>x</code> 的函数计算的。想象一下，我们想计算 <code>z</code> 关于 <code>x</code> 的梯度，但由于某种原因，希望将 <code>y</code> 视为一个常数，并且只考虑到 <code>x</code> 在 <code>y</code> 被计算后发挥的作用。</p>\n<p>这里可以分离 <code>y</code> 来返回一个新变量 <code>u</code>，该变量与 <code>y</code> 具有相同的值，但丢弃计算图中如何计算 <code>y</code> 的任何信息。换句话说，梯度不会向后流经 <code>u</code> 到 <code>x</code>。因此，下面的反向传播函数计算 <code>z = u * x</code> 关于 <code>x</code> 的偏导数，同时将 <code>u</code> 作为常数处理，而不是计算 <code>z = x * x * x</code> 关于 <code>x</code> 的偏导数。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.grad.zero_()</span><br><span class=\"line\">y = x * x</span><br><span class=\"line\">u = y.detach()</span><br><span class=\"line\">z = u * x</span><br><span class=\"line\"></span><br><span class=\"line\">z.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad == u)  <span class=\"comment\"># tensor([True, True, True, True])</span></span><br><span class=\"line\"></span><br><span class=\"line\">x.grad.zero_()</span><br><span class=\"line\">y.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad == <span class=\"number\">2</span> * x)  <span class=\"comment\"># tensor([True, True, True, True])</span></span><br></pre></td></tr></table></figure>\n<p>使用自动微分的一个好处是：即使构建函数的计算图需要通过 Python 控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到变量的梯度。在下面的代码中，<code>while</code> 循环的迭代次数和 <code>if</code> 语句的结果都取决于输入 <code>a</code> 的值。对于任何 <code>a</code>，存在某个常量标量 <code>k</code>，使得 <code>d = f(a) = k * a</code>，其中 <code>k</code> 的值取决于输入 <code>a</code>，因此可以用 <code>d / a</code> 验证梯度是否正确。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f</span>(<span class=\"params\">a</span>):</span><br><span class=\"line\">    b = a * <span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> b.norm() &lt; <span class=\"number\">1000</span>:</span><br><span class=\"line\">        b = b * <span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> b.<span class=\"built_in\">sum</span>() &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">        c = b</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        c = <span class=\"number\">100</span> * b</span><br><span class=\"line\">    <span class=\"keyword\">return</span> c</span><br><span class=\"line\"></span><br><span class=\"line\">a = torch.randn(size=(), requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">d = f(a)</span><br><span class=\"line\">d.backward()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(a.grad == d / a)  <span class=\"comment\"># tensor(True)</span></span><br></pre></td></tr></table></figure>\n<p>下一章：<a href=\"/posts/19931.html\">线性神经网络</a>。</p>\n",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/33687.html",
            "url": "https://asanosaki.github.io/posts/33687.html",
            "title": "实用机器学习课程笔记(Stanford)",
            "date_published": "2022-12-22T02:28:00.000Z",
            "content_html": "<blockquote>\n<p>Stanford 2021 秋季实用机器学习课程学习笔记。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-概论\">1. 概论</h2>\n<h3 id=\"1-1-课程介绍\">1.1 课程介绍</h3>\n<p><strong>Challenges</strong></p>\n<ul>\n<li>Formulate problem：关注最具影响力的行业问题（如自助超市、自动驾驶汽车等）。</li>\n<li>Data：高质量数据的稀缺、隐私问题。</li>\n<li>Train models：模型越来越复杂，需要大量数据，成本也越来越高。</li>\n<li>Deploy models：计算量大，不适合实时推理。</li>\n<li>Monitor：数据分布的变化、公平性问题。</li>\n</ul>\n<p><strong>Roles（不同类型的人在 ML 中的作用）</strong></p>\n<ul>\n<li>Domain experts（领域专家）：具有商业洞察力，知道什么数据是重要的以及在哪可以找到它，确定一个 ML 模型真正的影响。</li>\n<li>Data scientists：在 Data mining、Model training and deployment 方面做全栈的工作。</li>\n<li>ML experts：定制最先进（state of the art，SOTA）的 ML models。</li>\n<li>SDE（软件开发工程师）：开发/维护数据管道、模型训练和服务管道。</li>\n</ul>\n<p><strong>Corse topics</strong></p>\n<p>本课程的内容为数据科学家需要的技术，但是没有大学传统的 ML/统计/编程方面的教学。</p>\n<ul>\n<li>Data\n<ul>\n<li>收集、处理数据。</li>\n<li>部署的时候场景发生变化导致数据不一样，如 covariate（协变量）、concepts、label 的改变。</li>\n<li>独立同分布之外的数据。</li>\n</ul>\n</li>\n<li>Train\n<ul>\n<li>模型验证、融合、调优。</li>\n<li>迁移学习（Transfer learning）。</li>\n<li>多模态（Multi-modality）：如何把不同的数据源融合起来做一个比较大的模型。</li>\n</ul>\n</li>\n<li>Deploy\n<ul>\n<li>模型怎样部署。</li>\n<li>蒸馏（Distillation）：将比较大的模型提取出精华做的小一点。</li>\n</ul>\n</li>\n<li>Monitor\n<ul>\n<li>公平性，之后会讲模型的公平是什么含义。</li>\n<li>可解释性，怎样理解模型在干什么。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-2-数据获取\">1.2 数据获取</h3>\n<p><strong>Flow Chart for data acquisition</strong></p>\n<p><img src=\"data:image/png;base64,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\" alt=\"\"></p>\n<p><strong>Discover what data is available</strong></p>\n<ul>\n<li>找已经有的数据集。</li>\n<li>找基准数据集来检验我们的想法：\n<ul>\n<li>例如要做一个新的调超参数的算法，可能需要找一些数据集，且数据集不能太大，要小一点的或者中等大小的，为了客观地检验算法还要考虑找不同方面的数据集。</li>\n<li>如果训练比较深的神经网络就需要非常大的数据集。</li>\n</ul>\n</li>\n<li>收集新数据（做一个应用或产品很多时候没有现成的数据集）</li>\n</ul>\n<p><strong>Source of popular ML datasets</strong></p>\n<ul>\n<li>MNIST：手写数字。</li>\n<li>ImageNet：百万级别的来自搜索引擎的图片，可训练较为深度的模型。</li>\n<li>AudioSet：YouTube 上的一些声音的切片，用于做声音的分类。</li>\n<li>Kinetics：YouTube 上的一些视频的切片，用于人的行为分类。</li>\n<li>KITTI：摄像头或激光雷达等各种 sensor 记录下的交通场景。</li>\n<li>Amazon Review：Amazon 产品的用户评论。</li>\n<li>SQuAD：维基中的一些 Q-A 对。</li>\n<li>Librispeech：1000小时的有声读物。</li>\n</ul>\n<p>数据集两大来源：各个网站上爬取，采集数据。</p>\n<p><strong>Where to find datasets</strong></p>\n<ul>\n<li>Paperwithcodes Datasets：学术数据集与排行榜。</li>\n<li>Kaggle Datasets：数据科学家提交的 ML datasets。</li>\n<li>Google Dataset search：搜索网页上的数据集。</li>\n<li>Various toolkits datasets（开源工具包）：TensorFlow、HuggingFace。</li>\n<li>会议/公司的 ML 竞赛。</li>\n<li>自己组织的 Data lakes（数据湖）。</li>\n</ul>\n<p><strong>Datasets comparison</strong></p>\n<table>\n    <thead>\n        <tr>\n            <th>数据集</th>\n            <th>好处</th>\n            <th>坏处</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>学术数据集</td>\n            <td>数据干净、难度适中</td>\n            <td>选择面较小、太精简、规模小</td>\n        </tr>\n        <tr>\n            <td>竞赛数据集</td>\n            <td>更接近真实的 ML 应用</td>\n            <td>仍较精简，且只专注在较热门的应用</td>\n        </tr>\n        <tr>\n            <td>原生数据</td>\n            <td>很灵活</td>\n            <td>需要很多精力去预处理</td>\n        </tr>\n    </tbody>\n</table>\n<p><strong>Data integration</strong></p>\n<p>产品数据通常存放在不同的表中，因此要涉及到表的连接。</p>\n<table>\n    <caption>Table 1</caption>\n    <thead>\n        <tr>\n            <th>ID</th>\n            <th>Val 1</th>\n            <th>Val 2</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>1</td>\n            <td>1_val1</td>\n            <td>1_val2</td>\n        </tr>\n        <tr>\n            <td>2</td>\n            <td>2_val1</td>\n            <td>2_val2</td>\n        </tr>\n    </tbody>\n</table>\n<table>\n    <caption>Table 2</caption>\n    <thead>\n        <tr>\n            <th>ID</th>\n            <th>Val 3</th>\n            <th>Val 4</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>1</td>\n            <td>1_val3</td>\n            <td>1_val4</td>\n        </tr>\n        <tr>\n            <td>3</td>\n            <td>3_val3</td>\n            <td>3_val4</td>\n        </tr>\n    </tbody>\n</table>\n<table>\n    <caption>Inner Join T1 & T2</caption>\n    <thead>\n        <tr>\n            <th>ID</th>\n            <th>Val 1</th>\n            <th>Val 2</th>\n            <th>Val 3</th>\n            <th>Val 4</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>1</td>\n            <td>1_val1</td>\n            <td>1_val2</td>\n            <td>1_val3</td>\n            <td>1_val4</td>\n        </tr>\n    </tbody>\n</table>\n<table>\n    <caption>Left Join T1 & T2</caption>\n    <thead>\n        <tr>\n            <th>ID</th>\n            <th>Val 1</th>\n            <th>Val 2</th>\n            <th>Val 3</th>\n            <th>Val 4</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>1</td>\n            <td>1_val1</td>\n            <td>1_val2</td>\n            <td>1_val3</td>\n            <td>1_val4</td>\n        </tr>\n        <tr>\n            <td>2</td>\n            <td>2_val1</td>\n            <td>2_val2</td>\n            <td>/</td>\n            <td>/</td>\n        </tr>\n    </tbody>\n</table>\n<h3 id=\"1-3-网页数据抓取\">1.3 网页数据抓取</h3>\n<ul>\n<li>一般不能用 <code>curl</code>，因为网站所有者能使用各种方法阻止。</li>\n<li>使用无头浏览器（一个没有 GUI 的网络浏览器）：</li>\n</ul>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> selenium <span class=\"keyword\">import</span> webdriver</span><br><span class=\"line\"></span><br><span class=\"line\">chrome_options = webdriver.ChromeOptions()</span><br><span class=\"line\">chrome_options.headless = <span class=\"literal\">True</span></span><br><span class=\"line\">chrome = webdriver.Chrome(chrome_options = chrome_options)</span><br><span class=\"line\"></span><br><span class=\"line\">page = chrome.get(url)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>你需要大量的新 IP，可以通过云获得很多 IP。</li>\n</ul>\n<h3 id=\"1-4-数据标注\">1.4 数据标注</h3>\n<p><strong>半监督学习Semi-Supervised Learning（SSL）</strong></p>\n<ul>\n<li>重点关注有少量标记数据和大量未标记数据的场景。</li>\n<li>对没有标注的数据和有标注的数据的数据分布做一些假设：\n<ul>\n<li>连续性假设（Continuity assumption）：如果一个样本的特征和另外一个样本相似，那么这两个样本很可能具有相同的标号。</li>\n<li>聚类假设（Cluster assumption）：数据具有内在的聚类结构，那么假设一个类里面具有相同的标号。</li>\n<li>流形假设（Manifold assumption）：很有可能数据在本质上是在低维的一个流形上分布的。</li>\n</ul>\n</li>\n</ul>\n<p><strong>自学习Self-training</strong></p>\n<p><img src=\"data:image/png;base64,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\" alt=\"\"></p>\n<p><strong>主动学习Active Learning</strong></p>\n<ul>\n<li>关注的场景与 SSL 相同，但有人工干预，即选择最有趣的数据（最重要的没有标号的数据）给标注工标注。</li>\n<li>不确定性抽样（Uncertainty sampling）：选择一个最不确信的预测让人来判断。</li>\n</ul>\n<p><strong>Active Learning + Self-training</strong></p>\n<p><img src=\"data:image/png;base64,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\" alt=\"\"></p>\n<p><strong>弱监督学习Weak Supervision</strong></p>\n<ul>\n<li>半自动地生成标号，通常比手动标注的准确率差，但是也是好到可以训练一个还不错的模型。</li>\n<li>数据编程（Data programming）：用启发式的方法赋予标号：\n<ul>\n<li>关键字搜索、模式匹配、第三方模型。</li>\n<li>假设判断一个 YouTube 的评论是垃圾（spam）还是有用的东西（ham）：</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">check_out</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> SPAM <span class=\"keyword\">if</span> <span class=\"string\">&#x27;check out&#x27;</span> <span class=\"keyword\">in</span> x.lower() <span class=\"keyword\">else</span> ABSTAIN</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sentiment</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> HAM <span class=\"keyword\">if</span> sentiment_polarity(x) &gt; <span class=\"number\">0.9</span> <span class=\"keyword\">else</span> ABSTAIN</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-机器学习模型\">2. 机器学习模型</h2>\n<h3 id=\"2-1-机器学习模型概览\">2.1 机器学习模型概览</h3>\n<p><strong>ML 算法的种类</strong></p>\n<ul>\n<li>监督学习（Supervised）：训练有标签的数据来预测标签。\n<ul>\n<li>自监督学习（Self-supervised）：标签的生成来自于数据本身。</li>\n</ul>\n</li>\n<li>半监督学习（Semi-supervised）：在有标签和无标签的数据上进行训练，使用模型来预测无标签数据的标签。</li>\n<li>无监督学习（Unsupervised）：在未标记的数据上进行训练。</li>\n<li>强化学习（Reinforcement）：利用观察与环境互动的结果来采取行动以最大化收益。</li>\n</ul>\n<p>本课程最多讨论的内容为监督学习。</p>\n<p><strong>监督学习的组成部分</strong></p>\n<ul>\n<li>模型（Model）：将输入映射到标签的参数化函数。</li>\n<li>损失（Loss）：衡量模型在预测结果方面有多好，即衡量模型预测出来的值和真实值之间的差距，需要指导模型尽量向真实值靠近。</li>\n<li>目标函数（Objective）：优化模型参数的目标，例如需要优化模型在训练集合上的所有预测结果的损失之和最小。</li>\n<li>优化（Optimization）：解决 Objective 的算法，即把模型中没有指定的参数（可学习的参数）优化为合适的值，使得能够解决目标函数，也就是最小化损失。</li>\n</ul>\n<p><strong>监督学习的模型</strong></p>\n<ul>\n<li>决策树（Decision trees）：用树来做决定。</li>\n<li>线性模型（Linear methods）：决策是根据输入特征的线性组合做出的。</li>\n<li>核方法（Kernel machines）：使用核函数衡量两个样本的特征相似度，达到非线性的效果。</li>\n<li>神经网络（Neural Networks）：使用神经网络学习特征表示。</li>\n</ul>\n<h3 id=\"2-2-决策树\">2.2 决策树</h3>\n<p>优点：</p>\n<ul>\n<li>可以用来解释，即训练后的模型可以看到叶子结点是什么内容，决策是怎么一步步做下来的。</li>\n<li>能够处理数值和类别的特征。</li>\n</ul>\n<p>缺点：</p>\n<ul>\n<li>非常不稳定，可能数据内产生了一点噪音后整棵树构建出来的样子就不一样了。</li>\n<li>如果数据特别复杂，会生成一个特别复杂的树，可以把整个数据里面的各种情况列出来，生成大量的节点，最后会导致过拟合。</li>\n<li>不容易并行计算。</li>\n</ul>\n<p><strong>随机森林</strong></p>\n<ul>\n<li>训练多个决策树以提高稳定性。\n<ul>\n<li>树是并行地独立训练的。</li>\n<li>对于分类问题可以用多数投票法（例如超过一半的树觉得类别是1，那么它就是1），对于回归问题可以在多棵树上取平均。</li>\n</ul>\n</li>\n<li>为什么叫随机呢？\n<ul>\n<li>Bagging：<strong>随机</strong>抽取训练样本并进行替换。例如样本本来是 <code>[1, 2, 3, 4, 5]</code>，做 Bagging 的时候在里面随机采样5个出来，但是采样可能是有重复的，采样到的结果为 <code>[1, 2, 2, 3, 4]</code>，然后拿到这个 Bagging 出来的数据集后我们就在上面训练一棵树，然后一直重复训练 N 棵树为止。</li>\n<li>随机选择一个特征子集，即把 Bagging 出的数据拿出来之后，再从里面的特征中<strong>随机</strong>采样一些特征列出来（假设树是一个表，那么就是先随机采样出一些行，再随机采样出一些列）</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"2-3-线性模型\">2.3 线性模型</h3>\n<p><strong>线性回归</strong></p>\n<ul>\n<li>一个简单的房价预测模型：\n<ul>\n<li>假设有3个特征：卧室数量 <code>x1</code>、浴室数量 <code>x2</code>、居住面积 <code>x3</code>；</li>\n<li>预测价格为：<code>y_hat = w1 * x1 + w2 * x2 + w3 * x3 + b</code>；</li>\n<li>权重 <code>w1, w2, w3</code> 和偏置 <code>b</code> 将从训练数据中学习。</li>\n</ul>\n</li>\n<li>一般来说，给定数据 <code>x = [x1, x2, ..., xp]</code>，线性回归的预测为：<code>y_hat = w1 * x1 + w2 * x2 + ... + wp * xp + b = &lt;w, x&gt; + b</code>（其中 <code>w</code> 和 <code>x</code> 为长度为 <code>p</code> 的向量，<code>&lt;&gt;</code> 表示内积运算，<code>w</code> 和 <code>b</code> 都是可学习参数）。</li>\n</ul>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># weight w has shape (p, 1)</span></span><br><span class=\"line\"><span class=\"comment\"># bias b is a scalar</span></span><br><span class=\"line\"><span class=\"comment\"># data x has shape (p, 1)</span></span><br><span class=\"line\">y_hat = (x * w).<span class=\"built_in\">sum</span>() + b</span><br></pre></td></tr></table></figure>\n<p><strong>线性回归目标函数</strong></p>\n<p>假设我们收集了 <code>n</code> 个训练样本 <code>X = [x1, x2, ..., xn]</code>，其中每个 <code>xi</code> 均为长为 <code>p</code> 的向量，将其转置后即为一个 <code>n</code> 行 <code>p</code> 列的矩阵，其对应的标号为 <code>y = [y1, ..., yn]</code>，是一个长为 <code>n</code> 的向量。</p>\n<p>目标函数是最小化均方误差（MSE），即优化 <code>w, b</code> 的值使得 <code>sum((yi - &lt;xi, w&gt; - b)**2) / n</code> 最小。</p>\n<p><strong>线性回归在分类问题中的应用</strong></p>\n<p>回归的输出是一个连续的实数，而对于分类问题，我们要输出对某个样本的类别的预测。</p>\n<p>多类别分类：</p>\n<ul>\n<li>假设标签为独热编码，即 <code>y = [y1, y2, ..., ym]</code>，如果该样本为第 <code>i</code> 类则 <code>yi = 1</code>，否则 <code>yi = 0</code>。</li>\n<li>预测结果 <code>y_hat = [o1, o2, ..., om]</code>，其中 <code>oi</code> 表示预测该样本为第 <code>i</code> 类的概率。</li>\n<li>为每个类学习一个线性模型：<code>oi = &lt;x, wi&gt; + bi</code>。</li>\n<li>最小化 MSE 损失函数：<code>(y_hat - y)**2 / m</code>。</li>\n<li>预测结果所表示的类为 <code>m</code> 个概率中最大的那个，即 <code>argmax(y_hat)</code>。</li>\n</ul>\n<p><strong>Mini-batch 随机梯度下降</strong></p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># &#x27;batch_size&#x27; 为批大小，&#x27;features&#x27; 为所有样本的特征即 X，&#x27;labels&#x27; 为标签</span></span><br><span class=\"line\"><span class=\"comment\"># &#x27;features&#x27; shape is (n, p), `labels` shape is (n, 1)</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">data_iter</span>(<span class=\"params\">batch_size, features, labels</span>):</span><br><span class=\"line\">    num_examples = <span class=\"built_in\">len</span>(features)  <span class=\"comment\"># 样本数</span></span><br><span class=\"line\">    indices = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(num_examples))  <span class=\"comment\"># 下标</span></span><br><span class=\"line\">    random.shuffle(indices)  <span class=\"comment\"># 随机打乱</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, num_examples, batch_size):</span><br><span class=\"line\">        batch_indices = torch.tensor(</span><br><span class=\"line\">            indices[i:<span class=\"built_in\">min</span>(i + batch_size, num_examples)]</span><br><span class=\"line\">        )</span><br><span class=\"line\">        <span class=\"keyword\">yield</span> features[batch_indices], labels[batch_indices]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># w 用均值为0，方差为0.01的高斯分布初始化</span></span><br><span class=\"line\">w = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, size=(p, <span class=\"number\">1</span>), requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">b = torch.zeros(<span class=\"number\">1</span>, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> x, y <span class=\"keyword\">in</span> data_iter(batch_size, features, labels):  <span class=\"comment\"># 随机取出一个 batch</span></span><br><span class=\"line\">        y_hat = np.dot(x, w) + b</span><br><span class=\"line\">        loss = ((y_hat - y)**<span class=\"number\">2</span> / <span class=\"number\">2</span>).mean()</span><br><span class=\"line\">        loss.backward()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> [w, b]:</span><br><span class=\"line\">            param -= learning_rate * param.grad</span><br><span class=\"line\">            param.grad.zero_()</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-4-神经网络\">2.4 神经网络</h3>\n<p>神经网络就是将手工特征提取的部分换成了一个神经网络。</p>\n<ul>\n<li>神经网络通常需要更多的数据和更多的计算，一般都是大<strong>数个数量级</strong>。</li>\n<li>可以选择不同的神经网络架构来更有效地抽取我们的特征：\n<ul>\n<li>多层感知机。</li>\n<li>卷积神经网络。</li>\n<li>循环神经网络。</li>\n<li>Transformers。</li>\n</ul>\n</li>\n<li>设计神经网络以结合数据的先验知识。</li>\n</ul>\n<p><strong>线性模型到多层感知机（Multilayer Perceptron，MLP）</strong></p>\n<ul>\n<li>引入一种全连接层（稠密层，dense），假设输入样本数量为 <code>n</code>，每个样本的特征长度为 <code>m</code>，那么全连接层具有两个可学习参数 <code>w, b</code>，其中 <code>w</code> 是一个 <code>n</code> 行 <code>m</code> 列的实数矩阵，<code>b</code> 是一个长为 <code>n</code> 的向量。则全连接层的计算结果为：<code>y = np.dot(w, x) + b</code>。</li>\n<li>线性回归可以认为是一个只有一个输出的全连接层。</li>\n<li>Softmax 回归可以认为是一个有 C 个输出的全连接层，C 表示类别的数量。</li>\n</ul>\n<p>多层感知机的目的是实现一个非线性的模型，但是如果只是简单使用多个全连接层是没用的，多个线性操作的叠加还是一个线性操作，因此还需要加入非线性函数（激活函数）。</p>\n<ul>\n<li>激活函数是一个基于元素的非线性函数：\n<ul>\n<li><code>sigmoid(x) = 1 / (1 + np.exp(-x))</code>。</li>\n<li><code>relu(x) = max(x, 0)</code>。</li>\n<li>非线性的激活函数能让我们得到非线性模型。</li>\n</ul>\n</li>\n<li>可以堆叠多个隐藏层（例如多个 dense 层和 activation 层堆叠），得到更深层次的模型。</li>\n<li>超参数：隐藏层数量 <code>hidden layers</code>，每个隐藏层的输出大小 <code>outputs of each hidden layer</code>（最后一层的输出无法改变）。</li>\n</ul>\n<p>代码实现：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">relu</span>(<span class=\"params\">×</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.<span class=\"built_in\">max</span>(x, <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># &#x27;num_hiddens&#x27; 为超参数，randn() 产生均值为0，方差为1的正态分布</span></span><br><span class=\"line\">w1 = nn.Parameter(torch.randn(num_inputs, num_hiddens) * <span class=\"number\">0.01</span>)</span><br><span class=\"line\">b1 = nn.Parameter(torch.zeros(num_hiddens))</span><br><span class=\"line\">w2 = nn.Parameter(torch.randn(num_hiddens, num_outputs) * <span class=\"number\">0.01</span>)</span><br><span class=\"line\">b2 = nn.Parameter(torch.zeros(num_outputs))</span><br><span class=\"line\"></span><br><span class=\"line\">H = relu(np.dot(x, w1) + b1)</span><br><span class=\"line\">Y = np.dot(H, w2) + b2</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-5-卷积神经网络\">2.5 卷积神经网络</h3>\n<p><strong>全连接层到卷积神经网络</strong></p>\n<ul>\n<li>以一个图像识别任务为例，使用 MLP 模型学习 ImageNet（每张图像大小为300*300像素，有1000个类别），我们假设其中一个隐藏层具有10000个输出：\n<ul>\n<li>它会产生10亿个可学习参数，这太大了！</li>\n<li>因为全连接的输出是所有输入元素的加权和，而且每个输出的权重是不一样的。</li>\n</ul>\n</li>\n<li>识别图像中的物体：\n<ul>\n<li>平移不变性：无论对象在哪里，输出都是相似的。</li>\n<li>局部性：像素与其周围像素的相关性比较高，因为图像中的物体都是连续性的。</li>\n</ul>\n</li>\n<li>将先验知识构建到模型结构中：\n<ul>\n<li>用更少的参数（#params）实现相同的模型容量。</li>\n</ul>\n</li>\n</ul>\n<p><strong>卷积层（Convolution layer）</strong></p>\n<ul>\n<li>局部性：从 <code>k * k</code> 大小的输入窗口计算输出，即做局部的计算。</li>\n<li>平移不变性：输出使用相同的 <code>k * k</code> 权重（核）。</li>\n<li>卷积层的模型参数不依赖于输入/输出的大小。</li>\n<li>一个卷积核可以被学习成去识别一个图像里面的模式，比如识别绿色通道中的某个块状物体，识别某个方向上的纹理</li>\n</ul>\n<p>代码：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># both input `X` and weight `K` are matrices（矩阵）</span></span><br><span class=\"line\">h, w = K.shape  <span class=\"comment\"># 一般长和宽都是相等的，例如3、5</span></span><br><span class=\"line\">Y = torch.zeros((X.shape[<span class=\"number\">0</span>] - h + <span class=\"number\">1</span>, X.shape[<span class=\"number\">1</span>] - w + <span class=\"number\">1</span>))  <span class=\"comment\"># 卷积输出的矩阵</span></span><br><span class=\"line\"><span class=\"comment\"># stride = 1</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(Y.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(Y.shape[<span class=\"number\">1</span>]):</span><br><span class=\"line\">        Y[i, j] = (X[i:i + h, j:j + w] * K).<span class=\"built_in\">sum</span>()</span><br></pre></td></tr></table></figure>\n<p><strong>池化层（Pooling layer）</strong></p>\n<ul>\n<li>卷积层对输入的位置很敏感，即输入中模式的转换/旋转会导致输出中模式类似地变化，因此我们需要一定的对未知移动的鲁棒性。</li>\n<li>池化层在大小为 <code>k * k</code> 的窗口中计算平均值/最大值/最小值。</li>\n</ul>\n<p>代码：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># h, w: pooling window height and width</span></span><br><span class=\"line\"><span class=\"comment\"># mode: max or avg</span></span><br><span class=\"line\">Y = torch.zeros((X.shape[<span class=\"number\">0</span>] - h + <span class=\"number\">1</span>, X.shape[<span class=\"number\">1</span>] - w + <span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(Y.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(Y.shape[<span class=\"number\">1</span>]):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> mode == <span class=\"string\">&#x27;max&#x27;</span>:</span><br><span class=\"line\">            Y[i, j] = X[i:i + h, j:j + w].<span class=\"built_in\">max</span>()</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> mode == <span class=\"string\">&#x27;avg&#x27;</span>: </span><br><span class=\"line\">            Y[i, j] = X[i:i + h, j:j + w].mean() </span><br></pre></td></tr></table></figure>\n<p><strong>卷积神经网络（Convolutional Neural Networks，CNN）</strong></p>\n<ul>\n<li>卷积神经网络的原理为叠加卷积层来提取特征。\n<ul>\n<li>激活函数应用于每个卷积层之后。</li>\n<li>使用池化操作来降低位置敏感性。</li>\n</ul>\n</li>\n<li>现代 CNN 是具有各种超参数和层连接的深度神经网络（AlexNet, VGG, inception, ResNet, MobileNet）。</li>\n</ul>\n<h3 id=\"2-6-循环神经网络\">2.6 循环神经网络</h3>\n<p><strong>全连接层到循环神经网络</strong></p>\n<ul>\n<li>语言模型：给出一个句子前面的一些词，预测下一个词是什么。例如：<code>hello -&gt; world</code>、<code>hello world -&gt; !</code>。</li>\n<li>单纯使用 MLP 不能很好地处理序列信息，例如长度的变化和时序的变化。</li>\n</ul>\n<p>循环神经网络的原理为将上一个全连接层输出的状态复制一份作为隐藏状态 H，与下一个输入状态进行拼接后再进行预测。即：<code>h_t = RNN(W_hh * h' + W_hx * x_t + b_h)</code>，其中 <code>h'</code> 为隐藏状态，<code>x_t</code> 为当前输入。</p>\n<p>代码：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">W_xh = nn.Parameter(torch.randn(num_inputs, num_hiddens) * <span class=\"number\">0.01</span>)</span><br><span class=\"line\">W_hh = nn.Parameter(torch.rand(num_hiddens, num_hiddens) * <span class=\"number\">0.01</span>)</span><br><span class=\"line\">b_h = nn.Parameter(torch.zeros(num_hiddens))</span><br><span class=\"line\"></span><br><span class=\"line\">H = torch.zeros(num_hiddens)</span><br><span class=\"line\">outputs = []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> X <span class=\"keyword\">in</span> inputs:  <span class=\"comment\"># `inputs` shape : (num_steps, batch_size, num_inputs)，num_steps表示时间维度</span></span><br><span class=\"line\">    H = torch.tanh(np.dot(X, W_xh) + np.dot(H, W_hh) + b_h)</span><br><span class=\"line\">    outputs.append(H)</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-模型评估\">3. 模型评估</h2>\n<h3 id=\"3-1-评估指标\">3.1 评估指标</h3>\n<ul>\n<li>损失（Loss）衡量模型在预测监督学习结果的方面有多好。</li>\n<li>评估模型性能的其他指标：\n<ul>\n<li>模型相关的指标：例如分类的精度，物体检测的 mAP。</li>\n<li>商业相关的指标：例如收益，推理延迟（如模型能在100毫秒之内返回结果）。</li>\n</ul>\n</li>\n<li>我们一般通过考虑多种指标来选择模型。</li>\n</ul>\n<p><strong>二分类的评估指标</strong></p>\n<ul>\n<li>Accuracy：正确的预测数量/样本总数</li>\n</ul>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">sum</span>(y == y_hat) / y.size</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Precision：预测结果为类 <code>i</code> 且实际结果也为类 <code>i</code> 的数量/预测结果为类 <code>i</code> 的数量</li>\n</ul>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">sum</span>((y_hat == i) &amp; (y == i)) / <span class=\"built_in\">sum</span>(y_hat == i)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Recall：预测结果为类 <code>i</code> 且实际结果也为类 <code>i</code> 的数量/实际结果为类 <code>i</code> 的数量</li>\n</ul>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">sum</span>((y_hat == i) &amp; (y == i)) / <span class=\"built_in\">sum</span>(y == i)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>F1：平衡 Precision 和 Recall 的指标，为 Precision 和 Recall 的调和平均值：<code>2pr / (p + r)</code></li>\n</ul>\n<p><strong>二分类中的 AUC 和 ROC</strong></p>\n<ul>\n<li>AUC 为 ROC 曲线下的面积，大小范围为 <code>[0.5, 1]</code>。</li>\n<li>衡量模型分离这两个类的能力。</li>\n<li>选择决策阈值 <code>x</code>，如果输出 <code>y_hat &gt;= x</code> 则预测为正类，否则为负类。</li>\n</ul>\n<p><strong>展示广告的商业指标</strong></p>\n<ul>\n<li>最优化收入和客户体验。\n<ul>\n<li>Latency：广告应该与其他内容同时显示给用户。</li>\n<li>ASN：平均每页显示的广告数量。</li>\n<li>CTR：用户实际点击率。</li>\n<li>ACP：广告商每次点击支付的平均价格。</li>\n</ul>\n</li>\n<li>收益 = 页面浏览量 * ASN * CTR * ACP。</li>\n</ul>\n<!-- Image base64 code -->\n",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/48028.html",
            "url": "https://asanosaki.github.io/posts/48028.html",
            "title": "MySQL面试知识点总结",
            "date_published": "2022-12-04T09:13:00.000Z",
            "content_html": "<blockquote>\n<p>MySQL 常见面试题总结，文章将不断更新。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-基础\">1. 基础</h2>\n<h3 id=\"1-1-数据库的三范式是什么？\">1.1 数据库的三范式是什么？</h3>\n<ul>\n<li>第一范式（1NF）：强调的是列的原子性，即数据库表的每一列都是不可分割的原子数据项。</li>\n<li>第二范式（2NF）：要求实体的属性完全依赖于主关键字。所谓完全依赖是指不能存在仅依赖主关键字一部分的属性。</li>\n<li>第三范式（3NF）：任何非主属性不依赖于其它非主属性。</li>\n</ul>\n<h3 id=\"1-2-MySQL支持哪些存储引擎？\">1.2 MySQL支持哪些存储引擎？</h3>\n<p>MySQL 支持多种存储引擎，比如 InnoDB、MyISAM、Memory、Archive 等等。在大多数的情况下，直接选择使用 InnoDB 引擎都是最合适的，InnoDB 也是 MySQL 的默认存储引擎。</p>\n<p>MyISAM 和 InnoDB 的区别有哪些：</p>\n<ul>\n<li>InnoDB 支持<strong>事务</strong>，MyISAM 不支持。</li>\n<li>InnoDB 支持<strong>外键</strong>，MyISAM 不支持。</li>\n<li>InnoDB 是<strong>聚集索引</strong>，数据文件是和索引绑在一起的，必须要有主键，通过主键索引效率很高；MyISAM 是<strong>非聚集索引</strong>，数据文件是分离的，索引保存的是数据文件的指针，主键索引和辅助索引是独立的。</li>\n<li>InnoDB 不支持<strong>全文索引</strong>，MyISAM 支持全文索引，查询效率上 MyISAM 更高。</li>\n<li>InnoDB 不保存表的具体行数，MyISAM 用一个变量保存了整个表的行数。</li>\n<li>MyISAM 采用<strong>表级锁</strong>（table-level locking）；InnoDB 支持<strong>行级锁</strong>（row-level locking）和表级锁，默认为行级锁。</li>\n</ul>\n<h3 id=\"1-3-超键、候选键、主键、外键分别是什么？\">1.3 超键、候选键、主键、外键分别是什么？</h3>\n<ul>\n<li>超键：在关系中能唯一标识元组的属性集称为关系模式的超键。一个属性可以为作为一个超键，多个属性组合在一起也可以作为一个超键。<strong>超键包含候选键和主键</strong>。</li>\n<li>候选键：是<strong>最小超键</strong>，即没有冗余元素的超键。</li>\n<li>主键：数据库表中对储存数据对象予以唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（NULL）。</li>\n<li>外键：在一个表中存在的另一个表的主键称此表的外键。</li>\n</ul>\n<h3 id=\"1-4-SQL约束有哪几种？\">1.4 SQL约束有哪几种？</h3>\n<ul>\n<li><code>NOT NULL</code>：用于控制字段的内容一定不能为空（NULL）。</li>\n<li><code>UNIQUE</code>：控制字段内容不能重复，一个表允许有多个 <code>UNIQUE</code> 约束。</li>\n<li><code>PRIMARY KEY</code>：也是用于控制字段内容不能重复，但它在一个表只允许出现一个。</li>\n<li><code>FOREIGN KEY</code>：用于预防破坏表之间连接的动作，也能防止非法数据插入外键列，因为它必须是它指向的那个表中的值之一。</li>\n<li><code>CHECK</code>：用于控制字段的值范围。</li>\n</ul>\n<h3 id=\"1-5-MySQL中的varchar和char有什么区别？\">1.5 MySQL中的varchar和char有什么区别？</h3>\n<p><code>char</code> 是一个<strong>定长</strong>字段，假如申请了 <code>char(10)</code> 的空间，那么无论实际存储多少内容，该字段都占用10个字符；而 <code>varchar</code> 是<strong>变长</strong>的，也就是说申请的只是<strong>最大长度</strong>，占用的空间为实际字符长度 + 1，最后一个字符存储使用了多长的空间。</p>\n<p>在检索效率上来讲，<code>char &gt; varchar</code>，因此在使用中，如果确定某个字段的值的长度，可以使用 <code>char</code>，否则应该尽量使用 <code>varchar</code>，例如存储用户 MD5 加密后的密码，则可以使用 <code>char</code>。</p>\n<h3 id=\"1-6-MySQL中in和exists区别？\">1.6 MySQL中in和exists区别？</h3>\n<p>MySQL 中的 <code>in</code> 语句是把外表和内表作 Hash 连接，而 <code>exists</code> 语句是对外表作 Loop 循环，每次 Loop 循环再对内表进行查询。一直大家都认为 <code>exists</code> 比 <code>in</code> 语句的效率要高，这种说法其实是不准确的。这个是要区分环境的：</p>\n<ul>\n<li>如果查询的两个表大小相当，那么用 <code>in</code> 和 <code>exists</code> 差别不大。</li>\n<li>如果两个表中一个较小，一个是大表，则子查询表大的用 <code>exists</code>，子查询表小的用 <code>in</code>。</li>\n<li><code>not in</code> 和 <code>not exists</code>：如果查询语句使用了 <code>not in</code>，那么内外表都进行全表扫描，没有用到索引；而 <code>not extsts</code> 的子查询依然能用到表上的索引。所以无论哪个表大，用 <code>not exists</code> 都比 <code>not in</code> 要快。</li>\n</ul>\n<h3 id=\"1-7-drop、delete与truncate的区别？\">1.7 drop、delete与truncate的区别？</h3>\n<p>三者都表示删除，但是三者有一些差别：</p>\n<table>\n    <thead>\n        <tr>\n            <th></th>\n            <th>delete</th>\n            <th>truncate</th>\n            <th>drop</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>类型</td>\n            <td>属于 DML</td>\n            <td>属于 DDL</td>\n            <td>属于 DDL</td>\n        </tr>\n        <tr>\n            <td>回滚</td>\n            <td>可回滚</td>\n            <td>不可回滚</td>\n            <td>不可回滚</td>\n        </tr>\n        <tr>\n            <td>删除内容</td>\n            <td>表结构还在，删除表的全部或者一部分数据</td>\n            <td>表结构还在，删除表中的所有数据</td>\n            <td>从数据库中删除表，所有的数据行、索引和权限也会被删除</td>\n        </tr>\n        <tr>\n            <td>删除速度</td>\n            <td>删除速度慢，需要逐行删除</td>\n            <td>删除速度快</td>\n            <td>删除速度最快</td>\n        </tr>\n    </tbody>\n</table>\n<h3 id=\"1-8-什么是存储过程？有哪些优缺点？\">1.8 什么是存储过程？有哪些优缺点？</h3>\n<p>存储过程是一些预编译的 SQL 语句。</p>\n<ol>\n<li>更加直白的理解：存储过程可以说是一个记录集，它是由一些 T-SQL 语句组成的代码块，这些 T-SQL 语句代码像一个方法一样实现一些功能（对单表或多表的增删改查），然后再给这个代码块取一个名字，在用到这个功能的时候调用它就行了。</li>\n<li>存储过程是一个预编译的代码块，执行效率比较高，一个存储过程替代大量 T-SQL 语句，可以降低网络通信量，提高通信速率，可以一定程度上确保数据安全。</li>\n</ol>\n<p>但是，在互联网项目中，其实是不太推荐存储过程的，比较出名的就是阿里的《Java 开发手册》中禁止使用存储过程，我个人的理解是，在互联网项目中，迭代太快，项目的生命周期也比较短，人员流动相比于传统的项目也更加频繁，在这样的情况下，存储过程的管理确实是没有那么方便，同时，复用性也没有写在服务层那么好。</p>\n<h3 id=\"1-9-MySQL执行查询的过程？\">1.9 MySQL执行查询的过程？</h3>\n<ol>\n<li>客户端通过 TCP 连接发送连接请求到 MySQL 连接器，连接器会对该请求进行权限验证及连接资源分配。</li>\n<li>查缓存（当判断缓存是否命中时，MySQL 不会进行解析查询语句，而是直接使用 SQL 语句和客户端发送过来的其他原始信息。所以，任何字符上的不同，例如空格、注解等都会导致缓存的不命中）。</li>\n<li>语法分析（SQL 语法是否写错了）：如何把语句给到预处理器，检查数据表和数据列是否存在，解析别名看是否存在歧义。</li>\n<li>优化：是否使用索引，生成执行计划。</li>\n<li>交给执行器，将数据保存到结果集中，同时会逐步将数据缓存到查询缓存中，最终将结果集返回给客户端。</li>\n</ol>\n<p>更新语句执行会复杂一点，需要检查表是否有排它锁，写 <code>binlog</code>、刷盘、是否执行 <code>commit</code>。</p>\n<h2 id=\"2-事务\">2. 事务</h2>\n<h3 id=\"2-1-什么是数据库事务？\">2.1 什么是数据库事务？</h3>\n<p>事务是一个<strong>不可分割</strong>的数据库操作序列，也是数据库<strong>并发控制的基本单位</strong>，其执行的结果必须使数据库从一种一致性状态变到另一种一致性状态。事务是逻辑上的一组操作，要么都执行，要么都不执行。</p>\n<p>事务最经典也经常被拿出来说例子就是转账了。</p>\n<p>假如小明要给小红转账1000元，这个转账会涉及到两个关键操作就是：将小明的余额减少1000元，将小红的余额增加1000元。万一在这两个操作之间突然出现错误比如银行系统崩溃，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。</p>\n<h3 id=\"2-2-事务具有的四个特征？\">2.2 事务具有的四个特征？</h3>\n<p>事务就是一组<strong>原子性</strong>的操作，这些操作要么全部发生，要么全部不发生。事务把数据库从一种一致性状态转换成另一种一致性状态。</p>\n<ul>\n<li>原子性（Atomicity）：事务是数据库的逻辑工作单位，事务中包含的各操作要么都做，要么都不做。</li>\n<li>一致性（Consistency）：事务执行的结果必须是使数据库从一个一致性状态变到另一个一致性状态。因此当数据库只包含成功事务提交的结果时，就说数据库处于一致性状态。如果数据库系统运行中发生故障，有些事务尚未完成就被迫中断，这些未完成事务对数据库所做的修改有一部分已写入物理数据库，这时数据库就处于一种不正确的状态，或者说是不一致的状态。</li>\n<li>隔离性（Isolation）：一个事务的执行不能被其它事务干扰。即一个事务内部的操作及使用的数据对其它并发事务是隔离的，并发执行的各个事务之间不能互相干扰。</li>\n<li>持续性（Durability）：也称永久性，指一个事务一旦提交，它对数据库中的数据的改变就应该是永久性的。接下来的其它操作或故障不应该对其执行结果有任何影响。</li>\n</ul>\n<h3 id=\"2-3-MySQL的四种隔离级别？\">2.3 MySQL的四种隔离级别？</h3>\n<ul>\n<li>Read Uncommitted（读取未提交内容）<br>\n在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为<strong>脏读</strong>（Dirty Read）。</li>\n<li>Read Committed（读取提交内容）<br>\n这是大多数数据库系统的默认隔离级别（但不是 MySQL 默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交的事务所做的改变。这种隔离级别也支持所谓的<strong>不可重复读</strong>（Nonrepeatable Read），因为同一事务的其他实例在该实例处理其间可能会有新的 <code>commit</code>，所以同一 <code>select</code> 可能返回不同结果。</li>\n<li>Repeatable Read（可重读）<br>\n这是 MySQL 的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，这会导致另一个棘手的问题：<strong>幻读</strong>（Phantom Read）。</li>\n<li>Serializable（可串行化）<br>\n通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。</li>\n</ul>\n<table>\n    <thead>\n        <tr>\n            <th>隔离级别</th>\n            <th>脏读</th>\n            <th>不可重复读</th>\n            <th>幻影读</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>Read Uncommitted</td>\n            <td>有</td>\n            <td>有</td>\n            <td>有</td>\n        </tr>\n        <tr>\n            <td>Read Committed</td>\n            <td>无</td>\n            <td>有</td>\n            <td>有</td>\n        </tr>\n        <tr>\n            <td>Repeatable Read</td>\n            <td>无</td>\n            <td>无</td>\n            <td>有</td>\n        </tr>\n        <tr>\n            <td>Serializable</td>\n            <td>无</td>\n            <td>无</td>\n            <td>无</td>\n        </tr>\n    </tbody>\n</table>\n<p>MySQL 默认采用的是 <code>REPEATABLE-READ</code> 隔离级别，Oracle 默认采用的是 <code>READ-COMMITTED</code> 隔离级别。</p>\n<p>事务隔离机制的实现基于锁机制和并发调度。其中并发调度使用的是 MVVC（多版本并发控制），通过保存修改的旧版本信息来支持并发一致性读和回滚等特性。</p>\n<p>因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是 <code>READ-COMMITTED</code>（读取提交内容），但是你要知道的是 InnoDB 存储引擎默认使用 <code>REPEATABLE-READ</code>（可重读）并不会有任何性能损失。</p>\n<p>InnoDB 存储引擎在分布式事务的情况下一般会用到 <code>SERIALIZABLE</code>（可串行化）隔离级别。</p>\n<h3 id=\"2-4-什么是脏读、不可重复读与幻读？\">2.4 什么是脏读、不可重复读与幻读？</h3>\n<ul>\n<li>脏读：事务 A 读取了事务 B 更新的数据，然后 B 回滚操作，那么 A 读取到的数据是脏数据。</li>\n<li>不可重复读：事务 A 多次读取同一数据，事务 B 在事务 A 多次读取的过程中，对数据作了更新并提交，导致事务 A 多次读取同一数据时，结果不一致。</li>\n<li>幻读：系统管理员 A 将数据库中所有学生的成绩从具体分数改为 ABCDE 等级，但是系统管理员 B 就在这个时候插入了一条具体分数的记录，当系统管理员 A 改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。</li>\n</ul>\n<p>总结：不可重复读侧重于修改，幻读侧重于新增或删除（多了或少了行），脏读是一个事务回滚影响另外一个事务。</p>\n<h3 id=\"2-5-事务的实现原理？\">2.5 事务的实现原理？</h3>\n<p>事务是基于重做日志文件（redo log）和回滚日志（undo log）实现的。</p>\n<p>每提交一个事务必须先将该事务的所有日志写入到重做日志文件进行持久化，数据库就可以通过重做日志来保证事务的原子性和持久性。</p>\n<p>每当有修改事务时，还会产生 undo log，如果需要回滚，则根据 undo log 的反向语句进行逻辑操作，比如 <code>insert</code> 一条记录就 <code>delete</code> 一条记录。undo log 主要实现数据库的一致性。</p>\n<h3 id=\"2-6-介绍一下MySQL事务日志？\">2.6 介绍一下MySQL事务日志？</h3>\n<p>InnoDB 事务日志包括 redo log 和 undo log。</p>\n<p>undo log 指事务开始之前，在操作任何数据之前，首先将需操作的数据备份到一个地方。redo log 指事务中操作的任何数据，将最新的数据备份到一个地方。</p>\n<p>事务日志的目的：实例或者介质失败，事务日志文件就能派上用场。</p>\n<ul>\n<li>redu log<br>\nredo log 不是随着事务的提交才写入的，而是在事务的执行过程中，便开始写入 redo log 中。具体的落盘策略可以进行配置。防止在发生故障的时间点，尚有脏页未写入磁盘，在重启 MySQL 服务的时候，根据 redo log 进行重做，从而达到事务的未入磁盘数据进行持久化这一特性。redo log 是为了实现事务的持久性而出现的产物。</li>\n<li>undo log<br>\nundo log 用来回滚行记录到某个版本。事务未提交之前，undo log 保存了未提交之前的版本数据，undo log 中的数据可作为数据旧版本快照供其他并发事务进行快照读。是为了实现事务的原子性而出现的产物，在 MySQL InnoDB 存储引擎中用来实现多版本并发控制。</li>\n</ul>\n<h3 id=\"2-7-什么是MySQL的binlog？\">2.7 什么是MySQL的binlog？</h3>\n<p>MySQL 的 binlog 是记录所有数据库<strong>表结构变更</strong>（例如 <code>CREATE</code>、<code>ALTER TABLE</code>）以及<strong>表数据修改</strong>（例如 <code>INSERT</code>、<code>UPDATE</code>、<code>DELETE</code>）的二进制日志。binlog 不会记录 <code>SELECT</code> 和 <code>SHOW</code> 这类操作，因为这类操作对数据本身并没有修改，但你可以通过查询通用日志来查看 MySQL 执行过的所有语句。</p>\n<p>MySQL binlog 以事件形式记录，还包含语句执行所消耗的时间，MySQL 的二进制日志是事务安全型的。binlog 的主要目的是复制和恢复。</p>\n<p>binlog 有三种格式，各有优缺点：</p>\n<ul>\n<li><code>statement</code>：基于 SQL 语句的模式，某些语句和函数如 <code>UUID</code>、<code>LOAD DATA INFILE</code> 等在复制过程中可能导致数据不一致甚至出错。</li>\n<li><code>row</code>：基于行的模式，记录的是行的变化，很安全。但是 binlog 会比其他两种模式大很多，在一些大表中清除大量数据时在 binlog 中会生成很多条语句，可能导致从库延迟变大。</li>\n<li><code>mixed</code>：混合模式，根据语句来选用是 <code>statement</code> 还是 <code>row</code> 模式。</li>\n</ul>\n<h3 id=\"2-8-在事务中可以混合使用存储引擎吗？\">2.8 在事务中可以混合使用存储引擎吗？</h3>\n<p>尽量不要在同一个事务中使用多种存储引擎，MySQL 服务器层不管理事务，事务是由下层的存储引擎实现的。</p>\n<p>如果在事务中混合使用了事务型和非事务型的表（例如 InnoDB 和 MyISAM 表），在正常提交的情况下不会有什么问题。</p>\n<p>但如果该事务需要回滚，非事务型的表上的变更就无法撤销，这会导致数据库处于不一致的状态，这种情况很难修复，事务的最终结果将无法确定。所以，为每张表选择合适的存储引擎非常重要。</p>\n<h3 id=\"2-9-什么是MVCC？\">2.9 什么是MVCC？</h3>\n<p>MVCC，即多版木并发控制。MVCC 的实现，是通过保存数据在某个时间点的快照来实现的。根据事务开始的时间不同，每个事务对同一张表，同一时刻看到的数据可能是不一样的。</p>\n<h3 id=\"2-10-MVCC的实现\">2.10 MVCC的实现</h3>\n<p>对于 InnoDB，聚簇索引记录中包含3个隐藏的列：</p>\n<ul>\n<li>ROW ID：隐藏的自增 ID，如果表没有主键，InnoDB 会自动按 ROW ID 产生一个聚集索引树。</li>\n<li>事务 ID：记录最后一次修改该记录的事务 ID。</li>\n<li>回滚指针：指向这条记录的上一个版本。</li>\n</ul>\n<p>我们举个例子，假如现在有两个事务：</p>\n<ul>\n<li>事务1：<code>insert into t1(a, b) values (1, 1);</code></li>\n<li>事务2：<code>update t1 set b = 666 where a = 1;</code></li>\n</ul>\n<p>如图，首先 <code>insert</code> 语句向表 <code>t1</code> 中插入了一条数据，<code>a</code> 字段为1，<code>b</code> 字段为1，ROW ID 也为1，事务 ID 假设为1，回滚指针假设为 <code>null</code>。当执行 <code>update t1 set b = 666 where a = 1</code> 时，大致步骤如下：</p>\n<ul>\n<li>数据库会先对满足 <code>a = 1</code> 的行加排他锁；</li>\n<li>然后将原记录复制到 undo 表空间中；</li>\n<li>修改 <code>b</code> 字段的值为666，修改事务 ID 为2；</li>\n<li>并通过隐藏的回滚指针指向 undo log 中的历史记录；</li>\n<li>事务提交，释放前面对满足 <code>a = 1</code> 的行所加的排他锁。</li>\n</ul>\n<p>因此可以总结出 MVCC 实现的原理大致是：</p>\n<p>InnoDB 每一行数据都有一个隐藏的回滚指针，用于指向该行修改前的最后一个历史版本，这个历史版本存放在 undo log 中。如果要执行更新操作，会将原记录放入 undo log 中，并通过隐藏的回滚指针指向 undo log 中的原记录。其它事务此时需要查询时，就是查询 undo log 中这行数据的最后一个历史版本。</p>\n<p>MVCC 最大的好处是读不加锁，读写不冲突，极大地增加了 MySQL 的并发性。通过 MVCC，保证了事务 ACID 中的隔离性。</p>\n<h2 id=\"3-锁\">3. 锁</h2>\n<h3 id=\"3-1-为什么要加锁？\">3.1 为什么要加锁？</h3>\n<p>当多个用户并发地存取数据时，在数据库中就会产生多个事务同时存取同一数据的情况。若对并发操作不加控制就可能读取和存储不正确的数据，破坏数据库的一致性。因此需要加锁使得在多用户环境下保证数据库的完整性和一致性。</p>\n<h3 id=\"3-2-按照锁的粒度分数据库锁有哪些？\">3.2 按照锁的粒度分数据库锁有哪些？</h3>\n<p>在关系型数据库中，可以按照锁的粒度把数据库锁分为行级锁（InnoDB 引擎）、表级锁(MyISAM 引擎）和页级锁（BDB引擎）。</p>\n<ul>\n<li>行级锁\n<ul>\n<li>行级锁是 MySQL 中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁和排他锁。</li>\n<li>开销大，加锁慢，会出现死锁，锁定粒度最小，发生锁冲突的概率最低，并发度也最高。</li>\n</ul>\n</li>\n<li>表级锁\n<ul>\n<li>表级锁是 MySQL 中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分 MySQL 引擎支持。最常使用的 MyISAM 与 InnoDB 都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。</li>\n<li>开销小，加锁快，不会出现死锁，锁定粒度大，发生锁冲突的概率最高，并发度最低。</li>\n</ul>\n</li>\n<li>页级锁\n<ul>\n<li>页级锁是 MySQL 中锁定粒度介于行级锁和表级锁之间的一种锁。表级锁速度快，但冲突多，行级锁冲突少，但速度慢。所以取了折衷的页级锁，一次锁定相邻的一组记录。BDB 支持页级锁。</li>\n<li>开销和加锁时间界于表锁和行锁之间，会出现死锁，锁定粒度界于表锁和行锁之间，并发度一般。</li>\n</ul>\n</li>\n</ul>\n<p>MyISAM 和 InnoDB 存储引擎使用的锁：</p>\n<ul>\n<li>MyISAM 采用表级锁（table-level locking）。</li>\n<li>InnoDB 支持行级锁（row-level locking）和表级锁，默认为行级锁。</li>\n</ul>\n<h3 id=\"3-3-从锁的类别上分MySQL都有哪些锁呢？\">3.3 从锁的类别上分MySQL都有哪些锁呢？</h3>\n<p>从锁的类别上来讲，有共享锁和排他锁。</p>\n<ul>\n<li>共享锁：又叫做读锁，当用户要进行数据的读取时，对数据加上共享锁。共享锁可以同时加上多个。</li>\n<li>排他锁：又叫做写锁，当用户要进行数据的写入时，对数据加上排他锁。排他锁只可以加一个，它和其它的排他锁，共享锁都相斥。</li>\n</ul>\n<p>用上面的例子来说就是用户的行为有两种，一种是来看房，多个用户一起看房是可以接受的。一种是真正的入住一晚，在这期间，无论是想入住的还是想看房的都不可以。</p>\n<p>锁的粒度取决于具体的存储引擎，InnoDB 实现了行级锁，页级锁，表级锁。他们的加锁开销从大到小，并发能力也是从大到小。</p>\n<h3 id=\"3-4-数据库的乐观锁和悲观锁是什么？怎么实现的？\">3.4 数据库的乐观锁和悲观锁是什么？怎么实现的？</h3>\n<p>数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。乐观并发控制（乐观锁）和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。</p>\n<ul>\n<li>悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。在查询完数据的时候就把事务锁起来，直到提交事务。实现方式：使用数据库中的锁机制。</li>\n<li>乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。在修改数据的时候把事务锁起来，通过 version 的方式来进行锁定。实现方式：乐观锁一般会使用版本号机制或 CAS 算法实现。</li>\n</ul>\n<p>两种锁的使用场景：</p>\n<p>从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。</p>\n<p>但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行 <code>retry</code>，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。</p>\n<h3 id=\"3-5-InnoDB引擎的行锁是怎么实现的？\">3.5 InnoDB引擎的行锁是怎么实现的？</h3>\n<p>InnoDB 是基于索引来完成行锁的。</p>\n<p>例如：<code>select * from tab_with_index where id = 1 for update;</code></p>\n<p><code>for update</code> 可以根据条件来完成行锁锁定，并且 <code>id</code> 是有索引键的列，如果 <code>id</code> 不是索引键那么 InnoDB 将完成表锁，并发将无从谈起。</p>\n<h3 id=\"3-6-什么是死锁？怎么解决？\">3.6 什么是死锁？怎么解决？</h3>\n<p>死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方的资源，从而导致恶性循环的现象。常见的解决死锁的方法有：</p>\n<ul>\n<li>如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。</li>\n<li>在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率。</li>\n<li>对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率。</li>\n</ul>\n<p>如果业务处理不好可以用分布式事务锁或者使用乐观锁。</p>\n<h3 id=\"3-7-隔离级别与锁的关系？\">3.7 隔离级别与锁的关系？</h3>\n<ul>\n<li>在 Read Uncommitted 级别下，读取数据不需要加共享锁，这样就不会跟被修改的数据上的排他锁冲突。</li>\n<li>在 Read Committed 级别下，读操作需要加共享锁，但是在语句执行完以后释放共享锁。</li>\n<li>在 Repeatable Read 级别下，读操作需要加共享锁，但是在事务提交之前并不释放共享锁，也就是必须等待事务执行完毕以后才释放共享锁。</li>\n<li>SERIALIZABLE 是限制性最强的隔离级别，因为该级别锁定整个范围的键，并一直持有锁，直到事务完成。</li>\n</ul>\n<h3 id=\"3-8-优化锁方面的意见？\">3.8 优化锁方面的意见？</h3>\n<ul>\n<li>使用较低的隔离级别。</li>\n<li>设计索引，尽量使用索引去访问数据，加锁更加精确，从而减少锁冲突。</li>\n<li>选择合理的事务大小，给记录显示加锁时，最好一次性请求足够级别的锁。例如，修改数据的话最好申请排他锁，而不是先申请共享锁，修改时再申请排他锁，这样会导致死锁。</li>\n<li>不同的程序访问一组表的时候，应尽量约定一个相同的顺序访问各表，对于一个表而言，尽可能固定顺序地获取表中的行，这样将大大减少死锁的机会。</li>\n<li>尽量使用相等条件访问数据，这样可以避免间隙锁对并发插入的影响。</li>\n<li>不要申请超过实际需要的锁级别。</li>\n<li>数据查询的时候不是必要，不要使用加锁。MySQL 的 MVCC 可以实现事务中的查询不用加锁，优化事务性能：MVCC 只在 Read Committed（读提交）和 Repeatable Read（可重复读）两种隔离级别。</li>\n<li>对于特定的事务，可以使用表锁来提高处理速度或者减少死锁的可能。</li>\n</ul>\n<h2 id=\"4-索引\">4. 索引</h2>\n<h3 id=\"4-1-索引是什么？\">4.1 索引是什么？</h3>\n<p>索引是一种特殊的文件（InnoDB 数据表上的索引是表空间的一个组成部分），它们包含着对数据表里所有记录的引用指针。</p>\n<p>索引是一种数据结构。数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据。索引的实现通常使用 B 树及其变种 B+ 树。更通俗地说，索引就相当于目录。为了方便查找书中的内容，通过对内容建立索引形成目录。而且索引是一个文件，它是要占据物理空间的。</p>\n<p>MySQL 索引的建立对于 MySQL 的高效运行是很重要的，索引可以大大提高 MySQL 的检索速度。比如我们在查字典的时候，前面都有检索的拼音和偏旁、笔画等，然后找到对应字典页码，打开字典的页数就可以知道我们要搜索的某一个 key 的全部值的信息了。</p>\n<h3 id=\"4-2-索引有哪些优缺点？\">4.2 索引有哪些优缺点？</h3>\n<p>索引的优点：</p>\n<ul>\n<li>可以大大加快数据的检索速度，这也是创建索引的最主要的原因。</li>\n<li>通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。</li>\n</ul>\n<p>索引的缺点：</p>\n<ul>\n<li>时间方面：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，会降低增删改的执行效率。</li>\n<li>空间方面：索引需要占用物理空间。</li>\n</ul>\n<h3 id=\"4-3-MySQL有哪几种索引类型？\">4.3 MySQL有哪几种索引类型？</h3>\n<ul>\n<li>从存储结构上来划分：BTree 索引（B-Tree 或 B+Tree 索引）、Hash 索引、full-index 全文索引、R-Tree 索引。这里所描述的是索引存储时保存的形式。</li>\n<li>从应用层次来分：普通索引、唯一索引、复合索引。\n<ul>\n<li>普通索引：即一个索引只包含单个列，一个表可以有多个单列索引。</li>\n<li>唯一索引：索引列的值必须唯一，但允许有空值。</li>\n<li>复合索引：多列值组成一个索引，专门用于组合搜索，其效率大于索引合并。</li>\n<li>聚簇索引（聚集索引）：并不是一种单独的索引类型，而是一种数据存储方式。具体细节取决于不同的实现，InnoDB 的聚簇索引其实就是在同一个结构中保存了 B-Tree 索引（技术上来说是 B+Tree）和数据行。</li>\n<li>非聚簇索引：不是聚簇索引，就是非聚簇索引。</li>\n</ul>\n</li>\n<li>根据表中数据的物理顺序与键值的逻辑（索引）顺序关系：聚集索引，非聚集索引。</li>\n</ul>\n<h3 id=\"4-4-说一说索引的底层实现？\">4.4 说一说索引的底层实现？</h3>\n<ul>\n<li>Hash 索引：基于哈希表实现，只有精确匹配索引所有列的查询才有效，对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码（hash code），并且 Hash 索引将所有的哈希码存储在索引中，同时在索引表中保存指向每个数据行的指针。</li>\n<li>B-Tree 索引（MySQL 使用 B+Tree）：B-Tree 能加快数据的访问速度，因为存储引擎不再需要进行全表扫描来获取数据，数据分布在各个节点之中。</li>\n<li>B+Tree 索引：B-Tree 的改进版本，同时也是数据库索引所采用的存储结构。数据都在叶子节点上，并且增加了顺序访问指针，每个叶子节点都指向相邻的叶子节点的地址。相比 B-Tree 来说，进行范围查找时只需要查找两个节点，进行遍历即可。而 B-Tree 需要获取所有节点，相比之下 B+Tree 效率更高。<br>\nB+Tree 性质：\n<ul>\n<li><code>n</code> 棵子树的节点包含 <code>n</code> 个关键字，不用来保存数据而是保存数据的索引。</li>\n<li>所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身根据关键字的大小自小而大顺序链接。</li>\n<li>所有的非终端结点可以看成是索引部分，结点中仅含其子树中的最大（或最小）关键字。</li>\n<li>B+ 树中，数据对象的插入和删除仅在叶节点上进行。</li>\n<li>B+ 树有2个头指针，一个是树的根节点，一个是最小关键码的叶节点。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"4-5-为什么索引结构默认使用B-Tree，而不是B-Tree，Hash，二叉树，红黑树？\">4.5 为什么索引结构默认使用B+Tree，而不是B-Tree，Hash，二叉树，红黑树？</h3>\n<ul>\n<li>B-tree：从两个方面来回答：\n<ul>\n<li>B+ 树的磁盘读写代价更低：B+ 树的内部节点并没有指向关键字具体信息的指针，因此其内部节点相对 B/B- 树更小，如果把所有同一内部节点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多，一次性读入内存的需要查找的关键字也就越多，相对 IO 读写次数就降低了。</li>\n<li>由于 B+ 树的数据都存储在叶子结点中，分支结点均为索引，方便扫库，只需要扫一遍叶子结点即可，但是 B 树因为其分支结点同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序来扫，所以 B+ 树更加适合区间查询的情况，所以通常 B+ 树用于数据库索引。</li>\n</ul>\n</li>\n<li>Hash：\n<ul>\n<li>虽然可以快速定位，但是没有顺序，IO 复杂度高；</li>\n<li>基于 Hash 表实现，只有 Memory 存储引擎显式支持哈希索引；</li>\n<li>适合<strong>等值查询</strong>，如 <code>=</code>、<code>in()</code>、<code>&lt;=&gt;</code>，不支持范围查询；</li>\n<li>因为不是按照索引值顺序存储的，就不能像 B+Tree 索引一样利用索引完成排序；</li>\n<li>Hash 索引在查询等值时非常快；</li>\n<li>因为 Hash 索引始终索引<strong>所有列的全部内容</strong>，所以不支持部分索引列的匹配查找；</li>\n<li>如果有大量重复键值的情况下，哈希索引的效率会很低，因为存在<strong>哈希碰撞</strong>问题。</li>\n</ul>\n</li>\n<li>二叉树：树的高度不均匀，不能自平衡，查找效率跟数据有关（树的高度），并且 IO 代价高。</li>\n<li>红黑树：树的高度随着数据量增加而增加，IO 代价高。</li>\n</ul>\n<h3 id=\"4-6-讲一讲聚簇索引与非聚簇索引？\">4.6 讲一讲聚簇索引与非聚簇索引？</h3>\n<p>在 InnoDB 里，索引 B+Tree 的叶子节点存储了整行数据为主键索引，也被称之为聚簇索引，即将数据存储与索引放到了一块，找到索引也就找到了数据。</p>\n<p>而索引 B+Tree 的叶子节点存储了主键的值为非主键索引，也被称之为非聚簇索引、二级索引。</p>\n<p>聚簇索引与非聚簇索引的区别：</p>\n<ul>\n<li>非聚簇索引与聚簇索引的区别在于非聚簇索引的叶子节点不存储表中的数据，而是存储该列对应的主键（行号）。</li>\n<li>对于 InnoDB 来说，想要查找数据我们还需要根据主键再去聚簇索引中进行查找，这个再根据聚簇索引查找数据的过程，我们称为<strong>回表</strong>。第一次索引一般是顺序 IO，回表的操作属于随机 IO。需要回表的次数越多，即随机 IO 次数越多，我们就越倾向于使用全表扫描。</li>\n<li>通常情况下，主键索引（聚簇索引）查询只会查一次，而非主键索引（非聚簇索引）需要回表查询多次。当然，如果是覆盖索引的话，查一次即可。</li>\n</ul>\n<p>注意：MyISAM 无论主键索引还是二级索引都是非聚簇索引，而 InnoDB 的主键索引是聚簇索引，二级索引是非聚簇索引。我们自己建立的索引基本都是非聚簇索引。</p>\n<h3 id=\"4-7-非聚簇索引一定会回表查询吗？\">4.7 非聚簇索引一定会回表查询吗？</h3>\n<p>不一定，这涉及到查询语句所要求的字段是否全部命中了索引，如果全部命中了索引，那么就不必再进行回表查询。一个索引包含（覆盖）所有需要查询字段的值，被称之为“覆盖索引”。举个简单的例子，假设我们在员工表的年龄上建立了索引，那么当进行 <code>select score from stuaent where score &gt; 90</code> 的查询时，在索引的叶子节点上，已经包含了 <code>score</code> 信息，不会再次进行回表查询。</p>\n<h3 id=\"4-8-联合索引是什么？为什么需要注意联合索引中的顺序？\">4.8 联合索引是什么？为什么需要注意联合索引中的顺序？</h3>\n<p>MySQL 可以<strong>使用多个字段同时建立一个索引</strong>，叫做联合索引。在联合索引中，如果想要命中索引，需要按照建立索引时的字段顺序挨个使用，否则无法命中索引。具体原因为：</p>\n<ul>\n<li>MySQL 使用索引时需要索引有序，假设现在建立了 <code>name, age, school</code> 的联合索引，那么索引的排序为：先按照 <code>name</code> 排序，如果 <code>name</code> 相同，则按照 <code>age</code> 排序，如果 <code>age</code> 的值也相等，则按照 <code>school</code> 进行排序。</li>\n<li>当进行查询时，此时索引仅仅按照 <code>name</code> 严格有序，因此必须首先使用 <code>name</code> 字段进行<strong>等值查询</strong>，之后对于匹配到的列而言，其按照 <code>age</code> 字段严格有序，此时可以使用 <code>age</code> 字段用做索引查找，以此类推。因此在建立联合索引的时候应该注意索引列的顺序，一般情况下，将查询需求频繁或者字段选择性高的列放在前面。此外可以根据特例的查询或者表结构进行单独的调整。</li>\n</ul>\n",
            "tags": [
                "MySQL"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/4115.html",
            "url": "https://asanosaki.github.io/posts/4115.html",
            "title": "英语日常学习记录",
            "date_published": "2022-12-02T02:58:00.000Z",
            "content_html": "<blockquote>\n<p>记录日常积累的一些英语口语句子，日积月累不断进步！</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-短句\">1. 短句</h2>\n<h3 id=\"我不知道\">我不知道</h3>\n<p>I don’t know.（中性，根据语气判断凶不凶）</p>\n<p>Sorry, I have no idea. / I haven’t got a clue.（很地道）</p>\n<p>I’m afraid I don’t know.（更礼貌）</p>\n<p>How should/would I know? / Do I look like a walking encyclopedia?（不客气）</p>\n<h3 id=\"你会说英语吗\">你会说英语吗</h3>\n<p>Can you …（你会吗 / 你介不介意 / 你是不是被允许）</p>\n<p>Can you cook?（你会做饭吗 / 可以你做饭吗 / 你能做饭吗）</p>\n<p>Can you do sth.? 可能太直接、不礼貌，比如 Can you be quiet?</p>\n<p>Do you do sth.? 像是问你平时做不做某事，如果平时做，那肯定说明是会的</p>\n<p>Do you speak English? / Can you speak English? 都表示你会说英语吗，都可以用</p>\n<h3 id=\"我在外面\">我在外面</h3>\n<p>Outside 指的是在房间、建筑或某个地方的外面或附近，不能指出门办事、出门玩</p>\n<p>I’m coming, I’m just outside.（我到门口了）</p>\n<p>I’ll wait for you outside.（我在门口附近等你）</p>\n<p>Let’s eat outside.（在室外吃饭）</p>\n<p>Eat out.（下馆子）</p>\n<p>She is out with her friends.（她和朋友出去玩）</p>\n<p>They are out working.（他们外出办事）</p>\n<h3 id=\"喝咖啡\">喝咖啡</h3>\n<p>喝一般得说 have 或 want</p>\n<p>Let’s have coffee.</p>\n<p>I want a coke.</p>\n<p>Do you want a drink?</p>\n<p>如果说 grad a drink/food 一般是比较随意的、快的、看情况的这种感觉，意思就是如果你有时间，我们可以一起喝杯咖啡，如果你没时间那也没关系</p>\n<p>Let’s grab some coffee.</p>\n<p>Holding my coffee.（拿着咖啡）</p>\n<h3 id=\"很喜欢\">很喜欢</h3>\n<p>英式英语中不能说 Quite like，quite 一般表示有点儿，例如我有点累：I’m quite tired.</p>\n<p>所以在英式英语中顺序是 Don’t like、Quite like、Like、Really like</p>\n<p>而在美式英语中顺序是 Don’t like、Like、Quite like、Really like</p>\n<p>I love shopping.（我很喜欢购物）</p>\n<p>I’m a huge fan of British food.（我非常喜欢英国料理）</p>\n<p>I’m obsessed with American accents.（我非常喜欢美式口音）</p>\n<h3 id=\"我知道了\">我知道了</h3>\n<p>I see. / I get it. / I got it. / I understand. 表示以前不知道，现在才知道</p>\n<p>Got it? / Get it? / Got it. 可以不加 ‘I’，但是 I got it. 一定要加 I</p>\n<p>I know. / Of course. / Yep.（嗯哼）/ That goes without saying. 表示已经知道了，不需要你告诉我</p>\n<p>No shit, Sherlock. 阴阳怪气表达，这么明显还用你说？</p>\n<h3 id=\"美女-帅哥\">美女/帅哥</h3>\n<p>Beautiful girl. / Pretty girl. 指真正漂亮的女生</p>\n<p>Excuse me. / Sorry. / Hi. + (miss, mate, bro) 表示称呼陌生人（如服务员）美女或者帅哥</p>\n<p>Sorry mate, just to check…（不好意思帅哥，想问下…）</p>\n<h3 id=\"上车-下车\">上车/下车</h3>\n<p>In the car. 在车上</p>\n<p>Get in the car. 上车</p>\n<p>Get out (of) the car/taxi. 下车/下出租（从坐着的姿势直接下车）</p>\n<p>Get off the bus/train/plane. 下公交/动车/飞机（先站起来再下车）</p>\n<p>Get off my car. 表示别碰我的车</p>\n<p>Get down from the car. 表示别站在车顶/引擎盖上</p>\n<h2 id=\"2-情景对话\">2. 情景对话</h2>\n<h3 id=\"Day1\">Day1</h3>\n<p>Hey bro!（Hello 较正式，bro 更不正式，因此 Hello bro 很奇怪）</p>\n<p>I said hello to you, but you didn’t say hello back to me.</p>\n<p>Did your parents not teach you manners?（没有家教，该句特别 savage，即太过直接、犀利、甚至无理）</p>\n<p>What shall we eat?（别说 We eat what?）</p>\n<p>Anything is OK. / Anything will do. / Whatever you want. / I don’t mind. / You choose. / I’m easy.（表示都行，不能说 Both are OK，因为没有给选择）</p>\n<p>Hot pot.（火锅，别说成 Fire pot 了）</p>\n<p>Drink some beer? / Hot pot and beer?</p>\n<p>Definitely. / Great idea. / That sounds great to me.（必须的 / 好主意 / 我看行）</p>\n<h3 id=\"Day2\">Day2</h3>\n<p>Why are you late?（你怎么来的这么晚）</p>\n<p>Oh, my taxi was late. / There was a lot of traffic, so I was late.（出租车来晚了 / 堵车了）</p>\n<p>It’s dark now.（天黑了）</p>\n<p>What shall we eat?</p>\n<p>What do you wanna eat? / It’s your call. / You decide.（你想吃什么 / 你决定吧）</p>\n",
            "tags": [
                "Others"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/48394.html",
            "url": "https://asanosaki.github.io/posts/48394.html",
            "title": "PyTorch深度学习入门(CIFAR10分类)",
            "date_published": "2022-12-01T10:22:00.000Z",
            "content_html": "<blockquote>\n<p>通过 CIFAR10 数据集的分类问题初入门 Deep Learning，也是开坑 AI 系列的第一篇文章。<br>\n相关环境的搭建可以转至：<a href=\"/posts/15428.html\">Anaconda与PyTorch安装教程</a>。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-常用函数\">1. 常用函数</h2>\n<p>（1）路径函数</p>\n<p>在 <code>os</code> 模块中常用的路径相关函数有：</p>\n<ul>\n<li><code>os.listdir(path)</code>：将 <code>path</code> 目录下的内容列成一个 <code>list</code>。</li>\n<li><code>os.path.join(path1, path2)</code>：拼接路径：<code>path1\\path2</code>。</li>\n</ul>\n<p>例如：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"></span><br><span class=\"line\">dir_path = <span class=\"string\">&#x27;dataset/hymenoptera_data/train/ants_image&#x27;</span></span><br><span class=\"line\">img_path_list = os.listdir(dir_path)</span><br><span class=\"line\">img_full_path = os.path.join(dir_path, img_path_list[<span class=\"number\">0</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(img_path_list)  <span class=\"comment\"># [&#x27;0013035.jpg&#x27;, &#x27;1030023514_aad5c608f9.jpg&#x27;, ...]</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(img_full_path)  <span class=\"comment\"># dataset/hymenoptera_data/train/ants_image\\0013035.jpg</span></span><br></pre></td></tr></table></figure>\n<p>（2）辅助函数</p>\n<ul>\n<li><code>dir()</code>：不带参数时，返回当前范围内的变量、方法和定义的类型列表；带参数时，返回参数的属性、方法列表。</li>\n<li><code>help(func)</code>：查看函数 <code>func</code> 的使用说明。</li>\n</ul>\n<p>例如：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">dir</span>(torch))  <span class=\"comment\"># [&#x27;AVG&#x27;, &#x27;AggregationType&#x27;, ..., &#x27;cuda&#x27;, ...]</span></span><br><span class=\"line\"><span class=\"built_in\">help</span>(torch.cuda.is_available)  <span class=\"comment\"># Help on function is_available in module torch.cuda: is_available() -&gt; bool...</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"2-数据加载\">2. 数据加载</h2>\n<h3 id=\"2-1-Dataset\">2.1 Dataset</h3>\n<p>数据读取和预处理是进行机器学习的首要操作，PyTorch 提供了很多方法来完成数据的读取和预处理。</p>\n<p>其中 Dataset 表示数据集，<code>torch.utils.data.Dataset</code> 是代表这一数据的抽象类。你可以自己定义你的数据类，继承和重写这个抽象类，非常简单，只需要定义 <code>__len__</code> 和 <code>__getitem__</code> 这个两个函数即可，例如：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> Dataset</span><br><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MyData</span>(<span class=\"title class_ inherited__\">Dataset</span>):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, root_dir, label_dir</span>):</span><br><span class=\"line\">        self.root_dir = root_dir</span><br><span class=\"line\">        self.label_dir = label_dir</span><br><span class=\"line\">        self.path = os.path.join(self.root_dir, self.label_dir + <span class=\"string\">&#x27;_image&#x27;</span>)</span><br><span class=\"line\">        self.img_path_list = os.listdir(self.path)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__getitem__</span>(<span class=\"params\">self, idx</span>):</span><br><span class=\"line\">        img_path = self.img_path_list[idx]</span><br><span class=\"line\">        img_full_path = os.path.join(self.root_dir, self.label_dir + <span class=\"string\">&#x27;_image&#x27;</span>, img_path)</span><br><span class=\"line\">        img = Image.<span class=\"built_in\">open</span>(img_full_path)</span><br><span class=\"line\">        label = self.label_dir</span><br><span class=\"line\">        <span class=\"keyword\">return</span> img, label</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__len__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">len</span>(self.img_path_list)</span><br><span class=\"line\"></span><br><span class=\"line\">root_dir = <span class=\"string\">&#x27;dataset/hymenoptera_data/train&#x27;</span></span><br><span class=\"line\">ants_label_dir = <span class=\"string\">&#x27;ants&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">ants_data = MyData(root_dir, ants_label_dir)</span><br><span class=\"line\">img, label = ants_data[<span class=\"number\">0</span>]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(img, label)</span><br><span class=\"line\">img.show()</span><br></pre></td></tr></table></figure>\n<p>通过上面的方式，可以定义我们需要的数据类，可以通过迭代的方式来获取每一个数据，但这样很难实现取 batch、shuffle 或者是多线程去读取数据。</p>\n<h3 id=\"2-2-DataLoader\">2.2 DataLoader</h3>\n<p><code>torch.utils.data.DataLoader</code> 构建可迭代的数据装载器，我们在训练的时候，每一个 <code>for</code> 循环，每一次 iteration，就是从 <code>DataLoader</code> 中获取一个 <code>batch_size</code> 大小的数据的。打个比方如果 <code>Dataset</code> 是一副完整的扑克牌，那么 <code>DataLoader</code> 就是抽取几张组成的一部分扑克牌。</p>\n<p><code>DataLoader</code> 的参数很多，但我们常用的主要有以下几个：</p>\n<ul>\n<li><code>dataset</code>：<code>Dataset</code> 类，决定从哪个数据集读取数据。</li>\n<li><code>batch_size</code>：批大小。</li>\n<li><code>num_works</code>：是否多进程读取机制。</li>\n<li><code>shuffle</code>：每个 Epoch 是否乱序。</li>\n<li><code>drop_last</code>：当样本数不能被 <code>batch_size</code> 整除时，是否舍弃最后一批数据。</li>\n</ul>\n<p>要理解这个 <code>drop_last</code>，首先，得先理解 Epoch、Iteration 和 Batch_size 的概念：</p>\n<ul>\n<li>Epoch：所有训练样本都已输入到模型中，称为一个 Epoch。</li>\n<li>Iteration：一批样本输入到模型中，称为一个 Iteration。</li>\n<li>Batch_size：一批样本的大小，决定一个 Epoch 有多少个 Iteration。</li>\n</ul>\n<p><code>DataLoader</code> 的作用就是构建一个数据装载器，根据我们提供的 <code>batch_size</code> 的大小，将数据样本分成一个个的 Batch 去训练模型，而这个分的过程中需要把数据取到，这个就是借助 <code>Dataset</code> 的 <code>__getitem__</code> 方法。</p>\n<p>例如：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> Dataset, DataLoader</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms</span><br><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MyData</span>(<span class=\"title class_ inherited__\">Dataset</span>):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, root_dir, label_dir, transform</span>):</span><br><span class=\"line\">        self.root_dir = root_dir</span><br><span class=\"line\">        self.label_dir = label_dir</span><br><span class=\"line\">        self.path = os.path.join(self.root_dir, self.label_dir + <span class=\"string\">&#x27;_image&#x27;</span>)</span><br><span class=\"line\">        self.img_path_list = os.listdir(self.path)</span><br><span class=\"line\">        self.transform = transform  <span class=\"comment\"># transform 的方式</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__getitem__</span>(<span class=\"params\">self, idx</span>):</span><br><span class=\"line\">        img_path = self.img_path_list[idx]</span><br><span class=\"line\">        img_full_path = os.path.join(self.root_dir, self.label_dir + <span class=\"string\">&#x27;_image&#x27;</span>, img_path)</span><br><span class=\"line\">        img = Image.<span class=\"built_in\">open</span>(img_full_path).convert(<span class=\"string\">&#x27;RGB&#x27;</span>)  <span class=\"comment\"># 先将图片转换成三通道</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.transform <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            img = self.transform(img)</span><br><span class=\"line\">        label = self.label_dir</span><br><span class=\"line\">        <span class=\"keyword\">return</span> img, label</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__len__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">len</span>(self.img_path_list)</span><br><span class=\"line\"></span><br><span class=\"line\">root_dir = <span class=\"string\">&#x27;dataset/hymenoptera_data/train&#x27;</span></span><br><span class=\"line\">ants_label_dir = <span class=\"string\">&#x27;ants&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">trans_dataset = transforms.Compose([</span><br><span class=\"line\">    transforms.Resize((<span class=\"number\">83</span>, <span class=\"number\">100</span>)),  <span class=\"comment\"># tensor 大小必须统一</span></span><br><span class=\"line\">    transforms.ToTensor()</span><br><span class=\"line\">])</span><br><span class=\"line\"></span><br><span class=\"line\">ants_data = MyData(root_dir, ants_label_dir, trans_dataset)</span><br><span class=\"line\"></span><br><span class=\"line\">train_loader = DataLoader(dataset=ants_data, batch_size=<span class=\"number\">10</span>, shuffle=<span class=\"literal\">True</span>, num_workers=<span class=\"number\">0</span>, drop_last=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i, data <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(train_loader):</span><br><span class=\"line\">    imgs, labels = data</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"built_in\">type</span>(imgs))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(imgs[<span class=\"number\">0</span>])</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(labels)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(labels[<span class=\"number\">0</span>])</span><br></pre></td></tr></table></figure>\n<p>接下来使用 CIFAR10 数据集再展示一次 <code>DataLoader</code> 的用法：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms, datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> DataLoader</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.tensorboard <span class=\"keyword\">import</span> SummaryWriter</span><br><span class=\"line\"></span><br><span class=\"line\">test_set = datasets.CIFAR10(root=<span class=\"string\">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class=\"literal\">False</span>, transform=transforms.ToTensor(), download=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">test_loader = DataLoader(dataset=test_set, batch_size=<span class=\"number\">64</span>, shuffle=<span class=\"literal\">True</span>, num_workers=<span class=\"number\">0</span>, drop_last=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">writer = SummaryWriter(<span class=\"string\">&#x27;logs&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">2</span>):  <span class=\"comment\"># 循环两个 epoch</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> step, data <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(test_loader):  <span class=\"comment\"># step 表示第几个 batch</span></span><br><span class=\"line\">        imgs, targets = data</span><br><span class=\"line\">        writer.add_images(<span class=\"string\">&#x27;Epoch_&#123;&#125;&#x27;</span>.<span class=\"built_in\">format</span>(epoch), imgs, step)  <span class=\"comment\"># 注意是 add_images，图像默认格式为 NCHW</span></span><br><span class=\"line\"></span><br><span class=\"line\">writer.close()</span><br></pre></td></tr></table></figure>\n<p>PS：部分看不懂的代码可以先去学后面的 <code>transform</code> 以及 <code>tensorboard</code>。</p>\n<h2 id=\"3-TensorBoard\">3. TensorBoard</h2>\n<h3 id=\"3-1-add-scalar\">3.1 add_scalar</h3>\n<p>TensorBoard 原本是 TensorFlow 的可视化工具，PyTorch 从1.2.0开始支持 TensorBoard。之前的版本也可以使用 TensorBoardX 代替。</p>\n<p>先进入 Anaconda 的 PyTorch 环境，安装 TensorBoard：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda activate PyTorch</span><br><span class=\"line\">pip install tensorboard</span><br></pre></td></tr></table></figure>\n<p>在项目根目录下新建一个文件夹 <code>logs</code>，TensorBoard 的工作流程简单来说是将代码运行过程中的，某些你关心的数据保存在这个文件夹中（由代码中的 <code>writer</code> 完成），再读取这个文件夹中的数据，用浏览器显示出来（在命令行运行 TensorBoard 完成）。</p>\n<p>我们先绘制一个 <code>y = x</code> 的图像，运行以下代码：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.tensorboard <span class=\"keyword\">import</span> SummaryWriter</span><br><span class=\"line\"></span><br><span class=\"line\">writer = SummaryWriter(<span class=\"string\">&#x27;logs&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">100</span>):</span><br><span class=\"line\">    writer.add_scalar(<span class=\"string\">&#x27;y=x&#x27;</span>, x, x)  <span class=\"comment\"># tag=&#x27;y=x&#x27;, scalar_value=x, global_step=x</span></span><br><span class=\"line\"></span><br><span class=\"line\">writer.close()</span><br></pre></td></tr></table></figure>\n<p><code>add_scalar</code> 函数主要有三个参数：</p>\n<ul>\n<li><code>tag</code>：数据标识符，可以理解为数据图像的标题。</li>\n<li><code>scalar_value</code>：保存的值，即纵轴上的值。</li>\n<li><code>global_step</code>：记录的步长，即横轴的值，一般会设置一个不断增加的 <code>step</code>。</li>\n</ul>\n<p>运行后会看到 <code>logs</code> 文件夹下生成了一个文件，然后我们在 PyCharm 终端的 PyTorch 环境中打开 TensorBoard（要在当前项目中进入 PyTorch 环境，否则 <code>--logdir</code> 的路径就不能用相对路径了）：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensorboard --logdir logs</span><br></pre></td></tr></table></figure>\n<p>打开 <code>http://localhost:6006/</code> 即可看到绘制的图像。</p>\n<p>如果因为某些原因导致端口冲突可以指定端口：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensorboard --logdir logs --port 6007</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-2-add-image\">3.2 add_image</h3>\n<p><code>add_image</code> 函数主要有三个参数：</p>\n<ul>\n<li><code>tag</code>：同 <code>add_scalar</code>。</li>\n<li><code>img_tensor</code>：图像数据，类型必须是 <code>torch.Tensor</code>、<code>numpy.ndarry</code> 或 <code>string/blobname</code>。</li>\n<li><code>global_step</code>：同 <code>add_scalar</code>。</li>\n</ul>\n<p>可以看到传入的图片数据有类型限制，目前还没学到 <code>torch.Tensor</code> 类型，以 <code>numpy.ndarry</code> 为例，因此我们需要先安装一下 NumPy，还是在 PyTorch 环境中安装：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install numpy</span><br></pre></td></tr></table></figure>\n<p>使用 <code>PIL</code> 打开一个图像，将其转换成 NumPy 数组：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\">img_path = <span class=\"string\">&#x27;dataset/hymenoptera_data/train/ants_image/0013035.jpg&#x27;</span></span><br><span class=\"line\">img_PIL = Image.<span class=\"built_in\">open</span>(img_path)</span><br><span class=\"line\">img_array = np.array(img_PIL)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">type</span>(img_array))  <span class=\"comment\"># &lt;class &#x27;numpy.ndarray&#x27;&gt;</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(img_array.shape)  <span class=\"comment\"># (512, 768, 3)</span></span><br></pre></td></tr></table></figure>\n<p>可以看到图片的形状是三维的数据，前两个数据分别表示高度和宽度，第三个数据表示通道数，可以记为 <code>(H, W, C)</code>，简写为 <code>HWC</code>。</p>\n<p><code>add_image</code> 函数传入图片时格式默认为 <code>CHW</code>，如果格式不匹配需要设定函数中的 <code>dataformats</code> 参数，例如：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.tensorboard <span class=\"keyword\">import</span> SummaryWriter</span><br><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\">writer = SummaryWriter(<span class=\"string\">&#x27;logs&#x27;</span>)</span><br><span class=\"line\">img_path = <span class=\"string\">&#x27;dataset/hymenoptera_data/train/ants_image/0013035.jpg&#x27;</span></span><br><span class=\"line\">img_PIL = Image.<span class=\"built_in\">open</span>(img_path)</span><br><span class=\"line\">img_array = np.array(img_PIL)</span><br><span class=\"line\"></span><br><span class=\"line\">writer.add_image(<span class=\"string\">&#x27;img_test&#x27;</span>, img_array, <span class=\"number\">1</span>, dataformats=<span class=\"string\">&#x27;HWC&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">writer.close()</span><br></pre></td></tr></table></figure>\n<p>运行后打开 TensorBoard 即可在 IMAGES 页面下看到图片。</p>\n<h2 id=\"4-Transform\">4. Transform</h2>\n<h3 id=\"4-1-Transform的概念与基本用法\">4.1 Transform的概念与基本用法</h3>\n<p><code>transforms</code> 在计算机视觉工具包 <code>torchvision</code> 下，包含了很多种对图像数据进行变换的类，这些都是在我们进行图像数据读入步骤中必不可少的，通过图像变换可以将图片变成不同的类型，或者可以通过旋转、裁切等手段对图像数据集的图像进行变换，起到扩充数据集与数据增强的作用。</p>\n<p><code>transforms</code> 主要使用的类为：<code>transforms.ToTensor</code>，该类能够将 <code>PIL.Image</code> 或者 <code>ndarray</code> 类型的数据转换为 <code>tensor</code>，并且归一化至 <code>[0, 1]</code>。注意归一化至 <code>[0, 1]</code> 是直接除以255，若自己的 <code>ndarray</code> 数据尺度有变化，则需要自行修改。</p>\n<p>为什么需要 <code>tensor</code> 数据类型？因为它包装了反向传播神经网络所需要的一些基础的参数，因此在神经网络中需要将图片类型转换为 <code>tensor</code> 类型进行训练。</p>\n<p>例如：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms</span><br><span class=\"line\"><span class=\"keyword\">import</span> cv2</span><br><span class=\"line\"></span><br><span class=\"line\">img_path = <span class=\"string\">&#x27;dataset/hymenoptera_data/train/ants_image/0013035.jpg&#x27;</span></span><br><span class=\"line\">img_PIL = Image.<span class=\"built_in\">open</span>(img_path)  <span class=\"comment\"># &lt;class &#x27;PIL.JpegImagePlugin.JpegImageFile&#x27;&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">tensor_trans = transforms.ToTensor()  <span class=\"comment\"># 创建 ToTensor 的实例对象</span></span><br><span class=\"line\">img_tensor1 = tensor_trans(img_PIL)  <span class=\"comment\"># 将 PIL Image 转换成 tensor</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">type</span>(img_tensor1))  <span class=\"comment\"># &lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">img_cv = cv2.imread(img_path)  <span class=\"comment\"># &lt;class &#x27;numpy.ndarray&#x27;&gt;</span></span><br><span class=\"line\">img_tensor2 = tensor_trans(img_cv)  <span class=\"comment\"># 将 OpenCV Image 转换成 tensor</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">type</span>(img_tensor2))</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-2-Transform的常用类\">4.2 Transform的常用类</h3>\n<ul>\n<li><code>transforms.Compose</code>：<code>Compose</code> 能够将多种变换组合在一起。例如下面的代码可以先将 <code>PIL.Image</code> 中心裁切，然后再转换成 <code>tensor</code>：</li>\n</ul>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">img_path = <span class=\"string\">&#x27;dataset/hymenoptera_data/train/ants_image/0013035.jpg&#x27;</span></span><br><span class=\"line\">img_PIL = Image.<span class=\"built_in\">open</span>(img_path)</span><br><span class=\"line\"></span><br><span class=\"line\">trans = transforms.Compose([</span><br><span class=\"line\">    transforms.CenterCrop(<span class=\"number\">100</span>),</span><br><span class=\"line\">    transforms.ToTensor()</span><br><span class=\"line\">])</span><br><span class=\"line\"></span><br><span class=\"line\">img_trans = trans(img_PIL)</span><br></pre></td></tr></table></figure>\n<ul>\n<li><code>transforms.CenterCrop</code>：需要传入参数 <code>size</code>，表示以 <code>(size, size)</code> 的大小从中心裁剪，参数也可以为 <code>(height, width)</code>。例如：</li>\n</ul>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">img_PIL.show()</span><br><span class=\"line\"></span><br><span class=\"line\">trans_centercrop = transforms.CenterCrop((<span class=\"number\">100</span>, <span class=\"number\">150</span>))</span><br><span class=\"line\">img_centercrop = trans_centercrop(img_PIL)</span><br><span class=\"line\">img_centercrop.show()</span><br></pre></td></tr></table></figure>\n<ul>\n<li><code>transforms.RandomCrop</code>：需要传入参数 <code>size</code>，表示以 <code>(size, size)</code> 的大小随机裁剪，参数也可以为 <code>(height, width)</code>。</li>\n<li><code>transforms.Normalize(mean, std)</code>：对数据按通道进行标准化，即先减均值 <code>mean</code>，再除以标准差 <code>std</code>，注意是 <code>HWC</code> 格式，处理公式为：<code>output[channel] = (input[channel] - mean[channel]) / std[channel]</code>，例如：</li>\n</ul>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">trans_tensor = transforms.ToTensor()</span><br><span class=\"line\">img_tensor = trans_tensor(img_PIL)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 如果 input 的范围是[0, 1]，那么用该参数归一化后的范围就变为[-1, 1]</span></span><br><span class=\"line\">trans_norm = transforms.Normalize([<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>], [<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>])</span><br><span class=\"line\">img_norm = trans_norm(img_tensor)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(img_norm)</span><br></pre></td></tr></table></figure>\n<ul>\n<li><code>transforms.Resize</code>：需要传入参数 <code>(height, width)</code> 和 <code> interpolation</code>，表示重置图像的分辨率为 <code>(h, w)</code>，也可以传入一个整数 <code>size</code>，这样会将较短的那条边缩放至 <code>size</code>，另一条边按原图大小等比例缩放。<code>interpolation</code> 为插值方法选择，默认为 <code>PIL.Image.BILINEAR</code>，例如：</li>\n</ul>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">trans_tensor = transforms.ToTensor()</span><br><span class=\"line\">img_tensor = trans_tensor(img_PIL)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(img_tensor.size())  <span class=\"comment\"># torch.Size([3, 512, 768])，tensor 图像使用 size() 获取大小，PIL 图像使用 size</span></span><br><span class=\"line\"></span><br><span class=\"line\">trans_resize = transforms.Resize((<span class=\"number\">256</span>, <span class=\"number\">300</span>))</span><br><span class=\"line\">img_resize = trans_resize(img_tensor)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(img_resize.size())  <span class=\"comment\"># torch.Size([3, 256, 300])，修改比例</span></span><br><span class=\"line\"></span><br><span class=\"line\">trans_resize = transforms.Resize(<span class=\"number\">30</span>)</span><br><span class=\"line\">img_resize = trans_resize(img_tensor)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(img_resize.size())  <span class=\"comment\"># torch.Size([3, 30, 45])，与原图等比例</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li><code>transforms.ToPILImage</code>：：将 <code>tensor</code> 或者 <code>ndarray</code> 的数据转换为 <code>PIL.Image</code> 类型数据，参数 <code>mode</code> 默认为 <code>None</code>，表示1通道， <code>mode=3</code> 表示3通道，默认转换为 <code>RGB</code>，4通道默认转换为 <code>RGBA</code>。</li>\n</ul>\n<h2 id=\"5-Torchvision数据集使用方法\">5. Torchvision数据集使用方法</h2>\n<p>Torchvision 官方文档 <a href=\"https://pytorch.org/vision/stable/datasets.html\">Torchvision</a> 中的 <code>torchvision.datasets</code> 就是 Torchvision 提供的标准数据集，其中有很多已经构建和训练好的网络模型，在不同的领域下各自有着很优秀的性能。</p>\n<p>我们以 CIFAR10 为例，该数据集包括了60000张32*32像素的图像，总共有10个类别，每个类别有6000张图像，其中有50000张图像为训练图像，10000张为测试图像。其使用说明如下图所示：</p>\n<ul>\n<li><code>root</code>：数据集存放的路径。</li>\n<li><code>train</code>：如果为 <code>True</code>，创建的数据集就为训练集，否则创建的数据集就为测试集。</li>\n<li><code>transform</code>：使用 <code>transforms</code> 中的变换操作对数据集进行变换。</li>\n<li><code>target_transform</code>：对 <code>target</code> 进行 <code>transform</code>。</li>\n<li><code>download</code>：如果为 <code>True</code>，就会自动从网上下载这个数据集，否则就不会下载。</li>\n</ul>\n<p>例如：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torchvision</span><br><span class=\"line\"></span><br><span class=\"line\">train_data = torchvision.datasets.CIFAR10(root=<span class=\"string\">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class=\"literal\">True</span>, download=<span class=\"literal\">True</span>)</span><br><span class=\"line\">test_data = torchvision.datasets.CIFAR10(root=<span class=\"string\">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class=\"literal\">False</span>, download=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(train_data[<span class=\"number\">0</span>])  <span class=\"comment\"># (&lt;PIL.Image.Image image mode=RGB size=32x32 at 0x24011FC4F40&gt;, 6)</span></span><br></pre></td></tr></table></figure>\n<p>刚开始运行时可以看到正在从网上下载数据集，如果下载速度非常慢可以复制链接去迅雷之类的地方下载，下载好后自己创建设定的路径，将数据集放过来即可。</p>\n<p>然后设置断点，用 Debug 模式运行一下代码，我们可以查看一下数据集的内容，数据集 <code>train_data</code> 中的 <code>classes</code> 表示图像的种类，<code>classes_to_idx</code> 表示将种类映射为整数，<code>targets</code> 表示每张图像对应的种类编号，试着输出一下第一张图的信息：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">img, target = train_data[<span class=\"number\">0</span>]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(img)  <span class=\"comment\"># &lt;PIL.Image.Image image mode=RGB size=32x32 at 0x1EEAEC32190&gt;</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(target)  <span class=\"comment\"># 6</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(train_data.classes[target])  <span class=\"comment\"># frog</span></span><br><span class=\"line\">img.show()  <span class=\"comment\"># 图像显示为青蛙</span></span><br></pre></td></tr></table></figure>\n<p>现在展示如何使用 <code>transform</code> 参数，假设我们需要将数据集的图像都转换成 <code>tensor</code> 类型：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">trans_dataset = torchvision.transforms.Compose([</span><br><span class=\"line\">    torchvision.transforms.ToTensor()</span><br><span class=\"line\">])</span><br><span class=\"line\"></span><br><span class=\"line\">train_data = torchvision.datasets.CIFAR10(root=<span class=\"string\">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class=\"literal\">True</span>, transform=trans_dataset, download=<span class=\"literal\">True</span>)</span><br><span class=\"line\">test_data = torchvision.datasets.CIFAR10(root=<span class=\"string\">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class=\"literal\">False</span>, transform=trans_dataset, download=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">img, target = train_data[<span class=\"number\">0</span>]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">type</span>(img))  <span class=\"comment\"># &lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"6-神经网络Torch-NN基本骨架的使用\">6. 神经网络Torch.NN基本骨架的使用</h2>\n<p><code>torch.nn</code> 能够帮助我们更优雅地训练神经网络，使神经网络代码更加简洁和灵活。官方文档：<a href=\"https://pytorch.org/docs/stable/nn.html\">Torch.NN</a>。</p>\n<p>在文档中可以看到第一块内容叫做 <code>Container</code>（容器），这就相当于神经网络的骨架，<code>Container</code> 之后的东西就用于往骨架里面填充，如 Convolution Layers（卷积层）、Pooling Layers（池化层），有卷积神经网络基础的小伙伴对这些词应该都很熟悉了。</p>\n<p><code>Container</code> 中有六个模块：<code>Module</code>、<code>Sequential</code>、<code>ModuleList</code>、<code>ModuleDict</code>、<code>ParameterList</code>、<code>ParameterDict</code>，其中最常用的为 <code>Module</code>，这是所有神经网络的最基本的类，其基本的构造方式如下：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Model</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):  <span class=\"comment\"># 初始化</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">20</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\">        self.conv2 = nn.Conv2d(<span class=\"number\">20</span>, <span class=\"number\">20</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):  <span class=\"comment\"># 前向传播</span></span><br><span class=\"line\">        x = F.relu(self.conv1(x))  <span class=\"comment\"># 将 x 进行第一层卷积后用 ReLU 激活函数输出</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.relu(self.conv2(x))  <span class=\"comment\"># 将处理后的 x 再进行第二层卷积后用 ReLU 处理后返回最后结果</span></span><br></pre></td></tr></table></figure>\n<p>现在我们尝试自己创建一个简单的神经网络，并输出前向传播的结果：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Network</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):  <span class=\"comment\"># 初始化</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Network, self).__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, <span class=\"built_in\">input</span></span>):</span><br><span class=\"line\">        output = <span class=\"built_in\">input</span> + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br><span class=\"line\"></span><br><span class=\"line\">network = Network()</span><br><span class=\"line\">x = torch.tensor(<span class=\"number\">1.0</span>)  <span class=\"comment\"># x 为 tensor 类型</span></span><br><span class=\"line\">output = network(x)  <span class=\"comment\"># Module 中的 __call__ 函数会调用 forward 函数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(output)  <span class=\"comment\"># tensor(2.)</span></span><br></pre></td></tr></table></figure>\n<p>我们以 <code>Conv2d</code> 函数为例，该函数的官方文档：<a href=\"https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html#torch.nn.functional.conv2d\">TORCH.NN.FUNCTIONAL.CONV2D</a>。</p>\n<p>该函数有以下几个参数：</p>\n<ul>\n<li><code>input</code>：输入的图像，<code>size</code> 为 <code>(mini_batch, in_channels, height, width)</code>。</li>\n<li><code>weight</code>：卷积核的大小，<code>size</code> 为 <code>(out_channels, in_channels/groups, height, width)</code>。</li>\n<li><code>bias</code>：偏置，默认为 <code>None</code>。</li>\n<li><code>stride</code>：步长，用来控制卷积核移动间隔，如果为 <code>x</code> 则水平和竖直方向的步长都为 <code>x</code>，如果为 <code>(x, y)</code> 则竖直方向步长为 <code>x</code>，水平方向步长为 <code>y</code>。</li>\n<li><code>padding</code>：在输入图像的边沿进行扩边操作，以保证图像输入输出前后的尺寸大小不变，在 PyTorch 的卷积层定义中，默认的 <code>padding</code> 为零填充，即在边缘填充0。</li>\n<li><code>padding_mode</code>：扩边的方式。</li>\n<li><code>dilation</code>：设定了取数之间的间隔。</li>\n</ul>\n<p>例如：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">input</span> = torch.tensor([</span><br><span class=\"line\">    [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">    [<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>],</span><br><span class=\"line\">    [<span class=\"number\">3</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>],</span><br><span class=\"line\">    [<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\">])</span><br><span class=\"line\"></span><br><span class=\"line\">kernel = torch.tensor([</span><br><span class=\"line\">    [<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>],</span><br><span class=\"line\">    [<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>],</span><br><span class=\"line\">    [<span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">2</span>]</span><br><span class=\"line\">])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">input</span> = torch.reshape(<span class=\"built_in\">input</span>, (<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"number\">4</span>))  <span class=\"comment\"># batch_size = 1，channel = 1</span></span><br><span class=\"line\">kernel = torch.reshape(kernel, (<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">output = F.conv2d(<span class=\"built_in\">input</span>, kernel, stride=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(output)</span><br><span class=\"line\"><span class=\"comment\"># tensor([[[[15, 16],</span></span><br><span class=\"line\"><span class=\"comment\">#           [ 6, 15]]]])</span></span><br><span class=\"line\"></span><br><span class=\"line\">output = F.conv2d(<span class=\"built_in\">input</span>, kernel, stride=<span class=\"number\">1</span>, bias=torch.tensor([<span class=\"number\">3</span>]))  <span class=\"comment\"># 注意 bias 必须也是矩阵</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(output)</span><br><span class=\"line\"><span class=\"comment\"># tensor([[[[18, 19],</span></span><br><span class=\"line\"><span class=\"comment\">#           [ 9, 18]]]])</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"7-Convolution-Layers与Pooling-Layers\">7. Convolution Layers与Pooling Layers</h2>\n<p>由于图像是二维的，因此基本上最常用到的就是二维的卷积层和池化层：<code>torch.nn.Conv2d</code>、<code>torch.nn.MaxPool2d</code>，官方文档：<a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d\">torch.nn.Conv2d</a>、<a href=\"https://pytorch.org/docs/stable/nn.html#pooling-layers\">Pooling Layers</a>。</p>\n<h3 id=\"7-1-Convolution-Layers\">7.1 Convolution Layers</h3>\n<p>卷积运算能够<strong>提取输入图像的不同特征</strong>，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。</p>\n<p><code>torch.nn.Conv2d</code> 的主要参数有以下几个：</p>\n<ul>\n<li><code>in_channels</code>：输入图像的通道数，彩色图像一般都是三通道。</li>\n<li><code>out_channels</code>：通过卷积后产生的输出图像的通道数。</li>\n<li><code>kernel_size</code>：可以是一个数或一个元组，表示卷积核的大小，卷积核的参数是从数据的分布中采样得到的，这些数是多少无所谓，因为在神经网络训练的过程中就是对这些参数进行不断地调整。</li>\n<li><code>stride</code>：步长。</li>\n<li><code>padding</code>：填充。</li>\n<li><code>padding_mode</code>：填充模式，有 <code>zeros</code>、<code>reflect</code>、<code>replicate</code>、<code>circular</code>，默认为 <code>zeros</code>。</li>\n<li><code>dilation</code>：可以是一个数或一个元组，表示卷积核各个元素间的距离，也称空洞卷积。</li>\n<li><code>group</code>：一般设置为1，基本用不到。</li>\n<li><code>bias</code>：偏置，一般设置为 <code>True</code>。</li>\n</ul>\n<p>例如以下代码构建了一个只有一层卷积层的神经网络，该卷积层的输入和输出通道数都为三通道，卷积核大小为3*3，步长为1，无填充，然后用 CIFAR10 测试数据集进行测试：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms, datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> DataLoader</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.tensorboard <span class=\"keyword\">import</span> SummaryWriter</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\">test_data = datasets.CIFAR10(<span class=\"string\">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class=\"literal\">False</span>, transform=transforms.ToTensor())</span><br><span class=\"line\"></span><br><span class=\"line\">data_loader = DataLoader(test_data, batch_size=<span class=\"number\">64</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Network</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Network, self).__init__()</span><br><span class=\"line\">        self.conv1 = nn.Conv2d(in_channels=<span class=\"number\">3</span>, out_channels=<span class=\"number\">3</span>, kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, <span class=\"built_in\">input</span></span>):</span><br><span class=\"line\">        output = self.conv1(<span class=\"built_in\">input</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br><span class=\"line\"></span><br><span class=\"line\">network = Network()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(network)  <span class=\"comment\"># Network((conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1)))</span></span><br><span class=\"line\"></span><br><span class=\"line\">writer = SummaryWriter(<span class=\"string\">&#x27;logs&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> step, data <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(data_loader):</span><br><span class=\"line\">    imgs, targets = data</span><br><span class=\"line\">    output = network(imgs)</span><br><span class=\"line\">    writer.add_images(<span class=\"string\">&#x27;input&#x27;</span>, imgs, step)</span><br><span class=\"line\">    writer.add_images(<span class=\"string\">&#x27;output&#x27;</span>, output, step)</span><br><span class=\"line\"></span><br><span class=\"line\">writer.close()</span><br></pre></td></tr></table></figure>\n<p>运行后可以打开 TensorBoard 查看一下效果。</p>\n<h3 id=\"7-2-Pooling-Layers\">7.2 Pooling Layers</h3>\n<p>Pooling Layers 中的 <code>MaxPool</code> 表示最大池化，也称上采样；<code>MaxUnpool</code> 表示最小池化，也称下采样；<code>AvgPool</code> 表示平均池化。其中最常用的为 <code>MaxPool2d</code>，官方文档：<a href=\"https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d\">torch.nn.MaxPool2d</a>。</p>\n<p>最大池化的目的是<strong>保留输入数据的特征，同时减小特征的数据量</strong>。</p>\n<p><code>torch.nn.MaxPool2d</code> 的主要参数有以下几个：</p>\n<ul>\n<li><code>kernel_size</code>：用来取最大值的窗口（池化核）大小，和之前的卷积核类似。</li>\n<li><code>stride</code>：步长，注意默认值为 <code>kernel_size</code>。</li>\n<li><code>padding</code>：填充，和 <code>Conv2d</code> 一样。</li>\n<li><code>dilation</code>：池化核中各个元素间的距离，和 <code>Conv2d</code> 一样。</li>\n<li><code>return_indices</code>：如果为 <code>True</code>，表示返回值中包含最大值位置的索引。注意这个最大值指的是在所有窗口中产生的最大值，如果窗口产生的最大值总共有5个，就会有5个返回值。</li>\n<li><code>ceil_mode</code>：如果为 <code>True</code>，表示在计算输出结果形状的时候，使用向上取整，否则默认向下取整。</li>\n</ul>\n<p>输出图像的形状的计算公式可以在官方文档中查看。</p>\n<p>接下来我们用代码实现这个池化层：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms, datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> DataLoader</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.tensorboard <span class=\"keyword\">import</span> SummaryWriter</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Network</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Network, self).__init__()</span><br><span class=\"line\">        self.maxpool1 = nn.MaxPool2d(kernel_size=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, <span class=\"built_in\">input</span></span>):</span><br><span class=\"line\">        output = self.maxpool1(<span class=\"built_in\">input</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">input</span> = torch.tensor([</span><br><span class=\"line\">    [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">    [<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>],</span><br><span class=\"line\">    [<span class=\"number\">3</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>],</span><br><span class=\"line\">    [<span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\">], dtype=torch.float32)  <span class=\"comment\"># 注意池化层读入的数据需要为浮点型</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">input</span> = torch.reshape(<span class=\"built_in\">input</span>, (<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">network = Network()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(network)  <span class=\"comment\"># Network((maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False))</span></span><br><span class=\"line\"></span><br><span class=\"line\">output = network(<span class=\"built_in\">input</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(output)</span><br><span class=\"line\"><span class=\"comment\"># tensor([[[[2., 3.],</span></span><br><span class=\"line\"><span class=\"comment\">#           [4., 2.]]]])</span></span><br></pre></td></tr></table></figure>\n<p>我们用图像来试试效果：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms, datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> DataLoader</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.tensorboard <span class=\"keyword\">import</span> SummaryWriter</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\">test_data = datasets.CIFAR10(<span class=\"string\">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class=\"literal\">False</span>, transform=transforms.ToTensor())</span><br><span class=\"line\"></span><br><span class=\"line\">data_loader = DataLoader(test_data, batch_size=<span class=\"number\">64</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Network</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Network, self).__init__()</span><br><span class=\"line\">        self.maxpool1 = nn.MaxPool2d(kernel_size=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, <span class=\"built_in\">input</span></span>):</span><br><span class=\"line\">        output = self.maxpool1(<span class=\"built_in\">input</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br><span class=\"line\"></span><br><span class=\"line\">network = Network()</span><br><span class=\"line\"></span><br><span class=\"line\">writer = SummaryWriter(<span class=\"string\">&#x27;logs&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> step, data <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(data_loader):</span><br><span class=\"line\">    imgs, targets = data</span><br><span class=\"line\">    output = network(imgs)</span><br><span class=\"line\">    writer.add_images(<span class=\"string\">&#x27;input&#x27;</span>, imgs, step)</span><br><span class=\"line\">    writer.add_images(<span class=\"string\">&#x27;output&#x27;</span>, output, step)</span><br><span class=\"line\"></span><br><span class=\"line\">writer.close()</span><br></pre></td></tr></table></figure>\n<p>运行后可以打开 TensorBoard 查看一下效果。</p>\n<h2 id=\"8-Non-linear-Activations与Linear-Layers\">8. Non-linear Activations与Linear Layers</h2>\n<h3 id=\"8-1-Non-linear-Activations\">8.1 Non-linear Activations</h3>\n<p>非线性激活的目的是为了在网络中引入一些<strong>非线性特征</strong>，因为非线性特征越多才能训练出符合各种曲线（特征）的模型。</p>\n<p>非线性激活函数官方文档：<a href=\"https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\">Non-linear Activations</a>。</p>\n<p>有深度学习基础的同学应该知道最常用的非线性激活函数就是 ReLU 和 Sigmoid 函数，多分类问题会在输出层使用 Softmax 函数（如果损失函数使用的是交叉熵误差函数 <code>CrossEntropyLoss</code> 则会自动计算 Softmax，无需创建 Softmax 层）。这三个函数在 PyTorch 中分别为 <code>nn.ReLU</code>、<code>nn.Sigmoid</code> 和 <code>nn.Softmax</code>。</p>\n<p>这两个函数的输入都是只需指明 <code>batch_size</code> 即可，在 PyTorch1.0 之后的版本任何形状的数据都能被计算，无需指定 <code>batch_size</code>。</p>\n<p><code>nn.ReLU</code> 只有一个需要设置的参数 <code>inplace</code>，如果为 <code>True</code> 表示计算结果直接替换到输入数据上，例如：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">input</span> = -<span class=\"number\">1</span></span><br><span class=\"line\">nn.ReLU(<span class=\"built_in\">input</span>, inplace=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># input = 0</span></span><br></pre></td></tr></table></figure>\n<p>构建 ReLU 层代码如下：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Network</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Network, self).__init__()</span><br><span class=\"line\">        self.relu1 = nn.ReLU()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, <span class=\"built_in\">input</span></span>):</span><br><span class=\"line\">        output = self.relu1(<span class=\"built_in\">input</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br><span class=\"line\"></span><br><span class=\"line\">network = Network()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">input</span> = torch.tensor([</span><br><span class=\"line\">    [<span class=\"number\">1</span>, -<span class=\"number\">0.5</span>],</span><br><span class=\"line\">    [-<span class=\"number\">1</span>, <span class=\"number\">3</span>]</span><br><span class=\"line\">])</span><br><span class=\"line\"></span><br><span class=\"line\">output = network(<span class=\"built_in\">input</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(output)</span><br><span class=\"line\"><span class=\"comment\"># tensor([[1., 0.],</span></span><br><span class=\"line\"><span class=\"comment\">#         [0., 3.]])</span></span><br></pre></td></tr></table></figure>\n<p>由于 ReLU 对图像处理的直观效果不明显，我们使用 Sigmoid 对图像进行处理：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms, datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> DataLoader</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.tensorboard <span class=\"keyword\">import</span> SummaryWriter</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Network</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Network, self).__init__()</span><br><span class=\"line\">        self.sigmoid1 = nn.Sigmoid()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, <span class=\"built_in\">input</span></span>):</span><br><span class=\"line\">        output = self.sigmoid1(<span class=\"built_in\">input</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br><span class=\"line\"></span><br><span class=\"line\">test_data = datasets.CIFAR10(<span class=\"string\">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class=\"literal\">False</span>, transform=transforms.ToTensor())</span><br><span class=\"line\">data_loader = DataLoader(test_data, batch_size=<span class=\"number\">64</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">network = Network()</span><br><span class=\"line\"></span><br><span class=\"line\">writer = SummaryWriter(<span class=\"string\">&#x27;logs&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> step, data <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(data_loader):</span><br><span class=\"line\">    imgs, targets = data</span><br><span class=\"line\">    output = network(imgs)</span><br><span class=\"line\">    writer.add_images(<span class=\"string\">&#x27;input&#x27;</span>, imgs, step)</span><br><span class=\"line\">    writer.add_images(<span class=\"string\">&#x27;output&#x27;</span>, output, step)</span><br><span class=\"line\"></span><br><span class=\"line\">writer.close()</span><br></pre></td></tr></table></figure>\n<h3 id=\"8-2-Linear-Layers\">8.2 Linear Layers</h3>\n<p>线性层官方文档：<a href=\"https://pytorch.org/docs/stable/nn.html#linear-layers\">Linear Layers</a>。</p>\n<p>PyTorch 的 <code>nn.Linear</code> 是用于设置网络中的全连接层的，需要注意的是全连接层的输入与输出都是二维张量，一般形状为：<code>[batch_size, size]</code>，不同于卷积层要求输入输出是四维张量，因此在将图像传入全连接层之前一般都会展开成一维的。</p>\n<p><code>nn.Linear</code> 有三个参数分别如下：</p>\n<ul>\n<li><code>in_features</code>：指的是输入的二维张量的大小，即输入的 <code>[batch_size, size]</code> 中的 <code>size</code>。</li>\n<li><code>out_features</code>：指的是输出的二维张量的大小，即输出的二维张量的形状为 <code>[batch_size, output_size]</code>，当然，它也代表了该全连接层的神经元个数。从输入输出的张量的 <code>shape</code> 角度来理解，相当于一个输入为 <code>[batch_size, in_features]</code> 的张量变换成了 <code>[batch_size, out_features]</code> 的输出张量。</li>\n<li><code>bias</code>：偏置，相当于 <code>y = ax + b</code> 中的 <code>b</code>。</li>\n</ul>\n<p>代码示例如下：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Network</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Network, self).__init__()</span><br><span class=\"line\">        self.linear1 = nn.Linear(<span class=\"number\">24</span>, <span class=\"number\">30</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, <span class=\"built_in\">input</span></span>):</span><br><span class=\"line\">        output = self.linear1(<span class=\"built_in\">input</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">input</span> = torch.tensor([</span><br><span class=\"line\">    [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">    [<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>],</span><br><span class=\"line\">    [<span class=\"number\">3</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>],</span><br><span class=\"line\">], dtype=torch.float32)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">input</span>.shape)  <span class=\"comment\"># torch.Size([3, 8])</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">input</span> = torch.flatten(<span class=\"built_in\">input</span>)  <span class=\"comment\"># 将 input 拉平成一维</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">input</span>.shape)  <span class=\"comment\"># torch.Size([24])</span></span><br><span class=\"line\"></span><br><span class=\"line\">network = Network()</span><br><span class=\"line\"></span><br><span class=\"line\">output = network(<span class=\"built_in\">input</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(output.shape)  <span class=\"comment\"># torch.Size([30])</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"9-神经网络模型搭建小实战\">9. 神经网络模型搭建小实战</h2>\n<h3 id=\"9-1-Sequential\">9.1 Sequential</h3>\n<p><code>torch.nn.Sequential</code> 是一个 Sequential 容器，能够在容器中嵌套各种实现神经网络中具体功能相关的类，来完成对神经网络模型的搭建。模块的加入一般有两种方式，一种是直接嵌套，另一种是以 <code>OrderedDict</code> 有序字典的方式进行传入，这两种方式的唯一区别是：</p>\n<ul>\n<li>使用 <code>OrderedDict</code> 搭建的模型的每个模块都有我们自定义的名字。</li>\n<li>直接嵌套默认使用从零开始的数字序列作为每个模块的名字。</li>\n</ul>\n<p>（1）直接嵌套方法的代码如下：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\">model = nn.Sequential(</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">20</span>, <span class=\"number\">5</span>),</span><br><span class=\"line\">    nn.ReLU(),</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">20</span>, <span class=\"number\">64</span>, <span class=\"number\">5</span>),</span><br><span class=\"line\">    nn.ReLU()</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(model)</span><br><span class=\"line\"><span class=\"comment\"># Sequential(</span></span><br><span class=\"line\"><span class=\"comment\">#   (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class=\"line\"><span class=\"comment\">#   (1): ReLU()</span></span><br><span class=\"line\"><span class=\"comment\">#   (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class=\"line\"><span class=\"comment\">#   (3): ReLU()</span></span><br><span class=\"line\"><span class=\"comment\"># )</span></span><br></pre></td></tr></table></figure>\n<p>（2）使用 <code>OrderedDict</code> 的代码如下：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> OrderedDict</span><br><span class=\"line\"></span><br><span class=\"line\">model = nn.Sequential(OrderedDict([</span><br><span class=\"line\">    (<span class=\"string\">&#x27;Conv1&#x27;</span>, nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">20</span>, <span class=\"number\">5</span>)),</span><br><span class=\"line\">    (<span class=\"string\">&#x27;ReLU1&#x27;</span>, nn.ReLU()),</span><br><span class=\"line\">    (<span class=\"string\">&#x27;Conv2&#x27;</span>, nn.Conv2d(<span class=\"number\">20</span>, <span class=\"number\">64</span>, <span class=\"number\">5</span>)),</span><br><span class=\"line\">    (<span class=\"string\">&#x27;ReLU2&#x27;</span>, nn.ReLU())</span><br><span class=\"line\">]))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(model)</span><br><span class=\"line\"><span class=\"comment\"># Sequential(</span></span><br><span class=\"line\"><span class=\"comment\">#   (Conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class=\"line\"><span class=\"comment\">#   (ReLU1): ReLU()</span></span><br><span class=\"line\"><span class=\"comment\">#   (Conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class=\"line\"><span class=\"comment\">#   (ReLU2): ReLU()</span></span><br><span class=\"line\"><span class=\"comment\"># )</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"9-2-小实战\">9.2 小实战</h3>\n<p>由于代码很简单，都是学过的内容进行组装，因此直接看代码：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.tensorboard <span class=\"keyword\">import</span> SummaryWriter</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">CIFAR10_Network</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(CIFAR10_Network, self).__init__()</span><br><span class=\"line\">        self.model = nn.Sequential(</span><br><span class=\"line\">            nn.Conv2d(in_channels=<span class=\"number\">3</span>, out_channels=<span class=\"number\">32</span>, kernel_size=<span class=\"number\">5</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">2</span>),  <span class=\"comment\"># [32, 32, 32]</span></span><br><span class=\"line\">            nn.MaxPool2d(kernel_size=<span class=\"number\">2</span>),  <span class=\"comment\"># [32, 16, 16]</span></span><br><span class=\"line\">            nn.Conv2d(in_channels=<span class=\"number\">32</span>, out_channels=<span class=\"number\">32</span>, kernel_size=<span class=\"number\">5</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">2</span>),  <span class=\"comment\"># [32, 16, 16]</span></span><br><span class=\"line\">            nn.MaxPool2d(kernel_size=<span class=\"number\">2</span>),  <span class=\"comment\"># [32, 8, 8]</span></span><br><span class=\"line\">            nn.Conv2d(in_channels=<span class=\"number\">32</span>, out_channels=<span class=\"number\">64</span>, kernel_size=<span class=\"number\">5</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">2</span>),  <span class=\"comment\"># [64, 8, 8]</span></span><br><span class=\"line\">            nn.MaxPool2d(kernel_size=<span class=\"number\">2</span>),  <span class=\"comment\"># [64, 4, 4]</span></span><br><span class=\"line\">            nn.Flatten(),  <span class=\"comment\"># [1024]</span></span><br><span class=\"line\">            nn.Linear(in_features=<span class=\"number\">1024</span>, out_features=<span class=\"number\">64</span>),  <span class=\"comment\"># [64]</span></span><br><span class=\"line\">            nn.Linear(in_features=<span class=\"number\">64</span>, out_features=<span class=\"number\">10</span>) <span class=\"comment\"># [10]</span></span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, <span class=\"built_in\">input</span></span>):</span><br><span class=\"line\">        output = self.model(<span class=\"built_in\">input</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br><span class=\"line\"></span><br><span class=\"line\">network = CIFAR10_Network()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">input</span> = torch.randn(<span class=\"number\">64</span>, <span class=\"number\">3</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>)  <span class=\"comment\"># 返回一个包含了从标准正态分布中抽取的一组随机数的张量</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">input</span>.shape)  <span class=\"comment\"># torch.Size([64, 3, 32, 32])</span></span><br><span class=\"line\">output = network(<span class=\"built_in\">input</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(output.shape)  <span class=\"comment\"># torch.Size([64, 10])</span></span><br><span class=\"line\"></span><br><span class=\"line\">writer = SummaryWriter(<span class=\"string\">&#x27;logs&#x27;</span>)</span><br><span class=\"line\">writer.add_graph(network, <span class=\"built_in\">input</span>)  <span class=\"comment\"># 生成计算图</span></span><br><span class=\"line\">writer.close()</span><br></pre></td></tr></table></figure>\n<p>使用 <code>add_graph</code> 函数可以在 TensorBoard 中生成神经网络的计算图，通过计算图可以很清晰地看到每一层计算时数据流入流出的结果，双击相应的标签可以进一步深入查看更详细的信息。</p>\n<h2 id=\"10-损失函数与反向传播\">10. 损失函数与反向传播</h2>\n<h3 id=\"10-1-Loss-Functions\">10.1 Loss Functions</h3>\n<p>具有深度学习理论基础的同学对损失函数和反向传播一定不陌生，在此不详细展开理论介绍。损失函数是指用于计算标签值和预测值之间差异的函数，在机器学习过程中，有多种损失函数可供选择，典型的有距离向量，绝对值向量等。使用损失函数的流程概括如下：</p>\n<ol>\n<li>计算实际输出和目标之间的差距。</li>\n<li>为我们更新输出提供一定的依据（反向传播）。</li>\n</ol>\n<p>损失函数的官方文档：<a href=\"https://pytorch.org/docs/stable/nn.html#loss-functions\">Loss Functions</a>。</p>\n<p>（1）<code>nn.L1Loss</code>：平均绝对误差（MAE，Mean Absolute Error），计算方法很简单，取预测值和真实值的绝对误差的平均数即可。</p>\n<p>PyTorch1.13中 <code>nn.L1Loss</code> 数据形状规定如下：</p>\n<ul>\n<li><code>Input</code>：<code>(*)</code>，means any number of dimensions.</li>\n<li><code>Target</code>：<code>(*)</code>，same shape as the input.</li>\n<li><code>Output</code>：scalar. If <code>reduction</code> is <code>none</code>, then <code>(*)</code>, same shape as the input.</li>\n</ul>\n<p>早先的版本需要指定 <code>batch_size</code> 大小，现在不需要了。可以设置参数 <code>reduction</code>，默认为 <code>mean</code>，即取平均值，也可以设置为 <code>sum</code>，顾名思义就是取和。</p>\n<p>测试代码如下：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">input</span> = torch.tensor([<span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>, <span class=\"number\">3.0</span>])</span><br><span class=\"line\">target = torch.tensor([<span class=\"number\">4.0</span>, -<span class=\"number\">2.0</span>, <span class=\"number\">5.0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">loss = nn.L1Loss()</span><br><span class=\"line\">result = loss(<span class=\"built_in\">input</span>, target)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(result)  <span class=\"comment\"># tensor(3.)</span></span><br><span class=\"line\"></span><br><span class=\"line\">loss = nn.L1Loss(reduction=<span class=\"string\">&#x27;sum&#x27;</span>)</span><br><span class=\"line\">result = loss(<span class=\"built_in\">input</span>, target)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(result)  <span class=\"comment\"># tensor(9.)</span></span><br></pre></td></tr></table></figure>\n<p>（2）<code>nn.MSELoss</code>：均方误差（MSE，Mean Squared Error），即预测值和真实值之差的平方和的平均数。</p>\n<p>该损失函数的用法与 <code>nn.L1Loss</code> 相似，代码如下：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">input</span> = torch.tensor([<span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>, <span class=\"number\">3.0</span>])</span><br><span class=\"line\">target = torch.tensor([<span class=\"number\">4.0</span>, -<span class=\"number\">2.0</span>, <span class=\"number\">5.0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">loss = nn.MSELoss()</span><br><span class=\"line\">result = loss(<span class=\"built_in\">input</span>, target)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(result)  <span class=\"comment\"># tensor(9.6667)</span></span><br><span class=\"line\"></span><br><span class=\"line\">loss = nn.MSELoss(reduction=<span class=\"string\">&#x27;sum&#x27;</span>)</span><br><span class=\"line\">result = loss(<span class=\"built_in\">input</span>, target)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(result)  <span class=\"comment\"># tensor(29.)</span></span><br></pre></td></tr></table></figure>\n<p>（3）<code>nn.CrossEntropyLoss</code>：交叉熵误差，训练分类 C 个类别的模型的时候较常用这个损失函数，一般用在 Softmax 层后面，计算公式较为复杂，可以在官网中查看。</p>\n<p>测试代码如下：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">input</span> = torch.tensor([<span class=\"number\">0.1</span>, <span class=\"number\">0.7</span>, <span class=\"number\">0.2</span>])</span><br><span class=\"line\">target = torch.tensor(<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">loss = nn.CrossEntropyLoss()</span><br><span class=\"line\">result = loss(<span class=\"built_in\">input</span>, target)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(result)  <span class=\"comment\"># tensor(0.7679)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">input</span> = torch.tensor([<span class=\"number\">0.8</span>, <span class=\"number\">0.1</span>, <span class=\"number\">0.1</span>])</span><br><span class=\"line\">result = loss(<span class=\"built_in\">input</span>, target)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(result)  <span class=\"comment\"># tensor(1.3897)</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"10-2-Backward\">10.2 Backward</h3>\n<p>接下来以 CIFAR10 数据集为例，用上一节搭建的神经网络先设置 <code>batch_size</code> 为1，看一下输出结果：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms, datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> DataLoader</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">CIFAR10_Network</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(CIFAR10_Network, self).__init__()</span><br><span class=\"line\">        self.model = nn.Sequential(</span><br><span class=\"line\">            nn.Conv2d(in_channels=<span class=\"number\">3</span>, out_channels=<span class=\"number\">32</span>, kernel_size=<span class=\"number\">5</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">2</span>),  <span class=\"comment\"># [32, 32, 32]</span></span><br><span class=\"line\">            nn.MaxPool2d(kernel_size=<span class=\"number\">2</span>),  <span class=\"comment\"># [32, 16, 16]</span></span><br><span class=\"line\">            nn.Conv2d(in_channels=<span class=\"number\">32</span>, out_channels=<span class=\"number\">32</span>, kernel_size=<span class=\"number\">5</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">2</span>),  <span class=\"comment\"># [32, 16, 16]</span></span><br><span class=\"line\">            nn.MaxPool2d(kernel_size=<span class=\"number\">2</span>),  <span class=\"comment\"># [32, 8, 8]</span></span><br><span class=\"line\">            nn.Conv2d(in_channels=<span class=\"number\">32</span>, out_channels=<span class=\"number\">64</span>, kernel_size=<span class=\"number\">5</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">2</span>),  <span class=\"comment\"># [64, 8, 8]</span></span><br><span class=\"line\">            nn.MaxPool2d(kernel_size=<span class=\"number\">2</span>),  <span class=\"comment\"># [64, 4, 4]</span></span><br><span class=\"line\">            nn.Flatten(),  <span class=\"comment\"># [1024]</span></span><br><span class=\"line\">            nn.Linear(in_features=<span class=\"number\">1024</span>, out_features=<span class=\"number\">64</span>),  <span class=\"comment\"># [64]</span></span><br><span class=\"line\">            nn.Linear(in_features=<span class=\"number\">64</span>, out_features=<span class=\"number\">10</span>) <span class=\"comment\"># [10]</span></span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, <span class=\"built_in\">input</span></span>):</span><br><span class=\"line\">        output = self.model(<span class=\"built_in\">input</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br><span class=\"line\"></span><br><span class=\"line\">network = CIFAR10_Network()</span><br><span class=\"line\"></span><br><span class=\"line\">test_data = datasets.CIFAR10(<span class=\"string\">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class=\"literal\">False</span>, transform=transforms.ToTensor())</span><br><span class=\"line\">data_loader = DataLoader(test_data, batch_size=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">loss = nn.CrossEntropyLoss()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> step, data <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(data_loader):</span><br><span class=\"line\">    imgs, targets = data</span><br><span class=\"line\">    output = network(imgs)</span><br><span class=\"line\">    output_loss = loss(output, targets)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(output)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(targets)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(output_loss)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tensor([[ 0.1252, -0.1069, -0.0747,  0.0232,  0.0852,  0.1019,  0.0688, -0.1068,</span></span><br><span class=\"line\"><span class=\"comment\">#           0.0854, -0.0740]], grad_fn=&lt;AddmmBackward0&gt;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tensor([3])</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tensor(2.2960, grad_fn=&lt;NllLossBackward0&gt;)</span></span><br></pre></td></tr></table></figure>\n<p>现在我们来尝试解决第二个问题，即损失函数如何为我们更新输出提供一定的依据（反向传播）。</p>\n<p>例如对于卷积层来说，其中卷积核中的每个参数就是我们需要调整的，每个参数具有一个属性 <code>grad</code> 表示梯度，反向传播时每一个要更新的参数都会求出对应的梯度，在优化的过程中就可以根据这个梯度对参数进行优化，最终达到降低损失函数值的目的。</p>\n<p>PyTorch 中对损失函数计算出的结果使用 <code>backward</code> 函数即可计算出梯度：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms, datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> DataLoader</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">CIFAR10_Network</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(CIFAR10_Network, self).__init__()</span><br><span class=\"line\">        self.model = nn.Sequential(</span><br><span class=\"line\">            <span class=\"comment\"># Layers</span></span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, <span class=\"built_in\">input</span></span>):</span><br><span class=\"line\">        output = self.model(<span class=\"built_in\">input</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br><span class=\"line\"></span><br><span class=\"line\">network = CIFAR10_Network()</span><br><span class=\"line\"></span><br><span class=\"line\">test_data = datasets.CIFAR10(<span class=\"string\">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class=\"literal\">False</span>, transform=transforms.ToTensor())</span><br><span class=\"line\">data_loader = DataLoader(test_data, batch_size=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">loss = nn.CrossEntropyLoss()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> step, data <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(data_loader):</span><br><span class=\"line\">    imgs, targets = data</span><br><span class=\"line\">    output = network(imgs)</span><br><span class=\"line\">    output_loss = loss(output, targets)</span><br><span class=\"line\">    output_loss.backward()  <span class=\"comment\"># 反向传播</span></span><br></pre></td></tr></table></figure>\n<p>我们在计算反向传播之前设置断点，然后可以在 PyCharm 下方的变量区域通过目录 <code>network/model/Protected Attributes/_modules/'0'/weight/grad</code> 查看到某一层参数的梯度，在反向传播之前为 <code>None</code>，执行反向传播的代码后可以看到 <code>grad</code> 处有数值了。</p>\n<p>我们有了各个节点参数的梯度，接下来就可以选用一个合适的优化器，来对这些参数进行优化。</p>\n<h3 id=\"10-3-Optimizer\">10.3 Optimizer</h3>\n<p>优化器 <code>torch.optim</code> 的官方文档：<a href=\"https://pytorch.org/docs/stable/optim.html\">TORCH.OPTIM</a>。</p>\n<p>优化器主要是在模型训练阶段对模型的可学习参数进行更新，常用优化器有：SGD、RMSprop、Adam等。优化器初始化时传入传入模型的可学习参数，以及其他超参数如 <code>lr</code>、<code>momentum</code> 等，例如：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch.optim <span class=\"keyword\">as</span> optim</span><br><span class=\"line\"></span><br><span class=\"line\">optimizer = optim.SGD(model.parameters(), lr=<span class=\"number\">0.01</span>, momentum=<span class=\"number\">0.9</span>)</span><br><span class=\"line\">optimizer = optim.Adam([var1, var2], lr=<span class=\"number\">0.0001</span>)</span><br></pre></td></tr></table></figure>\n<p>在训练过程中先调用 <code>optimizer.zero_grad()</code> 清空梯度，再调用 <code>loss.backward()</code> 反向传播，最后调用 <code>optimizer.step()</code> 更新模型参数，例如：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> step, data <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(data_loader):</span><br><span class=\"line\">    imgs, targets = data</span><br><span class=\"line\">    output = network(imgs)</span><br><span class=\"line\">    loss = loss_function(output, targets)</span><br><span class=\"line\">    optimizer.zero_grad()</span><br><span class=\"line\">    loss.backward()</span><br><span class=\"line\">    optimizer.step()</span><br></pre></td></tr></table></figure>\n<p>接下来我们来训练20轮神经网络，看看损失函数值的变化：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms, datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> DataLoader</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.optim <span class=\"keyword\">as</span> optim</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">CIFAR10_Network</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(CIFAR10_Network, self).__init__()</span><br><span class=\"line\">        self.model = nn.Sequential(</span><br><span class=\"line\">            <span class=\"comment\"># Layers</span></span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, <span class=\"built_in\">input</span></span>):</span><br><span class=\"line\">        output = self.model(<span class=\"built_in\">input</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br><span class=\"line\"></span><br><span class=\"line\">network = CIFAR10_Network()</span><br><span class=\"line\"></span><br><span class=\"line\">test_data = datasets.CIFAR10(<span class=\"string\">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class=\"literal\">False</span>, transform=transforms.ToTensor())</span><br><span class=\"line\">data_loader = DataLoader(test_data, batch_size=<span class=\"number\">64</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">loss_function = nn.CrossEntropyLoss()</span><br><span class=\"line\">optimizer = optim.SGD(network.parameters(), lr=<span class=\"number\">0.01</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">20</span>):  <span class=\"comment\"># 学习20轮</span></span><br><span class=\"line\">    total_loss = <span class=\"number\">0.0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> step, data <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(data_loader):</span><br><span class=\"line\">        imgs, targets = data</span><br><span class=\"line\">        output = network(imgs)</span><br><span class=\"line\">        loss = loss_function(output, targets)</span><br><span class=\"line\">        total_loss += loss</span><br><span class=\"line\">        optimizer.zero_grad()</span><br><span class=\"line\">        loss.backward()</span><br><span class=\"line\">        optimizer.step()</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(total_loss)</span><br></pre></td></tr></table></figure>\n<p>可以看到每一轮所有 <code>batch</code> 的损失函数值的总和确实在不断降低了。</p>\n<h2 id=\"11-现有网络模型的使用及修改\">11. 现有网络模型的使用及修改</h2>\n<h3 id=\"11-1-VGG16模型的使用\">11.1 VGG16模型的使用</h3>\n<p>我们以 VGG16 为例，该网络模型是用于大规模图像识别的超深度卷积神经网络，官方文档：<a href=\"https://pytorch.org/vision/stable/models/generated/torchvision.models.vgg16.html#torchvision.models.vgg16\">VGG16</a>。</p>\n<p>该网络模型主要有以下参数：</p>\n<ul>\n<li><code>weights</code>：可以设置成 <code>torchvision.models.VGG16_Weights.DEFAULT</code>，<code>DEFAULT</code> 表示自动使用最新的数据。老版本为 <code>pretrained</code>，如果为 <code>True</code>，表示使用预先训练好的权重，在官网可以看到这个权重是在 <code>ImageNet-1K</code> 数据集训练的，默认为不使用预先训练好的权重。</li>\n<li><code>progress</code>：如果为 <code>True</code>，则显示下载的进度条，默认为 <code>True</code>。</li>\n</ul>\n<p>注意，下载网络时默认的下载路径是 <code>C:\\Users\\&lt;username&gt;\\.cache</code>，因此在下载模型前，我们需要修改路径：打开 <code>D:\\Anaconda3_Environments\\envs\\PyTorch\\Lib\\site-packages\\torch</code> 中的 <code>hub.py</code> 文件，搜索 <code>load_state_dict_from_url</code>，然后修改 <code>model_dir</code> 即可：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">model_dir: <span class=\"type\">Optional</span>[<span class=\"built_in\">str</span>] = <span class=\"string\">&#x27;D:\\\\Anaconda3_Environments\\\\envs\\\\PyTorch\\\\Torch-model&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>然后我们输出一下这个网络模型：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torchvision</span><br><span class=\"line\"></span><br><span class=\"line\">vgg = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(vgg)</span><br><span class=\"line\"><span class=\"comment\"># VGG(</span></span><br><span class=\"line\"><span class=\"comment\">#   (features): Sequential(</span></span><br><span class=\"line\"><span class=\"comment\">#     (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span></span><br><span class=\"line\"><span class=\"comment\">#     (1): ReLU(inplace=True)</span></span><br><span class=\"line\"><span class=\"comment\">#     (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span></span><br><span class=\"line\"><span class=\"comment\">#     (3): ReLU(inplace=True)</span></span><br><span class=\"line\"><span class=\"comment\">#     (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class=\"line\"><span class=\"comment\">#     ......</span></span><br><span class=\"line\"><span class=\"comment\">#   )</span></span><br><span class=\"line\"><span class=\"comment\">#   (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))</span></span><br><span class=\"line\"><span class=\"comment\">#   (classifier): Sequential(</span></span><br><span class=\"line\"><span class=\"comment\">#     (0): Linear(in_features=25088, out_features=4096, bias=True)</span></span><br><span class=\"line\"><span class=\"comment\">#     (1): ReLU(inplace=True)</span></span><br><span class=\"line\"><span class=\"comment\">#     (2): Dropout(p=0.5, inplace=False)</span></span><br><span class=\"line\"><span class=\"comment\">#     (3): Linear(in_features=4096, out_features=4096, bias=True)</span></span><br><span class=\"line\"><span class=\"comment\">#     (4): ReLU(inplace=True)</span></span><br><span class=\"line\"><span class=\"comment\">#     (5): Dropout(p=0.5, inplace=False)</span></span><br><span class=\"line\"><span class=\"comment\">#     (6): Linear(in_features=4096, out_features=1000, bias=True)</span></span><br><span class=\"line\"><span class=\"comment\">#   )</span></span><br><span class=\"line\"><span class=\"comment\"># )</span></span><br></pre></td></tr></table></figure>\n<p>可以看到这个模型的分类结果为1000类，那么假如我们需要分类 CIFAR10 该如何应用这个网络模型呢？一种方法就是直接将最后一层 <code>Linear</code> 的 <code>out_features</code> 改为10，还有一种方法就是再添加一层 <code>in_features=1000, out_features=10</code> 的 <code>Linear</code>：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms, datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> DataLoader</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.optim <span class=\"keyword\">as</span> optim</span><br><span class=\"line\"></span><br><span class=\"line\">vgg = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)</span><br><span class=\"line\"></span><br><span class=\"line\">vgg.classifier.add_module(<span class=\"string\">&#x27;add_linear&#x27;</span>, nn.Linear(in_features=<span class=\"number\">1000</span>, out_features=<span class=\"number\">10</span>))  <span class=\"comment\"># 在 classifier 中加一层 Linear</span></span><br><span class=\"line\"><span class=\"comment\"># vgg.classifier[6] = nn.Linear(in_features=4096, out_features=10)  # 修改 classifier 的最后一层 Linear</span></span><br><span class=\"line\"></span><br><span class=\"line\">test_data = datasets.CIFAR10(<span class=\"string\">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class=\"literal\">False</span>, transform=transforms.ToTensor())</span><br><span class=\"line\">data_loader = DataLoader(test_data, batch_size=<span class=\"number\">64</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">loss_function = nn.CrossEntropyLoss()</span><br><span class=\"line\">optimizer = optim.SGD(vgg.parameters(), lr=<span class=\"number\">0.01</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">20</span>):</span><br><span class=\"line\">    total_loss = <span class=\"number\">0.0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> step, data <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(data_loader):</span><br><span class=\"line\">        imgs, targets = data</span><br><span class=\"line\">        output = vgg(imgs)</span><br><span class=\"line\">        loss = loss_function(output, targets)</span><br><span class=\"line\">        total_loss += loss</span><br><span class=\"line\">        optimizer.zero_grad()</span><br><span class=\"line\">        loss.backward()</span><br><span class=\"line\">        optimizer.step()</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(total_loss)</span><br></pre></td></tr></table></figure>\n<p>可以看到效果是比之前自己构建的网络模型好很多的。</p>\n<h3 id=\"11-2-模型的保存与读取\">11.2 模型的保存与读取</h3>\n<p>我们在对某些模型进行修改后可能想将其保存下来，方便以后用到时无需再构建一遍网络，可以按以下的方式将整个模型保存到路径 <code>models/CIFAR10_VGG16.pth</code>：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torchvision</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\">model = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)</span><br><span class=\"line\">model.classifier.add_module(<span class=\"string\">&#x27;add_linear&#x27;</span>, nn.Linear(in_features=<span class=\"number\">1000</span>, out_features=<span class=\"number\">10</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">torch.save(model, <span class=\"string\">&#x27;models/CIFAR10_VGG16.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>其对应的加载模型的方式为：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">model = torch.load(<span class=\"string\">&#x27;models/CIFAR10_VGG16.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>还有一种保存方式是将模型中的参数保存成字典的形式，官方建议使用该方式：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.save(model.state_dict(), <span class=\"string\">&#x27;models/CIFAR10_VGG16_STATE.pkl&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>其对应的加载模型的方式为：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">model = torchvision.models.vgg16()</span><br><span class=\"line\">model.load_state_dict(torch.load(<span class=\"string\">&#x27;models/CIFAR10_VGG16_STATE.pkl&#x27;</span>))</span><br></pre></td></tr></table></figure>\n<p>注意如果是保存自己构建的网络模型，需要在模型的类的源代码中将该类导入进来，例如在 <code>test_save.py</code> 中用以下代码保存自己的网络：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MyNetwork</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(MyNetwork, self).__init__()</span><br><span class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, <span class=\"built_in\">input</span></span>):</span><br><span class=\"line\">        output = self.conv1(<span class=\"built_in\">input</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br><span class=\"line\"></span><br><span class=\"line\">model = MyNetwork()</span><br><span class=\"line\">torch.save(model, <span class=\"string\">&#x27;models/My_Network.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>在 <code>test_load.py</code> 中导入时需要这样写：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> test_save <span class=\"keyword\">import</span> MyNetwork</span><br><span class=\"line\"></span><br><span class=\"line\">model = torch.load(<span class=\"string\">&#x27;models/My_Network.pth&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(model)</span><br></pre></td></tr></table></figure>\n<h2 id=\"12-完整训练模型的方法\">12. 完整训练模型的方法</h2>\n<h3 id=\"12-1-训练模型时的注意事项\">12.1 训练模型时的注意事项</h3>\n<p>（1）通常我们会将超参数的设置放在一起，使代码更加直观且方便修改：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">BATCH_SIZE = <span class=\"number\">64</span></span><br><span class=\"line\">LEARNING_RATE = <span class=\"number\">0.01</span></span><br><span class=\"line\">EPOCH = <span class=\"number\">10</span></span><br></pre></td></tr></table></figure>\n<p>（2）我们在每一轮 epoch 中会先对训练集进行训练，然后使用测试集进行正确率的测试，因此一般我们会记录总共训练的次数 <code>total_train_step</code> 以及总共测试的次数 <code>total_test_step</code>，方便后续绘图使用。</p>\n<p>（3）在开始训练之前一般需要将模型设置成训练状态，在测试之前需要设置成评估状态，这两种状态会影响少部分的层例如 <code>Dropout</code> 和 <code>BatchNorm</code>：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">model.train()</span><br><span class=\"line\">    <span class=\"comment\"># training</span></span><br><span class=\"line\"></span><br><span class=\"line\">model.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\">    <span class=\"comment\"># evaluation</span></span><br></pre></td></tr></table></figure>\n<p>（4）在分类问题中计算准确率一般用以下方法：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\">a = torch.tensor([</span><br><span class=\"line\">    [<span class=\"number\">0.3</span>, <span class=\"number\">0.7</span>],</span><br><span class=\"line\">    [<span class=\"number\">0.6</span>, <span class=\"number\">0.4</span>]</span><br><span class=\"line\">])  <span class=\"comment\"># 假设两个物体二分类的结果</span></span><br><span class=\"line\"></span><br><span class=\"line\">b = torch.tensor([<span class=\"number\">0</span>, <span class=\"number\">0</span>])  <span class=\"comment\"># 正确的标签</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(a.argmax(dim=<span class=\"number\">1</span>)) <span class=\"comment\"># tensor([1, 0])，在第1维上取最大值，即对每一行求最大值，将最大值作为分类结果</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(a.argmax(dim=<span class=\"number\">1</span>) == b)  <span class=\"comment\"># tensor([False,  True])，与标签进行比较，第一个物体的结果与标签不符，第二个和标签相符</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>((a.argmax(dim=<span class=\"number\">1</span>) == b).<span class=\"built_in\">sum</span>())  <span class=\"comment\"># tensor(1)，将所有物体与标签的比较结果求和就是 True 的数量，也就是预测正确的数量</span></span><br></pre></td></tr></table></figure>\n<p>（5）测试时不能对模型进行任何干扰，即在测试的时候神经网络不能产生梯度，因此在每次测试前需要加上以下代码：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">    <span class=\"comment\"># evaluation</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"12-2-使用GPU进行训练\">12.2 使用GPU进行训练</h3>\n<p>前提：电脑有 NVIDIA 显卡，配置好了 CUDA，可以使用 <code>torch.cuda.is_available()</code> 来检查 CUDA 是否可用。</p>\n<p>使用 GPU 训练的时候，需要将 Module 对象和 Tensor 类型的数据转移到 GPU 上进行计算，一般来说即为将网络模型、数据、损失函数放到 GPU 上计算。</p>\n<p>使用 GPU 训练的方式有两种，第一种是使用 <code>cuda()</code> 函数，例如：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 网络模型</span></span><br><span class=\"line\">model = MyNetwork()</span><br><span class=\"line\">model = model.cuda()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 损失函数</span></span><br><span class=\"line\">loss_function = nn.CrossEntropyLoss()</span><br><span class=\"line\">loss_function = loss_function.cuda()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 数据</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> step, data <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(data_loader):</span><br><span class=\"line\">    imgs, targets = data</span><br><span class=\"line\">    imgs = imgs.cuda()</span><br><span class=\"line\">    targets = targets.cuda()</span><br></pre></td></tr></table></figure>\n<p>另一种是使用 <code>to(device)</code>，<code>device</code> 就是我们选择用来训练模型的设备，该方式与 <code>cuda()</code> 有一点细微的差别如下：</p>\n<ul>\n<li>对于 Tensor 类型的数据（图像、标签等），使用 <code>to(device)</code> 之后，需要接收返回值，返回值才是正确设置了 <code>device</code> 的 Tensor。</li>\n<li>对于 Module 对象（网络模型、损失函数），只用调用 <code>to(device)</code> 就可以将模型设置为指定的 <code>device</code>，不必接收返回值，当然接收返回值也是可以的。</li>\n</ul>\n<p>例如：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">device = torch.device(<span class=\"string\">&#x27;cuda&#x27;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&#x27;cpu&#x27;</span>)  <span class=\"comment\"># &#x27;cuda:0&#x27; 表示第 0 号 GPU</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 网络模型</span></span><br><span class=\"line\">model = MyNetwork()</span><br><span class=\"line\">model.to(device)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 损失函数</span></span><br><span class=\"line\">loss_function = nn.CrossEntropyLoss()</span><br><span class=\"line\">loss_function.to(device)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 数据</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> step, data <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(data_loader):</span><br><span class=\"line\">    imgs, targets = data</span><br><span class=\"line\">    imgs = imgs.to(device)</span><br><span class=\"line\">    targets = targets.to(device)</span><br></pre></td></tr></table></figure>\n<p>注意如果加载在 GPU 上训练好的模型，然后想在 CPU 上使用，需要映射回 CPU：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">model = torch.load(<span class=\"string\">&#x27;models/AFTER_TRAININGS_MODEL.pth&#x27;</span>, map_location=torch.device(<span class=\"string\">&#x27;cpu&#x27;</span>))</span><br></pre></td></tr></table></figure>\n<h3 id=\"12-3-CIFAR10-Net-Simple-v3\">12.3 CIFAR10_Net_Simple_v3</h3>\n<p>最后放上经过自己调参达到88%左右的正确率的模型和训练代码吧：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">CIFAR10_Net_Simple_v3</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(CIFAR10_Net_Simple_v3, self).__init__()</span><br><span class=\"line\">        self.model = nn.Sequential(</span><br><span class=\"line\">            nn.Conv2d(in_channels=<span class=\"number\">3</span>, out_channels=<span class=\"number\">32</span>, kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">1</span>),  <span class=\"comment\"># [32, 32, 32]</span></span><br><span class=\"line\">            nn.ReLU(inplace=<span class=\"literal\">True</span>),</span><br><span class=\"line\">            nn.BatchNorm2d(<span class=\"number\">32</span>),</span><br><span class=\"line\">            nn.Conv2d(in_channels=<span class=\"number\">32</span>, out_channels=<span class=\"number\">32</span>, kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">1</span>),  <span class=\"comment\"># [32, 32, 32]</span></span><br><span class=\"line\">            nn.ReLU(inplace=<span class=\"literal\">True</span>),</span><br><span class=\"line\">            nn.BatchNorm2d(<span class=\"number\">32</span>),</span><br><span class=\"line\">            nn.MaxPool2d(kernel_size=<span class=\"number\">2</span>),  <span class=\"comment\"># [32, 16, 16]</span></span><br><span class=\"line\"></span><br><span class=\"line\">            nn.Conv2d(in_channels=<span class=\"number\">32</span>, out_channels=<span class=\"number\">64</span>, kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">1</span>),  <span class=\"comment\"># [64, 16, 16]</span></span><br><span class=\"line\">            nn.ReLU(inplace=<span class=\"literal\">True</span>),</span><br><span class=\"line\">            nn.BatchNorm2d(<span class=\"number\">64</span>),</span><br><span class=\"line\">            nn.Conv2d(in_channels=<span class=\"number\">64</span>, out_channels=<span class=\"number\">64</span>, kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">1</span>),  <span class=\"comment\"># [64, 16, 16]</span></span><br><span class=\"line\">            nn.ReLU(inplace=<span class=\"literal\">True</span>),</span><br><span class=\"line\">            nn.BatchNorm2d(<span class=\"number\">64</span>),</span><br><span class=\"line\">            nn.MaxPool2d(kernel_size=<span class=\"number\">2</span>),  <span class=\"comment\"># [64, 8, 8]</span></span><br><span class=\"line\"></span><br><span class=\"line\">            nn.Conv2d(in_channels=<span class=\"number\">64</span>, out_channels=<span class=\"number\">128</span>, kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">1</span>),  <span class=\"comment\"># [128, 16, 16]</span></span><br><span class=\"line\">            nn.ReLU(inplace=<span class=\"literal\">True</span>),</span><br><span class=\"line\">            nn.BatchNorm2d(<span class=\"number\">128</span>),</span><br><span class=\"line\">            nn.Conv2d(in_channels=<span class=\"number\">128</span>, out_channels=<span class=\"number\">128</span>, kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">1</span>),  <span class=\"comment\"># [128, 16, 16]</span></span><br><span class=\"line\">            nn.ReLU(inplace=<span class=\"literal\">True</span>),</span><br><span class=\"line\">            nn.BatchNorm2d(<span class=\"number\">128</span>),</span><br><span class=\"line\">            nn.MaxPool2d(kernel_size=<span class=\"number\">2</span>),  <span class=\"comment\"># [128, 4, 4]</span></span><br><span class=\"line\"></span><br><span class=\"line\">            nn.Flatten(),  <span class=\"comment\"># [2048]</span></span><br><span class=\"line\">            nn.Dropout(p=<span class=\"number\">0.4</span>, inplace=<span class=\"literal\">False</span>),</span><br><span class=\"line\"></span><br><span class=\"line\">            nn.Linear(in_features=<span class=\"number\">2048</span>, out_features=<span class=\"number\">64</span>),  <span class=\"comment\"># [64]</span></span><br><span class=\"line\">            nn.ReLU(inplace=<span class=\"literal\">True</span>),</span><br><span class=\"line\">            nn.Dropout(p=<span class=\"number\">0.4</span>, inplace=<span class=\"literal\">False</span>),</span><br><span class=\"line\"></span><br><span class=\"line\">            nn.Linear(in_features=<span class=\"number\">64</span>, out_features=<span class=\"number\">10</span>) <span class=\"comment\"># [10]</span></span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, <span class=\"built_in\">input</span></span>):</span><br><span class=\"line\">        output = self.model(<span class=\"built_in\">input</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># model = CIFAR10_Net_Simple_v3()</span></span><br><span class=\"line\"><span class=\"comment\"># torch.save(model, &#x27;../models/CIFAR10_Net_Simple_v3.pth&#x27;)</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms, datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> DataLoader</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data.dataset <span class=\"keyword\">import</span> ConcatDataset</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.tensorboard <span class=\"keyword\">import</span> SummaryWriter</span><br><span class=\"line\"><span class=\"keyword\">from</span> util.CIFAR10_Net_Simple_v3 <span class=\"keyword\">import</span> *</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.optim <span class=\"keyword\">as</span> optim</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 超参数</span></span><br><span class=\"line\">BATCH_SIZE = <span class=\"number\">32</span></span><br><span class=\"line\">LEARNING_RATE = <span class=\"number\">0.01</span></span><br><span class=\"line\">EPOCH = <span class=\"number\">150</span></span><br><span class=\"line\">SHOW_INFO_STEP = <span class=\"number\">200</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 训练设备</span></span><br><span class=\"line\">device = torch.device(<span class=\"string\">&#x27;cuda&#x27;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&#x27;cpu&#x27;</span>)  <span class=\"comment\"># &#x27;cuda:0&#x27; 表示第 0 号 GPU</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 数据增强</span></span><br><span class=\"line\">trans = transforms.Compose([</span><br><span class=\"line\">    transforms.RandomCrop(<span class=\"number\">32</span>, padding=[<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>]),</span><br><span class=\"line\">    transforms.RandomHorizontalFlip(p=<span class=\"number\">0.5</span>),</span><br><span class=\"line\">    transforms.ToTensor()</span><br><span class=\"line\">])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 数据集</span></span><br><span class=\"line\">train_data = datasets.CIFAR10(<span class=\"string\">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class=\"literal\">True</span>, transform=trans)</span><br><span class=\"line\">test_data = datasets.CIFAR10(<span class=\"string\">&#x27;dataset/CIFAR10&#x27;</span>, train=<span class=\"literal\">False</span>, transform=transforms.ToTensor())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 扩充训练集</span></span><br><span class=\"line\"><span class=\"comment\"># trans_train_data = datasets.CIFAR10(&#x27;dataset/CIFAR10&#x27;, train=True, transform=trans)</span></span><br><span class=\"line\"><span class=\"comment\"># train_data = ConcatDataset([train_data, trans_train_data])</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载数据</span></span><br><span class=\"line\">train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=<span class=\"literal\">True</span>)</span><br><span class=\"line\">test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE)</span><br><span class=\"line\"></span><br><span class=\"line\">train_data_len = <span class=\"built_in\">len</span>(train_data)</span><br><span class=\"line\">test_data_len = <span class=\"built_in\">len</span>(test_data)</span><br><span class=\"line\"></span><br><span class=\"line\">model = torch.load(<span class=\"string\">&#x27;models/CIFAR10_Net_Simple_v3.pth&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">loss_function = nn.CrossEntropyLoss()</span><br><span class=\"line\"></span><br><span class=\"line\">optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=<span class=\"number\">0.9</span>)</span><br><span class=\"line\">scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[<span class=\"number\">8</span>, <span class=\"number\">16</span>, <span class=\"number\">24</span>, <span class=\"number\">32</span>], gamma=<span class=\"number\">0.5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">writer = SummaryWriter(<span class=\"string\">&#x27;logs/CIFAR10_Net_Simple_v3_Aug_Mom_logs&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">model.to(device)</span><br><span class=\"line\">loss_function.to(device)</span><br><span class=\"line\"></span><br><span class=\"line\">total_train_step = <span class=\"number\">0</span></span><br><span class=\"line\">total_test_step = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(EPOCH):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;---------- The &#123;&#125; epoch of training begins ----------&#x27;</span>.<span class=\"built_in\">format</span>(epoch))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Learning rate: &#123;&#125;&#x27;</span>.<span class=\"built_in\">format</span>(optimizer.state_dict()[<span class=\"string\">&#x27;param_groups&#x27;</span>][<span class=\"number\">0</span>][<span class=\"string\">&#x27;lr&#x27;</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\">    train_loss = <span class=\"number\">0.0</span></span><br><span class=\"line\">    train_acc = <span class=\"number\">0.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    model.train()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> step, data <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(train_dataloader):</span><br><span class=\"line\">        imgs, targets = data</span><br><span class=\"line\">        imgs = imgs.to(device)</span><br><span class=\"line\">        targets = targets.to(device)</span><br><span class=\"line\">        output = model(imgs)</span><br><span class=\"line\"></span><br><span class=\"line\">        acc = (output.argmax(dim=<span class=\"number\">1</span>)==targets).<span class=\"built_in\">float</span>().<span class=\"built_in\">sum</span>()</span><br><span class=\"line\">        loss = loss_function(output, targets)</span><br><span class=\"line\">        train_loss += loss.item()</span><br><span class=\"line\">        train_acc += acc</span><br><span class=\"line\"></span><br><span class=\"line\">        optimizer.zero_grad()</span><br><span class=\"line\">        loss.backward()</span><br><span class=\"line\">        optimizer.step()</span><br><span class=\"line\"></span><br><span class=\"line\">        total_train_step += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> total_train_step % SHOW_INFO_STEP == <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;The number of training: &#123;&#125;, Loss: &#123;&#125;&#x27;</span>.<span class=\"built_in\">format</span>(total_train_step, loss.item()))</span><br><span class=\"line\"></span><br><span class=\"line\">    train_acc /= train_data_len</span><br><span class=\"line\"></span><br><span class=\"line\">    writer.add_scalar(<span class=\"string\">&#x27;train_loss&#x27;</span>, train_loss, epoch)</span><br><span class=\"line\">    writer.add_scalar(<span class=\"string\">&#x27;train_acc&#x27;</span>, train_acc, epoch)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;The number of epoch: &#123;&#125;, train_loss: &#123;&#125;&#x27;</span>.<span class=\"built_in\">format</span>(epoch, train_loss))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;The number of epoch: &#123;&#125;, train_acc: &#123;&#125;&#x27;</span>.<span class=\"built_in\">format</span>(epoch, train_acc))</span><br><span class=\"line\"></span><br><span class=\"line\">    test_loss = <span class=\"number\">0.0</span></span><br><span class=\"line\">    test_acc = <span class=\"number\">0.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    model.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">        <span class=\"keyword\">for</span> step, data <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(test_dataloader):</span><br><span class=\"line\">            imgs, targets = data</span><br><span class=\"line\">            imgs = imgs.to(device)</span><br><span class=\"line\">            targets = targets.to(device)</span><br><span class=\"line\">            output = model(imgs)</span><br><span class=\"line\"></span><br><span class=\"line\">            acc = (output.argmax(dim=<span class=\"number\">1</span>) == targets).<span class=\"built_in\">float</span>().<span class=\"built_in\">sum</span>()</span><br><span class=\"line\">            loss = loss_function(output, targets)</span><br><span class=\"line\">            test_loss += loss.item()</span><br><span class=\"line\">            test_acc += acc</span><br><span class=\"line\"></span><br><span class=\"line\">            total_test_step += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">        test_acc /= test_data_len</span><br><span class=\"line\"></span><br><span class=\"line\">        writer.add_scalar(<span class=\"string\">&#x27;test_loss&#x27;</span>, test_loss, epoch)</span><br><span class=\"line\">        writer.add_scalar(<span class=\"string\">&#x27;test_acc&#x27;</span>, test_acc, epoch)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;The number of epoch: &#123;&#125;, test_loss: &#123;&#125;&#x27;</span>.<span class=\"built_in\">format</span>(epoch, test_loss))</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;The number of epoch: &#123;&#125;, test_acc: &#123;&#125;&#x27;</span>.<span class=\"built_in\">format</span>(epoch, test_acc))</span><br><span class=\"line\"></span><br><span class=\"line\">        writer.add_scalar(<span class=\"string\">&#x27;learning_rate&#x27;</span>, optimizer.state_dict()[<span class=\"string\">&#x27;param_groups&#x27;</span>][<span class=\"number\">0</span>][<span class=\"string\">&#x27;lr&#x27;</span>], epoch)</span><br><span class=\"line\">        scheduler.step()</span><br><span class=\"line\"></span><br><span class=\"line\">torch.save(model, <span class=\"string\">&#x27;models/CIFAR10_Net_Simple_v3_Aug_Mom_150TRAININGS.pth&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># torch.save(model.state_dict(), &#x27;models/CIFAR10_Net_Simple_v3_Aug_Mom_STATE.pkl&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\">writer.close()</span><br></pre></td></tr></table></figure>\n<p>至此已经成功入门 PyTorch 啦！可以正式进入 Deep Learning 的学习啦！</p>\n",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/9012.html",
            "url": "https://asanosaki.github.io/posts/9012.html",
            "title": "Kratos-Rebirth主题修改部分细节教程",
            "date_published": "2022-12-01T04:30:00.000Z",
            "content_html": "<blockquote>\n<p>记录一下自己在使用 Kratos-Rebirth 过程中的一些样式微调。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-部分文本的修改\">1. 部分文本的修改</h2>\n<h3 id=\"1-1-主页标题\">1.1 主页标题</h3>\n<p>在主题目录中的 <code>source/css/kratosr.min.css</code> 文件（之后也是在这个文件）中找到 <code>.kratos-cover .desc h2</code>，改成以下内容：</p>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"selector-class\">.kratos-cover</span> <span class=\"selector-class\">.desc</span> <span class=\"selector-tag\">h2</span>&#123;<span class=\"attribute\">color</span>:<span class=\"number\">#fff</span>;<span class=\"attribute\">display</span>:inline-block;<span class=\"attribute\">font-size</span>:<span class=\"number\">80px</span>;<span class=\"attribute\">font-family</span>:Garamond;<span class=\"attribute\">font-weight</span>:<span class=\"number\">800</span>;<span class=\"attribute\">text-shadow</span>: <span class=\"number\">0</span> <span class=\"number\">0</span> <span class=\"number\">0.5em</span> <span class=\"number\">#fc149b</span>, <span class=\"number\">0</span> <span class=\"number\">0</span> <span class=\"number\">0.2em</span> <span class=\"number\">#5c5c5c</span>;<span class=\"attribute\">margin-bottom</span>:<span class=\"number\">10px</span>;<span class=\"attribute\">text-transform</span>:none&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-2-主页副标题\">1.2 主页副标题</h3>\n<p>找到 <code>kratos-cover .desc p,.kratos-cover .desc span</code>，改成以下内容：</p>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kratos-cover <span class=\"selector-class\">.desc</span> <span class=\"selector-tag\">p</span>,<span class=\"selector-class\">.kratos-cover</span> <span class=\"selector-class\">.desc</span> <span class=\"selector-tag\">span</span>&#123;<span class=\"attribute\">color</span>:<span class=\"number\">#fff</span>;<span class=\"attribute\">display</span>:inline-block;<span class=\"attribute\">font-size</span>:<span class=\"number\">36px</span>;<span class=\"attribute\">font-family</span>:Consolas;<span class=\"attribute\">text-shadow</span>: <span class=\"number\">0</span> <span class=\"number\">0</span> <span class=\"number\">0.5em</span> <span class=\"number\">#ff3c00</span>, <span class=\"number\">0</span> <span class=\"number\">0</span> <span class=\"number\">0.2em</span> <span class=\"number\">#5c5c5c</span>;<span class=\"attribute\">letter-spacing</span>:<span class=\"number\">1px</span>;<span class=\"attribute\">margin-bottom</span>:<span class=\"number\">30px</span>&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-3-主页文章标题\">1.3 主页文章标题</h3>\n<p>找到 <code>.kratos-entry-header a,.kratos-entry-header span</code>，改成以下内容：</p>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"selector-class\">.kratos-entry-header</span> <span class=\"selector-tag\">a</span>,<span class=\"selector-class\">.kratos-entry-header</span> <span class=\"selector-tag\">span</span>&#123;<span class=\"attribute\">color</span>:<span class=\"number\">#000</span>;<span class=\"attribute\">font-family</span>:KaiTi;&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-4-进入文章页面时的标题\">1.4 进入文章页面时的标题</h3>\n<p>找到 <code>.kratos-entry-title</code>，改成以下内容：</p>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"selector-class\">.kratos-entry-title</span>&#123;<span class=\"attribute\">font-size</span>:<span class=\"number\">30px</span>;<span class=\"attribute\">margin</span>:<span class=\"number\">0</span> <span class=\"number\">0</span> <span class=\"number\">15px</span>;<span class=\"attribute\">font-family</span>:KaiTi;&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-主页主体部分宽度\">2. 主页主体部分宽度</h2>\n<p>找到 <code>@media (min-width:1200px)</code>，其后的 <code>.container&#123;width:xx&#125;</code> 即为主体部分宽度，改成以下内容：</p>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">@media</span> (<span class=\"attribute\">min-width</span>:<span class=\"number\">1200px</span>)&#123;<span class=\"selector-class\">.container</span>&#123;<span class=\"attribute\">width</span>:<span class=\"number\">1340px</span>&#125;<span class=\"selector-class\">.visible-lg</span>&#123;<span class=\"attribute\">display</span>:block<span class=\"meta\">!important</span>&#125;<span class=\"selector-class\">.hidden-lg</span>&#123;<span class=\"attribute\">display</span>:none<span class=\"meta\">!important</span>&#125;&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-代码块\">3. 代码块</h2>\n<p>在主题目录中的 <code>source/css/highlight/light.min.css</code> 文件中找到 <code>figure.highlight&#123;</code>，将代码块顶部横条部分背景改成以下内容：</p>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">background</span>:<span class=\"number\">#ececec</span>;</span><br></pre></td></tr></table></figure>\n<p>找到 <code>figure.highlight .code&#123;</code>，将代码块内容部分背景改成以下内容：</p>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">background-color</span>:<span class=\"number\">#f3f3f3</span>;</span><br></pre></td></tr></table></figure>\n<p>找到 <code>figure.highlight .gutter pre</code>，将代码块左侧代码行部分背景改成以下内容：</p>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">background-color</span>:<span class=\"number\">#ecf0f1</span>;</span><br></pre></td></tr></table></figure>\n",
            "tags": [
                "Hexo"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/15428.html",
            "url": "https://asanosaki.github.io/posts/15428.html",
            "title": "Anaconda与PyTorch安装教程",
            "date_published": "2022-11-25T13:56:00.000Z",
            "content_html": "<blockquote>\n<p>搭建 PyTorch 环境属实不容易，折腾了一整天，记录一下踩雷后的总结吧。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-Anaconda的安装与命令介绍\">1. Anaconda的安装与命令介绍</h2>\n<h3 id=\"1-1-安装Anaconda\">1.1 安装Anaconda</h3>\n<p>首先前往 Anaconda 官网：<a href=\"https://www.anaconda.com/\">Anaconda</a>，下载安装文件。本文下载的为 Windows Python3.9 版本。</p>\n<p>安装时没有需要特别注意的，设置好相应的安装路径即可，本文安装路径为：<code>D:\\Anaconda3</code>。</p>\n<p>安装好后打开开始菜单能看到启动项：<code>Anaconda Prompt</code>，打开后如果看到命令行最左侧有 <code>(base)</code> 标识说明安装成功，可以查看版本号：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda -V</span><br></pre></td></tr></table></figure>\n<p>在 Anaconda 中我们会创建很多环境，那么我们需要先设置环境创建的路径，默认是在 <code>C:\\Users\\XXX\\.conda</code> 下的。</p>\n<p>先在想要存放的地方创建文件夹 <code>Anaconda3_Environments</code>，本文创建在 D 盘，然后在该文件夹中再创两个文件夹：<code>D:\\Anaconda3_Environments\\envs</code> 和 <code>D:\\Anaconda3_Environments\\pkgs</code>。</p>\n<p>在开始菜单打开 Anaconda Navigator，点击左上角的 <code>File-Preferences-Configure Conda</code>，修改为以下信息，然后保存即可：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">channels:</span><br><span class=\"line\"> - defaults</span><br><span class=\"line\">envs_dirs:</span><br><span class=\"line\"> - D:\\Anaconda3_Environments\\envs</span><br><span class=\"line\">pkgs_dirs:</span><br><span class=\"line\"> - D:\\Anaconda3_Environments\\pkgs</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-2-Anaconda常用命令\">1.2 Anaconda常用命令</h3>\n<p>创建名为 PyTorch 的环境，Python 版本为3.9：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create -n PyTorch python=3.9</span><br></pre></td></tr></table></figure>\n<p>删除名为 PyTorch 的环境（注意删除环境时要在 base 环境下，别在要删除的环境下）：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda remove -n PyTorch --all</span><br></pre></td></tr></table></figure>\n<p>查看当前的所有环境：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda env list</span><br><span class=\"line\">conda info --envs  # 另一种方式</span><br></pre></td></tr></table></figure>\n<p>激活 PyTorch 环境：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda activate PyTorch</span><br></pre></td></tr></table></figure>\n<p>退出环境：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda deactivate</span><br></pre></td></tr></table></figure>\n<p>更新 conda 及 Anaconda：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda update conda</span><br><span class=\"line\">conda update Anaconda</span><br></pre></td></tr></table></figure>\n<p>如果下载速度很慢，例如下载 PyTorch 时，可以先按以下命令的方式修改镜像源：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br></pre></td></tr></table></figure>\n<p>查看相关镜像源：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda config --show</span><br></pre></td></tr></table></figure>\n<p>将镜像源恢复成默认设置：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda config --remove-key channels</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-PyTorch的安装与配置\">2. PyTorch的安装与配置</h2>\n<h3 id=\"2-1-安装PyTorch\">2.1 安装PyTorch</h3>\n<p>首先查看本机的 CUDA 版本，CUDA Version 即为版本号，本文的版本号为11.6：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nvidia-smi</span><br></pre></td></tr></table></figure>\n<p>前往 PyTorch 官网：<a href=\"https://pytorch.org/\">PyTorch</a>，在 Get Started 中选择好相应的选项：Stable、Windows、Conda、Python、CUDA 11.6，然后会生成一条安装命令（注意如果想在自己电脑上跑通代码，就选 CUDA，如果不需要在自己电脑上跑，而是在服务器上跑，或者没有独立显卡，就选 CPU。独立显卡需要 NVIDIA 显卡。这里我们一定要选择和自己版本相同或更低的 CUDA）。</p>\n<p>本文使用离线与在线相结合的方式进行安装，也可以直接使用官网的命令安装但是速度很慢，或者修改镜像源后再安装。</p>\n<p>前往清华大学镜像源：<a href=\"https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/win-64/\">清华大学 PyTorch 镜像源</a>，手动下载 <code>pytorch</code>、<code>torchvision</code> 以及 <code>torchaudio</code>。注意版本号要对应，本文下载的为：<code>pytorch-1.13.0-py3.9_cuda11.6_cudnn8_0.tar.bz2</code>、<code>torchvision-0.14.0-py39_cu116.tar.bz2</code>、<code>torchaudio-0.13.0-py39_cu116.tar.bz2</code>。<code>py</code> 和 <code>cu</code> 后面的数字分别表示 Python 和 CUDA 的版本号，找到对应版本进行下载即可。</p>\n<p>下载好后进入 Anaconda 环境进行离线安装，本文下载路径为 D 盘根目录，在 PyTorch 环境中安装：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda activate PyTorch</span><br><span class=\"line\">conda install --offline D:/pytorch-1.13.0-py3.9_cuda11.6_cudnn8_0.tar.bz2</span><br><span class=\"line\">conda install --offline D:/torchvision-0.14.0-py39_cu116.tar.bz2</span><br><span class=\"line\">conda install --offline D:/torchaudio-0.13.0-py39_cu116.tar.bz2</span><br></pre></td></tr></table></figure>\n<p>前往<a href=\"https://developer.nvidia.com/cuda-toolkit-archive\"> CUDA Toolkit Archive </a>下载对应版本的 CUDA 套件（注意如果电脑是 Win10，Version 需要选10）。</p>\n<p>下载好后打开程序安装 <code>NVIDIA GPU Computing Toolkit</code>，安装时路径需要使用默认的，即 <code>C:\\Program Files\\NVIDIA GPU Computing Toolkit</code> 以及 <code>C:\\Program Files\\NVIDIA Corporation</code>。</p>\n<p>安装好后打开命令行检查版本：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nvcc -V</span><br></pre></td></tr></table></figure>\n<p>进入 PyTorch 环境，安装剩余的包，此处还需要等待一段时间，但是最大的包已经离线安装了所以会快很多：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install pytorch torchvision torchaudio pytorch-cuda=11.6 -c pytorch -c nvidia</span><br></pre></td></tr></table></figure>\n<p>打开 Python，输入以下内容进行测试，没有报错即安装成功：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>torch.cuda</span><br><span class=\"line\">&lt;module <span class=\"string\">&#x27;torch.cuda&#x27;</span> <span class=\"keyword\">from</span> <span class=\"string\">&#x27;D:\\\\Anaconda3_Environments\\\\envs\\\\PyTorch\\\\lib\\\\site-packages\\\\torch\\\\cuda\\\\__init__.py&#x27;</span>&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>torch.cuda.is_available()</span><br><span class=\"line\"><span class=\"literal\">True</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">from</span> __future__ <span class=\"keyword\">import</span> print_function</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>x = torch.rand(<span class=\"number\">5</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">print</span>(x)</span><br><span class=\"line\">tensor([[<span class=\"number\">0.4175</span>, <span class=\"number\">0.7341</span>, <span class=\"number\">0.7712</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.3714</span>, <span class=\"number\">0.4031</span>, <span class=\"number\">0.8727</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.3453</span>, <span class=\"number\">0.1515</span>, <span class=\"number\">0.6602</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.6127</span>, <span class=\"number\">0.2680</span>, <span class=\"number\">0.4209</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.0118</span>, <span class=\"number\">0.3593</span>, <span class=\"number\">0.7251</span>]])</span><br></pre></td></tr></table></figure>\n<p>由于安装好 Anaconda 后顺带装了 Jupyter，但是他默认是装在 base 环境中的，因此我们还需要进入 PyTorch 环境中安装相应的包：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install nb_conda_kernels</span><br></pre></td></tr></table></figure>\n<p>安装好后输入以下命令打开 Jupyter：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jupyter notebook</span><br></pre></td></tr></table></figure>\n<p>如果在 D 盘启动需要加上路径：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jupyter notebook D:</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-2-PyCharm配置PyTorch\">2.2 PyCharm配置PyTorch</h3>\n<p>在 PyCharm 中设置 Python 解释器，在 Conda 环境中选择现有环境，解释器选择：<code>D:\\Anaconda3_Environments\\envs\\PyTorch\\python.exe</code>，Conda 可执行文件选择：<code>D:\\Anaconda3\\Scripts\\conda.exe</code>。</p>\n<p>设置好后即可在 Python 解释器选择菜单中找到 <code>Python 3.9 (PyTorch)</code> 选项，应用该环境后可以在底部导航栏打开 Python 控制台，用之前测试过的代码进行测试：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>torch.cuda.is_available()</span><br><span class=\"line\"><span class=\"literal\">True</span></span><br></pre></td></tr></table></figure>\n<p>接下来需要修改 PyCharm 的终端，使其打开不是 Windows 默认的终端而是 Anaconda 的终端。先找到开始菜单中 Anaconda Prompt 的文件路径，然后查看属性，目标中有一段内容为：<code>%windir%\\System32\\cmd.exe &quot;/K&quot; D:\\Anaconda3\\Scripts\\activate.bat D:\\Anaconda3</code>。</p>\n<p>将目标中的路径从 <code>cmd.exe</code> 开始之后的内容复制下来，进入 PyCharm，在文件-设置-工具-终端中修改 Shell 路径：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cmd.exe &quot;/K&quot; D:\\Anaconda3\\Scripts\\activate.bat D:\\Anaconda3</span><br></pre></td></tr></table></figure>\n<p>然后打开终端即可找到熟悉的感觉，即出现了 <code>(base)</code>。</p>\n",
            "tags": [
                "Others"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/54431.html",
            "url": "https://asanosaki.github.io/posts/54431.html",
            "title": "计算机网络面试知识点总结",
            "date_published": "2022-11-24T03:14:00.000Z",
            "content_html": "<blockquote>\n<p>计算机网络常见面试题总结，文章将不断更新。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-概述\">1. 概述</h2>\n<h3 id=\"1-1-计算机网络的各层协议及作用？\">1.1 计算机网络的各层协议及作用？</h3>\n<p>计算机网络体系可以大致分为三种：OSI 七层模型、TCP/IP 四层模型和五层模型。</p>\n<ul>\n<li>OSI 七层模型：大而全，但是比较复杂、而且是先有了理论模型，没有实际应用。</li>\n<li>TCP/IP 四层模型：是由实际应用发展总结出来的，从实质上讲，TCP/IP 只有最上面两层，最下面一层没有什么具体内容，TCP/IP 参考模型没有真正描述这一层的实现。</li>\n<li>TCP/IP 五层模型：五层模型只出现在计算机网络教学过程中，这是对七层模型和四层模型的一个折中，既简洁又能将概念阐述清楚。</li>\n</ul>\n<p>七层网络体系结构各层的主要功能：</p>\n<ul>\n<li>应用层：为应用程序提供交互服务。在互联网中的应用层协议有很多，如域名系统 DNS，支持万维网应用的 HTTP 协议，支持电子邮件的 SMTP 协议等。</li>\n<li>表示层：主要负责数据格式的转换，如加密解密、转换翻译、压缩解压缩等。</li>\n<li>会话层：负责在网络中的两节点之间建立、维持和终止通信，如服务器验证用户登录便是由会话层完成的。</li>\n<li>运输层：有时也译为传输层，向主机进程提供通用的数据传输服务。该层主要有以下两种协议：\n<ul>\n<li>TCP：提供面向连接的、可靠的数据传输服务。</li>\n<li>UDP：提供无连接的、尽最大努力的数据传输服务，但不保证数据传输的可靠性。</li>\n</ul>\n</li>\n<li>网络层：选择合适的路由和交换结点，确保数据及时传送。主要包括 IP 协议。</li>\n<li>数据链路层：数据链路层通常简称为链路层。将网络层传下来的 IP 数据包组装成帧，并在相邻节点的链路上传送帧。</li>\n<li>物理层：实现相邻节点间比特流的透明传输，尽可能屏蔽传输介质和通信手段的差异。</li>\n</ul>\n<h2 id=\"2-TCP-IP\">2. TCP/IP</h2>\n<h3 id=\"2-1-TCP和UDP的区别？\">2.1 TCP和UDP的区别？</h3>\n<table>\n    <thead>\n        <tr>\n            <th></th>\n            <th>TCP</th>\n            <th>UDP</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>是否连接</td>\n            <td>面向连接</td>\n            <td>无连接</td>\n        </tr>\n        <tr>\n            <td>是否可靠</td>\n            <td>可靠传输，使用流量控制和拥塞控制</td>\n            <td>不可靠传输，不使用流量控制和拥塞控制</td>\n        </tr>\n        <tr>\n            <td>是否有序</td>\n            <td>有序，消息在传输过程中可能会乱序，TCP 会重新排序</td>\n            <td>无序</td>\n        </tr>\n        <tr>\n            <td>传输速度</td>\n            <td>慢</td>\n            <td>快</td>\n        </tr>\n        <tr>\n            <td>连接对象个数</td>\n            <td>只能一对一通信</td>\n            <td>支持一对一、一对多、多对一和多对多交互通信</td>\n        </tr>\n        <tr>\n            <td>传输方式</td>\n            <td>面向字节流</td>\n            <td>面向报文</td>\n        </tr>\n        <tr>\n            <td>首部开销</td>\n            <td>首部开销大，最小20字节，最大60字节</td>\n            <td>首部开销小，仅8字节</td>\n        </tr>\n        <tr>\n            <td>适用场景</td>\n            <td>适用于要求可靠传输的应用，例如文件传输</td>\n            <td>适用于实时应用例如 IP 电话、视频会议、直播等</td>\n        </tr>\n    </tbody>\n</table>\n<p>总结：TCP 用于在传输层有必要实现可靠传输的情况，UDP 用于对高速传输和实时性有较高要求的通信。TCP 和 UDP 应该根据应用目的按需使用。</p>\n<h3 id=\"2-2-TCP和UDP对应的应用场景是什么？\">2.2 TCP和UDP对应的应用场景是什么？</h3>\n<ul>\n<li>TCP 是面向连接的，能保证数据的可靠性交付，因此经常用于：\n<ul>\n<li>FTP 文件传输。</li>\n<li>HTTP/HTTPS。</li>\n<li>SMTP 简单邮件传输。</li>\n</ul>\n</li>\n<li>UDP 是无连接的，它可以随时发送数据，再加上 UDP 本身的处理既简单又高效，因此经常用于：\n<ul>\n<li>包总量较少的通信，如 DNS、SNMP 等。</li>\n<li>视频、音频等多媒体通信。</li>\n<li>广播通信。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"2-3-TCP的三次握手机制？\">2.3 TCP的三次握手机制？</h3>\n<ul>\n<li>第一次握手：客户端请求建立连接，向服务端发送一个同步报文（<code>SYN = 1</code>），同时选择一个随机数 <code>seq = x</code> 作为初始序列号，并进入 <code>SYN_SENT</code>（同步已发送）状态，等待服务器确认。</li>\n<li>第二次握手：服务端收到连接请求报文后，如果同意建立连接，则向客户端发送同步确认报文（<code>SYN = 1, ACK = 1</code>），确认号为 <code>ack = x + 1</code>，同时选择一个随机数 <code>seq = y</code> 作为初始序列号，此时服务器进入 <code>SYN_RECV</code>（同步收到）状态。</li>\n<li>第三次握手：客户端收到服务端的确认后，向服务端发送一个确认报文（<code>ACK = 1</code>），确认号为 <code>ack = y + 1</code>，序列号为 <code>seq = x + 1</code>，客户端和服务器进入 <code>ESTABLISHED</code>（已建立连接）状态，完成三次握手。</li>\n</ul>\n<p>理想状态下，TCP 连接一旦建立，在通信双方中的任何一方主动关闭连接之前，TCP 连接都将被一直保持下去。</p>\n<h3 id=\"2-4-为什么需要三次握手，而不是两次？\">2.4 为什么需要三次握手，而不是两次？</h3>\n<p>主要有三个原因：</p>\n<ol>\n<li>防止已过期的连接请求报文突然又传送到服务器，因而产生错误和资源浪费。<br>\n在双方两次握手即可建立连接的情况下，假设客户端发送报文段A请求建立连接，由于网络原因造成A暂时无法到达服务器，服务器接收不到请求报文段就不会返回确认报文段。<br>\n客户端在长时间得不到应答的情况下重新发送请求报文段B，这次B顺利到达服务器，服务器随即返回确认报文并进入 <code>ESTABLISHED</code> 状态，客户端在收到确认报文后也进入 <code>ESTABLISHED</code> 状态，双方建立连接并传输数据，之后正常断开连接。<br>\n此时姗姗来迟的报文段A才到达服务器，服务器随即返回确认报文并进入 <code>ESTABLISHED</code> 状态，但是已经进入 <code>CLOSED</code> 状态的客户端无法再接受确认报文段，更无法进入 <code>ESTABLISHED</code> 状态，这将导致服务器长时间单方面等待，造成资源浪费。</li>\n<li>三次握手才能让双方均确认自己和对方的发送和接收能力都正常。<br>\n第一次握手：客户端只是发送处请求报文段，什么都无法确认，而服务器可以确认自己的接收能力和对方的发送能力正常。<br>\n第二次握手：客户端可以确认自己发送能力和接收能力正常，对方发送能力和接收能力正常。<br>\n第三次握手：服务器可以确认<strong>自己发送能力</strong>和接收能力正常，<strong>对方</strong>发送能力和<strong>接收能力</strong>正常。<br>\n可见三次握手才能让双方都确认自己和对方的发送和接收能力全部正常，这样就可以愉快地进行通信了。</li>\n<li>告知对方自己的初始序号值，并确认收到对方的初始序号值。<br>\nTCP 实现了可靠的数据传输，原因之一就是 TCP 报文段中维护了序号字段和确认序号字段，通过这两个字段双方都可以知道在自己发出的数据中，哪些是已经被对方确认接收的。这两个字段的值会在初始序号值的基础上递增，如果是两次握手，只有发起方的初始序号可以得到确认，而另一方的初始序号则得不到确认。</li>\n</ol>\n<h3 id=\"2-5-为什么需要三次握手，而不是四次？\">2.5 为什么需要三次握手，而不是四次？</h3>\n<p>因为三次握手已经可以确认双方的发送和接收能力正常，双方都知道彼此已经准备好，而且也可以完成对双方初始序号值的确认，也就无需第四次握手了。</p>\n<ul>\n<li>第一次握手：服务端确认<strong>自己收、对方发</strong>报文功能正常。</li>\n<li>第二次握手：客户端确认<strong>自己发、自己收、对方收、对方发</strong>报文功能正常，客户端认为连接己建立。</li>\n<li>第三次握手：服务端确认<strong>自己发、对方收</strong>报文功能正常，此时双方均建立连接，可以正常通信。</li>\n</ul>\n<h3 id=\"2-6-什么是SYN洪泛攻击？如何防范？\">2.6 什么是SYN洪泛攻击？如何防范？</h3>\n<p>SYN 洪泛攻击属于 DOS 攻击的一种，它利用 TCP 协议缺陷，通过发送大量的<strong>半连接</strong>请求，耗费 CPU 和内存资源。</p>\n<p>原理：</p>\n<ul>\n<li>在三次握手过程中，服务器发送 <code>[SYN/ACK]</code> 包（即第二个包）之后、收到客户端的 <code>[ACK]</code> 包（即第三个包）之前的 TCP 连接称为半连接（half-open connect），此时服务器处于 <code>SYN_RECV</code>（等待客户端响应）状态。如果接收到客户端的 <code>[ACK]</code>，则 TCP 连接成功，如果未接收到，则会不断重发请求直至成功。</li>\n<li>SYN 攻击的攻击者在短时间内伪造大量不存在的 IP 地址，向服务器不断地发送 <code>[SYN]</code> 包，服务器回复 <code>[SYN/ACK]</code> 包，并等待客户的确认。由于源地址是不存在的，服务器需要不断的重发直至超时。</li>\n<li>这些伪造的 <code>[SYN]</code> 包将长时间占用未连接队列，影响了正常的 SYN，导致目标系统运行缓慢、网络堵塞甚至系统瘫痪。</li>\n</ul>\n<p>检测：当在服务器上看到大量的半连接状态时，特别是源 IP 地址是随机的，基本上可以断定这是一次 SYN 攻击。</p>\n<p>防范：</p>\n<ul>\n<li>通过防火墙、路由器等过滤网关防护。</li>\n<li>通过加固 TCP/IP 协议栈防范，如增加最大半连接数，缩短超时时间。</li>\n<li>SYN Cookies 技术。SYN Cookies 是对 TCP 服务器端的三次握手做一些修改，专门用来防范 SYN 洪泛攻击的一种手段。</li>\n</ul>\n<h3 id=\"2-7-三次握手连接阶段，如果最后一次ACK包丢失，会发生什么？\">2.7 三次握手连接阶段，如果最后一次ACK包丢失，会发生什么？</h3>\n<p>服务端：</p>\n<ul>\n<li>第三次的 <code>ACK</code> 包在网络中丢失，那么服务端该 TCP 连接的状态为 <code>SYN_RECV</code>,并且会根据 TCP 的超时重传机制，会等待3秒、6秒、12秒后重新发送 <code>SYN + ACK</code> 包，以便客户端重新发送 <code>ACK</code> 包。</li>\n<li>如果重发指定次数之后，仍然未收到客户端的 ACK 应答，那么一段时间后，服务端自动关闭这个连接。</li>\n</ul>\n<p>客户端：</p>\n<ul>\n<li>客户端认为这个连接已经建立，如果客户端向服务端发送数据，服务端将以 <code>RST</code> 包（Reset，表示复位，用于异常的关闭连接）响应。此时，客户端便知道第三次握手失败。</li>\n</ul>\n<h3 id=\"2-8-TCP的四次挥手过程？\">2.8 TCP的四次挥手过程？</h3>\n<ul>\n<li>第一次挥手：客户端向服务端发送连接释放报文（<code>FIN = 1, ACK = 1</code>），主动关闭连接，同时等待服务端的确认，客户端进入 <code>FIN_WAIT_1</code>（终止等待1）状态。序列号 <code>seq = u</code>，为客户端上次发送的报文的最后一个字节的序号 + 1。</li>\n<li>第二次挥手：服务端收到连接释放报文后，立即发出确认报文（<code>ACK = 1</code>），序列号 <code>seq = v</code>，为服务端上次发送的报文的最后一个字节的序号 + 1，确认号 <code>ack = u + 1</code>，服务端进入 <code>CLOSE_WAIT</code>（关闭等待）状态。<br>\n此时 TCP 连接处于<strong>半关闭</strong>状态，即客户端到服务端的连接已经释放了，但是服务端到客户端的连接还未释放。这表示客户端已经没有数据发送了，但是服务端可能还要给客户端发送数据。</li>\n<li>第三次挥手：客户端收到服务端的确认后进入 <code>FIN_WAIT_2</code>（终止等待2）状态，等待服务端发出连接释放报文段。服务端向客户端发送连接释放报文（<code>FIN = 1, ACK = 1</code>），主动关闭连接，同时等待A的确认，服务端进入 <code>LAST_ACK</code>（最后确认）状态。\n<ul>\n<li>序列号 <code>seq = w</code>，即服务端上次发送的报文的最后一个字节的序号 + 1，可能在半关闭状态服务端又发送了一些数据。</li>\n<li>确认号 <code>ack = u + 1</code>，与第二次挥手相同，因为这段时间客户端没有发送数据。</li>\n</ul>\n</li>\n<li>第四次挥手：客户端收到服务端的连接释放报文后，立即发出确认报文（<code>ACK = 1</code>），序列号 <code>seq = u + 1</code>，确认号为 <code>ack = w + 1</code>。<br>\n此时，客户端就进入了 <code>TIME_WAIT</code>（时间等待）状态。注意此时客户端到 TCP 连接还没有释放，必须经过2 * MSL（最长报文段寿命）的时间后，才进入 <code>CLOSED</code> 状态。而服务端只要收到客户端发出的确认，就立即进入 <code>CLOSED</code> 状态。可以看到，服务端结束 TCP 连接的时间要比客户端早一些。</li>\n</ul>\n<h3 id=\"2-9-为什么连接的时候是三次握手，关闭的时候却是四次握手？\">2.9 为什么连接的时候是三次握手，关闭的时候却是四次握手？</h3>\n<p>服务器在收到客户端的 <code>FIN</code> 报文段后，可能还有一些数据要传输，所以不能马上关闭连接，但是会做出应答，返回 <code>ACK</code> 报文段.</p>\n<p>接下来可能会继续发送数据，在数据发送完后，服务器会向客户单发送 <code>FIN</code> 报文，表示数据已经发送完毕，请求关团连接。服务器的 <code>ACK</code> 和 <code>FIN</code> 一般都会分开发送，从而导致多了一次，因此一共需要四次挥手。</p>\n<h3 id=\"2-10-为什么客户端的TIME-WAIT状态必须等待2MSL？\">2.10 为什么客户端的TIME_WAIT状态必须等待2MSL？</h3>\n<p>主要有两个原因:</p>\n<ol>\n<li>确保最后一个 <code>ACK</code> 报文段能够到达服务端，从而使服务端正常关闭连接。<br>\n第四次挥手时，客户端第四次挥手的 <code>ACK</code> 报文段不一定会到达服务端。服务端会超时重传 <code>FIN/ACK</code> 报文段，此时如果客户端已经断开了连接，那么就无法响应服务端的二次请求，这样服务端迟迟收不到 <code>FIN/ACK</code> 报文段的确认，就无法正常断开连接。<br>\nMSL 是报文段在网络上存活的最长时间。客户端等待 2MSL 时间，即：客户端 <code>ACK</code> 报文段 1MSL 超时 + 服务端 <code>FIN</code> 报文段 1MSL 传输，就能够收到服务端重传的 <code>FIN/ACK</code> 报文段，然后客户端重传一次 <code>ACK</code> 报文段，并重新启动 2MSL 计时器。如此保证服务端能够正常关闭。<br>\n如果服务端重发的 <code>FIN</code> 报文段没有成功地在 2MSL 时间里传给客户端，服务端则会继续超时重试直到断开连接。</li>\n<li>防止已失效的连接请求报文段出现在之后的连接中。<br>\nTCP 要求在 2MSL 内不使用相同的序列号。客户端在发送完最后一个 <code>ACK</code> 报文段后，再经过时间 2MSL，就可以保证本连接持续的时间内产生的所有报文段都从网络中消失。这样就可以使下一个连接中不会出现这种旧的连接请求报文段。或者即使收到这些过时的报文，也可以不处理它。</li>\n</ol>\n<h3 id=\"2-11-如果已经建立了连接，但是客户端出现故障了怎么办？\">2.11 如果已经建立了连接，但是客户端出现故障了怎么办？</h3>\n<p>通过<strong>定时器 + 超时重试机制</strong>，尝试获取确认，直到最后会自动断开连接。</p>\n<p>具体而言，TCP 设有一个<strong>保活计时器</strong>。服务器每收到一次客户端的数据，都会重新设置这个计时器，时间通常是设置为2小时。若2小时还没有收到客户端的任何数据，服务器就发送一个探测报文段，之后则每隔75秒钟发送一次，若一连发送10个探测报文段后客户端依然没有响应，那么服务器就认为客户端出现故障，接着就关闭这个连接。</p>\n<h3 id=\"2-12-TIME-WAIT是服务器端还是客户端的状态\">2.12 TIME_WAIT是服务器端还是客户端的状态?</h3>\n<p><code>TIME_WAIT</code> 是<strong>主动断开连接</strong>的一方会进入的状态，一般情况下，都是客户端所处的状态，服务器端一般设置不主动关闭连接。</p>\n<p><code>TIME_WAIT</code> 需要等待 2MSL，在大量短连接的情况下，<code>TIME_WAIT</code> 会太多，这也会消耗很多系统资源。对于服务器来说，在 HTTP 协议里指定 KeepAlive（浏览器重用一个 TCP 连接来处理多个 HTTP 请求），由浏览器来主动断开连接，可以一定程度上减少服务器的这个问题。</p>\n<h3 id=\"2-13-TCP协议如何保证可靠性，即如何实现可靠传输？\">2.13 TCP协议如何保证可靠性，即如何实现可靠传输？</h3>\n<p>TCP 主要提供了检验和、序列号/确认应答、超时重传、滑动窗口、拥塞控制和流量控制等方法实现了可靠性传输。</p>\n<ul>\n<li>检验和：通过检验和的方式，接收端可以检测出来数据是否有差错和异常，假如有差错就会直接丢弃 TCP 报文段，重新发送。</li>\n<li>序列号/确认应答：序列号的作用不仅仅是应答的作用，有了序列号能够将接收到的数据根据序列号排序，并且去掉重复序列号的数据。<br>\nTCP 传输的过程中，每次接收方收到数据后，都会对传输方进行确认应答。也就是发送 ACK 报文段，这个 ACK 报文段当中带有对应的确认序列号，告诉发送方，接收到了哪些数据，下一次的数据从哪里发。</li>\n<li>滑动窗口：滑动窗口既提高了报文传输的效率，也避免了发送方发送过多的数据而导致接收方无法正常处理的异常。</li>\n<li>超时重传：超时重传的时间是指发送出去的数据包到接收到确认包之间的时间，如果超过了这个时间会被认为是丢包了，需要重传。最大超时时间是动态计算的。</li>\n<li>拥塞控制：在数据传输过程中，可能由于网络状态的问题，造成网络拥堵，此时引入拥塞控制机制，在保证 TCP 可靠性的同时，提高性能。</li>\n<li>流量控制：如果主机A一直向主机B发送数据，不考虑主机B的接收能力，则可能导致主机B的接收缓冲区满了而无法再接收数据，从而会导致大量的数据丢包，引发重传机制。而在重传的过程中，若主机B的接收缓冲区情况仍未好转，则会将大量的时间浪费在重传数据上，降低传送数据的效率。所以引入流量控制机制，主机B通过告诉主机A自己接收缓冲区的大小，来使主机A控制发送的数据量。流量控制与 TCP 协议报头中的窗口大小有关。</li>\n</ul>\n<h3 id=\"2-14-详细讲一下TCP的滑动窗口？\">2.14 详细讲一下TCP的滑动窗口？</h3>\n<p>在进行数据传输时，如果传输的数据比较大，就需要拆分为多个数据包进行发送。TCP 协议需要对数据进行确认后，才可以发送下一个数据包。这样一来，就会在等待确认应答包环节浪费时间。</p>\n<p>为了避免这种情况，TCP 引入了窗口概念。窗口大小指的是不需要等待确认应答包而可以继续发送数据包的最大值。</p>\n<p>滑动窗口里面也分为有三种类型的数据，第一种是已经发送且收到确认但是未按序到达，即没有在窗口尾部形成一段连续的序列；第二种是已经发送但是未被确认的数据；第三种是等待发送的数据。随着已发送的数据不断被确认，窗口内等待发送的数据也会不断被发送。整个窗口就会不断往前移动，让还没轮到的数据进入窗口内。</p>\n<p>可以看到滑动窗口起到了一个限流的作用，也就是说当前滑动窗口的大小决定了当前 TCP 发送包的速率，而滑动窗口的大小取决于拥塞控制窗口和流量控制窗口的两者间的最小值。</p>\n<h3 id=\"2-15-详细讲一下拥塞控制？\">2.15 详细讲一下拥塞控制？</h3>\n<p>TCP 一共使用了四种算法来实现拥塞控制：</p>\n<ul>\n<li>慢开始（slow-start）</li>\n<li>拥塞避免（congestion avoidance）</li>\n<li>快重传（fast retransmit）</li>\n<li>快恢复（fast recovery）</li>\n</ul>\n<p>发送方维持一个叫做拥塞窗口 cwnd（congestion window）的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。</p>\n<ul>\n<li>慢开始：不要一开始就发送大量的数据，<strong>由小到大逐渐增加拥塞窗口的大小</strong>。<br>\n例如一开始发送方先设置 cwnd = 1，发送第一个报文段，等发送方接收到对方的确认后把 cwnd 从1增大到2。此后每经过一个传输轮次，拥塞窗口 cwnd 就加倍。<br>\n为了防止拥塞窗口 cwnd 增长过大引起网络拥塞，还需要设置一个慢开始门限 <code>ssthresh</code> 状态变量。\n<ul>\n<li>当 <code>cwnd &lt; ssthresh</code> 时，使用慢开始算法。</li>\n<li>当 <code>cwnd &gt; ssthresh</code> 时，停止使用慢开始算法改用拥塞避免算法。</li>\n<li>当 <code>cwnd = ssthresh</code> 时，即可使用慢开始算法，也可使用拥塞避免算法。</li>\n</ul>\n</li>\n<li>拥塞避免：拥塞避免算法让拥塞窗口缓慢增长，即每经过一个往返时间 RTT 就把发送方的拥塞窗口 cwnd 加一而不是加倍。这样拥塞窗口按线性规律缓慢增长。</li>\n<li>快重传：我们可以剔除一些不必要的拥塞报文，提高网络吞吐量。比如接收方在收到一个失序的报文段后就立即发出重复确认，而不要等到自己发送数据时捎带确认。快重传规定：发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传计时器时间到期。</li>\n<li>快恢复：主要是配合快重传，当发送方连续收到三个重复确认时，就执行<strong>乘法减小</strong>算法，把 <code>ssthresh</code> 门限减半（为了预防网络发生拥塞），但接下来并不执行慢开始算法，因为如果网络出现拥塞的话就不会收到好几个重复的确认，收到三个重复确认说明网络状况还可以。</li>\n</ul>\n<h2 id=\"3-HTTP-HTTPS\">3. HTTP/HTTPS</h2>\n<h3 id=\"3-1-HTTP常见的状态码有哪些？\">3.1 HTTP常见的状态码有哪些？</h3>\n<p>常见状态码：</p>\n<ul>\n<li>200：服务器已成功处理了请求。通常，这表示服务器提供了请求的网页。</li>\n<li>301：请求的网页已永久移动到新位置。服务器返回此响应（对 GET 或 HEAD 请求的响应）时，会自动将请求者转到新位置（永久移动）。</li>\n<li>302：服务器目前从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求（临时移动）。</li>\n<li>400：客户端请求有语法错误，不能被服务器所理解。</li>\n<li>403：服务器收到请求，但是拒绝提供服务。</li>\n<li>404：服务器找不到请求的网页。</li>\n<li>500：服务器遇到错误，无法完成请求。</li>\n</ul>\n<p>状态码开头代表类型：</p>\n<table>\n    <thead>\n        <tr>\n            <th></th>\n            <th>类别</th>\n            <th>原因短语</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>1XX</td>\n            <td>Informational（信息性状态码）</td>\n            <td>接收的请求正在处理</td>\n        </tr>\n        <tr>\n            <td>2XX</td>\n            <td>Success（成功状态码）</td>\n            <td>请求正常处理完毕</td>\n        </tr>\n        <tr>\n            <td>3XX</td>\n            <td>Redirection（重定向状态码）</td>\n            <td>需要进行附加操作以完成请求</td>\n        </tr>\n        <tr>\n            <td>4XX</td>\n            <td>Client Error（客户端错误状态码）</td>\n            <td>服务器无法处理请求</td>\n        </tr>\n        <tr>\n            <td>5XX</td>\n            <td>Server Error（服务器错误状态码）</td>\n            <td>服务器处理请求出错</td>\n        </tr>\n    </tbody>\n</table>\n<h3 id=\"3-2-状态码301和302的区别是什么？\">3.2 状态码301和302的区别是什么？</h3>\n<ul>\n<li>共同点：301和302状态码都表示重定向，就是说浏览器在拿到服务器返回的这个状态码后会自动跳转到一个新的 URL 地址，这个地址可以从响应的 Location 首部中获取（用户看到的效果就是他输入的地址A瞬间变成了另一个地址B）。</li>\n<li>不同点：301表示旧地址A的资源已经被永久地移除了（这个资源不可访问了），搜索引擎在抓取新内容的同时也将旧的网址交换为重定向之后的网址。302表示旧地址A的资源还在（仍然可以访问），这个重定向只是临时地从旧地址A跳转到地址B，搜索引擎会抓取新的内容而保存旧的网址。SEO 中302好于301。</li>\n</ul>\n<p>重定向原因：</p>\n<ul>\n<li>网站调整（如改变网页目录结构）。</li>\n<li>网页被移到一个新地址。</li>\n<li>网页扩展名改变（如应用需要把 <code>.php</code> 改成 <code>.html</code> 或 <code>.shtml</code>）。</li>\n</ul>\n<h3 id=\"3-3-HTTP常用的请求方式有哪些？\">3.3 HTTP常用的请求方式有哪些？</h3>\n<table>\n    <thead>\n        <tr>\n            <th>方法</th>\n            <th>作用</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>GET</td>\n            <td>获取资源</td>\n        </tr>\n        <tr>\n            <td>POST</td>\n            <td>传输实体主体</td>\n        </tr>\n        <tr>\n            <td>PUT</td>\n            <td>上传文件</td>\n        </tr>\n        <tr>\n            <td>DELETE</td>\n            <td>删除文件</td>\n        </tr>\n        <tr>\n            <td>HEAD</td>\n            <td>和 GET 方法类似，但是只返回报文首部，不返回报文实体主体部分</td>\n        </tr>\n        <tr>\n            <td>PATCH</td>\n            <td>对资源进行部分修改</td>\n        </tr>\n        <tr>\n            <td>OPTIONS</td>\n            <td>查询指定的 URL 支持的方法</td>\n        </tr>\n        <tr>\n            <td>CONNECT</td>\n            <td>要求用隧道协议连接代理</td>\n        </tr>\n        <tr>\n            <td>TRACE</td>\n            <td>服务器会将通信路径返回给客户端</td>\n        </tr>\n    </tbody>\n</table>\n<p>为了方便记忆，可以将 PUT、DELETE、POST、GET 理解为客户端对服务端的增删改查：</p>\n<ul>\n<li>PUT：上传文件，向服务器添加数据。</li>\n<li>DELETE：删除文件。</li>\n<li>POST：传输数据，向服务器提交数据，对服务器数据进行更新。</li>\n<li>GET：获取资源，查询服务器资源。</li>\n</ul>\n<h3 id=\"3-4-GET请求和POST请求的区别？\">3.4 GET请求和POST请求的区别？</h3>\n<p>使用上的区别：</p>\n<ul>\n<li>作用：GET 用于获取资源，而 POST 用于传输实体。</li>\n<li>参数：GET 使用 URL 或 Cookie 传参，而 POST 将数据放在 Request Body 中，这个是因为 HTTP 协议用法的约定。</li>\n<li>缓存：GET 请求会被浏览器主动缓存，而 POST 不会，除非手动设置。</li>\n<li>请求长度：GET 方式提交的数据有长度限制，基本为2kb，而 POST 的数据则可以非常大，这个是因为它们使用的操作系统和浏览器设置的不同引起的区别。</li>\n<li>安全性：POST 比 GET 安全，因为数据在地址栏上不可见，而 GET 的参数直接暴露在 URL 上。这个说法没毛病，但依然不是 GET 和 POST 本身的区别。</li>\n</ul>\n<p>本质区别：GET 和 POST 最大的区别主要是 GET 请求是幂等性的，POST 请求不是。这个是它们本质区别。（幂等性是指一次和多次请求某一个资源应该具有同样的副作用。简单来说意味着对同一 URL 的多个请求应该返回同样的结果）</p>\n<h3 id=\"3-5-解释一下HTTP长连接和短连接？\">3.5 解释一下HTTP长连接和短连接？</h3>\n<p>在 <strong>HTTP/1.0</strong> 中，默认使用的是短连接。也就是说，浏览器和服务器每进行一次 HTTP 操作，就建立一次连接，但任务结束就中断连接。如果客户端浏览器访问的某个 HTML 或其他类型的 Web 页中包含有其他的 Web 资源，如 JavaScript 文件、图像文件、CSS 文件等，当浏览器每遇到这样一个 Web 资源，就会建立一个 HTTP 会话。</p>\n<p>但从 <strong>HTTP/1.1</strong> 起，默认使用长连接，用以保持连接特性。使用长连接的 HTTP 协议，会在响应头加入这行代码：<code>Connection:keep-alive</code>。</p>\n<p>在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输 HTTP 数据的 TCP 连接不会关闭，如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的连接。Keep-Alive 不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（例如 Apache）中设定这个时间。实现长连接要客户端和服务端都支持长连接。</p>\n<p><strong>HTTP 协议的长连接和短连接，实质上是 TCP 协议的长连接和短连接</strong>。</p>\n<h3 id=\"3-6-HTTP1-0和HTTP1-1的区别？\">3.6 HTTP1.0和HTTP1.1的区别？</h3>\n<ul>\n<li>长连接：HTTP1.1 支持长连接（Persistent Connection）和请求的流水线（Pipelining）处理，在一个 TCP 连接上可以传送多个 HTTP 请求和响应，减少了建立和关闭连接的消耗和延迟，在 HTTP1.1 中默认开启 <code>Connection: keep-alive</code>，一定程度上弥补了 HTTP1.0 每次请求都要创建连接的缺点。</li>\n<li>缓存处理：在 HTTP1.0 中主要使用 header 里的 <code>If-Modified-Since, Expires</code> 来做为缓存判断的标准，HTTP1.1 则引入了更多的缓存控制策略，可供选择的缓存头来控制缓存策略。</li>\n<li>带宽优化及网络连接的使用：HTTP1.0 中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP1.1 则在请求头引入了 range 头域，它允许只请求资源的某个部分，即返回码是206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。</li>\n<li>错误通知的管理：在 HTTP1.1 中新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突，410（Gone）表示服务器上的某个资源被永久性的删除。</li>\n<li>Host 头处理：在 HTTP1.0 中认为每台服务器都绑定一个唯一的 IP 地址，因此，请求消息中的 URL 并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个 IP 地址。HTTP1.1 的请求消息和响应消息都应支持 Host 头域，且请求消息中如果没有 Host 头域会报告一个错误（400 Bad Request）。</li>\n</ul>\n<h3 id=\"3-7-HTTP1-1和HTTP2-0的区别？\">3.7 HTTP1.1和HTTP2.0的区别？</h3>\n<p>HTTP2.0 相比 HTTP1.1 支持的特性：</p>\n<ul>\n<li>新的二进制格式：HTTP1.1 的解析是基于文本的。基于文本协议的格式解析存在天然缺陷，文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，二进制则不同，只认0和1的组合。基于这种考虑 HTTP2.0 的协议解析决定采用二进制格式，实现方便且健壮。</li>\n<li>多路复用：即连接共享，每一个 request 都是用作连接共享机制的。一个 request 对应一个 id，这样一个连接上可以有多个 request，每个连接的 request 可以随机的混杂在一起，接收方可以根据 request 的 id 将 request 再归属到各自不同的服务端请求里面。</li>\n<li>头部压缩：HTTP1.1 的头部（header）带有大量信息，而且每次都要重复发送。HTTP2.0 使用 encoder 来减少需要传输的 header 大小，通讯双方各自 cache 一份 header fields 表，既避免了重复 header 的传输，又减小了需要传输的大小。</li>\n<li>服务端推送：服务器除了对最初请求的响应外，服务器还可以额外地向客户端推送资源，而无需客户端明确的请求。</li>\n</ul>\n<h3 id=\"3-8-HTTP和HTTPS的区别？\">3.8 HTTP和HTTPS的区别？</h3>\n<table>\n    <thead>\n        <tr>\n            <th></th>\n            <th>HTTP</th>\n            <th>HTTPS</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>端口</td>\n            <td>80</td>\n            <td>443</td>\n        </tr>\n        <tr>\n            <td>安全性</td>\n            <td>无加密，安全性较差</td>\n            <td>有加密机制，安全性较高</td>\n        </tr>\n        <tr>\n            <td>资源消耗</td>\n            <td>较少</td>\n            <td>由于加密处理，资源消耗更多</td>\n        </tr>\n        <tr>\n            <td>是否需要证书</td>\n            <td>不需要</td>\n            <td>需要</td>\n        </tr>\n        <tr>\n            <td>协议</td>\n            <td>运行在 TCP 协议之上</td>\n            <td>运行在 SSL 协议之上，SSL 运行在 TCP 协议之上</td>\n        </tr>\n    </tbody>\n</table>\n<h3 id=\"3-9-HTTPS的优缺点？\">3.9 HTTPS的优缺点？</h3>\n<p>优点：</p>\n<ul>\n<li>安全性：\n<ul>\n<li>使用 HTTPS 协议可认证用户和服务器，确保数据发送到正确的客户机和服务器。</li>\n<li>HTTPS 协议是由 SSL + HTTP 协议构建的可进行加密传输、身份认证的网络协议，要比 HTTP 协议安全，可防止数据在传输过程中不被窃取、改变，确保数据的完整性。</li>\n<li>HTTPS 是现行架构下最安全的解决方案，虽然不是绝对安全，但它大幅增加了中间人攻击的成本。</li>\n</ul>\n</li>\n<li>SEO 方面：谷歌曾在2014年8月份调整搜索引擎算法，并称比起同等 HTTP 网站，采用 HTTPS 加密的网站在搜索结果中的排名将会更高。</li>\n</ul>\n<p>缺点：</p>\n<ul>\n<li>在相同网络环境中，HTTPS 相比 HTTP 无论是响应时间还是耗电量都有大幅度上升。</li>\n<li>HTTPS 的安全是有范围的，在黑客攻击、服务器劫持等情况下几乎起不到作用。</li>\n<li>在现有的证书机制下，中间人攻击依然有可能发生。</li>\n<li>HTTPS 需要更多的服务器资源，也会导致成本的升高。</li>\n</ul>\n<h3 id=\"3-10-HTTPS的原理？\">3.10 HTTPS的原理？</h3>\n<ol>\n<li>客户端请求 HTTPS 网址，例如：<code>https://www.baidu.com</code>，然后连接到 Server 的443端口（HTTPS 默认端口，类似于 HTTP 的80端口）。</li>\n<li>采用 HTTPS 协议的服务器必须要有一套数字 CA（Certification Authority）证书。颁发证书的同时会产生一个私钥和公钥。私钥由服务端自己保存，不可泄漏。公钥则是附带在证书的信息中，可以公开的。证书本身也附带一个证书电子签名，这个签名用来验证证书的完整性和真实性，可以防止证书被篡改。</li>\n<li>服务器响应客户端请求，将证书传递给客户端，证书包含公钥和大量其他信息，比如证书颁发机构信息，公司信息和证书有效期等。</li>\n<li>客户端解析证书并对其进行验证。如果证书不是可信机构颁布，或者证书中的域名与实际域名不一致，或者证书已经过期，就会向访问者显示一个警告，由其选择是否还要继续通信。<br>\n如果证书没有问题，客户端就会从服务器证书中取出服务器的公钥A。然后客户端还会生成一个随机码 <code>KEY</code>，并使用公钥A将其加密。</li>\n<li>客户端把加密后的随机码 <code>KEY</code> 发送给服务器，作为后面对称加密的密钥。</li>\n<li>服务器在收到随机码 <code>KEY</code> 之后会使用私钥B将其解密。经过以上这些步骤，客户端和服务器终于建立了安全连接，完美解决了对称加密的密钥泄露问题，接下来就可以用对称加密愉快地进行通信了。</li>\n<li>服务器使用密钥（随机码 <code>KEY</code>）对数据进行对称加密并发送给客户端，客户端使用相同的密钥（随机码 <code>KEY</code>）解密数据。</li>\n<li>双方使用对称加密愉快地传输所有数据。</li>\n</ol>\n<h3 id=\"3-11-什么是Cookie和Session？\">3.11 什么是Cookie和Session？</h3>\n<p>HTTP Cookie（也叫 Web Cookie 或浏览器 Cookie）是服务器发送到用户浏览器并保存在本地的一小块数据，它会在浏览器下次向同一服务器再发起请求时被携带并发送到服务器上。通常，它用于告知服务端两个请求是否来自同一浏览器，如保持用户的登录状态。Cookie 使基于无状态的 HTTP 协议记录稳定的状态信息成为了可能。</p>\n<p>Cookie 主要用于以下三个方面：</p>\n<ul>\n<li>会话状态管理（如用户登录状态、购物车、游戏分数或其它需要记录的信息）。</li>\n<li>个性化设置（如用户自定义设置、主题等）。</li>\n<li>浏览器行为跟踪（如跟踪分析用户行为等）。</li>\n</ul>\n<p>Session 代表着服务器和客户端一次会话的过程。Session 对象存储特定用户会话所需的属性及配置信息。这样，当用户在应用程序的 Web 页之间跳转时，存储在 Session 对象中的变量将不会丢失，而是在整个用户会话中一直存在下去。当客户端关闭会话，或者 Session 超时失效时会话结束。</p>\n<h3 id=\"3-12-Cookie和Session是如何配合的呢？\">3.12 Cookie和Session是如何配合的呢？</h3>\n<p>用户第一次请求服务器的时候，服务器根据用户提交的相关信息，创建对应的 Session，请求返回时将此 Session 的唯一标识信息 SessionID 返回给浏览器，浏览器接收到服务器返回的 SessionID 信息后，会将此信息存入到 Cookie 中，同时 Cookie 记录此 SessionID 属于哪个域名。</p>\n<p>当用户第二次访问服务器的时候，请求会自动判断此域名下是否存在 Cookie 信息，如果存在则自动将 Cookie 信息也发送给服务端，服务端会从 Cookie 中获取 SessionID，再根据 SessionID 查找对应的 Session 信息，如果没有找到说明用户没有登录或者登录失效，如果找到 Session 证明用户已经登录可执行后面操作。</p>\n<p>根据以上流程可知，SessionID 是连接 Cookie 和 Session 的一道桥梁，大部分系统也是根据此原理来验证用户登录状态。</p>\n<h3 id=\"3-13-Cookie和Session的区别？\">3.13 Cookie和Session的区别？</h3>\n<ul>\n<li>作用范围不同：Cookie 保存在客户端（浏览器），Session 保存在服务器端。</li>\n<li>存取方式的不同：Cookie 只能保存 ASCII，Session 可以存任意数据类型，一般情况下我们可以在 Session 中保持一些常用变量信息，比如说UserID等。</li>\n<li>有效期不同：Cookie 可设置为长时间保持，比如我们经常使用的默认登录功能，Session 一般失效时间较短，客户端关闭或者 Session 超时都会失效。</li>\n<li>隐私策略不同：Cookie 存储在客户端，比较容易遭到不法获取，早期有人将用户的登录名和密码存储在 Cookie 中导致信息被窃取；Session 存储在服务端，安全性相对 Cookie 要好一些。</li>\n<li>存储大小不同：单个 Cookie 保存的数据不能超过4K，Session 可存储数据远高于 Cookie。</li>\n</ul>\n",
            "tags": [
                "Network"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/18884.html",
            "url": "https://asanosaki.github.io/posts/18884.html",
            "title": "Hexo使用Abbrlink生成文章固定链接",
            "date_published": "2022-11-23T03:11:00.000Z",
            "content_html": "<blockquote>\n<p>本文介绍如何使用 Abbrlink 插件生成形如 <code>http://id.github.io/posts/38175.html</code> 的 Hexo 文章固定编号链接。</p>\n</blockquote>\n<span id=\"more\"></span>\n<p>Hexo 默认的静态 <code>url</code> 格式是：<code>:year/:month/:day/:title</code>，也就是按照年、月、日、文章标题来生成固定链接的。如：<code>http://id.github.io/2022/11/23/hello-world</code>。</p>\n<p>使用 Abbrlink 插件可以使每篇文章都有一个唯一的编号，并将文章的链接用这个编号唯一区别，这样链接中不会出现中文，也不会因为修改文章的日期而导致链接的改变。</p>\n<p>首先我们先安装插件，在博客根目录中打开命令行，输入以下命令：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install hexo-abbrlink --save</span><br></pre></td></tr></table></figure>\n<p>修改根目录下的 <code>_config.yml</code> 文件，修改文件中的 <code>permalink:</code> 配置项，且添加一个配置项 <code>abbrlink:</code>，修改后的结果如下：</p>\n<figure class=\"highlight yml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">permalink:</span> <span class=\"string\">posts/:abbrlink.html</span>  <span class=\"comment\"># 此处可以自己设置，也可以直接使用 /:abbrlink.html</span></span><br><span class=\"line\"><span class=\"attr\">abbrlink:</span></span><br><span class=\"line\">    <span class=\"attr\">alg:</span> <span class=\"string\">crc16</span>   <span class=\"comment\">#算法：crc16(default) and crc32</span></span><br><span class=\"line\">    <span class=\"attr\">rep:</span> <span class=\"string\">dec</span>     <span class=\"comment\">#进制：dec(default) and hex</span></span><br></pre></td></tr></table></figure>\n<p>其中，<code>alg</code> 属性表示算法，目前支持 <code>crc16</code> 和 <code>crc32</code> 算法，默认值是 <code>crc16</code>。<code>rep</code> 表示形式，即生成的链接可以是十六进制格式也可以是十进制格式，默认值是十进制格式，示例如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">crc16 &amp; hex</span><br><span class=\"line\">https://id.github.io/posts/3ab8.html</span><br><span class=\"line\">crc16 &amp; dec</span><br><span class=\"line\">https://id.github.io/posts/28591.html</span><br><span class=\"line\"></span><br><span class=\"line\">crc32 &amp; hex</span><br><span class=\"line\">https://id.github.io/posts/23ab1cd3.html</span><br><span class=\"line\">crc32 &amp; dec</span><br><span class=\"line\">https://id.github.io/posts/5471416323.html</span><br></pre></td></tr></table></figure>\n<p>注意：在生成之前就要改好算法和形式，不然后面再改的话会导致链接不统一。</p>\n",
            "tags": [
                "Hexo"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/62882.html",
            "url": "https://asanosaki.github.io/posts/62882.html",
            "title": "本科阶段学习生活回顾",
            "date_published": "2022-11-20T03:33:00.000Z",
            "content_html": "<blockquote>\n<p>大学本科生涯即将结束，浅浅地在此总结回忆一下这段生活吧~</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"2019-2020\">2019-2020</h2>\n<h3 id=\"2019-09\">2019.09</h3>\n<blockquote>\n<p>录取的专业是兰州理工大学土木工程学院测绘工程专业，高中同班同学ycy去了同校的电子信息科学与技术专业（我的一志愿，他比我高了一分被他抢了hhh）</p>\n</blockquote>\n<p>第一次出远门挺不适应的，和爸妈分别也是很难受QAQ。</p>\n<p>刚去学校没有经验，和ycy坐动车先到武汉再去兰州，从早上一直坐到晚上十一点多，还花了一千多块，简直俩冤大头。到站后去旅馆住了一晚，第二天早上去学校。</p>\n<blockquote>\n<p>我拖了一个29寸的大行李箱和一个20寸的登机箱，带了被子还有几件厚外套</p>\n</blockquote>\n<p>刚到学校时认不清路，学长学姐来问我时我也没搭理hhh，自己摸索着办完了注册手续然后找到了宿舍楼。</p>\n<p>到学校的第一顿午餐就和ycy干了一碗牛肉面（兰州拉面），感觉不算很好吃，但是确实便宜，5r一碗。</p>\n<p>我的舍友都挺不错的，有一个是福建的老乡lcq，一个辽宁的lty，然后剩下三个是甘肃本地的yhy、bm、zx。</p>\n<p>学校的图书馆确实很大很新，夜景也很漂亮，让人有种想进去学习的欲望~</p>\n<blockquote>\n<p>进入了为期两周的军训生活</p>\n</blockquote>\n<p>军训期间有点累，突然更想家了，加上刚来学校，那段时间就挺难受的。</p>\n<p>军训结束后和舍友出去吃了一顿，顺便逛了一圈（怀念爆发 COVID-19 前的生活）</p>\n<blockquote>\n<p>然后到了大一正式的学习生活</p>\n</blockquote>\n<p>上了大学感觉和高中最明显的差别就是班集体的概念淡化了很多，基本上平时只会和舍友走在一起。</p>\n<p>大一每天早上有跑操，晚上有晚自习，有时候感觉自己像个高中生。</p>\n<p>刚上高数课时感觉大家都很积极，每天前排都老早就被抢了，我和我舍友基本也是抢前两排。</p>\n<p>入学英语考试考进了A班，可以在12月就报名英语四级考试，因此开始每天碎片化背单词。</p>\n<p>课余时间基本都会跑去图书馆，第一次体验在图书馆学习的感觉，和家里氛围确实差太多了，在图书馆基本不会有摸鱼的想法hhh。</p>\n<hr>\n<h3 id=\"2019-10\">2019.10</h3>\n<blockquote>\n<p>国庆节和舍友一起去了成都，在火车上对面做了几个本校的学姐，聊了几句</p>\n</blockquote>\n<p>在成都去了杜甫草堂，感觉不是很有意思，有个舍友特别喜欢文学，在那里面待了一整天。然后去了动物园，第一次看到大熊猫ovo。春熙路、太古里也是必去的地方，还有九眼桥下的整条 Bar 街，属实是体会到了什么叫做灯红酒绿。</p>\n<p>顺便去逛了一圈四川大学，985名校确实大多了，建筑也豪华不少，还在纪念品店买了一些川大纪念品。</p>\n<p>国庆结束后回学校的晚上感觉天气就变凉了不少，感觉已经快和福建的冬天差不多了。</p>\n<hr>\n<h3 id=\"2019-11-2020-01\">2019.11-2020.01</h3>\n<blockquote>\n<p>国庆假期结束后就该好好学习了，什么时候努力都不晚~</p>\n</blockquote>\n<p>大一上册基本就是学那几门必修课，时间比较充裕，因此课余时间基本都在学英语。</p>\n<p>而且当时想着是跨专业考研计算机，考研的话英语也是非常重要的，可以提前积累。</p>\n<p>考四级的时候我的英语词汇量从高中的2k提升到了8k左右，语法还是零基础，买了一套四级练习基本全刷完了，第一次四级也挺顺利的，554分通过！</p>\n<p>四级考完就该准备期末考了。因为对计算机感兴趣，就提前学完了C语言，很多宿舍的人都跑来这问我题目hhh。</p>\n<p>最后大一上的期末考也很顺利，高数和C语言这类高学分的课都是接近满分，总成绩自然也被拉上去了。</p>\n<hr>\n<h3 id=\"2020-02-2020-08\">2020.02-2020.08</h3>\n<blockquote>\n<p>年前去逛了两场漫展，然后差不多就爆发了 COVID-19</p>\n</blockquote>\n<p>过年的这段时间基本天天在家里，然后看新闻，不过我家这边的小县城倒是很安全。</p>\n<p>大一下的开学也延期了，正式开始了网课生活。网课期间我在家也不像在学校那样有动力了，基本天天挂着网课打游戏。</p>\n<p>那时候在肝 Destiny 2 和 GTA V，感觉又找回了高中打游戏的状态hhh。</p>\n<p>到了六月左右学校又通知返校，那时候我的其他高中同学就没听谁说还要返校的。</p>\n<p>回学校抓紧速成了一下，顺利通过期末考，再次回家。</p>\n<hr>\n<h2 id=\"2020-2021\">2020-2021</h2>\n<h3 id=\"2020-09\">2020.09</h3>\n<blockquote>\n<p>这时候通知可以报名转专业，当时感觉转专业可能会很难，但还是报了一下</p>\n</blockquote>\n<p>转专业期间认识了班上的一位女生cl，发现她也报的和我一个专业，然后慢慢地就熟了。</p>\n<p>最后我竟然总分第一成功转来了计算机科学与技术！当时自己也想不到。cl调剂去了同院的大数据。</p>\n<p>那么这时候目标就改成了考研本专业！这样就不用跨专业了。</p>\n<p>大概在同一时间段，大一学年的评奖评优也出来了，评上了校一等奖学金和校三好学生。</p>\n<blockquote>\n<p>转专业后分配了一个转专业宿舍，一共5个人，其中有一个是通信工程的zaj</p>\n</blockquote>\n<p>我转专业后没多久也搬宿舍了，这时候新宿舍只有我和zaj，剩下几位还待原宿舍不搬。</p>\n<p>zaj简直是带动整个宿舍的优良作息规律，必须早睡早起。</p>\n<p>九月份考了因为 COVID-19 影响而推迟的英语六级，最后六级也是一次过，虽然是428分低分飘过hhh。</p>\n<hr>\n<h3 id=\"2020-10-2021-01\">2020.10-2021.01</h3>\n<blockquote>\n<p>继续进入学习状态，学计算机后才知道自学远比上课听讲更有效</p>\n</blockquote>\n<p>转专业后得补修没修过的大一课程，最难搞的就是大物了。</p>\n<p>我也是提前自学完了 C++，然后十月份报名了团体程序设计天梯赛的校赛，排在八十多名，打了个酱油hhh。</p>\n<p>此后开始正式进入算法学习阶段，每天一整天在图书馆都在学算法和写题，后来颈椎都快出问题了。</p>\n<p>在做 C++ 课设的时候认识了lj老师，是个很不错的老师，她知道课设都是我自己写的，答辩的时候就没问我问题了，然后开始和我闲聊，问我是不是想打比赛，然后说之后培训我可以去听。</p>\n<p>到了十二月，先是考了第二次六级，这次511分通过！</p>\n<p>这时候苦练了大概两个月的算法，继续参加了天梯赛校赛，拿了第十名！lj和zh老师说我有潜力。</p>\n<p>同月的 ACM-ICPC 校队选拔赛也是第一名！</p>\n<p>到了学期末的时候另一个舍友hxc也搬过来了，感觉宿舍热闹了一点，还建了一个三个人的小群。</p>\n<p>比赛结束后又是进入紧张的期末备考阶段，然后顺利考完，回家过年。</p>\n<hr>\n<h3 id=\"2021-02-2021-08\">2021.02-2021.08</h3>\n<blockquote>\n<p>大二下学期就进入了比赛的主场了，基本所有比赛的省赛国赛都在下学期</p>\n</blockquote>\n<p>返校后也是复健了一下算法，寒假在家基本又颓废了。</p>\n<p>寒假组队 ICPC 的时候认识了dyy，大学真正的导师来了hhh。</p>\n<p>三四月开始继续训练算法，四月有 ICPC 昆明站、蓝桥杯省赛和天梯赛国赛。</p>\n<p>第一次打 ICPC 的时候属实被打蒙了，感觉自己是个 Newbie，成功打铁。</p>\n<p>蓝桥杯比较水，拿下省一，天梯赛状态也不错，wjt第一名220+，我211分，拿下个人国三。</p>\n<p>打完算法比赛后dyy拉我开发项目参加计算机设计大赛。当时我对开发技术一无所知。</p>\n<p>dyy教我学了 Qt，然后用 C++ 开发了一个线上考试系统，但是当时时间紧张加上我太 Newbie 所以没实现什么功能，但是最后拿下西北赛区一等奖，震惊。</p>\n<p>五月份还和dyy参加了数学建模校赛，教我 Word 的排版方法，感觉要被嫌弃死hhh。之后去了宁夏理工学院参加 ICPC 银川站，成功打铁，但是和dyy还有sy玩的还挺开心的（旅游组实锤）。</p>\n<p>六月打完了蓝桥杯国赛，拿下国二。去了西北工业大学参加 ICPC 全国邀请赛，成功打铁，这次没怎么旅游，还是考完数据库连夜赶过去的，都没休息好。</p>\n<p>大二学年结束后发现成绩排到了专业第一，这时候又改变目标了，要不冲保研吧ovo。</p>\n<hr>\n<h2 id=\"2021-2022\">2021-2022</h2>\n<h3 id=\"2021-09-2022-01\">2021.09-2022.01</h3>\n<blockquote>\n<p>大二结束后暑假没有回家，而是留学校培训数学建模，然后继续训练算法</p>\n</blockquote>\n<p>暑假期间第四个舍友zyj也搬过来了，因为原学院要搬去本部了，被迫驱逐。</p>\n<p>我、hxc、twl、zyj都留校培训数学建模，兰州夏天也是挺难熬的，早上热晚上蚊子多，天天晚上打蚊子打到深夜。</p>\n<p>暑假结束后直接建模国赛，感觉做的很好但是最后没获奖，不会被一眼假了吧，震惊。</p>\n<p>然后就是继续训练算法，十月份的天梯赛校赛拿下第一名。这次校赛鲲鹏有赞助，还拿了个华为耳机ovo。</p>\n<p>到了期末还是冲刺考试，想保研成绩肯定得稳住，学期结束还是稳住了专业第一。</p>\n<hr>\n<h3 id=\"2022-02-2022-08\">2022.02-2022.08</h3>\n<blockquote>\n<p>保研前最后一个学期的冲刺了</p>\n</blockquote>\n<p>这个学期的天梯赛不是cy姥姥出题了，换了一个，难度比之前大了不少，直接翻车车，国赛169分没拿到个人国三，最后是团队国三。</p>\n<p>蓝桥杯国赛 LCA 公式写错，DJ 模板题建成单向边，继续翻车车，只有国三。</p>\n<p>被dyy拉着搞计算机设计大赛，开发一个线上编辑器，我和舍友hxc基本都是划水的。最后拿下西北赛区一等奖，合理，没拿到国奖，震惊。</p>\n<p>到了七月份，突然爆发 COVID-19，7月9日中午还在学校做课设，突然就通知能回家的赶紧回家，提前放假，当晚就和hxc还有dyy到了机场，第二天直接润回家。</p>\n<p>我先飞到了南昌，然后路上还认识了一个江西的也是我们学校的一个能动院学弟，最后没有留联系方式，短暂的缘分hhh。</p>\n<hr>\n<h2 id=\"2022-2023\">2022-2023</h2>\n<h3 id=\"2022-09-2023-01\">2022.09-2023.01</h3>\n<p>九月突然爆发 COVID-19，没办法返校，保研的事情线上办完了，最后也是以第一名的总分保研了。</p>\n<p>9月28日，填了推免系统信息，最后录取了西安电子科技大学的人工智能学院，也算是给过去三年的努力画上了句号。</p>\n<p>十月除学校组织返校，我去福州度假了hhh，就申请了不返校，结果这波是我预判了兰州的预判。</p>\n<p>这波返校后兰州 COVID-19 大爆发，一直到现在十一月都快结束了，那边还很严重，突然感觉当时没返校待家里是多幸福的一件事。</p>\n<h3 id=\"2023-02-2023-06\">2023.02-2023.06</h3>\n<blockquote>\n<p>一转眼就到了本科的最后一个学期了呀，不得不感慨时间过的真的太快了</p>\n</blockquote>\n<p>这是本科生涯的最后一次返校了，有大半年没见到舍友了。</p>\n<p>通信的舍友zaj考研南理工353分，本来我们寝室几个舍友还都挺开心的，结果出来复试线是350分后大家都惊了，去年复试线是325，今年由于 COVID-19 原因很多考研er都很不容易，我们宿舍本以为复试线大概会在320左右顶天了，350也是真的从来没想过。</p>\n<p>擦线进复试，他情绪瞬间低落到谷底，开始纠结要不要去线下复试，我们其他几个安慰了好久最后还是决定冲一把，由于hxc之前大创挂过他的名字，因此就给他讲了很多项目内容帮助他充实简历和自我介绍，实验部分找了对门的zsy来辅导，就这么持续了一两周后他就去南京复试了，但是很遗憾最后还是被刷了。</p>\n<p>复试完他先回家休息了几天才来学校，在这期间我和hxc商量着等他回来咱宿舍一起去吃顿饭，然后我俩一起请他，他回来后我们去吃了一顿海底捞，还叫上了隔壁班的好兄弟nzx，吃完后去兰州老街逛了一圈，在回学校的路上下起了雨夹雪，105路车上的显示屏放的是去年祝福兰理工毕业生的海报，感觉在车上心情复杂，即将要毕业了。</p>\n<p>zaj过了一段时间恢复心态后决定二战，我们也很支持他，hxc是今年一战，因为去年秋招就业不太好找工作，所以他们俩就互相鼓励，最后一定会上岸的！！！</p>\n<blockquote>\n<p>2023.04.17这也是在学校过的最后一个生日了</p>\n</blockquote>\n<p>生日的时候cl给我买了个蛋糕，当天把蛋糕带到了东三和学弟们一起吃，wfy和cxq分别还带了小红包和零食，学弟们真的都很好。</p>\n<p>四五月份的主要工作就是毕业设计了，我的题目是《基于机器学习的北半球积雪覆盖数据分析系统》，指导老师是lj，对于这个毕设我还是比较划水的，因为主要心思还是放在学习未来研究方向的基础，然后师兄偶尔也会给我派点小活，这个学期基本都在创客和学弟们吹水水，偶尔还会出去玩玩。</p>\n<p>这段时间真的过的很快很快，从开题报告到中期答辩很快就到了终期答辩，老师让我和dyy去6.6的系级答辩，然后我俩都进了院级答辩，时间就在第二天6.7，最后我俩在院级答辩时被刷了，这下是真正毕业啦！</p>\n<p>答辩完后由于其他同学都是在6.12才小组答辩，我就先和对门的zsy出去拍了几张照片（偷穿他们电信院的学士服hhh）。</p>\n<p>其实那段时间还挺欢乐的，并没有多想什么，很快就到了6.16，这是我们院集体拍毕业照的时候，早上我们班拍完后下午我们宿舍就约上zsy一起出去拍，还叫上了学弟们一起来拍，拍的过程还是很欢乐的，傍晚和nzx回红柳创客梦工厂把照片导出来，并且挑了一部分比较好的稍微修了一下（其实很多照片都很nice！），这时候看到这些照片中大家的笑容后心情就变得有些复杂了起来，可能这就是定格我们本科时候的最后一组照片了。</p>\n<p>过了两天cl约上我和hxc出去用手机拍了几张，还叫上了dyy，到了6.20就是我们院拿双证的时候了，拿证时遇到了曾经一起搞数学建模的女同学qmh，她也拉着我还有身边的zaj一起在求是楼5楼很简单地拍了一张。</p>\n<p>6.21一大早七点zaj就要走了，刚好我醒了，伸出头看了一眼，他说了句再见了兄弟们，然后就出门了，简简单单的告别，瞬间让我感觉到了太多的不舍，随后我和nzx去本部给成绩单盖章，我拉上了原专业的舍友也是简单地拍了几张合影，可惜福建老乡老早就润回家了，他考上了福大，剩下几个考上的学校都是在西安，还有一个找工作也是找在了西安那边，大家未来还有很多机会团建hhh。</p>\n<p>江西舍友zyj在中午的时候也走了，我那时候还在本部，下午回来后，简单在宿舍和hxc随便聊了会，到了差不多六点，他也准备走了，我和他还有nzx一起去东门吃了顿“叫了只炸鸡”，吃完后我和nzx就送他上车了，还是很简单的告别，走了兄弟们，以后再见。</p>\n<p>这晚回宿舍后就剩我一个人了，作为最后一个走的确实感觉到有很多的不舍，一地凌乱的垃圾，被搬空的床位，还有空荡荡的桌子…</p>\n<p>6.21晚上几个学弟们把我叫出去通宵，先是去玩了剧本杀，然后去了一家酒吧喝到了早上六点，这一晚过的也很快乐，这段时间nlh动不动就说“易哥你延毕吧我舍不得你”，虽然说的时候感觉都是在说说笑笑，但是内心确实多少会有波澜，只是真的没办法阻挡时间的流逝。</p>\n<p>早上回学校后头有点晕就去小睡了一会，起来后也差不多要到我要走的时候了，这时候对门zsy也还没走，突然过来和我说了一句“昨天几句很简单的话语不经意间就送走了好多兄弟”，我说没事以后还有机会再见的，他说“话都是这么说，实际上很难再完整地聚在一块了”。</p>\n<p>这时候内心的心情真的很难受，看着宿舍，一幕幕往事瞬间浮现出来，确实，未来真的有机会再见吗？要几年后，五年还是十年？还是等到有人结婚？</p>\n<p>临走之前拍了一张宿舍的照片，还有对门zsy的宿舍，zsy刚好不在宿舍，就给他发了条简单的消息，真是造化弄人唉。</p>\n<p>nzx送我去南门坐105路，上车后也是简单的告别，祝我路上注意安全，一路顺风。</p>\n<p>在车上越想越难受，流了眼泪但是没滴下来，果然人多想就会陷入其中，不过最后还是得离开这边，再见了兰州。</p>\n<h3 id=\"写在最后\">写在最后</h3>\n<p>2019~2023，四年大学时光到此告一段落，并不后悔不远千里来到了兰州，当然并不是因为学校也不是因为兰州，是因为在这里遇到的朋友，勤奋上进还有趣的所有舍友们（hxc、zaj、zyj、twl）、严厉但是热心而且实力超强的dyy、外向热心但是说话攻击性极强的cl、说话亲切幽默还有实力的nzx、帅气的苏州富豪学霸zsy、班级唯一一个主动找我聊天的女生lr、幽默风趣动不动闯到我们宿舍来的wh、帅气的全栈大佬lsl、在各个协会风生水起的铁兄弟nlh、经常送礼十分热情的cxq、物联网算法卷王也很热情的wfy、气场强大的syh、整天修仙看妹妹的ycl、创客第一大帅哥ylq、…</p>\n<p>虽然不善于交际，认识的人不算多，但是完全足够了，每一个朋友都值得作为一份永久的回忆，天下没有不散的筵席，大家最后都会各奔东西，为了各自的生活继续努力着，每个人都只是去到祖国最需要他们的地方了，但是希望未来真的还能有机会再见面，不管在多少年后。</p>\n<p>如舍友hxc所说的，不怀念兰州，怀念兄弟们，怀念朋友们。</p>\n",
            "tags": [
                "Essay"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/47192.html",
            "url": "https://asanosaki.github.io/posts/47192.html",
            "title": "Hexo搭建Github博客教程",
            "date_published": "2022-11-19T16:11:00.000Z",
            "content_html": "<blockquote>\n<p>使用 Hexo 搭建我的 Github 个人网站：<a href=\"https://asanosaki.github.io/\">My Github Blog</a>。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-环境配置\">1. 环境配置</h2>\n<p>（1）安装 Git Bash：<a href=\"/posts/31252.html\">Windows 安装配置 Git 教程</a>。</p>\n<p>（2）安装 NodeJS：<a href=\"/posts/11062.html\">NodeJS 的安装及配置</a>。</p>\n<p>（3）修改 <code>npm</code> 镜像源：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm config set registry https://registry.npm.taobao.org</span><br></pre></td></tr></table></figure>\n<p>（4）安装 Hexo：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install hexo-cli -g</span><br></pre></td></tr></table></figure>\n<p>（5）安装部署插件：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>\n<p>此处如果出现 <code>npm ERR! Error: EPERM: operation not permitted, mkdir 'E:\\NodeJS\\node_modules\\.corepack-xTCBGLKh</code> 之类的错误需要以管理员身份打开 <code>cmd</code>，然后在 <code>cmd</code> 中安装。</p>\n<h2 id=\"2-本地博客搭建\">2. 本地博客搭建</h2>\n<p>首先创建文件夹 <code>Hexo</code>，然后进入该文件夹，创建文件夹 <code>blog</code>，使用管理员身份打开 <code>cmd</code>，进入 <code>blog</code> 文件夹，初始化 Hexo 博客：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo init</span><br></pre></td></tr></table></figure>\n<p>然后在本地启动一下看看效果：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo s</span><br></pre></td></tr></table></figure>\n<p>然后打开链接：<code>http://localhost:4000/</code> 查看一下页面内容，之后我们进行页面调试都是在这个本地链接进行的。</p>\n<p>使用 VS Code 打开 <code>blog</code> 文件夹，其中，<code>source/_posts</code> 文件夹下存放我们写的文章，<code>themes</code> 文件夹存放博客的主题，<code>_config.yml</code> 是博客的全局配置文件，<code>_config.landscape.yml</code> 是博客的主题配置文件。</p>\n<h2 id=\"3-部署至Github\">3. 部署至Github</h2>\n<p>在 Github 创建一个名为 <code>用户名.github.io</code> 的仓库，例如：<code>AsanoSaki.github.io</code></p>\n<p>在 VS Code 中打开 <code>blog</code> 文件夹，找到 <code>_config.yml</code> 文件，找到 <code>deploy</code>，按照以下格式进行修改：</p>\n<figure class=\"highlight yml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">deploy:</span></span><br><span class=\"line\">  <span class=\"attr\">type:</span> <span class=\"string\">git</span></span><br><span class=\"line\">  <span class=\"attr\">repo:</span> <span class=\"string\">git@github.com:你的用户名/你的用户名.github.io.git</span></span><br><span class=\"line\">  <span class=\"attr\">branch:</span> <span class=\"string\">master</span></span><br></pre></td></tr></table></figure>\n<p>最后执行以下命令：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo clean  # 清除缓存</span><br><span class=\"line\">hexo g      # 生成静态网页</span><br><span class=\"line\">hexo d      # 部署到Github，注意需要在Git Bash中部署</span><br></pre></td></tr></table></figure>\n<p>在部署的时候如果出现警告：<code>LF will be replaced by CRLF the next time Git touches it</code>，可以用以下指令禁用将 LF 自动转换为 CRLF：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git config --global core.autocrlf false</span><br></pre></td></tr></table></figure>\n<p>然后访问域名：<code>https://用户名.github.io/</code> 即可进入自己的博客啦。</p>\n<h2 id=\"4-博客主题设置\">4. 博客主题设置</h2>\n<p>Hexo 主题官网：<a href=\"https://hexo.io/themes/\">Hexo Themes</a>。</p>\n<p>将下载好的主题放到 <code>blog/themes</code> 文件夹中，然后将根目录下的 <code>_config.yml</code> 中的主题修改为下载的主题即可，例如：</p>\n<figure class=\"highlight yml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">theme:</span> <span class=\"string\">shoka</span></span><br></pre></td></tr></table></figure>\n<p>然后进入主题的文件夹，该文件夹下也有一个 <code>_config.yml</code> 文件，修改这个文件的内容即可修改当前博客主页的样式。</p>\n<h2 id=\"5-博客备份\">5. 博客备份</h2>\n<p>在 Github 新建一个名为 <code>blog</code> 的私有仓库，然后在 <code>Hexo/blog</code> 目录下打开 Git Bash，执行以下命令：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git init  <span class=\"comment\"># 初始化仓库</span></span><br><span class=\"line\">git add .  <span class=\"comment\"># 添加文件到暂存区</span></span><br><span class=\"line\">git commit -m <span class=\"string\">&quot;initial blog&quot;</span>  <span class=\"comment\"># 将暂存区内容添加到仓库</span></span><br><span class=\"line\">git branch -M main  <span class=\"comment\"># 重命名分支为main，和博客的分支master区分开</span></span><br><span class=\"line\">git remote add origin git@github.com:你的ID/blog.git  <span class=\"comment\"># 添加到远程版本库</span></span><br><span class=\"line\">git push -u origin main  <span class=\"comment\"># 提交到Github，注意只有第一次提交需要用这个指令</span></span><br><span class=\"line\">git push  <span class=\"comment\"># 之后提交用这个指令</span></span><br></pre></td></tr></table></figure>\n<p>如果在执行 <code>git add .</code> 时出现警告：<code>You've added another git repository inside your current repository.</code>，可以将 <code>blog</code> 目录中的 <code>.deploy_git</code> 文件夹删除。</p>\n",
            "tags": [
                "Hexo"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/60453.html",
            "url": "https://asanosaki.github.io/posts/60453.html",
            "title": "Web学习笔记-React",
            "date_published": "2022-11-18T10:00:00.000Z",
            "content_html": "<blockquote>\n<p>本文记录 React 的学习过程。<br>\nReact 是一个用于构建用户界面的库。React 不是一个框架，它的应用甚至不局限于 Web 开发，它可以与其他库一起使用以渲染到特定环境。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-React配置环境\">1. React配置环境</h2>\n<p>React 官网：<a href=\"https://zh-hans.reactjs.org/\">React</a>。</p>\n<p>React 是一个声明式，高效且灵活的用于构建用户界面的 JavaScript 库。使用 React 可以将一些简短、独立的代码片段组合成复杂的 UI 界面，这些代码片段被称作 <code>components</code>。React 能够构建那些数据会随时间改变的大型应用。</p>\n<p>React 特性：</p>\n<ul>\n<li>React 为了能够方便地去维护我们的页面，它在内存里面创建了一个虚拟的 DOM 树：<code>Virtual DOM</code>，这是一个轻量级的虚拟的 DOM，就是 React 抽象出来的一个对象，描述 DOM 应该什么样子的，应该如何呈现。通过这个 <code>Virtual DOM</code> 去更新真实的 DOM，由这个 <code>Virtual DOM</code> 管理真实 DOM 的更新。</li>\n<li>数据驱动：当某一个元素里的数据发生变化后，React 会重新将有可能修改的元素都修改一遍，然后与真实的 DOM 树对比是否有区别，React 分析完后最终只会修改真实改变的结点。由于在内存里修改对象的速度很快，因此 React 效率很高。</li>\n<li>React 一般不直接手写 JS，而是通过编写 JSX 文件，JSX 比 JS 更好写一点，React 会先将 JSX 编译成 JS。</li>\n</ul>\n<p>（1）安装 <code>Git Bash</code>：<a href=\"/posts/31252.html\">Windows 安装配置 Git 教程</a>。</p>\n<p>（2）安装 <code>NodeJS</code>：<a href=\"/posts/11062.html\">NodeJS 的安装及配置</a>。</p>\n<p>（3）安装 <code>create-react-app</code>：</p>\n<p>打开 <code>Git bash</code>，执行以下命令：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm i -g create-react-app</span><br></pre></td></tr></table></figure>\n<p>如果速度很慢，可以先修改镜像源再尝试安装：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm config set registry https://registry.npm.taobao.org</span><br><span class=\"line\">npm i -g create-react-app</span><br></pre></td></tr></table></figure>\n<p>如果安装完成后出现警告：<code>npm WARN deprecated tar@2.2.2: This version of tar is no longer supported, and will not receive security updates. Please upgrade asap.</code>，可以先更新 <code>tar</code> 试试：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install -g tar</span><br></pre></td></tr></table></figure>\n<p>如果还是有警告，且创建项目时（例如执行 <code>create-react-app react-app</code>）报错：<code>bash: create-react-app: command not found</code>，使用 <code>npx</code> 创建项目：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npx create-react-app my-app</span><br></pre></td></tr></table></figure>\n<p>或者用 <code>npm</code> 创建项目：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm init react-app my-app</span><br></pre></td></tr></table></figure>\n<p>创建好后进入项目文件夹启动项目：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd my-app</span><br><span class=\"line\">npm start</span><br></pre></td></tr></table></figure>\n<p>启动后访问 <code>localhost:3000</code> 即可访问页面，<code>ctrl+c</code> 可停止服务。</p>\n<p>（4）配置 VS Code 插件：Simple React Snippets、Prettier - Code formatter</p>\n<p><code>Simple React Snippets</code> 为 React 智能化自动补全插件。</p>\n<p>例如输入 <code>imrc</code> 即可补全出以下内容：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"title class_\">React</span>, &#123; <span class=\"title class_\">Component</span> &#125; <span class=\"keyword\">from</span> <span class=\"string\">&#x27;react&#x27;</span>;</span><br></pre></td></tr></table></figure>\n<p>输入 <code>cc</code> 即可补全出以下内容：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Example</span> <span class=\"keyword\">extends</span> <span class=\"title class_ inherited__\">Component</span> &#123;</span><br><span class=\"line\">    state = &#123;  &#125; </span><br><span class=\"line\">    <span class=\"title function_\">render</span>(<span class=\"params\"></span>) &#123; </span><br><span class=\"line\">        <span class=\"keyword\">return</span> ();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"keyword\">default</span> <span class=\"title class_\">Example</span>;</span><br></pre></td></tr></table></figure>\n<p><code>Prettier - Code formatter</code> 为代码格式化插件。</p>\n<p>（5）创建 <code>React App</code>：</p>\n<p>在目标目录下右键打开 Git Bash，在终端中执行：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npx create-react-app react-app  # react-app是新建项目的名字，可以替换为其他名称</span><br><span class=\"line\">cd react-app</span><br><span class=\"line\">npm start  # 启动应用</span><br></pre></td></tr></table></figure>\n<p>启动成功后会在本地开一个3000端口，页面效果已在上文展示。此时使用 VS Code 打开 <code>react-app</code> 文件夹可以看到项目的目录结构。</p>\n<p>其中，<code>node_modules</code> 用来维护各种 JS 库，未来安装的所有依赖项都会放在该文件夹下；<code>public</code> 中的 <code>index.html</code> 就是我们未来渲染出的页面，该文件中只有一个 <code>&lt;div id=&quot;root&quot;&gt;&lt;/div&gt;</code>；<code>src</code> 中的 <code>index.js</code> 代码如下：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"title class_\">React</span> <span class=\"keyword\">from</span> <span class=\"string\">&#x27;react&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"title class_\">ReactDOM</span> <span class=\"keyword\">from</span> <span class=\"string\">&#x27;react-dom/client&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"string\">&#x27;./index.css&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"title class_\">App</span> <span class=\"keyword\">from</span> <span class=\"string\">&#x27;./App&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">const</span> root = <span class=\"title class_\">ReactDOM</span>.<span class=\"title function_\">createRoot</span>(<span class=\"variable language_\">document</span>.<span class=\"title function_\">getElementById</span>(<span class=\"string\">&#x27;root&#x27;</span>));  <span class=\"comment\">// 获取div同时创建为一个React对象</span></span><br><span class=\"line\">root.<span class=\"title function_\">render</span>(</span><br><span class=\"line\">  <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">React.StrictMode</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">App</span> /&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">  <span class=\"tag\">&lt;/<span class=\"name\">React.StrictMode</span>&gt;</span></span></span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure>\n<p>其中 <code>App</code> 的定义在 <code>App.js</code> 中：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> logo <span class=\"keyword\">from</span> <span class=\"string\">&#x27;./logo.svg&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"string\">&#x27;./App.css&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">function</span> <span class=\"title function_\">App</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">  <span class=\"keyword\">return</span> (</span><br><span class=\"line\">    <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">className</span>=<span class=\"string\">&quot;App&quot;</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;<span class=\"name\">header</span> <span class=\"attr\">className</span>=<span class=\"string\">&quot;App-header&quot;</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">        <span class=\"tag\">&lt;<span class=\"name\">img</span> <span class=\"attr\">src</span>=<span class=\"string\">&#123;logo&#125;</span> <span class=\"attr\">className</span>=<span class=\"string\">&quot;App-logo&quot;</span> <span class=\"attr\">alt</span>=<span class=\"string\">&quot;logo&quot;</span> /&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">        <span class=\"tag\">&lt;<span class=\"name\">p</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">          Edit <span class=\"tag\">&lt;<span class=\"name\">code</span>&gt;</span>src/App.js<span class=\"tag\">&lt;/<span class=\"name\">code</span>&gt;</span> and save to reload.</span></span><br><span class=\"line\"><span class=\"language-xml\">        <span class=\"tag\">&lt;/<span class=\"name\">p</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">        <span class=\"tag\">&lt;<span class=\"name\">a</span></span></span></span><br><span class=\"line\"><span class=\"tag\"><span class=\"language-xml\">          <span class=\"attr\">className</span>=<span class=\"string\">&quot;App-link&quot;</span></span></span></span><br><span class=\"line\"><span class=\"tag\"><span class=\"language-xml\">          <span class=\"attr\">href</span>=<span class=\"string\">&quot;https://reactjs.org&quot;</span></span></span></span><br><span class=\"line\"><span class=\"tag\"><span class=\"language-xml\">          <span class=\"attr\">target</span>=<span class=\"string\">&quot;_blank&quot;</span></span></span></span><br><span class=\"line\"><span class=\"tag\"><span class=\"language-xml\">          <span class=\"attr\">rel</span>=<span class=\"string\">&quot;noopener noreferrer&quot;</span></span></span></span><br><span class=\"line\"><span class=\"tag\"><span class=\"language-xml\">        &gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">          Learn React</span></span><br><span class=\"line\"><span class=\"language-xml\">        <span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">      <span class=\"tag\">&lt;/<span class=\"name\">header</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span></span><br><span class=\"line\">  );</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"keyword\">default</span> <span class=\"title class_\">App</span>;</span><br></pre></td></tr></table></figure>\n<p>该 <code>App</code> 组件就定义了页面的具体内容，且我们能够发现该 JS 文件中有 HTML 代码，因此该文件即为 JSX 文件，能够在 JavaScript 的基础上支持 XML（可扩展标记语言），HTML 也是一种特殊的 XML。</p>\n<p>JSX 是 React 中的一种语言，会被 <a href=\"https://babeljs.io/repl/#?browsers=defaults,%20not%20ie%2011,%20not%20ie_mob%2011&amp;build=&amp;builtIns=false&amp;corejs=3.21&amp;spec=false&amp;loose=false&amp;code_lz=MYewdgzgLgBApgGzgWzmWBeGAeAFgRgD4AJRBEAGhgHcQAnBAEwEJsB6AwgbgChRJY_KAEMAlmDh0YWRiGABXVOgB0AczhQAokiVQAQgE8AkowAUAcjogQUcwEpeAJTjDgUACIB5ALLK6aRklTRBQ0KCohMQk6Bx4gA&amp;debug=false&amp;forceAllTransforms=false&amp;shippedProposals=false&amp;circleciRepo=&amp;evaluate=false&amp;fileSize=false&amp;timeTravel=false&amp;sourceType=module&amp;lineWrap=true&amp;presets=react&amp;prettier=false&amp;targets=&amp;version=7.20.4&amp;externalPlugins=&amp;assumptions=%7B%7D\">Babel</a> 编译成标准的 JavaScript。</p>\n<h2 id=\"2-ES6语法补充\">2. ES6语法补充</h2>\n<p>ES6，全称 ECMAScript 6.0，是 JavaScript 的版本标准。此处添加一些 React 中常用的语法糖。</p>\n<p><strong>（1）使用 <code>bind()</code> 函数绑定 <code>this</code> 取值</strong></p>\n<p>在 JavaScript 中，函数里的 <code>this</code> 指向的是执行时的调用者，而非定义时所在的对象。例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">const</span> person = &#123;</span><br><span class=\"line\">  <span class=\"attr\">name</span>: <span class=\"string\">&quot;abc&quot;</span>,</span><br><span class=\"line\">  <span class=\"attr\">talk</span>: <span class=\"keyword\">function</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">    <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(<span class=\"variable language_\">this</span>);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">person.<span class=\"title function_\">talk</span>();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">const</span> talk = person.<span class=\"property\">talk</span>;</span><br><span class=\"line\"><span class=\"title function_\">talk</span>();</span><br></pre></td></tr></table></figure>\n<p>运行结果为：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;name: &#x27;abc&#x27;, talk: f&#125;</span><br><span class=\"line\">undefined</span><br></pre></td></tr></table></figure>\n<p>使用 <code>bind()</code> 函数绑定 <code>this</code> 的取值为 <code>person</code>。例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">const</span> talk = person.<span class=\"property\">talk</span>.<span class=\"title function_\">bind</span>(person);</span><br></pre></td></tr></table></figure>\n<p>运行结果为：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;name: &#x27;abc&#x27;, talk: f&#125;</span><br><span class=\"line\">&#123;name: &#x27;abc&#x27;, talk: f&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>（2）箭头函数的简写方式</strong></p>\n<p>当函数参数只有一个时可以将括号去掉，当函数体只有一个 <code>return</code> 语句时可以把 <code>return</code> 和 <code>&#123;&#125;</code> 一起去掉，例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">const</span> <span class=\"title function_\">f</span> = (<span class=\"params\">x</span>) =&gt; &#123;</span><br><span class=\"line\">  <span class=\"keyword\">return</span> x * x;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>等价于：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">const</span> <span class=\"title function_\">f</span> = x =&gt; x * x;</span><br></pre></td></tr></table></figure>\n<p><strong>（3）箭头函数不重新绑定 <code>this</code> 的取值</strong></p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">const</span> person = &#123;</span><br><span class=\"line\">  <span class=\"attr\">talk</span>: <span class=\"keyword\">function</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">    <span class=\"built_in\">setTimeout</span>(<span class=\"keyword\">function</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">      <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(<span class=\"variable language_\">this</span>);</span><br><span class=\"line\">    &#125;, <span class=\"number\">1000</span>);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">person.<span class=\"title function_\">talk</span>();  <span class=\"comment\">// 输出Window</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">const</span> person = &#123;</span><br><span class=\"line\">  <span class=\"attr\">talk</span>: <span class=\"keyword\">function</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">    <span class=\"built_in\">setTimeout</span>(<span class=\"function\">() =&gt;</span> &#123;</span><br><span class=\"line\">      <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(<span class=\"variable language_\">this</span>);</span><br><span class=\"line\">    &#125;, <span class=\"number\">1000</span>);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">person.<span class=\"title function_\">talk</span>();  <span class=\"comment\">// 输出 &#123;talk: f&#125;</span></span><br></pre></td></tr></table></figure>\n<p><strong>（4）对象的解构</strong></p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">const</span> person = &#123;</span><br><span class=\"line\">  <span class=\"attr\">name</span>: <span class=\"string\">&quot;abc&quot;</span>,</span><br><span class=\"line\">  <span class=\"attr\">age</span>: <span class=\"number\">18</span>,</span><br><span class=\"line\">  <span class=\"attr\">height</span>: <span class=\"number\">180</span>,</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">const</span> &#123;name : new_name, age&#125; = person;  <span class=\"comment\">// new_name是name的别名</span></span><br></pre></td></tr></table></figure>\n<p><strong>（5）数组和对象的展开</strong></p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">let</span> a = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>];</span><br><span class=\"line\"><span class=\"keyword\">let</span> b = [...a];  <span class=\"comment\">// b是a的复制，和a不是一个数组，如果写let b = a则b和a是同一个数组</span></span><br><span class=\"line\"><span class=\"keyword\">let</span> c = [...a, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>];  <span class=\"comment\">// c = [1, 2, 3, 4, 5, 6]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">let</span> d = &#123;<span class=\"attr\">name</span>: <span class=\"string\">&quot;abc&quot;</span>&#125;;</span><br><span class=\"line\"><span class=\"keyword\">let</span> e = &#123;<span class=\"attr\">age</span>: <span class=\"number\">18</span>, <span class=\"attr\">height</span>: <span class=\"number\">180</span>&#125;;</span><br><span class=\"line\"><span class=\"keyword\">let</span> f = &#123;...d, ...e, <span class=\"attr\">weight</span>: <span class=\"number\">120</span>&#125;;  <span class=\"comment\">// f = &#123;name: &quot;abc&quot;, age: 18, height: 180, weight: 120&#125;</span></span><br></pre></td></tr></table></figure>\n<p><strong>（6）Named exports 与 Default exports</strong></p>\n<ul>\n<li><code>Named Export</code>：可以 <code>export</code> 多个，<code>import</code> 的时候需要加大括号，名称需要匹配，即之前使用的方式。</li>\n<li><code>Default Export</code>：最多 <code>export</code> 一个，<code>import</code> 的时候不需要加大括号，可以直接定义别名。</li>\n</ul>\n<h2 id=\"3-Components\">3. Components</h2>\n<p><strong>（1）创建项目</strong></p>\n<p>首先创建一个新项目 <code>box-app</code>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npx create-react-app box-app</span><br><span class=\"line\">cd box-app</span><br><span class=\"line\">npm start</span><br></pre></td></tr></table></figure>\n<p>安装 <code>bootstrap</code> 库：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm i bootstrap</span><br></pre></td></tr></table></figure>\n<p><code>bootstrap</code> 的引入方式：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"string\">&#x27;bootstrap/dist/css/bootstrap.css&#x27;</span>;</span><br></pre></td></tr></table></figure>\n<p><strong>（2）创建 Component</strong></p>\n<p>在 <code>src</code> 文件夹中创建一个文件夹 <code>components</code> 存放组件，然后在 <code>components</code> 文件夹中创建一个 JSX 文件 <code>box.jsx</code>（使用 <code>.js</code> 后缀也一样，只是用 <code>.jsx</code> 后区分起来跟明显一点）其框架如下：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"title class_\">React</span>, &#123; <span class=\"title class_\">Component</span> &#125; <span class=\"keyword\">from</span> <span class=\"string\">&#x27;react&#x27;</span>;  <span class=\"comment\">// 输入imrc即可补全</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Box</span> <span class=\"keyword\">extends</span> <span class=\"title class_ inherited__\">Component</span> &#123;  <span class=\"comment\">// 输入cc即可补全</span></span><br><span class=\"line\">    state = &#123;  &#125; </span><br><span class=\"line\">    <span class=\"title function_\">render</span>(<span class=\"params\"></span>) &#123;  <span class=\"comment\">// Component类的函数，用来返回当前组件最后渲染的HTML结构是什么</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> (<span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">h1</span>&gt;</span>Hello World!<span class=\"tag\">&lt;/<span class=\"name\">h1</span>&gt;</span></span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"keyword\">default</span> <span class=\"title class_\">Box</span>;</span><br></pre></td></tr></table></figure>\n<p>然后我们需要在 <code>index.js</code> 中将组件渲染出来：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"title class_\">React</span> <span class=\"keyword\">from</span> <span class=\"string\">&#x27;react&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"title class_\">ReactDOM</span> <span class=\"keyword\">from</span> <span class=\"string\">&#x27;react-dom/client&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"string\">&#x27;./index.css&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"string\">&#x27;bootstrap/dist/css/bootstrap.css&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"title class_\">Box</span> <span class=\"keyword\">from</span> <span class=\"string\">&#x27;./components/box&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">const</span> root = <span class=\"title class_\">ReactDOM</span>.<span class=\"title function_\">createRoot</span>(<span class=\"variable language_\">document</span>.<span class=\"title function_\">getElementById</span>(<span class=\"string\">&#x27;root&#x27;</span>));</span><br><span class=\"line\">root.<span class=\"title function_\">render</span>(</span><br><span class=\"line\">  <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">React.StrictMode</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">    <span class=\"tag\">&lt;<span class=\"name\">Box</span> /&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">  <span class=\"tag\">&lt;/<span class=\"name\">React.StrictMode</span>&gt;</span></span></span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure>\n<p><strong>（3）创建按钮</strong></p>\n<p>由于 Component 中的 <code>render()</code> 函数只能 <code>return</code> 一个元素，因此当子节点数量大于1时，可以用 <code>&lt;div&gt;</code> 或 <code>&lt;React.Fragment&gt;</code> 将其括起来。</p>\n<p><strong>（4）内嵌表达式</strong></p>\n<p>JSX 中使用 <code>&#123;&#125;</code> 在 HTML 标签中嵌入表达式。</p>\n<p><strong>（5）设置属性</strong></p>\n<p>由于 <code>class</code> 是 JS 中的关键字，因此 HTML 标签中的 <code>class</code> 需要改为 <code>className</code>。CSS 属性也需要修改，例如：<code>background-color</code> 修改为 <code>backgroundColor</code>，其它属性也是类似的。</p>\n<p>以上的综合示例：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"title class_\">React</span>, &#123; <span class=\"title class_\">Component</span> &#125; <span class=\"keyword\">from</span> <span class=\"string\">&#x27;react&#x27;</span>;  <span class=\"comment\">// 输入imrc即可补全</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Box</span> <span class=\"keyword\">extends</span> <span class=\"title class_ inherited__\">Component</span> &#123;  <span class=\"comment\">// 输入cc即可补全</span></span><br><span class=\"line\">    state = &#123; </span><br><span class=\"line\">        <span class=\"attr\">x</span>: <span class=\"number\">0</span></span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    styles = &#123;</span><br><span class=\"line\">        <span class=\"attr\">width</span>: <span class=\"string\">&#x27;50px&#x27;</span>,</span><br><span class=\"line\">        <span class=\"attr\">height</span>: <span class=\"string\">&#x27;50px&#x27;</span>,</span><br><span class=\"line\">        <span class=\"attr\">backgroundColor</span>: <span class=\"string\">&#x27;lightblue&#x27;</span>,</span><br><span class=\"line\">        <span class=\"attr\">color</span>: <span class=\"string\">&#x27;white&#x27;</span>,</span><br><span class=\"line\">        <span class=\"attr\">textAlign</span>: <span class=\"string\">&#x27;center&#x27;</span>,</span><br><span class=\"line\">        <span class=\"attr\">lineHeight</span>: <span class=\"string\">&#x27;50px&#x27;</span>,</span><br><span class=\"line\">        <span class=\"attr\">borderRadius</span>: <span class=\"string\">&#x27;5px&#x27;</span></span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"title function_\">render</span>(<span class=\"params\"></span>) &#123;  <span class=\"comment\">// Component类的函数，用来返回当前组件最后渲染的HTML结构是什么</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> (</span><br><span class=\"line\">        <span class=\"comment\">// HTML标签中可以使用&#123;&#125;写一个表达式</span></span><br><span class=\"line\">        <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">React.Fragment</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">style</span>=<span class=\"string\">&#123;this.styles&#125;</span>&gt;</span>&#123;this.state.x&#125;<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;<span class=\"name\">button</span> <span class=\"attr\">className</span>=<span class=\"string\">&#x27;btn btn-primary m-2&#x27;</span>&gt;</span>Left<span class=\"tag\">&lt;/<span class=\"name\">button</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;<span class=\"name\">button</span> <span class=\"attr\">className</span>=<span class=\"string\">&#x27;btn btn-success m-2&#x27;</span>&gt;</span>Right<span class=\"tag\">&lt;/<span class=\"name\">button</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">        <span class=\"tag\">&lt;/<span class=\"name\">React.Fragment</span>&gt;</span></span></span><br><span class=\"line\">        );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"keyword\">default</span> <span class=\"title class_\">Box</span>;</span><br></pre></td></tr></table></figure>\n<p><strong>（6）数据驱动改变 Style</strong></p>\n<p>例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"title class_\">React</span>, &#123; <span class=\"title class_\">Component</span> &#125; <span class=\"keyword\">from</span> <span class=\"string\">&#x27;react&#x27;</span>;  <span class=\"comment\">// 输入imrc即可补全</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Box</span> <span class=\"keyword\">extends</span> <span class=\"title class_ inherited__\">Component</span> &#123;  <span class=\"comment\">// 输入cc即可补全</span></span><br><span class=\"line\">    state = &#123; </span><br><span class=\"line\">        <span class=\"attr\">x</span>: <span class=\"number\">1</span></span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"title function_\">render</span>(<span class=\"params\"></span>) &#123;  <span class=\"comment\">// Component类的函数，用来返回当前组件最后渲染的HTML结构是什么</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> (</span><br><span class=\"line\">        <span class=\"comment\">// HTML标签中可以使用&#123;&#125;写一个表达式</span></span><br><span class=\"line\">        <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">React.Fragment</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">style</span>=<span class=\"string\">&#123;this.getStyles()&#125;</span>&gt;</span>&#123;this.state.x&#125;<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;<span class=\"name\">button</span> <span class=\"attr\">className</span>=<span class=\"string\">&#x27;btn btn-primary m-2&#x27;</span>&gt;</span>Left<span class=\"tag\">&lt;/<span class=\"name\">button</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;<span class=\"name\">button</span> <span class=\"attr\">className</span>=<span class=\"string\">&#x27;btn btn-success m-2&#x27;</span>&gt;</span>Right<span class=\"tag\">&lt;/<span class=\"name\">button</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">        <span class=\"tag\">&lt;/<span class=\"name\">React.Fragment</span>&gt;</span></span></span><br><span class=\"line\">        );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"title function_\">getStyles</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> styles = &#123;</span><br><span class=\"line\">            <span class=\"attr\">width</span>: <span class=\"string\">&#x27;50px&#x27;</span>,</span><br><span class=\"line\">            <span class=\"attr\">height</span>: <span class=\"string\">&#x27;50px&#x27;</span>,</span><br><span class=\"line\">            <span class=\"attr\">backgroundColor</span>: <span class=\"string\">&#x27;lightblue&#x27;</span>,</span><br><span class=\"line\">            <span class=\"attr\">color</span>: <span class=\"string\">&#x27;white&#x27;</span>,</span><br><span class=\"line\">            <span class=\"attr\">textAlign</span>: <span class=\"string\">&#x27;center&#x27;</span>,</span><br><span class=\"line\">            <span class=\"attr\">lineHeight</span>: <span class=\"string\">&#x27;50px&#x27;</span>,</span><br><span class=\"line\">            <span class=\"attr\">borderRadius</span>: <span class=\"string\">&#x27;5px&#x27;</span></span><br><span class=\"line\">        &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (<span class=\"variable language_\">this</span>.<span class=\"property\">state</span>.<span class=\"property\">x</span> === <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            styles.<span class=\"property\">backgroundColor</span> = <span class=\"string\">&#x27;orange&#x27;</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> styles;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"keyword\">default</span> <span class=\"title class_\">Box</span>;</span><br></pre></td></tr></table></figure>\n<p><strong>（7）渲染列表</strong></p>\n<p>可以使用 <code>map</code> 函数渲染一个列表，每个元素需要具有唯一的 <code>key</code> 属性，用来帮助 React 快速找到被修改的 DOM 元素，例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"title class_\">React</span>, &#123; <span class=\"title class_\">Component</span> &#125; <span class=\"keyword\">from</span> <span class=\"string\">&#x27;react&#x27;</span>;  <span class=\"comment\">// 输入imrc即可补全</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Box</span> <span class=\"keyword\">extends</span> <span class=\"title class_ inherited__\">Component</span> &#123;  <span class=\"comment\">// 输入cc即可补全</span></span><br><span class=\"line\">    state = &#123; </span><br><span class=\"line\">        <span class=\"attr\">x</span>: <span class=\"number\">1</span>,</span><br><span class=\"line\">        <span class=\"attr\">colors</span>: [<span class=\"string\">&#x27;red&#x27;</span>, <span class=\"string\">&#x27;green&#x27;</span>, <span class=\"string\">&#x27;blue&#x27;</span>]</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"title function_\">render</span>(<span class=\"params\"></span>) &#123;  <span class=\"comment\">// Component类的函数，用来返回当前组件最后渲染的HTML结构是什么</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> (</span><br><span class=\"line\">        <span class=\"comment\">// HTML标签中可以使用&#123;&#125;写一个表达式</span></span><br><span class=\"line\">        <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">React.Fragment</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;<span class=\"name\">div</span>&gt;</span>&#123;this.state.x&#125;<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;<span class=\"name\">button</span> <span class=\"attr\">className</span>=<span class=\"string\">&#x27;btn btn-primary m-2&#x27;</span>&gt;</span>Left<span class=\"tag\">&lt;/<span class=\"name\">button</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;<span class=\"name\">button</span> <span class=\"attr\">className</span>=<span class=\"string\">&#x27;btn btn-success m-2&#x27;</span>&gt;</span>Right<span class=\"tag\">&lt;/<span class=\"name\">button</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            &#123;this.state.colors.map(color =&gt; (</span></span><br><span class=\"line\"><span class=\"language-xml\">                <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">key</span>=<span class=\"string\">&#123;color&#125;</span>&gt;</span>&#123;color&#125;<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            ))&#125;</span></span><br><span class=\"line\"><span class=\"language-xml\">        <span class=\"tag\">&lt;/<span class=\"name\">React.Fragment</span>&gt;</span></span></span><br><span class=\"line\">        );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"keyword\">default</span> <span class=\"title class_\">Box</span>;</span><br></pre></td></tr></table></figure>\n<p><strong>（8）Conditional Rendering</strong></p>\n<p>利用逻辑表达式的短路原则：</p>\n<ul>\n<li>与表达式中 <code>expr1 &amp;&amp; expr2</code>，当 <code>expr1</code> 为假时返回 <code>expr1</code> 的值，否则返回 <code>expr2</code> 的值。</li>\n<li>或表达式中 <code>expr1 || expr2</code>，当 <code>expr1</code> 为真时返回 <code>expr1</code> 的值，否则返回 <code>expr2</code> 的值。</li>\n</ul>\n<p><strong>（9）绑定事件</strong></p>\n<p>例如可以使用 <code>onClick</code> 绑定按钮的点击事件，注意需要妥善处理好绑定事件函数的 <code>this</code>，示例如下：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"title class_\">React</span>, &#123; <span class=\"title class_\">Component</span> &#125; <span class=\"keyword\">from</span> <span class=\"string\">&#x27;react&#x27;</span>;  <span class=\"comment\">// 输入imrc即可补全</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Box</span> <span class=\"keyword\">extends</span> <span class=\"title class_ inherited__\">Component</span> &#123;  <span class=\"comment\">// 输入cc即可补全</span></span><br><span class=\"line\">    state = &#123; </span><br><span class=\"line\">        <span class=\"attr\">x</span>: <span class=\"number\">1</span></span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"title function_\">handleClickLeft</span>(<span class=\"params\"></span>) &#123;  <span class=\"comment\">// 如果不用箭头函数或者下面调用函数时使用bind(this)的话this会变成undefined</span></span><br><span class=\"line\">        <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(<span class=\"string\">&#x27;click left&#x27;</span>, <span class=\"variable language_\">this</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    handleClickRight = <span class=\"function\">() =&gt;</span> &#123;</span><br><span class=\"line\">        <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(<span class=\"string\">&#x27;click right&#x27;</span>, <span class=\"variable language_\">this</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"title function_\">render</span>(<span class=\"params\"></span>) &#123;  <span class=\"comment\">// Component类的函数，用来返回当前组件最后渲染的HTML结构是什么</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> (</span><br><span class=\"line\">        <span class=\"comment\">// HTML标签中可以使用&#123;&#125;写一个表达式</span></span><br><span class=\"line\">        <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">React.Fragment</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;<span class=\"name\">div</span>&gt;</span>&#123;this.state.x&#125;<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;<span class=\"name\">button</span> <span class=\"attr\">onClick</span>=<span class=\"string\">&#123;this.handleClickLeft&#125;</span> <span class=\"attr\">className</span>=<span class=\"string\">&#x27;btn btn-primary m-2&#x27;</span>&gt;</span>Left<span class=\"tag\">&lt;/<span class=\"name\">button</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;<span class=\"name\">button</span> <span class=\"attr\">onClick</span>=<span class=\"string\">&#123;this.handleClickRight&#125;</span> <span class=\"attr\">className</span>=<span class=\"string\">&#x27;btn btn-success m-2&#x27;</span>&gt;</span>Right<span class=\"tag\">&lt;/<span class=\"name\">button</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">        <span class=\"tag\">&lt;/<span class=\"name\">React.Fragment</span>&gt;</span></span></span><br><span class=\"line\">        );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"keyword\">default</span> <span class=\"title class_\">Box</span>;</span><br></pre></td></tr></table></figure>\n<p><strong>（10）修改 state</strong></p>\n<p>需要使用 <code>this.setState()</code> 函数，每次调用 <code>this.setState()</code> 函数后，会自动重新调用 <code>this.render()</code> 函数，用来修改虚拟 DOM 树。React 只会修改不同步的实际 DOM 树节点。例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"title class_\">React</span>, &#123; <span class=\"title class_\">Component</span> &#125; <span class=\"keyword\">from</span> <span class=\"string\">&#x27;react&#x27;</span>;  <span class=\"comment\">// 输入imrc即可补全</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Box</span> <span class=\"keyword\">extends</span> <span class=\"title class_ inherited__\">Component</span> &#123;  <span class=\"comment\">// 输入cc即可补全</span></span><br><span class=\"line\">    state = &#123; </span><br><span class=\"line\">        <span class=\"attr\">x</span>: <span class=\"number\">1</span></span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    handleClickLeft = <span class=\"function\">() =&gt;</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// this.state.x--;  这样写的话React不会修改前端页面的效果</span></span><br><span class=\"line\">        <span class=\"variable language_\">this</span>.<span class=\"title function_\">setState</span>(&#123;</span><br><span class=\"line\">            <span class=\"attr\">x</span>: <span class=\"variable language_\">this</span>.<span class=\"property\">state</span>.<span class=\"property\">x</span> - <span class=\"number\">1</span></span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    handleClickRight = <span class=\"function\">() =&gt;</span> &#123;</span><br><span class=\"line\">        <span class=\"variable language_\">this</span>.<span class=\"title function_\">setState</span>(&#123;</span><br><span class=\"line\">            <span class=\"attr\">x</span>: <span class=\"variable language_\">this</span>.<span class=\"property\">state</span>.<span class=\"property\">x</span> + <span class=\"number\">1</span></span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"title function_\">render</span>(<span class=\"params\"></span>) &#123;  <span class=\"comment\">// Component类的函数，用来返回当前组件最后渲染的HTML结构是什么</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> (</span><br><span class=\"line\">        <span class=\"comment\">// HTML标签中可以使用&#123;&#125;写一个表达式</span></span><br><span class=\"line\">        <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">React.Fragment</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">style</span>=<span class=\"string\">&#123;this.getStyles()&#125;</span>&gt;</span>&#123;this.state.x&#125;<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;<span class=\"name\">button</span> <span class=\"attr\">onClick</span>=<span class=\"string\">&#123;this.handleClickLeft&#125;</span> <span class=\"attr\">className</span>=<span class=\"string\">&#x27;btn btn-primary m-2&#x27;</span>&gt;</span>Left<span class=\"tag\">&lt;/<span class=\"name\">button</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;<span class=\"name\">button</span> <span class=\"attr\">onClick</span>=<span class=\"string\">&#123;this.handleClickRight&#125;</span> <span class=\"attr\">className</span>=<span class=\"string\">&#x27;btn btn-success m-2&#x27;</span>&gt;</span>Right<span class=\"tag\">&lt;/<span class=\"name\">button</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">        <span class=\"tag\">&lt;/<span class=\"name\">React.Fragment</span>&gt;</span></span></span><br><span class=\"line\">        );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"title function_\">getStyles</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> styles = &#123;</span><br><span class=\"line\">            <span class=\"attr\">width</span>: <span class=\"string\">&#x27;50px&#x27;</span>,</span><br><span class=\"line\">            <span class=\"attr\">height</span>: <span class=\"string\">&#x27;50px&#x27;</span>,</span><br><span class=\"line\">            <span class=\"attr\">backgroundColor</span>: <span class=\"string\">&#x27;lightblue&#x27;</span>,</span><br><span class=\"line\">            <span class=\"attr\">color</span>: <span class=\"string\">&#x27;white&#x27;</span>,</span><br><span class=\"line\">            <span class=\"attr\">textAlign</span>: <span class=\"string\">&#x27;center&#x27;</span>,</span><br><span class=\"line\">            <span class=\"attr\">lineHeight</span>: <span class=\"string\">&#x27;50px&#x27;</span>,</span><br><span class=\"line\">            <span class=\"attr\">borderRadius</span>: <span class=\"string\">&#x27;5px&#x27;</span>,</span><br><span class=\"line\">            <span class=\"attr\">position</span>: <span class=\"string\">&#x27;relative&#x27;</span>,</span><br><span class=\"line\">            <span class=\"attr\">left</span>: <span class=\"variable language_\">this</span>.<span class=\"property\">state</span>.<span class=\"property\">x</span></span><br><span class=\"line\">        &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (<span class=\"variable language_\">this</span>.<span class=\"property\">state</span>.<span class=\"property\">x</span> === <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            styles.<span class=\"property\">backgroundColor</span> = <span class=\"string\">&#x27;orange&#x27;</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> styles;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"keyword\">default</span> <span class=\"title class_\">Box</span>;</span><br></pre></td></tr></table></figure>\n<p><strong>（11）给事件函数添加参数</strong></p>\n<p>可以定义一个临时函数绑定事件，然后在该函数中调用原函数并传入参数，或者直接在绑定事件的时候用一个临时的箭头函数返回传入参数的原函数。例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"title class_\">React</span>, &#123; <span class=\"title class_\">Component</span> &#125; <span class=\"keyword\">from</span> <span class=\"string\">&#x27;react&#x27;</span>;  <span class=\"comment\">// 输入imrc即可补全</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Box</span> <span class=\"keyword\">extends</span> <span class=\"title class_ inherited__\">Component</span> &#123;  <span class=\"comment\">// 输入cc即可补全</span></span><br><span class=\"line\">    state = &#123; </span><br><span class=\"line\">        <span class=\"attr\">x</span>: <span class=\"number\">1</span></span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    handleClickLeft = <span class=\"function\">(<span class=\"params\">step</span>) =&gt;</span> &#123;</span><br><span class=\"line\">        <span class=\"variable language_\">this</span>.<span class=\"title function_\">setState</span>(&#123;</span><br><span class=\"line\">            <span class=\"attr\">x</span>: <span class=\"variable language_\">this</span>.<span class=\"property\">state</span>.<span class=\"property\">x</span> - step</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    handleClickRight = <span class=\"function\">(<span class=\"params\">step</span>) =&gt;</span> &#123;</span><br><span class=\"line\">        <span class=\"variable language_\">this</span>.<span class=\"title function_\">setState</span>(&#123;</span><br><span class=\"line\">            <span class=\"attr\">x</span>: <span class=\"variable language_\">this</span>.<span class=\"property\">state</span>.<span class=\"property\">x</span> + step</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    handleClickLeftTmp = <span class=\"function\">() =&gt;</span> &#123;</span><br><span class=\"line\">        <span class=\"variable language_\">this</span>.<span class=\"title function_\">handleClickLeft</span>(<span class=\"number\">10</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"title function_\">render</span>(<span class=\"params\"></span>) &#123;  <span class=\"comment\">// Component类的函数，用来返回当前组件最后渲染的HTML结构是什么</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> (</span><br><span class=\"line\">        <span class=\"comment\">// HTML标签中可以使用&#123;&#125;写一个表达式</span></span><br><span class=\"line\">        <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">React.Fragment</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">style</span>=<span class=\"string\">&#123;this.getStyles()&#125;</span>&gt;</span>&#123;this.state.x&#125;<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;<span class=\"name\">button</span> <span class=\"attr\">onClick</span>=<span class=\"string\">&#123;this.handleClickLeftTmp&#125;</span> <span class=\"attr\">className</span>=<span class=\"string\">&#x27;btn btn-primary m-2&#x27;</span>&gt;</span>Left<span class=\"tag\">&lt;/<span class=\"name\">button</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">            <span class=\"tag\">&lt;<span class=\"name\">button</span> <span class=\"attr\">onClick</span>=<span class=\"string\">&#123;()</span> =&gt;</span> this.handleClickRight(10)&#125; className=&#x27;btn btn-success m-2&#x27;&gt;Right<span class=\"tag\">&lt;/<span class=\"name\">button</span>&gt;</span></span></span><br><span class=\"line\"><span class=\"language-xml\">        <span class=\"tag\">&lt;/<span class=\"name\">React.Fragment</span>&gt;</span></span></span><br><span class=\"line\">        );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"title function_\">getStyles</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> styles = &#123;</span><br><span class=\"line\">            <span class=\"attr\">width</span>: <span class=\"string\">&#x27;50px&#x27;</span>,</span><br><span class=\"line\">            <span class=\"attr\">height</span>: <span class=\"string\">&#x27;50px&#x27;</span>,</span><br><span class=\"line\">            <span class=\"attr\">backgroundColor</span>: <span class=\"string\">&#x27;lightblue&#x27;</span>,</span><br><span class=\"line\">            <span class=\"attr\">color</span>: <span class=\"string\">&#x27;white&#x27;</span>,</span><br><span class=\"line\">            <span class=\"attr\">textAlign</span>: <span class=\"string\">&#x27;center&#x27;</span>,</span><br><span class=\"line\">            <span class=\"attr\">lineHeight</span>: <span class=\"string\">&#x27;50px&#x27;</span>,</span><br><span class=\"line\">            <span class=\"attr\">borderRadius</span>: <span class=\"string\">&#x27;5px&#x27;</span>,</span><br><span class=\"line\">            <span class=\"attr\">position</span>: <span class=\"string\">&#x27;relative&#x27;</span>,</span><br><span class=\"line\">            <span class=\"attr\">left</span>: <span class=\"variable language_\">this</span>.<span class=\"property\">state</span>.<span class=\"property\">x</span></span><br><span class=\"line\">        &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (<span class=\"variable language_\">this</span>.<span class=\"property\">state</span>.<span class=\"property\">x</span> === <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            styles.<span class=\"property\">backgroundColor</span> = <span class=\"string\">&#x27;orange&#x27;</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> styles;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"keyword\">default</span> <span class=\"title class_\">Box</span>;</span><br></pre></td></tr></table></figure>\n",
            "tags": [
                "Web"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/31252.html",
            "url": "https://asanosaki.github.io/posts/31252.html",
            "title": "Windows安装配置Git教程",
            "date_published": "2022-11-18T03:45:00.000Z",
            "content_html": "<blockquote>\n<p>Windows 下安装 Git 以及配置与 Github 的远程连接。</p>\n</blockquote>\n<span id=\"more\"></span>\n<p>（1）首先前往 Git 官网：<a href=\"https://git-scm.com/\">Git</a>，下载安装文件：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/a3464f035edc4602b77b0a30e7e20927.png\" alt=\"\"></p>\n<p>（2）打开安装程序，把 <code>Only show new options</code> 的勾去掉，点击 <code>Next</code>：<br>\n<img src=\"https://img-blog.csdnimg.cn/acfeb99dcbc348678ed3018896486dff.png\" alt=\"\"></p>\n<p>（3）此处可以选用默认设置，也可以勾上 <code>Add a Git Bash Profile to Windows Terminal</code>：<br>\n<img src=\"https://img-blog.csdnimg.cn/44e0f21c3f3d4518a6baee9ddcd0a6db.png\" alt=\"\"></p>\n<p>（4）此处选择编辑器，一般直接使用 <code>Vim</code> 即可：<br>\n<img src=\"https://img-blog.csdnimg.cn/ceff4c6d201349bbb69382327320e5fb.png\" alt=\"\"></p>\n<p>（5）此处是设置 Git 初始化分支的名称，默认为 <code>master</code>，也可以选择自定义：<br>\n<img src=\"https://img-blog.csdnimg.cn/c877b79b65c04960b59d0183fccd5acb.png\" alt=\"\"></p>\n<p>（6）此处选择使用 Git 的方式，通常选第一个：<br>\n<img src=\"https://img-blog.csdnimg.cn/e77844da80104b9c81d714bd6fa4547f.png\" alt=\"\"></p>\n<p>（7）选择 <code>SSH</code>，第一个即可：<br>\n<img src=\"https://img-blog.csdnimg.cn/fe9648ad17b444c887cf825a1f50eba3.png\" alt=\"\"></p>\n<p>（8）选择 <code>https</code> 传输后端，第一个选项使用 OpenSSL 库，第二个选项使用本机 Windows 安全通道库，选第一个即可：<br>\n<img src=\"https://img-blog.csdnimg.cn/fb0d613fa4f24216b9d974700a8ce0b0.png\" alt=\"\"></p>\n<p>（9）配置结束行转换方式，也就是 Git 处理文本结束行的方式，Windows 选择第一个即可：<br>\n<img src=\"https://img-blog.csdnimg.cn/c2a183145ddf4661b5a229a2130dc3df.png\" alt=\"\"></p>\n<p>（10）配置终端使用 Git Bash，第一个选项是使用 MinTTY 作为终端模拟器，第二个选项是使用 Windows 的默认控制台，一般选择第一个：<br>\n<img src=\"https://img-blog.csdnimg.cn/6145b71262154b27b82367b43805479a.png\" alt=\"\"></p>\n<p>（11）选择 <code>git pull</code> 的默认行为，第一个选项是 <code>git pull</code> 的标准行为：尽可能快进当前分支到一个被捕获的分支，否则创建合并提交；第二个选项是将当前分支改为获取的分支。如果没有要重基的本地提交，这相当于快进；第三个选项是仅仅快进，快进到获取的分支，如果不可能，就失败。此处选择第一个选项即可：<br>\n<img src=\"https://img-blog.csdnimg.cn/772eb5d50aaf40e5970707d5539c9f1b.png\" alt=\"\"></p>\n<p>（12）选择 Git 凭证助手，选第一个即可：<br>\n<img src=\"https://img-blog.csdnimg.cn/9a45284b2dc3438dab6354c6530a896a.png\" alt=\"\"></p>\n<p>（13）配置额外特性，第一个选项是启用文件系统缓存，第二个选项是支持符号链接，勾上第一个即可：<br>\n<img src=\"https://img-blog.csdnimg.cn/7bda5e591a354174aa735643c740a42e.png\" alt=\"\"></p>\n<p>（14）实验特性，一般不用选，直接安装即可：<br>\n<img src=\"https://img-blog.csdnimg.cn/9a4e60e623fb4a23a7a0eb7a493c96f1.png\" alt=\"\"></p>\n<p>（15）安装好后打开 Git Bash，输入 <code>git -v</code>，即可看到以下结果：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/af0ad4d1be3c4a2094581ea9fe10e1c3.png\" alt=\"\"></p>\n<p>（16）输入以下命令配置 Git：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git config --global user.name &quot;Github用户名&quot;</span><br><span class=\"line\">git config --global user.email &quot;Github邮箱&quot;</span><br></pre></td></tr></table></figure>\n<p>例如：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git config --global user.name &quot;AsanoSaki&quot;</span><br><span class=\"line\">git config --global user.email &quot;Yujie_Yi@foxmail.com&quot;</span><br></pre></td></tr></table></figure>\n<p>生成公钥（输完命令连按几次回车）：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>\n<p>将 <code>~/.ssh/id_rsa.pub</code> 中的内容复制到 GitHub 的 SSH Keys 中：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/0ee1b8da92b34025a146c8e8ee27e5d6.png\" alt=\"\"></p>\n<p>测试是否连接上 GitHub：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh -T git@github.com</span><br></pre></td></tr></table></figure>\n<p>结果如下说明配置成功：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/093e9bbe95264d6594e40fda280bd519.png\" alt=\"\"></p>\n<p>如果出现报错提示：<code>ssh: connect to host github.com port 22: Connection timed out</code>，说明22端口可能被防火墙屏蔽了，可以尝试连接 GitHub 的443端口，我们将 <code>~/.ssh/config</code> 文件修改成以下内容，这样 SSH 连接 GitHub 的时候就会使用443端口：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Host github.com</span><br><span class=\"line\">    HostName ssh.github.com</span><br><span class=\"line\">    User 你的GitHub用户名</span><br><span class=\"line\">    Port 443</span><br></pre></td></tr></table></figure>\n",
            "tags": [
                "Others"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/40580.html",
            "url": "https://asanosaki.github.io/posts/40580.html",
            "title": "Web学习笔记-JavaScript",
            "date_published": "2022-11-14T03:14:00.000Z",
            "content_html": "<blockquote>\n<p>本文记录 JavaScript 的学习过程。<br>\nJavaScript 是一种具有函数优先的轻量级，解释型或即时编译型的编程语言。目前 JavaScript 的标准是 ECMAScript6，简称 ES6。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-JS的调用方式与执行顺序\">1. JS的调用方式与执行顺序</h2>\n<p>JS 常见使用方式有以下几种：</p>\n<ul>\n<li>直接在 HTML 的 <code>&lt;script type=&quot;module&quot;&gt;&lt;/script&gt;</code> 标签内写 JS 代码。</li>\n<li>直接引入 <code>.js</code> 文件：<code>&lt;script type=&quot;module&quot; src=&quot;/static/js/index.js&quot;&gt;&lt;/script&gt;</code>。</li>\n<li>将所需的代码通过 <code>import</code> 关键字引入到当前作用域。</li>\n</ul>\n<p>例如 <code>/static/js/index.js</code> 文件中的内容为：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">let</span> name = <span class=\"string\">&quot;acwing&quot;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">function</span> <span class=\"title function_\">print</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">    <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(<span class=\"string\">&quot;Hello World!&quot;</span>);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">export</span> &#123;</span><br><span class=\"line\">    name,</span><br><span class=\"line\">    print</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><code>&lt;script type=&quot;module&quot;&gt;&lt;/script&gt;</code> 中的内容为：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">script</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;module&quot;</span>&gt;</span><span class=\"language-javascript\"></span></span><br><span class=\"line\"><span class=\"language-javascript\">    <span class=\"keyword\">import</span> &#123; name, print &#125; <span class=\"keyword\">from</span> <span class=\"string\">&quot;/static/js/index.js&quot;</span>;</span></span><br><span class=\"line\"><span class=\"language-javascript\"></span></span><br><span class=\"line\"><span class=\"language-javascript\">    <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(name);</span></span><br><span class=\"line\"><span class=\"language-javascript\">    <span class=\"title function_\">print</span>();</span></span><br><span class=\"line\"><span class=\"language-javascript\"></span><span class=\"tag\">&lt;/<span class=\"name\">script</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>执行顺序：</p>\n<ol>\n<li>类似于 HTML 与 CSS，按从上到下的顺序执行。</li>\n<li>事件驱动执行。</li>\n</ol>\n<p>HTML、CSS、JavaScript 三者之间的关系：</p>\n<ul>\n<li>CSS 控制 HTML；</li>\n<li>JavaScript 控制 HTML 与 CSS；</li>\n<li>为了方便开发与维护，尽量按照上述顺序写代码。例如：不要在 HTML 中调用 JavaScript 中的函数。</li>\n</ul>\n<h2 id=\"2-变量与运算符\">2. 变量与运算符</h2>\n<p>（1）<code>let</code> 与 <code>const</code>：用来声明变量，作用范围为当前作用域。</p>\n<ul>\n<li><code>let</code> 用来定义变量。</li>\n<li><code>const</code> 用来定义常量。</li>\n</ul>\n<p>例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">const</span> N = <span class=\"number\">100</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">let</span> s = <span class=\"string\">&quot;Hello World!&quot;</span>, x = <span class=\"number\">10</span>;</span><br><span class=\"line\"><span class=\"keyword\">let</span> dict = &#123;</span><br><span class=\"line\">    <span class=\"attr\">name</span>: <span class=\"string\">&quot;AsanoSaki&quot;</span>,</span><br><span class=\"line\">    <span class=\"attr\">age</span>: <span class=\"number\">18</span></span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(N);</span><br><span class=\"line\"><span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(s + <span class=\"string\">&#x27; &#x27;</span> + x);</span><br><span class=\"line\"><span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(dict.<span class=\"property\">name</span>, dict.<span class=\"property\">age</span>);</span><br></pre></td></tr></table></figure>\n<p>（2）变量类型：</p>\n<ul>\n<li><code>number</code>：数值变量，例如：<code>1, 2.5</code>。</li>\n<li><code>string</code>：字符串，例如：<code>&quot;acwing&quot;, 'AsanoSaki'</code>，单引号与双引号均可。字符串中的每个字符为只读类型。</li>\n<li><code>boolean</code>：布尔值，例如：<code>true, false</code>。</li>\n<li><code>object</code>：对象，类似于 C++ 中的指针，例如：<code>[1, 2, 3]</code>，<code>&#123; name: &quot;AsanoSaki&quot;, age: 18 &#125;</code>，<code>null</code>。</li>\n<li><code>undefined</code>：未定义的变量。</li>\n</ul>\n<p>类似于 Python，JavaScript 中的变量类型可以动态变化。</p>\n<p>（3）运算符：<br>\n与 C++、Python、Java 类似，不同点：</p>\n<ul>\n<li><code>**</code> 表示乘方。</li>\n<li>等于与不等于用 <code>===</code> 和 <code>!==</code>。</li>\n</ul>\n<h2 id=\"3-输入与输出\">3. 输入与输出</h2>\n<p>（1）输入</p>\n<ul>\n<li>从 HTML 与用户的交互中输入信息，例如通过 <code>input</code>、<code>textarea</code> 等标签获取用户的键盘输入，通过 <code>click</code>、<code>hover</code> 等事件获取用户的鼠标输入。</li>\n<li>通过 <code>Ajax</code> 与 <code>WebSocket</code> 从服务器端获取输入。</li>\n<li>标准输入。</li>\n</ul>\n<p>（2）输出</p>\n<ul>\n<li>调试用 <code>console.log()</code>，会将信息输出到浏览器控制台。</li>\n<li>改变当前页面的 HTML 与 CSS。</li>\n<li>通过 <code>Ajax</code> 与 <code>WebSocket</code> 将结果返回到服务器。</li>\n</ul>\n<p>通过 HTML 输入输出示例：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;!DOCTYPE <span class=\"keyword\">html</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">html</span> <span class=\"attr\">lang</span>=<span class=\"string\">&quot;en&quot;</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">charset</span>=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">http-equiv</span>=<span class=\"string\">&quot;X-UA-Compatible&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;IE=edge&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;viewport&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">title</span>&gt;</span>Document<span class=\"tag\">&lt;/<span class=\"name\">title</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">link</span> <span class=\"attr\">rel</span>=<span class=\"string\">&quot;stylesheet&quot;</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/Web Application Lesson/static/css/index.css&quot;</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\">    输入：</span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">br</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">textarea</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;inputText&quot;</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;&quot;</span> <span class=\"attr\">id</span>=<span class=\"string\">&quot;&quot;</span> <span class=\"attr\">cols</span>=<span class=\"string\">&quot;30&quot;</span> <span class=\"attr\">rows</span>=<span class=\"string\">&quot;10&quot;</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">textarea</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">br</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">button</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;run&quot;</span>&gt;</span>Run<span class=\"tag\">&lt;/<span class=\"name\">button</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">br</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    输出：</span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">br</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">pre</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;outputText&quot;</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">pre</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">script</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;module&quot;</span>&gt;</span><span class=\"language-javascript\"></span></span><br><span class=\"line\"><span class=\"language-javascript\">        <span class=\"keyword\">import</span> &#123; runOnclick &#125; <span class=\"keyword\">from</span> <span class=\"string\">&quot;/Web Application Lesson/static/js/index.js&quot;</span>;</span></span><br><span class=\"line\"><span class=\"language-javascript\"></span></span><br><span class=\"line\"><span class=\"language-javascript\">        <span class=\"title function_\">runOnclick</span>();</span></span><br><span class=\"line\"><span class=\"language-javascript\">    </span><span class=\"tag\">&lt;/<span class=\"name\">script</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">html</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">let</span> inputText = <span class=\"variable language_\">document</span>.<span class=\"title function_\">querySelector</span>(<span class=\"string\">&quot;.inputText&quot;</span>);</span><br><span class=\"line\"><span class=\"keyword\">let</span> run = <span class=\"variable language_\">document</span>.<span class=\"title function_\">querySelector</span>(<span class=\"string\">&quot;.run&quot;</span>);</span><br><span class=\"line\"><span class=\"keyword\">let</span> outputText = <span class=\"variable language_\">document</span>.<span class=\"title function_\">querySelector</span>(<span class=\"string\">&quot;.outputText&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">function</span> <span class=\"title function_\">runOnclick</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">    <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(inputText);</span><br><span class=\"line\">    run.<span class=\"title function_\">addEventListener</span>(<span class=\"string\">&quot;click&quot;</span>, <span class=\"keyword\">function</span> (<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> s = inputText.<span class=\"property\">value</span>;</span><br><span class=\"line\">        outputText.<span class=\"property\">innerHTML</span> = s;</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">export</span> &#123;</span><br><span class=\"line\">    runOnclick</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>通过标准输入输出示例：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">let</span> fs = <span class=\"built_in\">require</span>(<span class=\"string\">&#x27;fs&#x27;</span>);</span><br><span class=\"line\"><span class=\"keyword\">let</span> buf = <span class=\"string\">&#x27;&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">process.<span class=\"property\">stdin</span>.<span class=\"title function_\">on</span>(<span class=\"string\">&#x27;readable&#x27;</span>, <span class=\"keyword\">function</span> (<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">let</span> chunk = process.<span class=\"property\">stdin</span>.<span class=\"title function_\">read</span>();</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (chunk) buf += chunk.<span class=\"title function_\">toString</span>();</span><br><span class=\"line\">&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">process.<span class=\"property\">stdin</span>.<span class=\"title function_\">on</span>(<span class=\"string\">&#x27;end&#x27;</span>, <span class=\"keyword\">function</span> (<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">    buf.<span class=\"title function_\">split</span>(<span class=\"string\">&#x27;\\n&#x27;</span>).<span class=\"title function_\">forEach</span>(<span class=\"keyword\">function</span> (<span class=\"params\">line</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> tokens = line.<span class=\"title function_\">split</span>(<span class=\"string\">&#x27; &#x27;</span>).<span class=\"title function_\">map</span>(<span class=\"keyword\">function</span> (<span class=\"params\">x</span>) &#123; <span class=\"keyword\">return</span> <span class=\"built_in\">parseInt</span>(x); &#125;);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (tokens.<span class=\"property\">length</span> != <span class=\"number\">2</span>) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(tokens.<span class=\"title function_\">reduce</span>(<span class=\"keyword\">function</span> (<span class=\"params\">a, b</span>) &#123; <span class=\"keyword\">return</span> a + b; &#125;));</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure>\n<p>（3）格式化字符串</p>\n<ul>\n<li>字符串中填入数值：</li>\n</ul>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">let</span> name = <span class=\"string\">&#x27;AsanoSaki&#x27;</span>, age = <span class=\"number\">18</span>;</span><br><span class=\"line\"><span class=\"keyword\">let</span> s = <span class=\"string\">`My name is <span class=\"subst\">$&#123;name&#125;</span>, I&#x27;m <span class=\"subst\">$&#123;age&#125;</span> years old.`</span>;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>定义多行字符串：</li>\n</ul>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">let</span> s = </span><br><span class=\"line\"><span class=\"string\">`&lt;div&gt;</span></span><br><span class=\"line\"><span class=\"string\">    &lt;h2&gt;标题&lt;/h2&gt;</span></span><br><span class=\"line\"><span class=\"string\">    &lt;p&gt;段落&lt;/p&gt;</span></span><br><span class=\"line\"><span class=\"string\">/div&gt;`</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>保留两位小数：</li>\n</ul>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">let</span> x = <span class=\"number\">1.234567</span>;</span><br><span class=\"line\"><span class=\"keyword\">let</span> s = <span class=\"string\">`<span class=\"subst\">$&#123;x.toFixed(<span class=\"number\">2</span>)&#125;</span>`</span>;</span><br></pre></td></tr></table></figure>\n<h2 id=\"4-判断语句\">4. 判断语句</h2>\n<p>JavaScript 中的 <code>if-else</code> 语句与 C++、Python、Java 中类似。例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">let</span> score = <span class=\"number\">90</span>;</span><br><span class=\"line\"><span class=\"keyword\">if</span> (score &gt;= <span class=\"number\">85</span>) &#123;</span><br><span class=\"line\">    <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(<span class=\"string\">&quot;A&quot;</span>);</span><br><span class=\"line\">&#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (score &gt;= <span class=\"number\">70</span>) &#123;</span><br><span class=\"line\">    <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(<span class=\"string\">&quot;B&quot;</span>);</span><br><span class=\"line\">&#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (score &gt;= <span class=\"number\">60</span>) &#123;</span><br><span class=\"line\">    <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(<span class=\"string\">&quot;C&quot;</span>);</span><br><span class=\"line\">&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(<span class=\"string\">&quot;D&quot;</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>JavaScript 中的逻辑运算符也与 C++、Java 中类似：<code>&amp;&amp;</code> 表示与、<code>||</code> 表示或、<code>!</code> 表示非。</p>\n<h2 id=\"5-循环语句\">5. 循环语句</h2>\n<p>JavaScript 中的循环语句与 C++ 中类似，也包含 <code>for</code>、<code>while</code>、<code>do while</code> 循环。</p>\n<p>（1）<code>for</code> 循环</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">let</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">10</span>; i++) &#123;</span><br><span class=\"line\">    <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(i);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>枚举对象或数组时可以使用：</p>\n<ul>\n<li><code>for-in</code> 循环：可以枚举数组中的下标，以及对象中的 <code>key</code>。</li>\n<li><code>for-of</code> 循环：可以枚举数组中的值，以及对象中的 <code>value</code>。</li>\n</ul>\n<p>例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">let</span> a = [<span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">let</span> key <span class=\"keyword\">in</span> a) &#123;</span><br><span class=\"line\">    <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(key);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">let</span> val <span class=\"keyword\">of</span> a) &#123;</span><br><span class=\"line\">    <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(val);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>（2）<code>while</code> 循环</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">let</span> i = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">while</span> (i &lt; <span class=\"number\">10</span>) &#123;</span><br><span class=\"line\">    <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(i);</span><br><span class=\"line\">    i++;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>（3）<code>do while</code> 循环</p>\n<p><code>do while</code> 语句与 <code>while</code> 语句非常相似。唯一的区别是，<code>do while</code> 语句限制先循环体后检查条件。不管条件的值如何，我们都要至少执行一次循环体。</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">let</span> i = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">do</span> &#123;</span><br><span class=\"line\">    <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(i);</span><br><span class=\"line\">    i++;</span><br><span class=\"line\">&#125; <span class=\"keyword\">while</span> (i &lt; <span class=\"number\">10</span>);</span><br></pre></td></tr></table></figure>\n<h2 id=\"6-对象\">6. 对象</h2>\n<p>英文名称：<code>Object</code>。<br>\n类似于 C++ 中的 <code>map</code>，由 <code>key:value</code> 对构成。</p>\n<ul>\n<li><code>value</code> 可以是变量、数组、对象、函数等。</li>\n<li>函数定义中的 <code>this</code> 用来引用该函数的“拥有者”。</li>\n</ul>\n<p>例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">let</span> person = &#123;</span><br><span class=\"line\">    <span class=\"attr\">name</span>: <span class=\"string\">&quot;John&quot;</span>,</span><br><span class=\"line\">    <span class=\"attr\">age</span>: <span class=\"number\">20</span>,</span><br><span class=\"line\">    <span class=\"attr\">money</span>: <span class=\"number\">0</span>,</span><br><span class=\"line\">    <span class=\"attr\">friends</span>: [<span class=\"string\">&quot;Tom&quot;</span>, <span class=\"string\">&quot;Alice&quot;</span>, <span class=\"string\">&quot;Bob&quot;</span>],</span><br><span class=\"line\">    <span class=\"attr\">clothes</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">color</span>: <span class=\"string\">&quot;blue&quot;</span>,</span><br><span class=\"line\">        <span class=\"attr\">price</span>: <span class=\"number\">20</span></span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    <span class=\"attr\">add_money</span>: <span class=\"keyword\">function</span> (<span class=\"params\">x</span>) &#123;</span><br><span class=\"line\">        <span class=\"variable language_\">this</span>.<span class=\"property\">money</span> += x;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>对象属性与函数的调用方式：</p>\n<ul>\n<li>（1）用 <code>.</code> 调用：<code>person.name</code>、<code>person.add_money()</code>。</li>\n<li>（2）用 <code>[]</code> 调用：<code>person[&quot;name&quot;]</code>、<code>person[&quot;add_money&quot;]()</code>。</li>\n</ul>\n<h2 id=\"7-数组\">7. 数组</h2>\n<p>数组是一种特殊的对象，类似于 C++ 中的数组，但是 JavaScript 数组中的元素类型可以不同（数组中的元素可以是变量、数组、对象、函数）。例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">let</span> a = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"string\">&quot;a&quot;</span>, <span class=\"string\">&quot;abc&quot;</span>];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">let</span> b = [</span><br><span class=\"line\">    <span class=\"number\">1</span>,  <span class=\"comment\">// 变量</span></span><br><span class=\"line\">    <span class=\"string\">&quot;abc&quot;</span>,  <span class=\"comment\">// 变量</span></span><br><span class=\"line\">    [<span class=\"string\">&#x27;a&#x27;</span>, <span class=\"string\">&#x27;b&#x27;</span>, <span class=\"number\">3</span>],  <span class=\"comment\">// 数组</span></span><br><span class=\"line\">    <span class=\"keyword\">function</span> (<span class=\"params\"></span>) &#123;  <span class=\"comment\">// 函数</span></span><br><span class=\"line\">        <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(<span class=\"string\">&quot;Hello World&quot;</span>);</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    &#123; <span class=\"attr\">name</span>: <span class=\"string\">&quot;abc&quot;</span>, <span class=\"attr\">age</span>: <span class=\"number\">18</span> &#125;  <span class=\"comment\">// 对象</span></span><br><span class=\"line\">];</span><br></pre></td></tr></table></figure>\n<p>可以通过下标访问数组元素，例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a[<span class=\"number\">0</span>] = <span class=\"number\">1</span>;  <span class=\"comment\">// 访问数组a[]的第0个元素</span></span><br><span class=\"line\"><span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(a[<span class=\"number\">0</span>]);</span><br></pre></td></tr></table></figure>\n<p>数组的常用属性和函数：</p>\n<ul>\n<li>属性 <code>length</code>：返回数组长度。注意 <code>length</code> 是属性，不是函数，因此调用的时候不要加 <code>()</code>。</li>\n<li>函数 <code>push()</code>：向数组末尾添加元素。</li>\n<li>函数 <code>pop()</code>：删除数组末尾的元素。</li>\n<li>函数 <code>splice(a, b)</code>：删除从下标 <code>a</code> 开始的 <code>b</code> 个元素。</li>\n<li>函数 <code>sort()</code>：将整个数组从小到大排序。\n<ul>\n<li>自定义比较函数：<code>array.sort(cmp)</code>，函数 <code>cmp</code> 输入两个需要比较的元素，返回一个实数，负数表示第一个参数排在第二个参数前面，零表示相等，正数表示第一个参数排在第二个参数后面。因此如果要实现从大到小排序只需要令函数为：<code>function(a, b) &#123; return b - a; &#125;</code>。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"8-函数\">8. 函数</h2>\n<p>JavaScript 中的函数是用对象来实现的，定义完函数后是允许再对这个对象进行修改的。函数的定义方式如下：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">function</span> <span class=\"title function_\">add</span>(<span class=\"params\">a, b</span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> a + b;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">let</span> add = <span class=\"keyword\">function</span> (<span class=\"params\">a, b</span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> a + b;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">let</span> <span class=\"title function_\">add</span> = (<span class=\"params\">a, b</span>) =&gt; &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> a + b;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>函数返回值：如果未定义返回值，则返回 <code>undefined</code>。</p>\n<h2 id=\"9-类\">9. 类</h2>\n<p>与 C++ 中的 <code>Class</code> 类似，但是不存在私有成员，<code>this</code> 指向类的实例。</p>\n<p>（1）定义</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Point</span> &#123;</span><br><span class=\"line\">    <span class=\"title function_\">constructor</span>(<span class=\"params\">x, y</span>) &#123;</span><br><span class=\"line\">        <span class=\"variable language_\">this</span>.<span class=\"property\">x</span> = x;</span><br><span class=\"line\">        <span class=\"variable language_\">this</span>.<span class=\"property\">y</span> = y;</span><br><span class=\"line\">        <span class=\"variable language_\">this</span>.<span class=\"title function_\">init</span>();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"title function_\">init</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">        <span class=\"variable language_\">this</span>.<span class=\"property\">sum</span> = <span class=\"variable language_\">this</span>.<span class=\"property\">x</span> + <span class=\"variable language_\">this</span>.<span class=\"property\">y</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"title function_\">toString</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">`Point: (<span class=\"subst\">$&#123;<span class=\"variable language_\">this</span>.x&#125;</span>, <span class=\"subst\">$&#123;<span class=\"variable language_\">this</span>.y&#125;</span>), Sum: <span class=\"subst\">$&#123;<span class=\"variable language_\">this</span>.sum&#125;</span>`</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">let</span> p = <span class=\"keyword\">new</span> <span class=\"title class_\">Point</span>(<span class=\"number\">3</span>, <span class=\"number\">4</span>);</span><br><span class=\"line\"><span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(p.<span class=\"title function_\">toString</span>());</span><br></pre></td></tr></table></figure>\n<p>（2）继承</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ColorPoint</span> <span class=\"keyword\">extends</span> <span class=\"title class_ inherited__\">Point</span> &#123;</span><br><span class=\"line\">    <span class=\"title function_\">constructor</span>(<span class=\"params\">x, y, c</span>) &#123;</span><br><span class=\"line\">        <span class=\"variable language_\">super</span>(x, y);</span><br><span class=\"line\">        <span class=\"variable language_\">this</span>.<span class=\"property\">color</span> = c;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"title function_\">toString</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">`<span class=\"subst\">$&#123;<span class=\"variable language_\">super</span>.toString()&#125;</span>, Color: <span class=\"subst\">$&#123;<span class=\"variable language_\">this</span>.color&#125;</span>`</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>注意：</p>\n<ul>\n<li><code>super</code> 这个关键字，既可以当作函数使用，也可以当作对象使用。\n<ul>\n<li>作为函数调用时，代表父类的构造函数，且只能用在子类的构造函数之中。</li>\n<li>作为对象时，指向父类的原型对象。</li>\n</ul>\n</li>\n<li>在子类的构造函数中，只有调用 <code>super</code> 之后，才可以使用 <code>this</code> 关键字。</li>\n<li>成员重名时，子类的成员会覆盖父类的成员，类似于 C++ 中的多态。</li>\n</ul>\n<p>（3）静态方法</p>\n<p>在成员函数前添加 <code>static</code> 关键字即可。静态方法可以被子类继承，但是不会被类的实例继承，只能通过类名来调用。例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Point</span> &#123;</span><br><span class=\"line\">    <span class=\"title function_\">constructor</span>(<span class=\"params\">x, y</span>) &#123;</span><br><span class=\"line\">        <span class=\"variable language_\">this</span>.<span class=\"property\">x</span> = x;</span><br><span class=\"line\">        <span class=\"variable language_\">this</span>.<span class=\"property\">y</span> = y;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"title function_\">toString</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">`(<span class=\"subst\">$&#123;<span class=\"variable language_\">this</span>.x&#125;</span>, <span class=\"subst\">$&#123;<span class=\"variable language_\">this</span>.y&#125;</span>)`</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">static</span> <span class=\"title function_\">print_class_name</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">        <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(<span class=\"string\">&quot;Point&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">let</span> p = <span class=\"keyword\">new</span> <span class=\"title class_\">Point</span>(<span class=\"number\">3</span>, <span class=\"number\">4</span>);</span><br><span class=\"line\"><span class=\"title class_\">Point</span>.<span class=\"title function_\">print_class_name</span>();</span><br><span class=\"line\">p.<span class=\"title function_\">print_class_name</span>();  <span class=\"comment\">// 报错</span></span><br></pre></td></tr></table></figure>\n<p>（4）静态变量</p>\n<p>在 ES6 中，只能通过 <code>class.propname</code> 定义和访问，子类可以继承父类的静态变量，即可以通过子类名访问静态变量。例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Point</span> &#123;</span><br><span class=\"line\">    <span class=\"title function_\">constructor</span>(<span class=\"params\">x, y</span>) &#123;</span><br><span class=\"line\">        <span class=\"variable language_\">this</span>.<span class=\"property\">x</span> = x;</span><br><span class=\"line\">        <span class=\"variable language_\">this</span>.<span class=\"property\">y</span> = y;</span><br><span class=\"line\">        <span class=\"title class_\">Point</span>.<span class=\"property\">cnt</span>++;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"title function_\">toString</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">`(<span class=\"subst\">$&#123;<span class=\"variable language_\">this</span>.x&#125;</span>, <span class=\"subst\">$&#123;<span class=\"variable language_\">this</span>.y&#125;</span>)`</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"title class_\">Point</span>.<span class=\"property\">cnt</span> = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">let</span> p = [];</span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">let</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">5</span>; i++) &#123;</span><br><span class=\"line\">    p[i] = <span class=\"keyword\">new</span> <span class=\"title class_\">Point</span>(<span class=\"number\">3</span>, <span class=\"number\">4</span>);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(<span class=\"title class_\">Point</span>.<span class=\"property\">cnt</span>);</span><br></pre></td></tr></table></figure>\n<h2 id=\"10-事件\">10. 事件</h2>\n<p>JavaScript 的代码一般通过事件触发。<br>\n可以通过 <code>addEventListener</code> 函数为元素绑定事件的触发函数。<br>\n常见的触发函数如下：</p>\n<p>（1）鼠标</p>\n<ul>\n<li><code>click</code>：鼠标左键点击。</li>\n<li><code>dblclick</code>：鼠标左键双击。</li>\n<li><code>contextmenu</code>：鼠标右键点击。</li>\n<li><code>mousedown</code>：鼠标按下，包括左键、滚轮、右键。\n<ul>\n<li><code>event.button</code>：0表示左键，1表示中键，2表示右键。</li>\n</ul>\n</li>\n<li><code>mouseup</code>：鼠标弹起，包括左键、滚轮、右键。\n<ul>\n<li><code>event.button</code>：0表示左键，1表示中键，2表示右键。</li>\n</ul>\n</li>\n</ul>\n<p>例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">let</span> div = <span class=\"variable language_\">document</span>.<span class=\"title function_\">querySelector</span>(<span class=\"string\">&#x27;div&#x27;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">function</span> <span class=\"title function_\">main</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">    div.<span class=\"title function_\">addEventListener</span>(<span class=\"string\">&#x27;click&#x27;</span>, <span class=\"keyword\">function</span> (<span class=\"params\">event</span>) &#123;</span><br><span class=\"line\">        <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(event.<span class=\"property\">type</span>, event.<span class=\"property\">button</span>);</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">export</span> &#123;</span><br><span class=\"line\">    main</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>（2）键盘</p>\n<ul>\n<li><code>keydown</code>：某个键是否被按住，事件会连续触发。\n<ul>\n<li><code>event.code</code>：返回按的是哪个键。</li>\n<li><code>event.altKey</code>、<code>event.ctrlKey</code>、<code>event.shiftKey</code> 分别表示是否同时按下了 <code>alt</code>、<code>ctrl</code>、<code>shift</code> 键。</li>\n</ul>\n</li>\n<li><code>keyup</code>：某个按键是否被释放。\n<ul>\n<li><code>event</code>常用属性同上。</li>\n</ul>\n</li>\n<li><code>keypress</code>：紧跟在 <code>keydown</code> 事件后触发，只有按下字符键时触发，适用于判定用户输入的字符。\n<ul>\n<li><code>event</code> 常用属性同上。</li>\n</ul>\n</li>\n</ul>\n<p><code>keydown</code>、<code>keyup</code>、<code>keypress</code> 的关系类似于鼠标的 <code>mousedown</code>、<code>mouseup</code>、<code>click</code>。</p>\n<p>（3）表单</p>\n<ul>\n<li><code>focus</code>：聚焦某个元素。</li>\n<li><code>blur</code>：取消聚焦某个元素。</li>\n<li><code>change</code>：某个元素的内容发生了改变。</li>\n</ul>\n<p>（4）窗口</p>\n<p>需要作用到 <code>window</code> 元素上。</p>\n<ul>\n<li><code>resize</code>：当窗口大小放生变化。</li>\n<li><code>scroll</code>：滚动指定的元素。</li>\n<li><code>load</code>：当元素全部被加载完成。</li>\n</ul>\n<h2 id=\"11-常用库\">11. 常用库</h2>\n<h3 id=\"11-1-jQuery\">11.1 jQuery</h3>\n<p>jQuery 能够让我们更加方便地去获取前端的某一个标签、绑定某个事件、改变前端的某个标签的 CSS 属性。</p>\n<p>（1）下载地址：<a href=\"https://jquery.com/download/\">jQuery 官网</a>。</p>\n<p>（2）使用方式：在 <code>&lt;head&gt;</code> 元素中添加：<code>&lt;script src=&quot;/Web Application Lesson/static/js/jquery-3.6.1.min.js&quot;&gt;&lt;/script&gt;</code>。</p>\n<p>（3）选择器<br>\n<code>$(selector)</code>，<code>selector</code> 类似于 CSS 的选择器。例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$(<span class=\"string\">&#x27;div&#x27;</span>);</span><br><span class=\"line\">$(<span class=\"string\">&#x27;.big-div&#x27;</span>);</span><br><span class=\"line\">$(<span class=\"string\">&#x27;div &gt; p&#x27;</span>);</span><br></pre></td></tr></table></figure>\n<p>（4）事件<br>\n<code>$(selector).on(event, func)</code> 绑定事件，例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$(<span class=\"string\">&#x27;div&#x27;</span>).<span class=\"title function_\">on</span>(<span class=\"string\">&#x27;click&#x27;</span>, <span class=\"keyword\">function</span> (<span class=\"params\">e</span>) &#123;</span><br><span class=\"line\">    <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(<span class=\"string\">&quot;click div&quot;</span>);</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n<p><code>$(selector).off(event, func)</code> 删除事件，例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$(<span class=\"string\">&#x27;div&#x27;</span>).<span class=\"title function_\">on</span>(<span class=\"string\">&#x27;click&#x27;</span>, <span class=\"keyword\">function</span> (<span class=\"params\">e</span>) &#123;</span><br><span class=\"line\">    <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(<span class=\"string\">&quot;click div&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    $(<span class=\"string\">&#x27;div&#x27;</span>).<span class=\"title function_\">off</span>(<span class=\"string\">&#x27;click&#x27;</span>);</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure>\n<p>当存在多个相同类型的事件触发函数时，可以通过 <code>click.name</code> 来区分，例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$(<span class=\"string\">&#x27;div&#x27;</span>).<span class=\"title function_\">on</span>(<span class=\"string\">&#x27;click.first&#x27;</span>, <span class=\"keyword\">function</span> (<span class=\"params\">e</span>) &#123;</span><br><span class=\"line\">    <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(<span class=\"string\">&quot;click div&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    $(<span class=\"string\">&#x27;div&#x27;</span>).<span class=\"title function_\">off</span>(<span class=\"string\">&#x27;click.first&#x27;</span>);</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure>\n<p>在事件触发的函数中的 <code>return false</code> 等价于同时执行：</p>\n<ul>\n<li><code>e.stopPropagation()</code>：阻止事件向上传递。例如 <code>a</code> 是 <code>div</code> 的子标签，当点击 <code>a</code> 时同样会触发 <code>div</code> 的 <code>click</code> 事件，当在 <code>a</code> 的事件触发函数中加上该语句时点击 <code>a</code> 就不会触发 <code>div</code> 的 <code>click</code> 事件。</li>\n<li><code>e.preventDefault()</code>：阻止事件的默认行为。例如点击 <code>a</code> 时不打开链接，并向上传递触发 <code>div</code> 的 <code>click</code> 事件。</li>\n</ul>\n<p>（5）元素的隐藏、展现</p>\n<ul>\n<li><code>$A.hide()</code>：隐藏，可以添加参数，表示消失时间（毫秒）。</li>\n<li><code>$A.show()</code>：展现，可以添加参数，表示出现时间。</li>\n<li><code>$A.fadeOut()</code>：颜色淡退至消失，可以添加参数，表示消失时间。</li>\n<li><code>$A.fadeIn()</code>：颜色淡增至出现，可以添加参数，表示出现时间。</li>\n</ul>\n<p>（6）元素的添加、删除</p>\n<ul>\n<li><code>$('&lt;div class=&quot;mydiv&quot;&gt;&lt;span&gt;Hello World&lt;/span&gt;&lt;/div&gt;')</code>：构造一个 <code>jQuery</code> 对象。</li>\n<li><code>$A.append($B)</code>：将 <code>$B</code> 添加到 <code>$A</code> 的末尾。</li>\n<li><code>$A.prepend($B)</code>：将 <code>$B</code> 添加到 <code>$A</code> 的开头。</li>\n<li><code>$A.remove()</code>：删除元素 <code>$A</code>。</li>\n<li><code>$A.empty()</code>：清空元素 <code>$A</code> 的所有儿子。</li>\n</ul>\n<p>（7）对类的操作（此处 <code>class_name</code> 无需加 <code>.</code>）</p>\n<ul>\n<li><code>$A.addClass(class_name)</code>：添加某个类。</li>\n<li><code>$A.removeClass(class_name)</code>：删除某个类。</li>\n<li><code>$A.hasClass(class_name)</code>：判断某个类是否存在。</li>\n</ul>\n<p>（8）对 CSS 的操作</p>\n<ul>\n<li><code>$(&quot;div&quot;).css(&quot;background-color&quot;)</code>：获取某个 CSS 的属性。</li>\n<li><code>$(&quot;div&quot;).css(&quot;background-color&quot;, &quot;yellow&quot;)</code>：设置某个 CSS 的属性。</li>\n<li>同时设置多个 CSS 的属性（注意 JS 中带 <code>-</code> 的标签必须加引号，如果不加会被当做减号）：</li>\n</ul>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$(<span class=\"string\">&#x27;div&#x27;</span>).<span class=\"title function_\">css</span>(&#123;</span><br><span class=\"line\">    <span class=\"attr\">width</span>: <span class=\"string\">&quot;200px&quot;</span>,</span><br><span class=\"line\">    <span class=\"attr\">height</span>: <span class=\"string\">&quot;200px&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;background-color&quot;</span>: <span class=\"string\">&quot;orange&quot;</span></span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure>\n<p>（9）对标签属性的操作（除 <code>class</code>、<code>id</code> 外可以随意创造新的属性，例如：<code>&lt;div abc=&quot;abc&quot;&gt;&lt;/div&gt;</code>）</p>\n<ul>\n<li><code>$('div').attr('id')</code>：获取属性。</li>\n<li><code>$('div').attr('id', 'ID')</code>：设置属性。</li>\n</ul>\n<p>（10）对 HTML 内容、文本的操作<br>\n不需要背每个标签该用哪种，用到的时候 Google 或者 Bing 即可。</p>\n<ul>\n<li><code>$A.html()</code>：获取、修改 HTML 内容（加参数即可修改），例如：<code>&lt;div&gt;&lt;span&gt;span content&lt;/span&gt;&lt;/div&gt;</code> 输出为 <code>&lt;span&gt;span content&lt;/span&gt;</code>。</li>\n<li><code>$A.text()</code>：获取、修改文本信息，例如：<code>&lt;div&gt;&lt;span&gt;span content&lt;/span&gt;&lt;/div&gt;</code> 输出为 <code>span content</code>。</li>\n<li><code>$A.val()</code>：获取、修改文本的值，一般用在 <code>input</code>、<code>textarea</code> 中。</li>\n</ul>\n<p>（11）查找</p>\n<ul>\n<li><code>$(selector).parent(filter)</code>：查找父元素。</li>\n<li><code>$(selector).parents(filter)</code>：查找所有祖先元素。</li>\n<li><code>$(selector).children(filter)</code>：在所有子元素中查找。</li>\n<li><code>$(selector).find(filter)</code>：在所有后代元素中查找。</li>\n</ul>\n<p>（12）<code>ajax</code><br>\n<code>ajax</code> 可以让我们在不刷新页面的情况下只从服务器端获取某些数据，一般是获取一个 <code>json</code> 数据。</p>\n<p><code>GET</code> 方法（从服务器端获取内容）：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$.<span class=\"title function_\">ajax</span>(&#123;</span><br><span class=\"line\">    <span class=\"attr\">url</span>: url,</span><br><span class=\"line\">    <span class=\"attr\">type</span>: <span class=\"string\">&quot;GET&quot;</span>,</span><br><span class=\"line\">    <span class=\"attr\">data</span>: &#123;</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    <span class=\"attr\">dataType</span>: <span class=\"string\">&quot;json&quot;</span>,</span><br><span class=\"line\">    <span class=\"attr\">success</span>: <span class=\"keyword\">function</span> (<span class=\"params\">resp</span>) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure>\n<p><code>POST</code> 方法（把表单内容提交给服务器）：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$.<span class=\"title function_\">ajax</span>(&#123;</span><br><span class=\"line\">    <span class=\"attr\">url</span>: url,</span><br><span class=\"line\">    <span class=\"attr\">type</span>: <span class=\"string\">&quot;POST&quot;</span>,</span><br><span class=\"line\">    <span class=\"attr\">data</span>: &#123;</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    <span class=\"attr\">dataType</span>: <span class=\"string\">&quot;json&quot;</span>,</span><br><span class=\"line\">    <span class=\"attr\">success</span>: <span class=\"keyword\">function</span> (<span class=\"params\">resp</span>) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure>\n<h3 id=\"11-2-setTimeout与setInterval\">11.2 setTimeout与setInterval</h3>\n<p>（1）<code>setTimeout(func, delay)</code><br>\n经过 <code>delay</code> 毫秒后，执行函数 <code>func()</code>，可以使用 <code>clearTimeout()</code> 关闭定时器。例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">let</span> timeout_id = <span class=\"built_in\">setTimeout</span>(<span class=\"function\">() =&gt;</span> &#123;</span><br><span class=\"line\">    <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(<span class=\"string\">&quot;Hello World!&quot;</span>);</span><br><span class=\"line\">&#125;, <span class=\"number\">2000</span>);  <span class=\"comment\">// 2秒后在控制台输出&quot;Hello World!&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">clearTimeout</span>(timeout_id);  <span class=\"comment\">// 清除定时器</span></span><br></pre></td></tr></table></figure>\n<p>（2）<code>setInterval(func, delay)</code><br>\n每隔 <code>delay</code> 毫秒，执行一次函数 <code>func()</code>，第一次在第 <code>delay</code> 毫秒后执行，可以使用 <code>clearInterval()</code> 关闭周期执行的函数。例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">let</span> interval_id = <span class=\"built_in\">setInterval</span>(<span class=\"function\">() =&gt;</span> &#123;</span><br><span class=\"line\">    <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(<span class=\"string\">&quot;Hello World!&quot;</span>);</span><br><span class=\"line\">&#125;, <span class=\"number\">2000</span>);  <span class=\"comment\">// 每隔2秒，输出一次&quot;Hello World!&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">clearInterval</span>(interval_id);  <span class=\"comment\">// 清除周期执行的函数</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"11-3-requestAnimationFrame\">11.3 requestAnimationFrame</h3>\n<p><code>requestAnimationFrame(func)</code> 函数会在下次浏览器刷新页面之前执行一次，一般浏览器每秒刷新60次，因此通常会用递归写法使其每秒执行60次 <code>func()</code> 函数。调用时会向 <code>func()</code> 传入一个参数，表示函数执行的时间戳，单位为毫秒。例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">let</span> $div = $(<span class=\"string\">&#x27;div&#x27;</span>);</span><br><span class=\"line\"><span class=\"keyword\">let</span> $window = $(<span class=\"variable language_\">window</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">function</span> <span class=\"title function_\">main</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">function</span> <span class=\"title function_\">step</span>(<span class=\"params\">timestamp</span>) &#123;</span><br><span class=\"line\">        <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(timestamp);</span><br><span class=\"line\">        $div.<span class=\"title function_\">width</span>($div.<span class=\"title function_\">width</span>() + <span class=\"number\">1</span>);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> ($div.<span class=\"title function_\">width</span>() &gt;= $window.<span class=\"title function_\">width</span>()) <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">        <span class=\"title function_\">requestAnimationFrame</span>(step);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"title function_\">requestAnimationFrame</span>(step);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>使用 <code>setTimeout</code> 和 <code>setInterval</code> 实现以上效果的代码如下：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">let</span> <span class=\"title function_\">timeout_func</span> = (<span class=\"params\"></span>) =&gt; &#123;</span><br><span class=\"line\">    $div.<span class=\"title function_\">width</span>($div.<span class=\"title function_\">width</span>() + <span class=\"number\">1</span>);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> ($div.<span class=\"title function_\">width</span>() === $window.<span class=\"title function_\">width</span>()) <span class=\"built_in\">clearTimeout</span>(timeout_id);</span><br><span class=\"line\">    <span class=\"keyword\">else</span> timeout_id = <span class=\"built_in\">setTimeout</span>(timeout_func, <span class=\"number\">16.67</span>);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">let</span> timeout_id = <span class=\"built_in\">setTimeout</span>(timeout_func, <span class=\"number\">16.67</span>);</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">let</span> interval_id = <span class=\"built_in\">setInterval</span>(<span class=\"keyword\">function</span> (<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">    $div.<span class=\"title function_\">width</span>($div.<span class=\"title function_\">width</span>() + <span class=\"number\">1</span>);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> ($div.<span class=\"title function_\">width</span>() === $window.<span class=\"title function_\">width</span>()) <span class=\"built_in\">clearInterval</span>(interval_id);</span><br><span class=\"line\">&#125;, <span class=\"number\">16.67</span>);</span><br></pre></td></tr></table></figure>\n<p>与 <code>setTimeout</code> 和 <code>setInterval</code> 的区别：</p>\n<ul>\n<li><code>requestAnimationFrame</code> 渲染动画的效果更好，性能更佳。该函数可以保证每两次调用之间的时间间隔相同，但 <code>setTimeout</code> 与 <code>setInterval</code> 不能保证这点。<code>setTmeout</code> 两次调用之间的间隔包含回调函数的执行时间；<code>setInterval</code> 只能保证按固定时间间隔将回调函数压入栈中，但具体的执行时间间隔仍然受回调函数的执行时间影响。</li>\n<li>当页面在后台时，因为页面不再渲染，因此 <code>requestAnimationFrame</code> 不再执行。但 <code>setTimeout</code> 与 <code>setInterval</code> 函数会继续执行。</li>\n</ul>\n<h3 id=\"11-4-Map与Set\">11.4 Map与Set</h3>\n<p>（1）Map<br>\n<code>Map</code> 对象保存键值对。</p>\n<ul>\n<li>用 <code>for...of</code> 或者 <code>forEach</code> 可以按<strong>插入</strong>顺序遍历。</li>\n<li>键值可以为任意值，包括函数、对象或任意基本类型。</li>\n</ul>\n<p>常用 API：</p>\n<ul>\n<li><code>set(key, value)</code>：插入键值对，如果 <code>key</code> 已存在，则会覆盖原有的 <code>value</code>。</li>\n<li><code>get(key)</code>：查找关键字，如果不存在，返回 <code>undefined</code>。</li>\n<li><code>size</code>：返回键值对数量。</li>\n<li><code>has(key)</code>：返回是否包含关键字 <code>key</code>。</li>\n<li><code>delete(key)</code>：删除关键字 <code>key</code>。</li>\n<li><code>clear()</code>：删除所有元素。</li>\n</ul>\n<p>例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">let</span> mp = <span class=\"keyword\">new</span> <span class=\"title class_\">Map</span>();</span><br><span class=\"line\">mp.<span class=\"title function_\">set</span>(<span class=\"string\">&#x27;name&#x27;</span>, <span class=\"string\">&#x27;AsanoSaki&#x27;</span>);</span><br><span class=\"line\">mp.<span class=\"title function_\">set</span>(<span class=\"string\">&#x27;phone&#x27;</span>, <span class=\"string\">&#x27;123456&#x27;</span>);</span><br><span class=\"line\"><span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(mp, mp.<span class=\"title function_\">get</span>(<span class=\"string\">&#x27;name&#x27;</span>), mp.<span class=\"title function_\">has</span>(<span class=\"string\">&#x27;name&#x27;</span>), mp.<span class=\"property\">size</span>);</span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">let</span> [k, v] <span class=\"keyword\">of</span> mp) &#123;</span><br><span class=\"line\">    <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(k, v);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>（2）Set<br>\nSet 对象允许你存储任何类型的唯一值，无论是原始值或者是对象引用。</p>\n<ul>\n<li>用 <code>for...of</code> 或者 <code>forEach</code> 可以按插入顺序遍历。</li>\n</ul>\n<p>常用 API：</p>\n<ul>\n<li><code>add()</code>：添加元素。</li>\n<li><code>has()</code>：返回是否包含某个元素。</li>\n<li><code>size</code>：返回元素数量。</li>\n<li><code>delete()</code>：删除某个元素。</li>\n<li><code>clear()</code>：删除所有元素</li>\n</ul>\n<p>例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">let</span> st = <span class=\"keyword\">new</span> <span class=\"title class_\">Set</span>();</span><br><span class=\"line\">st.<span class=\"title function_\">add</span>(<span class=\"string\">&#x27;AsanoSaki&#x27;</span>);</span><br><span class=\"line\">st.<span class=\"title function_\">add</span>(<span class=\"number\">20</span>);</span><br><span class=\"line\"><span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(st, st.<span class=\"title function_\">has</span>(<span class=\"number\">20</span>), st.<span class=\"property\">size</span>);</span><br><span class=\"line\">st.<span class=\"title function_\">forEach</span>(<span class=\"function\">(<span class=\"params\">v</span>) =&gt;</span> &#123;</span><br><span class=\"line\">    <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(v);</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure>\n<h3 id=\"11-5-localStorage\">11.5 localStorage</h3>\n<p><code>localStorage</code> 可以在用户的浏览器上存储键值对。常用 API：</p>\n<ul>\n<li><code>setItem(key, value)</code>：插入。</li>\n<li><code>getItem(key)</code>：查找。</li>\n<li><code>removeItem(key)</code>：删除。</li>\n<li><code>clear()</code>：清空。</li>\n</ul>\n<p>例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"variable language_\">localStorage</span>.<span class=\"title function_\">setItem</span>(<span class=\"string\">&#x27;name&#x27;</span>, <span class=\"string\">&#x27;AsanoSaki&#x27;</span>);</span><br><span class=\"line\"><span class=\"variable language_\">localStorage</span>.<span class=\"title function_\">setItem</span>(<span class=\"string\">&#x27;age&#x27;</span>, <span class=\"number\">18</span>);</span><br><span class=\"line\"><span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(<span class=\"variable language_\">localStorage</span>.<span class=\"title function_\">getItem</span>(<span class=\"string\">&#x27;name&#x27;</span>));</span><br><span class=\"line\"><span class=\"variable language_\">localStorage</span>.<span class=\"title function_\">clear</span>();</span><br></pre></td></tr></table></figure>\n<h3 id=\"11-6-JSON\">11.6 JSON</h3>\n<p>JSON 对象用于序列化对象、数组、数值、字符串、布尔值和 <code>null</code>。常用 API：</p>\n<ul>\n<li><code>JSON.parse()</code>：将字符串解析成对象。</li>\n<li><code>JSON.stringify()</code>：将对象转化为字符串。</li>\n</ul>\n<p>例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">let</span> obj = &#123;</span><br><span class=\"line\">    <span class=\"attr\">name</span>: <span class=\"string\">&#x27;AsanoSaki&#x27;</span>,</span><br><span class=\"line\">    <span class=\"attr\">age</span>: <span class=\"number\">18</span></span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">let</span> str = <span class=\"title class_\">JSON</span>.<span class=\"title function_\">stringify</span>(obj);</span><br><span class=\"line\"><span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(str);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">let</span> new_obj = <span class=\"title class_\">JSON</span>.<span class=\"title function_\">parse</span>(str);</span><br><span class=\"line\"><span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(new_obj);</span><br></pre></td></tr></table></figure>\n<h3 id=\"11-7-日期\">11.7 日期</h3>\n<p>（1）返回值为整数的 API，数值为 <code>1970-1-1 00:00:00 UTC</code>（世界标准时间）到某个时刻所经过的毫秒数：</p>\n<ul>\n<li><code>Date.now()</code>：返回现在时刻。</li>\n<li><code>Date.parse(&quot;2022-04-15T15:30:00.000+08:00&quot;)</code>：返回北京时间2022年4月15日15:30:00的时刻。</li>\n</ul>\n<p>（2）与 <code>Date</code> 对象的实例相关的 API：</p>\n<ul>\n<li><code>new Date()</code>：返回现在时刻。</li>\n<li><code>new Date(&quot;2022-04-15T15:30:00.000+08:00&quot;)</code>：返回北京时间2022年4月15日15:30:00的时刻。</li>\n<li>两个 <code>Date</code> 对象实例的差值为毫秒数。</li>\n<li><code>getDay()</code>：返回星期，0表示星期日，1-6表示星期一至星期六。</li>\n<li><code>getDate()</code>：返回日，数值为1-31。</li>\n<li><code>getMonth()</code>：返回月，数值为0-11。</li>\n<li><code>getFullYear()</code>：返回年份。</li>\n<li><code>getHours()</code>：返回小时。</li>\n<li><code>getMinutes()</code>：返回分钟。</li>\n<li><code>getSeconds()</code>：返回秒。</li>\n<li><code>getMilliseconds()</code>：返回毫秒。</li>\n</ul>\n<p>例如：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">let</span> time = <span class=\"keyword\">new</span> <span class=\"title class_\">Date</span>();</span><br><span class=\"line\"><span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(time);</span><br><span class=\"line\"><span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(time.<span class=\"title function_\">getDay</span>());</span><br><span class=\"line\"><span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(time.<span class=\"title function_\">getDate</span>());</span><br><span class=\"line\"><span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(time.<span class=\"title function_\">getMonth</span>());</span><br></pre></td></tr></table></figure>\n<h3 id=\"11-8-WebSocket\">11.8 WebSocket</h3>\n<p>与服务器建立全双工连接。常用 API：</p>\n<ul>\n<li><code>new WebSocket('ws://localhost:8080');</code>：建立 WS 连接。</li>\n<li><code>send()</code>：向服务器端发送一个字符串。一般用 <code>JSON</code> 将传入的对象序列化为字符串。</li>\n<li><code>onopen</code>：类似于 <code>onclick</code>，当连接建立时触发。</li>\n<li><code>onmessage</code>：当从服务器端接收到消息时触发。</li>\n<li><code>close()</code>：关闭连接。</li>\n<li><code>onclose</code>：当连接关闭后触发。</li>\n</ul>\n<h3 id=\"11-9-window\">11.9 window</h3>\n<ul>\n<li><code>window.open(&quot;https://www.acwing.com&quot;)</code>：在新标签栏中打开页面。</li>\n<li><code>location.reload()</code>：刷新页面。</li>\n<li><code>location.href = &quot;https://www.acwing.com&quot;</code>：在当前标签栏中打开页面。</li>\n</ul>\n<h3 id=\"11-10-Canvas\">11.10 Canvas</h3>\n<p>Canvas 教程参考：<a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API/Tutorial\">Canvas Tutorial (English Version)</a>、<a href=\"https://developer.mozilla.org/zh-CN/docs/Web/API/Canvas_API/Tutorial\">Canvas 教程（中文）</a>。</p>\n",
            "tags": [
                "Web"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/11050.html",
            "url": "https://asanosaki.github.io/posts/11050.html",
            "title": "Web学习笔记-CSS",
            "date_published": "2022-11-09T07:49:00.000Z",
            "content_html": "<blockquote>\n<p>本文记录 CSS 的学习过程。<br>\nCSS（层叠样式表）是一种用来为结构化文档（如 HTML 文档或 XML 应用）添加样式（字体、间距和颜色等）的计算机语言，CSS 文件扩展名为：<code>.css</code>。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-样式定义方式\">1. 样式定义方式</h2>\n<p>（1）行内样式表（inline style sheet）<br>\n直接定义在标签的 <code>style</code> 属性中，仅对<strong>当前标签</strong>产生影响。</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">img</span> <span class=\"attr\">src</span>=<span class=\"string\">&quot;/Web Application Lesson/static/images/logo.png&quot;</span> <span class=\"attr\">alt</span>=<span class=\"string\">&quot;&quot;</span> <span class=\"attr\">style</span>=<span class=\"string\">&quot;width: 300px; height: 300px;&quot;</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>（2）内部样式表（internal style sheet）<br>\n定义在 <code>style</code> 标签中，通过选择器影响对应的标签，可以对<strong>同一个页面中的多个标签</strong>产生影响。</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;!DOCTYPE <span class=\"keyword\">html</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">html</span> <span class=\"attr\">lang</span>=<span class=\"string\">&quot;en&quot;</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">charset</span>=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">http-equiv</span>=<span class=\"string\">&quot;X-UA-Compatible&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;IE=edge&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;viewport&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">title</span>&gt;</span>Document<span class=\"tag\">&lt;/<span class=\"name\">title</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">style</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;text/css&quot;</span>&gt;</span><span class=\"language-css\"></span></span><br><span class=\"line\"><span class=\"language-css\">        <span class=\"selector-tag\">img</span> &#123;</span></span><br><span class=\"line\"><span class=\"language-css\">            <span class=\"attribute\">width</span>: <span class=\"number\">100px</span>;</span></span><br><span class=\"line\"><span class=\"language-css\">            <span class=\"attribute\">height</span>: <span class=\"number\">100px</span>;</span></span><br><span class=\"line\"><span class=\"language-css\">        &#125;</span></span><br><span class=\"line\"><span class=\"language-css\"></span></span><br><span class=\"line\"><span class=\"language-css\">        <span class=\"selector-tag\">p</span> &#123;</span></span><br><span class=\"line\"><span class=\"language-css\">            <span class=\"attribute\">width</span>: <span class=\"number\">50px</span>;</span></span><br><span class=\"line\"><span class=\"language-css\">            <span class=\"attribute\">height</span>: <span class=\"number\">50px</span>;</span></span><br><span class=\"line\"><span class=\"language-css\">            <span class=\"attribute\">background-color</span>: lightgreen;</span></span><br><span class=\"line\"><span class=\"language-css\">        &#125;</span></span><br><span class=\"line\"><span class=\"language-css\"></span></span><br><span class=\"line\"><span class=\"language-css\">        <span class=\"comment\">/* 注意自定义class时首部要加上.号 */</span></span></span><br><span class=\"line\"><span class=\"language-css\">        <span class=\"selector-class\">.lightblue_p</span> &#123;</span></span><br><span class=\"line\"><span class=\"language-css\">            <span class=\"attribute\">background-color</span>: lightblue;</span></span><br><span class=\"line\"><span class=\"language-css\">        &#125;</span></span><br><span class=\"line\"><span class=\"language-css\"></span></span><br><span class=\"line\"><span class=\"language-css\">        <span class=\"selector-class\">.big</span> &#123;</span></span><br><span class=\"line\"><span class=\"language-css\">            <span class=\"attribute\">width</span>: <span class=\"number\">150px</span>;</span></span><br><span class=\"line\"><span class=\"language-css\">            <span class=\"attribute\">height</span>: <span class=\"number\">150px</span>;</span></span><br><span class=\"line\"><span class=\"language-css\">        &#125;</span></span><br><span class=\"line\"><span class=\"language-css\">    </span><span class=\"tag\">&lt;/<span class=\"name\">style</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">img</span> <span class=\"attr\">src</span>=<span class=\"string\">&quot;/Web Application Lesson/static/images/logo.png&quot;</span> <span class=\"attr\">alt</span>=<span class=\"string\">&quot;&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">img</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;big&quot;</span> <span class=\"attr\">src</span>=<span class=\"string\">&quot;/Web Application Lesson/static/images/logo.png&quot;</span> <span class=\"attr\">alt</span>=<span class=\"string\">&quot;&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">p</span>&gt;</span>1<span class=\"tag\">&lt;/<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">p</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;lightblue_p big&quot;</span>&gt;</span>2<span class=\"tag\">&lt;/<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">p</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;big&quot;</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">p</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;lightblue_p&quot;</span>&gt;</span>4<span class=\"tag\">&lt;/<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">html</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>（3）外部样式表（external style sheet）<br>\n定义在 <code>.css</code> 样式文件中，通过选择器影响对应的标签。可以用 <code>link</code> 标签引入某些页面，可以对<strong>多个页面</strong>产生影响。</p>\n<p>首先在 <code>/static/css</code> 文件夹下创建 <code>style.css</code> 文件，将之前定义的样式代码移到该文件下：</p>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"selector-tag\">img</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-tag\">p</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">50px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">50px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: lightgreen;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/* 注意自定义class时首部要加上.号 */</span></span><br><span class=\"line\"><span class=\"selector-class\">.lightblue_p</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: lightblue;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-class\">.big</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">150px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">150px</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>然后在 <code>.html</code> 文件中用 <code>link</code> 链接该样式表即可：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;!DOCTYPE <span class=\"keyword\">html</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">html</span> <span class=\"attr\">lang</span>=<span class=\"string\">&quot;en&quot;</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">charset</span>=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">http-equiv</span>=<span class=\"string\">&quot;X-UA-Compatible&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;IE=edge&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;viewport&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">title</span>&gt;</span>Document<span class=\"tag\">&lt;/<span class=\"name\">title</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">&lt;!-- 在此处链接样式表 --&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">link</span> <span class=\"attr\">rel</span>=<span class=\"string\">&quot;stylesheet&quot;</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/Web Application Lesson/static/css/style.css&quot;</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;text/css&quot;</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">img</span> <span class=\"attr\">src</span>=<span class=\"string\">&quot;/Web Application Lesson/static/images/logo.png&quot;</span> <span class=\"attr\">alt</span>=<span class=\"string\">&quot;&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">img</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;big&quot;</span> <span class=\"attr\">src</span>=<span class=\"string\">&quot;/Web Application Lesson/static/images/logo.png&quot;</span> <span class=\"attr\">alt</span>=<span class=\"string\">&quot;&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">p</span>&gt;</span>1<span class=\"tag\">&lt;/<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">p</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;lightblue_p big&quot;</span>&gt;</span>2<span class=\"tag\">&lt;/<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">p</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;big&quot;</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">p</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;lightblue_p&quot;</span>&gt;</span>4<span class=\"tag\">&lt;/<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">html</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"2-选择器\">2. 选择器</h2>\n<p>（1）标签选择器<br>\n例如选择所有 <code>div</code> 标签：</p>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"selector-tag\">div</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">200px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">200px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: gray;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>（2）ID 选择器<br>\n例如选择 ID 为 <code>rect_1</code> 的标签：</p>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"selector-id\">#rect_1</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">200px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">200px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: gray;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>（3）类选择器<br>\n例如选择所有 <code>rectangle</code> 类的标签（注意：习惯上一个页面的 <code>id</code> 是唯一的，而 <code>class</code> 不是唯一的；且一个标签可以同时有多个 <code>class</code>，用空格隔开即可，多个 <code>class</code> 的效果根据 <code>.css</code> 文件中的定义顺序进行覆盖，后定义的覆盖先定义的样式）：</p>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"selector-class\">.rectangle</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">200px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">200px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: gray;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>（4）伪类选择器<br>\n伪类用于定义元素的特殊状态。</p>\n<p>链接伪类选择器：</p>\n<ul>\n<li><code>:link</code>：链接访问前的样式</li>\n<li><code>:visited</code>：链接访问后的样式</li>\n<li><code>:hover</code>：鼠标悬停时的样式</li>\n<li><code>:active</code>：鼠标点击后长按时的样式</li>\n<li><code>:focus</code>：聚焦后的样式</li>\n</ul>\n<p>位置伪类选择器：<code>:nth-child(n)</code>：选择是其父标签第 <code>n</code> 个子元素的所有元素。</p>\n<p>目标伪类选择器：<code>:target</code>：当 <code>url</code> 指向该元素时生效。</p>\n<p>以上就是较为常用的选择器，现在来看一个综合示例，首先是 <code>index.html</code> 代码：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;!DOCTYPE <span class=\"keyword\">html</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">html</span> <span class=\"attr\">lang</span>=<span class=\"string\">&quot;en&quot;</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">charset</span>=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">http-equiv</span>=<span class=\"string\">&quot;X-UA-Compatible&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;IE=edge&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;viewport&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">title</span>&gt;</span>Document<span class=\"tag\">&lt;/<span class=\"name\">title</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">link</span> <span class=\"attr\">rel</span>=<span class=\"string\">&quot;stylesheet&quot;</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/Web Application Lesson/static/css/style.css&quot;</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;text/css&quot;</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;effect&quot;</span>&gt;</span>1<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">id</span>=<span class=\"string\">&quot;mydiv&quot;</span>&gt;</span>2<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">id</span>=<span class=\"string\">&quot;mydiv2&quot;</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;https://www.baidu.com&quot;</span>&gt;</span>Baidu<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;#mydiv2&quot;</span>&gt;</span>MyDiv2<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">input</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;text&quot;</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">html</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p><code>style.css</code>代码：</p>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"selector-tag\">div</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: lightblue;</span><br><span class=\"line\">    <span class=\"attribute\">margin-bottom</span>: <span class=\"number\">10px</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-tag\">div</span><span class=\"selector-pseudo\">:nth-child</span>(<span class=\"number\">3</span>) &#123;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: lightgreen;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-class\">.effect</span><span class=\"selector-pseudo\">:hover</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/* 鼠标悬停时扩大为原来的1.1倍 */</span></span><br><span class=\"line\">    <span class=\"attribute\">transform</span>: <span class=\"built_in\">scale</span>(<span class=\"number\">1.1</span>);</span><br><span class=\"line\">    <span class=\"comment\">/* 变化过程时间为300ms */</span></span><br><span class=\"line\">    <span class=\"attribute\">transition</span>: <span class=\"number\">300ms</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-id\">#mydiv</span><span class=\"selector-pseudo\">:hover</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: lightcoral;</span><br><span class=\"line\">    <span class=\"attribute\">transition</span>: <span class=\"number\">300ms</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-tag\">a</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">font-size</span>: <span class=\"number\">30px</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-tag\">a</span><span class=\"selector-pseudo\">:link</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">color</span>: lightblue;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-tag\">a</span><span class=\"selector-pseudo\">:visited</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">color</span>: lightcoral;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-tag\">a</span><span class=\"selector-pseudo\">:hover</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">color</span>: lightskyblue;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-tag\">a</span><span class=\"selector-pseudo\">:active</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">color</span>: lightpink;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-tag\">input</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">25px</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-tag\">input</span><span class=\"selector-pseudo\">:focus</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">200px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: lightgray;</span><br><span class=\"line\">    <span class=\"attribute\">transition</span>: <span class=\"number\">300ms</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-id\">#mydiv2</span><span class=\"selector-pseudo\">:target</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">150px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">150px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: lightsalmon;</span><br><span class=\"line\">    <span class=\"attribute\">transition</span>: <span class=\"number\">300ms</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>（5）复合选择器<br>\n由两个及以上基础选择器组合而成的选择器。</p>\n<ul>\n<li><code>element1, element2</code>：同时选择元素 <code>element1</code> 和元素 <code>element2</code>。</li>\n<li><code>element.class</code>：选则包含某类的 <code>element</code> 元素。</li>\n<li><code>element1 + element2</code>：选择紧跟 <code>element1</code> 的 <code>element2</code> 元素。</li>\n<li><code>element1 element2</code>：选择 <code>element1</code> 内的所有 <code>element2</code> 元素。</li>\n<li><code>element1 &gt; element2</code>：选择父标签是 <code>element1</code> 的所有 <code>element2</code> 元素。</li>\n</ul>\n<p>（6）通配符选择器</p>\n<ul>\n<li><code>*</code>：选择所有标签。</li>\n<li><code>[attribute]</code>：选择具有某个属性的所有标签。</li>\n<li><code>[attribute=value]</code>：选择 <code>attribute</code> 值为 <code>value</code> 的所有标签。</li>\n</ul>\n<p>（7）伪元素选择器<br>\n将特定内容当做一个元素，选择这些元素的选择器被称为伪元素选择器。</p>\n<ul>\n<li><code>::first-letter</code>：选择第一个字母。</li>\n<li><code>::first-line</code>：选择第一行。</li>\n<li><code>::selection</code>：选择已被选中的内容。</li>\n<li><code>::after</code>：可以在元素后插入内容。</li>\n<li><code>::before</code>：可以在元素前插入内容。</li>\n</ul>\n<p>（8）样式渲染优先级</p>\n<ul>\n<li>权重大小，越具体的选择器权重越大：<code>!important</code> &gt; 行内样式 &gt; ID 选择器 &gt; 类与伪类选择器 &gt; 标签选择器 &gt; 通用选择器。</li>\n<li>权重相同时，后面的样式会覆盖前面的样式。</li>\n<li>继承自父元素的权重最低。</li>\n</ul>\n<h2 id=\"3-颜色\">3. 颜色</h2>\n<p>（1）预定义的颜色值<br>\n<code>black</code>、<code>white</code>、<code>red</code>、<code>green</code>、<code>blue</code>、<code>lightblue</code> 等。</p>\n<p>（2）16进制表示法<br>\n使用6位16进制数表示颜色，例如：<code>#ADD8E6</code>。<br>\n其中第1-2位表示红色，第3-4位表示绿色，第5-6位表示蓝色。<br>\n简写方式：<code>#ABC</code>，等价于 <code>#AABBCC</code>。</p>\n<p>（3）RGB 表示法<br>\n<code>rgb(173, 216, 230)</code>，其中第一个数表示红色，第二个数表示绿色，第三个数表示蓝色。</p>\n<p>（4）RGBA 表示法<br>\n<code>rgba(173, 216, 230, 0.5)</code>，前三个数同上，第四个数表示透明度。</p>\n<p>（5）取色方式</p>\n<ul>\n<li>网页里的颜色，可以在 Chrome 浏览器的调试模式下获取</li>\n<li>其他颜色可以使用 QQ 的截图软件：直接按 <code>c</code> 键，可以复制 RGB 颜色值；按住 <code>shift</code> 再按 <code>c</code> 键，可以复制16进制颜色值。</li>\n</ul>\n<h2 id=\"4-文本\">4. 文本</h2>\n<p>长度单位：</p>\n<ul>\n<li><code>px</code>：设备上的像素点</li>\n<li><code>%</code>：相对于父元素的百分比</li>\n<li><code>em</code>：相对于当前元素的字体大小（倍）</li>\n<li><code>rem</code>：相对于根元素的字体大小（倍）</li>\n<li><code>vw</code>：相对于视窗宽度的百分比</li>\n<li><code>vh</code>：相对于视窗高度的百分比</li>\n</ul>\n<p>（1）<code>text-align</code><br>\n<code>text-align</code> 属性定义行内内容（例如文字）如何相对它的块父元素对齐。<code>text-align</code> 并不控制块元素自己的对齐，只控制它的行内内容的对齐。</p>\n<p>（2）<code>line-height</code><br>\n<code>line-height</code> 属性用于设置多行元素的空间量，如多行文本的间距。对于块级元素，它指定元素行盒（line boxes）的最小高度。对于非替代的 <code>inline</code> 元素，它用于计算行盒（line box）的高度。当 <code>line-height</code> 与 <code>height</code> 相等时可以让字体竖直居中。</p>\n<p>（3）<code>letter-spacing</code><br>\n<code>letter-spacing</code> 属性用于设置文本字符的间距。</p>\n<p>（4）<code>text-indent</code><br>\n<code>text-indent</code> 属性能定义一个块元素首行文本内容之前的缩进量。</p>\n<p>（5）<code>text-decoration</code><br>\n<code>text-decoration</code> 属性是用于设置文本的修饰线外观的（下划线、上划线、贯穿线/删除线或闪烁）它是 <code>text-decoration-line</code>，<code>text-decoration-color</code>，<code>text-decoration-style</code>，和新出现的<code>text-decoration-thickness</code> 属性的缩写。</p>\n<p>（6）<code>text-shadow</code><br>\n<code>text-shadow</code> 为文字添加阴影。可以为文字与 <code>text-decorations</code> 添加多个阴影，阴影值之间用逗号隔开。每个阴影值由<code>(X方向的偏移量 Y方向的偏移量 模糊半径 颜色值)</code>组成。</p>\n<p>综合示例：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;!DOCTYPE <span class=\"keyword\">html</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">html</span> <span class=\"attr\">lang</span>=<span class=\"string\">&quot;en&quot;</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">charset</span>=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">http-equiv</span>=<span class=\"string\">&quot;X-UA-Compatible&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;IE=edge&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;viewport&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">title</span>&gt;</span>Document<span class=\"tag\">&lt;/<span class=\"name\">title</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">link</span> <span class=\"attr\">rel</span>=<span class=\"string\">&quot;stylesheet&quot;</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/Web Application Lesson/static/css/style.css&quot;</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;text/css&quot;</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;mydiv1&quot;</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">h1</span>&gt;</span>Title<span class=\"tag\">&lt;/<span class=\"name\">h1</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;mydiv2&quot;</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">p</span>&gt;</span>First paragraph.<span class=\"tag\">&lt;/<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;mydiv3&quot;</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\">                First paragraph.First paragraph.First paragraph.First paragraph.</span><br><span class=\"line\">                First paragraph.First paragraph.First paragraph.First paragraph.</span><br><span class=\"line\">                First paragraph.First paragraph.First paragraph.First paragraph.</span><br><span class=\"line\">                First paragraph.First paragraph.First paragraph.First paragraph.</span><br><span class=\"line\">                First paragraph.First paragraph.First paragraph.First paragraph.</span><br><span class=\"line\">                First paragraph.First paragraph.First paragraph.First paragraph.</span><br><span class=\"line\">                First paragraph.First paragraph.First paragraph.First paragraph.</span><br><span class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\">                Second paragraph.Second paragraph.Second paragraph.Second paragraph.</span><br><span class=\"line\">                Second paragraph.Second paragraph.Second paragraph.Second paragraph.</span><br><span class=\"line\">                Second paragraph.Second paragraph.Second paragraph.Second paragraph.</span><br><span class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">p</span>&gt;</span>Third paragraph.<span class=\"tag\">&lt;/<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;mydiv4&quot;</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">img</span> <span class=\"attr\">width</span>=<span class=\"string\">&quot;70&quot;</span> <span class=\"attr\">src</span>=<span class=\"string\">&quot;/Web Application Lesson/static/images/logo.png&quot;</span> <span class=\"attr\">alt</span>=<span class=\"string\">&quot;&quot;</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;https://www.acwing.com&quot;</span>&gt;</span>AcWing<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">html</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"selector-class\">.mydiv1</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/* 文本居中 */</span></span><br><span class=\"line\">    <span class=\"attribute\">text-align</span>: center;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-class\">.mydiv2</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">line-height</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: lightblue;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-class\">.mydiv3</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/* 字体大小 */</span></span><br><span class=\"line\">    <span class=\"attribute\">font-size</span>: <span class=\"number\">1.5rem</span>;</span><br><span class=\"line\">    <span class=\"comment\">/* 文本两端对齐 */</span></span><br><span class=\"line\">    <span class=\"attribute\">text-align</span>: justify;</span><br><span class=\"line\">    <span class=\"comment\">/* 文本首行缩进2倍默认大小的长度 */</span></span><br><span class=\"line\">    <span class=\"attribute\">text-indent</span>: <span class=\"number\">2em</span>;</span><br><span class=\"line\">    <span class=\"attribute\">text-shadow</span>: <span class=\"number\">3px</span> <span class=\"number\">3px</span> <span class=\"number\">2px</span> lightgray;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-class\">.mydiv4</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: lightgreen;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-tag\">a</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/* 去掉超链接的下划线 */</span></span><br><span class=\"line\">    <span class=\"attribute\">text-decoration</span>: none;</span><br><span class=\"line\">    <span class=\"attribute\">font-size</span>: <span class=\"number\">2rem</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/* 使图像在div中竖直居中 */</span></span><br><span class=\"line\"><span class=\"selector-tag\">img</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">position</span>: relative;</span><br><span class=\"line\">    <span class=\"attribute\">top</span>: <span class=\"number\">50%</span>;</span><br><span class=\"line\">    <span class=\"attribute\">transform</span>: <span class=\"built_in\">translateY</span>(-<span class=\"number\">50%</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"5-字体\">5. 字体</h2>\n<p>（1）<code>font-size</code><br>\n<code>font-size</code> 属性指定字体的大小。因为该属性的值会被用于计算 <code>em</code> 和 <code>ex</code> 长度单位，定义该值可能改变其他元素的大小。</p>\n<p>（2）<code>font-style</code><br>\n<code>font-style</code> 属性允许你选择 <code>font-family</code> 字体下的 <code>italic</code> 或 <code>oblique</code> 样式。</p>\n<p>（3）<code>font-weight</code><br>\n<code>font-weight</code> 属性指定了字体的粗细程度。一些字体只提供 <code>normal</code> 和 <code>bold</code> 两种值。</p>\n<p>（4）<code>font-family</code><br>\n<code>font-family</code> 属性允许您通过给定一个有先后顺序的，由字体名或者字体族名组成的列表来为选定的元素设置字体。属性值用逗号隔开。浏览器会选择列表中第一个该计算机上有安装的字体，或者是通过 <code>@font-face</code> 指定的可以直接下载的字体。</p>\n<p>综合示例：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;!DOCTYPE <span class=\"keyword\">html</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">html</span> <span class=\"attr\">lang</span>=<span class=\"string\">&quot;en&quot;</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">charset</span>=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">http-equiv</span>=<span class=\"string\">&quot;X-UA-Compatible&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;IE=edge&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;viewport&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">title</span>&gt;</span>Document<span class=\"tag\">&lt;/<span class=\"name\">title</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">link</span> <span class=\"attr\">rel</span>=<span class=\"string\">&quot;stylesheet&quot;</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/Web Application Lesson/static/css/style.css&quot;</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;text/css&quot;</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;mydiv&quot;</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\">            First paragraph.First paragraph.First paragraph.First paragraph.</span><br><span class=\"line\">            First paragraph.First paragraph.First paragraph.First paragraph.</span><br><span class=\"line\">            First paragraph.First paragraph.First paragraph.First paragraph.</span><br><span class=\"line\">            First paragraph.First paragraph.First paragraph.First paragraph.</span><br><span class=\"line\">            First paragraph.First paragraph.First paragraph.First paragraph.</span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\">            第二个段落。第二个段落。第二个段落。第二个段落。第二个段落。第二个段落。</span><br><span class=\"line\">            第二个段落。第二个段落。第二个段落。第二个段落。第二个段落。第二个段落。</span><br><span class=\"line\">            第二个段落。第二个段落。第二个段落。第二个段落。第二个段落。第二个段落。</span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\">            第三个段落。第三个段落。第三个段落。第三个段落。第三个段落。第三个段落。</span><br><span class=\"line\">            第三个段落。第三个段落。第三个段落。第三个段落。第三个段落。第三个段落。</span><br><span class=\"line\">            第三个段落。第三个段落。第三个段落。第三个段落。第三个段落。第三个段落。</span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\">            第四个段落。第四个段落。第四个段落。第四个段落。第四个段落。第四个段落。</span><br><span class=\"line\">            第四个段落。第四个段落。第四个段落。第四个段落。第四个段落。第四个段落。</span><br><span class=\"line\">            第四个段落。第四个段落。第四个段落。第四个段落。第四个段落。第四个段落。</span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">html</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"selector-class\">.mydiv</span>&gt;<span class=\"selector-tag\">p</span><span class=\"selector-pseudo\">:nth-child</span>(<span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">    <span class=\"attribute\">font-size</span>: <span class=\"number\">1.5rem</span>;</span><br><span class=\"line\">    <span class=\"attribute\">font-style</span>: oblique;</span><br><span class=\"line\">    <span class=\"attribute\">font-weight</span>: bold;</span><br><span class=\"line\">    <span class=\"attribute\">font-family</span>: Consolas;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-class\">.mydiv</span>&gt;<span class=\"selector-tag\">p</span><span class=\"selector-pseudo\">:nth-child</span>(<span class=\"number\">2</span>) &#123;</span><br><span class=\"line\">    <span class=\"attribute\">font-size</span>: <span class=\"number\">1.2rem</span>;</span><br><span class=\"line\">    <span class=\"attribute\">font-family</span>: <span class=\"string\">&quot;SimSun&quot;</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-class\">.mydiv</span>&gt;<span class=\"selector-tag\">p</span><span class=\"selector-pseudo\">:nth-child</span>(<span class=\"number\">3</span>) &#123;</span><br><span class=\"line\">    <span class=\"attribute\">font-size</span>: <span class=\"number\">1.3rem</span>;</span><br><span class=\"line\">    <span class=\"attribute\">font-family</span>: <span class=\"string\">&quot;KaiTi&quot;</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-class\">.mydiv</span>&gt;<span class=\"selector-tag\">p</span><span class=\"selector-pseudo\">:nth-child</span>(<span class=\"number\">4</span>) &#123;</span><br><span class=\"line\">    <span class=\"attribute\">font-size</span>: <span class=\"number\">1.4rem</span>;</span><br><span class=\"line\">    <span class=\"attribute\">font-family</span>: <span class=\"string\">&quot;Microsoft Yahei&quot;</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"6-背景\">6. 背景</h2>\n<p>（1）<code>background-color</code><br>\n<code>background-color</code> 属性会设置元素的背景色，属性的值为颜色值或关键字 <code>transparent</code>（透明）二者选其一。</p>\n<p>（2）<code>background-image</code><br>\n<code>background-image</code> 属性用于为一个元素设置一个或者多个背景图像。渐变色：<code>linear-gradient(rgba(0, 0, 255, 0.5), rgba(255, 255, 0, 0.5))</code>。</p>\n<p>（3）<code>background-size</code><br>\n<code>background-size</code> 属性设置背景图片大小。图片可以保有其原有的尺寸，或者拉伸到新的尺寸，或者在保持其原有比例的同时缩放到元素的可用空间的尺寸。</p>\n<p>（4）<code>background-repeat</code><br>\n<code>background-repeat</code> 属性定义背景图像的重复方式。背景图像可以沿着水平轴，垂直轴，两个轴重复，或者根本不重复。</p>\n<p>（5）<code>background-position</code><br>\n<code>background-position</code> 属性为背景图片设置初始位置。</p>\n<p>（6）<code>background-attachment</code><br>\n<code>background-attachment</code> 属性决定背景图像的位置是在视口内固定，或者随着包含它的区块滚动。</p>\n<p>综合示例：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;!DOCTYPE <span class=\"keyword\">html</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">html</span> <span class=\"attr\">lang</span>=<span class=\"string\">&quot;en&quot;</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">charset</span>=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">http-equiv</span>=<span class=\"string\">&quot;X-UA-Compatible&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;IE=edge&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;viewport&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">title</span>&gt;</span>Document<span class=\"tag\">&lt;/<span class=\"name\">title</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">link</span> <span class=\"attr\">rel</span>=<span class=\"string\">&quot;stylesheet&quot;</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/Web Application Lesson/static/css/style.css&quot;</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;text/css&quot;</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;mydiv1&quot;</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;mydiv2&quot;</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">div</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">html</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"selector-class\">.mydiv1</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">200px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">200px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">background-image</span>: <span class=\"built_in\">url</span>(<span class=\"string\">&#x27;/Web Application Lesson/static/images/logo.png&#x27;</span>);</span><br><span class=\"line\">    <span class=\"attribute\">background-size</span>: cover;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-class\">.mydiv2</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">200px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">200px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">background-image</span>: <span class=\"built_in\">url</span>(<span class=\"string\">&#x27;/Web Application Lesson/static/images/logo.png&#x27;</span>),</span><br><span class=\"line\">        <span class=\"built_in\">url</span>(<span class=\"string\">&#x27;/Web Application Lesson/static/images/image1.png&#x27;</span>);</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: lightblue;</span><br><span class=\"line\">    <span class=\"attribute\">background-size</span>: <span class=\"number\">100px</span> <span class=\"number\">200px</span>, <span class=\"number\">100px</span> <span class=\"number\">200px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">background-repeat</span>: no-repeat;</span><br><span class=\"line\">    <span class=\"attribute\">background-position</span>: left top, <span class=\"number\">100px</span>, top;</span><br><span class=\"line\">    <span class=\"attribute\">background-attachment</span>: scroll;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-class\">.mydiv2</span>&gt;<span class=\"selector-tag\">div</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: lightblue;</span><br><span class=\"line\">    <span class=\"attribute\">opacity</span>: <span class=\"number\">0.5</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"7-边框\">7. 边框</h2>\n<p>（1）<code>border-style</code><br>\n<code>border-style</code> 属性用来设定元素所有边框的样式。其内容为：<code>(border-top-style border-right-style border-bottom-style border-left-style)</code>，之后的所有属性设置内容格式也如此。</p>\n<p>（2）<code>border-width</code><br>\n<code>border-width</code> 属性用于设置元素的边框宽度。</p>\n<p>（3）<code>border-color</code><br>\n<code>border-color</code> 属性用于设置元素四个边框的颜色。</p>\n<p>（4）<code>border-radius</code><br>\n<code>border-radius</code> 属性允许你设置元素外边框的圆角。当使用一个半径时确定一个圆形，当使用两个半径时确定一个椭圆。这个（椭）圆与边框的交集形成圆角效果。</p>\n<p>（5）<code>border-collapse</code><br>\n<code>border-collapse</code> 属性是用来决定表格的边框是分开的还是合并的。在分隔模式下，相邻的单元格都拥有独立的边框。在合并模式下，相邻单元格共享边框。</p>\n<p>综合示例：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;!DOCTYPE <span class=\"keyword\">html</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">html</span> <span class=\"attr\">lang</span>=<span class=\"string\">&quot;en&quot;</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">charset</span>=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">http-equiv</span>=<span class=\"string\">&quot;X-UA-Compatible&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;IE=edge&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;viewport&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">title</span>&gt;</span>Document<span class=\"tag\">&lt;/<span class=\"name\">title</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">link</span> <span class=\"attr\">rel</span>=<span class=\"string\">&quot;stylesheet&quot;</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/Web Application Lesson/static/css/style.css&quot;</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;text/css&quot;</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">div</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">div</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">div</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">img</span> <span class=\"attr\">src</span>=<span class=\"string\">&quot;/Web Application Lesson/static/images/background.jpg&quot;</span> <span class=\"attr\">alt</span>=<span class=\"string\">&quot;&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">table</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">tbody</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">tr</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">td</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">td</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">td</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">td</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">td</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">td</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">tr</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">tr</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">td</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">td</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">td</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">td</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">td</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">td</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">tr</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">tr</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">td</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">td</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">td</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">td</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">td</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">td</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">tr</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">tbody</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">table</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">html</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"selector-tag\">div</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">200px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">200px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">margin</span>: <span class=\"number\">20px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: lightblue;</span><br><span class=\"line\">    <span class=\"attribute\">border-style</span>: solid dotted solid inset;</span><br><span class=\"line\">    <span class=\"attribute\">border-width</span>: <span class=\"number\">2px</span> <span class=\"number\">3px</span> <span class=\"number\">4px</span> <span class=\"number\">5px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">border-color</span>: lightcoral lightgreen lightpink lightsalmon;</span><br><span class=\"line\">    <span class=\"attribute\">border-radius</span>: <span class=\"number\">10px</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-tag\">img</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">border-radius</span>: <span class=\"number\">50%</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-tag\">td</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">border-style</span>: solid;</span><br><span class=\"line\">    <span class=\"attribute\">border-width</span>: <span class=\"number\">3px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">20px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">20px</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-tag\">table</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">border-style</span>: solid;</span><br><span class=\"line\">    <span class=\"attribute\">border-width</span>: <span class=\"number\">3px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">border-collapse</span>: collapse;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"8-元素展示格式\">8. 元素展示格式</h2>\n<p>（1）<code>display</code></p>\n<ul>\n<li><code>block</code>：独占一行，<code>width</code>、<code>height</code>、<code>margin</code>、<code>padding</code> 均可控制，<code>width</code> 默认100%。例如 <code>&lt;div&gt;</code>。</li>\n<li><code>inline</code>：可以共占一行，<code>width</code> 与 <code>height</code> 无效，水平方向的 <code>margin</code> 与 <code>padding</code> 有效，竖直方向的 <code>margin</code> 与 <code>padding</code> 无效，<code>width</code> 默认为本身内容宽度。例如 <code>&lt;span&gt;</code>。</li>\n<li><code>inline-block</code>：可以共占一行，<code>width</code>、<code>height</code>、<code>margin</code>、<code>padding</code> 均可控制，<code>width</code> 默认为本身内容宽度。例如 <code>&lt;img&gt;</code>。</li>\n</ul>\n<p>（2）<code>white-space</code><br>\n<code>white-space</code> 属性是用来设置如何处理元素中的空白。</p>\n<p>（3）<code>text-overflow</code><br>\n<code>text-overflow</code> 属性确定如何向用户发出未显示的溢出内容信号。它可以被剪切，显示一个省略号或显示一个自定义字符串。</p>\n<p>（4）<code>overflow</code><br>\n<code>overflow</code> 属性定义当一个元素的内容太大而无法适应块级格式化上下文的时候该做什么。它是 <code>overflow-x</code> 和 <code>overflow-y</code> 的简写属性。</p>\n<h2 id=\"9-内边距与外边距\">9. 内边距与外边距</h2>\n<p>（1）<code>margin</code><br>\n<code>margin</code> 属性为给定元素设置所有四个（上下左右）方向的外边距属性。</p>\n<ul>\n<li>可以接受1~4个值（上、右、下、左的顺序）。</li>\n<li>可以分别指明四个方向：<code>margin-top</code>、<code>margin-right</code>、<code>margin-bottom</code>、<code>margin-left</code>。</li>\n<li>可取值：\n<ul>\n<li><code>length</code>：固定值，例如：<code>20px</code>。</li>\n<li><code>percentage</code>：相对于包含块的宽度，以百分比值为外边距，例如：<code>20%</code>。</li>\n<li><code>auto</code>：让浏览器自己选择一个合适的外边距。有时，在一些特殊情况下，该值可以使元素居中。</li>\n</ul>\n</li>\n<li>外边距重叠：\n<ul>\n<li>块的上外边距 <code>margin-top</code> 和下外边距 <code>margin-bottom</code> 有时合并（折叠）为单个边距，其大小为单个边距的最大值（或如果它们相等，则仅为其中一个），这种行为称为边距折叠。</li>\n<li>父元素与后代元素：父元素没有上边框和 <code>padding</code> 时，后代元素的 <code>margin-top</code> 会溢出，溢出后父元素的 <code>margin-top</code> 会与后代元素取最大值。</li>\n</ul>\n</li>\n</ul>\n<p>（2）<code>padding</code><br>\n<code>padding</code> 属性控制元素所有四条边的内边距区域。</p>\n<ul>\n<li>可以接受1~4个值（上、右、下、左的顺序）。</li>\n<li>可以分别指明四个方向：<code>padding-top</code>、<code>padding-right</code>、<code>padding-bottom</code>、<code>padding-left</code>。</li>\n<li>可取值：\n<ul>\n<li><code>length</code>：固定值。</li>\n<li><code>percentage</code>：相对于包含块的宽度，以百分比值为内边距。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"10-盒子模型\">10. 盒子模型</h2>\n<p><code>box-sizing</code>：定义了 <code>user agent</code> 应该如何计算一个元素的总宽度和总高度。</p>\n<ul>\n<li><code>content-box</code>：是默认值，设置 <code>border</code> 和 <code>padding</code> 均会增加元素的宽高。</li>\n<li><code>border-box</code>：设置 <code>border</code> 和 <code>padding</code> 不会改变元素的宽高，而是挤占内容区域。</li>\n</ul>\n<h2 id=\"11-位置\">11. 位置</h2>\n<p><code>position</code>：用于指定一个元素在文档中的定位方式。</p>\n<p>定位类型：</p>\n<ul>\n<li>定位元素（positioned element）是 <code>position</code> 值为 <code>relative</code>、<code>absolute</code>、<code>fixed</code> 或 <code>sticky</code> 的元素。（换句话说，它是除 <code>static</code> 以外的任何东西）。</li>\n<li>相对定位元素（relatively positioned element）是 <code>position</code> 值为 <code>relative</code> 的元素。<code>top</code> 和 <code>bottom</code> 属性指定相对其正常位置的垂直偏移量，<code>left</code> 和 <code>right</code> 属性指定水平偏移量。</li>\n<li>绝对定位元素（absolutely positioned element）是 <code>position</code> 值为 <code>absolute</code> 或 <code>fixed</code> 的元素。</li>\n<li>粘性定位元素（stickily positioned element）是 <code>position</code> 值为 <code>sticky</code> 的元素。</li>\n</ul>\n<p>取值：</p>\n<ul>\n<li><code>static</code>：该关键字指定元素使用正常的布局行为，即元素在文档常规流中当前的布局位置。此时 <code>top</code>、<code>right</code>、<code>bottom</code>、<code>left</code> 和 <code>z-index</code> 属性无效，其中 <code>z-index</code> 属性指定元素在Z轴上在第几层，也就是垂直于屏幕朝外的方向。</li>\n<li><code>relative</code>：该关键字下，元素先放置在未添加定位时的位置，然后在不改变页面布局的前提下调整元素位置（因此会在此元素未添加定位时所在位置即初始位置留下空白）。<code>top</code>、<code>right</code>、<code>bottom</code>、<code>left</code> 等调整元素相对于<strong>初始位置</strong>的偏移量。</li>\n<li><code>absolute</code>：元素会被移出正常文档流，并不为元素预留空间，通过指定元素相对于最近的非 <code>static</code> 定位祖先元素的偏移，来确定元素位置。绝对定位的元素可以设置外边距（margins），且不会与其他边距合并。</li>\n<li><code>fixed</code>：元素会被移出正常文档流，并不为元素预留空间，而是通过指定元素相对于屏幕视口（viewport）的位置来指定元素位置。元素的位置在屏幕滚动时不会改变。</li>\n<li><code>sticky</code>：元素根据正常文档流进行定位，然后相对它的最近滚动祖先（nearest scrolling ancestor）和 containing block（最近块级祖先，nearest block-level ancestor），包括 <code>table-related</code> 元素，基于 <code>top</code>、<code>right</code>、<code>bottom</code> 和 <code>left</code> 的值进行偏移。偏移值不会影响任何其他元素的位置。</li>\n</ul>\n<p>综合示例：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;!DOCTYPE <span class=\"keyword\">html</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">html</span> <span class=\"attr\">lang</span>=<span class=\"string\">&quot;en&quot;</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">charset</span>=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">http-equiv</span>=<span class=\"string\">&quot;X-UA-Compatible&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;IE=edge&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;viewport&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">title</span>&gt;</span>Document<span class=\"tag\">&lt;/<span class=\"name\">title</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">link</span> <span class=\"attr\">rel</span>=<span class=\"string\">&quot;stylesheet&quot;</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/Web Application Lesson/static/css/style.css&quot;</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;text/css&quot;</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;div_outer&quot;</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;div_inner1&quot;</span>&gt;</span>1<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;div_inner2&quot;</span>&gt;</span>2<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;div_inner3&quot;</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;div_inner4&quot;</span>&gt;</span>4<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">html</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"selector-class\">.div_outer</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">300px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">400px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: lightblue;</span><br><span class=\"line\">    <span class=\"attribute\">margin</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/* 防止后代元素的margin-top溢出 */</span></span><br><span class=\"line\"><span class=\"selector-class\">.div_outer</span><span class=\"selector-pseudo\">::before</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">content</span>: <span class=\"string\">&quot;&quot;</span>;</span><br><span class=\"line\">    <span class=\"attribute\">display</span>: table;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-class\">.div_inner1</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: darkred;</span><br><span class=\"line\">    <span class=\"attribute\">color</span>: white;</span><br><span class=\"line\">    <span class=\"attribute\">margin</span>: <span class=\"number\">10px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">display</span>: inline-block;</span><br><span class=\"line\">    <span class=\"attribute\">position</span>: relative;</span><br><span class=\"line\">    <span class=\"attribute\">z-index</span>: <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-class\">.div_inner2</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: darkgreen;</span><br><span class=\"line\">    <span class=\"attribute\">color</span>: white;</span><br><span class=\"line\">    <span class=\"attribute\">margin</span>: <span class=\"number\">10px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">display</span>: inline-block;</span><br><span class=\"line\">    <span class=\"attribute\">position</span>: relative;</span><br><span class=\"line\">    <span class=\"attribute\">top</span>: <span class=\"number\">30px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">right</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-class\">.div_inner3</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: darkred;</span><br><span class=\"line\">    <span class=\"attribute\">color</span>: white;</span><br><span class=\"line\">    <span class=\"attribute\">margin</span>: <span class=\"number\">10px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">display</span>: inline-block;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-class\">.div_inner4</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: darkblue;</span><br><span class=\"line\">    <span class=\"attribute\">color</span>: white;</span><br><span class=\"line\">    <span class=\"attribute\">margin</span>: <span class=\"number\">10px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">display</span>: inline-block;</span><br><span class=\"line\">    <span class=\"attribute\">position</span>: absolute;</span><br><span class=\"line\">    <span class=\"attribute\">top</span>: <span class=\"number\">20px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">left</span>: <span class=\"number\">20px</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"12-浮动\">12. 浮动</h2>\n<p>（1）<code>float</code><br>\n<code>float</code> 属性指定一个元素应沿其容器的左侧或右侧放置，允许文本和内联元素环绕它。该元素从网页的正常流动（文档流）中移除，尽管仍然保持部分的流动性（与绝对定位相反）。</p>\n<p>由于 <code>float</code> 意味着使用块布局，它在某些情况下修改 <code>display</code> 值的计算值：<code>display</code> 为 <code>inline</code> 或 <code>inline-block</code> 时，使用 <code>float</code> 后会统一变成 <code>block</code>。</p>\n<p>取值：</p>\n<ul>\n<li><code>left</code>：表明元素必须浮动在其所在的块容器左侧的关键字。</li>\n<li><code>right</code>：表明元素必须浮动在其所在的块容器右侧的关键字。</li>\n</ul>\n<p>（2）<code>clear</code><br>\n有时，你可能想要强制元素移至任何浮动元素下方。比如说，你可能希望某个段落与浮动元素保持相邻的位置，但又希望这个段落从头开始强制独占一行。此时可以使用 <code>clear</code>。</p>\n<p>取值：</p>\n<ul>\n<li><code>left</code>：清除左侧浮动。</li>\n<li><code>right</code>：清除右侧浮动。</li>\n<li><code>both</code>：清除左右两侧浮动。</li>\n</ul>\n<h2 id=\"13-中期实战\">13. 中期实战</h2>\n<p>现在来实现一个效果如下图所示的用户个人信息卡以及B站个人资料卡，限于篇幅就不放上代码了：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/de967cfe0780468bad8f16f2897d0710.png\" alt=\"\"></p>\n<p><img src=\"https://img-blog.csdnimg.cn/10f6560605b84378ba17eb6e2b3b30f3.png\" alt=\"\"></p>\n<h2 id=\"14-flex布局\">14. flex布局</h2>\n<p><code>flex</code> 属性设置了 flex 项目如何增大或缩小以适应其 flex 容器中可用的空间。</p>\n<p>（1）<code>flex-direction</code><br>\n<code>flex-direction</code> 属性指定了内部元素是如何在 flex 容器中布局的，定义了主轴的方向（正方向或反方向）。</p>\n<p>取值：</p>\n<ul>\n<li><code>row</code>：flex 容器的主轴被定义为与文本方向相同。主轴起点和主轴终点与内容方向相同。</li>\n<li><code>row-reverse</code>：表现和 <code>row</code> 相同，但是置换了主轴起点和主轴终点。</li>\n<li><code>column</code>：flex 容器的主轴和交叉轴相同。主轴起点与主轴终点和书写模式的前后点相同。</li>\n<li><code>column-reverse</code>：表现和 <code>column</code> 相同，但是置换了主轴起点和主轴终点。</li>\n</ul>\n<p>（2）<code>flex-wrap</code><br>\n<code>flex-wrap</code> 属性指定 flex 元素单行显示还是多行显示。如果允许换行，这个属性允许你控制行的堆叠方向。</p>\n<p>取值：</p>\n<ul>\n<li><code>nowrap</code>：默认值，不换行。</li>\n<li><code>wrap</code>：换行，第一行在上方。</li>\n<li><code>wrap-reverse</code>：换行，第一行在下方。</li>\n</ul>\n<p>（3）<code>flex-flow</code><br>\n<code>flex-flow</code> 属性是 <code>flex-direction</code> 和 <code>flex-wrap</code> 的简写。默认值为：<code>row nowrap</code>。</p>\n<p>（4）<code>justify-content</code> 属性定义了浏览器如何沿 flex 容器的主轴和 grid 容器的内联轴分配内容项之间和周围的空间。</p>\n<p>取值：</p>\n<ul>\n<li><code>flex-start</code>：默认值，沿主轴起点方向对齐。</li>\n<li><code>flex-end</code>：沿主轴终点方向对齐。</li>\n<li><code>start</code>：如果主轴是 <code>row</code> 或 <code>row-reverse</code>，则左对齐，如果主轴是 <code>column</code> 或 <code>column-reverse</code>，则上对齐。</li>\n<li><code>end</code>：如果主轴是 <code>row</code> 或 <code>row-reverse</code>，则右对齐，如果主轴是 <code>column</code> 或 <code>column-reverse</code>，则下对齐。</li>\n<li><code>space-between</code>：沿主轴的两端对齐。</li>\n<li><code>space-around</code>：在主轴上均匀分配弹性元素。相邻元素间距离相同。第一个元素到主轴起始位置的距离和最后一个元素到主轴结束位置的距离将会是相邻元素之间距离的一半。</li>\n<li><code>space-evenly</code>：沿着主轴均匀分布在指定的对齐容器中。相邻元素之间的间距，主轴起始位置到第一个元素的间距，主轴结束位置到最后一个元素的间距，都完全一样。</li>\n<li><code>center</code>：容器中的元素居中，且元素之间紧贴没有空隙。</li>\n</ul>\n<p>（5）<code>align-items</code><br>\n<code>align-items</code> 属性将所有直接子节点上的 <code>align-self</code> 值设置为一个组。<code>align-self</code> 属性设置项目在其包含块中在交叉轴方向上的对齐方式。</p>\n<p>取值：</p>\n<ul>\n<li><code>flex-start</code>：元素向交叉轴起点对齐。</li>\n<li><code>flex-end</code>：元素向交叉轴终点对齐。</li>\n<li><code>center</code>：元素在交叉轴居中。</li>\n<li><code>stretch</code>：元素在交叉轴方向被拉伸到与 flex 容器相同的高度或宽度（元素未被设定高度或宽度的前提下）。</li>\n</ul>\n<p>（6）<code>align-content</code><br>\n<code>align-content</code> 属性设置了浏览器如何沿着 flex 布局的交叉轴和 grid 布局的主轴在内容项之间和周围分配空间。</p>\n<p>取值：</p>\n<ul>\n<li><code>flex-start</code>：所有行从交叉轴起点开始填充，第一行的交叉轴起点边和容器的交叉轴起点边对齐，接下来的每一行紧跟前一行中间没有空隙。</li>\n<li><code>flex-end</code>：所有行从交叉轴末尾开始填充，最后一行的交叉轴终点边和容器的交叉轴终点边对齐，同时所有后续行与前一行紧贴。</li>\n<li><code>center</code>：所有行朝向容器的交叉轴中心填充，每行互相紧挨，相对于容器居中对齐，容器的交叉轴起点边和第一行的距离相等于容器的交叉轴终点边和最后一行的距离。</li>\n<li><code>stretch</code>：拉伸所有行来填满剩余空间。剩余空间平均地分配给每一行。</li>\n</ul>\n<p>（7）<code>order</code><br>\n<code>order</code> 属性定义 flex 项目的顺序，值越小越靠前。</p>\n<p>（8）<code>flex-grow</code><br>\n<code>flex-grow</code> 属性设置 flex 容器主尺寸的 <code>flex</code> 增长系数，也就是 flex 容器中的元素随着容器尺寸的增大而增大（<code>nowrap</code> 前提下）。负值无效，默认为0。</p>\n<p>（9）<code>flex-shrink</code><br>\n<code>flex-shrink</code> 属性指定了 flex 元素的收缩规则。flex 元素仅在默认宽度之和大于 flex 容器的时候才会发生收缩，其收缩的大小是依据 <code>flex-shrink</code> 的值。负值无效，默认为1。</p>\n<p>（10）<code>flex-basis</code><br>\n<code>flex-basis</code> 属性指定了 flex 元素在主轴方向上的初始大小。取值可以是长度例如：<code>100px</code>，也可以是一个相对于其父 flex 容器主轴尺寸的百分比。不允许为负值。默认为 <code>auto</code>。</p>\n<p>（11）<code>flex</code><br>\n<code>flex-grow</code>、<code>flex-shrink</code>、<code>flex-basis</code> 的缩写。</p>\n<p>常用取值：</p>\n<ul>\n<li><code>auto</code>：<code>flex: 1 1 auto</code></li>\n<li><code>none</code>：<code>flex: 0 0 auto</code></li>\n</ul>\n<p>综合样例：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;!DOCTYPE <span class=\"keyword\">html</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">html</span> <span class=\"attr\">lang</span>=<span class=\"string\">&quot;en&quot;</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">charset</span>=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">http-equiv</span>=<span class=\"string\">&quot;X-UA-Compatible&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;IE=edge&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;viewport&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">title</span>&gt;</span>Document<span class=\"tag\">&lt;/<span class=\"name\">title</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">link</span> <span class=\"attr\">rel</span>=<span class=\"string\">&quot;stylesheet&quot;</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/Web Application Lesson/static/css/flex.css&quot;</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;div_flex&quot;</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;div_flex_item&quot;</span>&gt;</span>1<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;div_flex_item&quot;</span>&gt;</span>2<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;div_flex_item&quot;</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;div_flex_item&quot;</span>&gt;</span>4<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;div_flex_item&quot;</span>&gt;</span>5<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;div_flex_item&quot;</span>&gt;</span>6<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;div_flex_item&quot;</span>&gt;</span>7<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;div_flex_item&quot;</span>&gt;</span>8<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;div_flex_item&quot;</span>&gt;</span>9<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;div_flex_item&quot;</span>&gt;</span>10<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">html</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"selector-class\">.div_flex</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">50%</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">50vh</span>;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: lightgray;</span><br><span class=\"line\">    <span class=\"attribute\">display</span>: flex;</span><br><span class=\"line\">    <span class=\"attribute\">flex-direction</span>: row;</span><br><span class=\"line\">    <span class=\"attribute\">flex-wrap</span>: wrap;</span><br><span class=\"line\">    <span class=\"attribute\">justify-content</span>: flex-end;</span><br><span class=\"line\">    <span class=\"attribute\">align-items</span>: center;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-class\">.div_flex_item</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-class\">.div_flex_item</span><span class=\"selector-pseudo\">:nth-child</span>(odd) &#123;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: lightpink;</span><br><span class=\"line\">    <span class=\"attribute\">order</span>: <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-class\">.div_flex_item</span><span class=\"selector-pseudo\">:nth-child</span>(even) &#123;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: lightgreen;</span><br><span class=\"line\">    <span class=\"attribute\">order</span>: <span class=\"number\">2</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"15-响应式布局\">15. 响应式布局</h2>\n<p>（1）<code>media</code> 查询：可以查询屏幕的各种信息，比如查询宽度，当屏幕宽度满足特定条件时应用某种 CSS。例如：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;!DOCTYPE <span class=\"keyword\">html</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">html</span> <span class=\"attr\">lang</span>=<span class=\"string\">&quot;en&quot;</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">charset</span>=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">http-equiv</span>=<span class=\"string\">&quot;X-UA-Compatible&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;IE=edge&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;viewport&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">title</span>&gt;</span>Document<span class=\"tag\">&lt;/<span class=\"name\">title</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">link</span> <span class=\"attr\">rel</span>=<span class=\"string\">&quot;stylesheet&quot;</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/Web Application Lesson/static/css/responsive.css&quot;</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;container&quot;</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;card&quot;</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">html</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"selector-class\">.container</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: lightgray;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">80%</span>;</span><br><span class=\"line\">    <span class=\"attribute\">margin</span>: <span class=\"number\">0</span> auto;</span><br><span class=\"line\">    <span class=\"attribute\">padding</span>: <span class=\"number\">10px</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-class\">.card</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">80%</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">100vh</span>;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: blueviolet;</span><br><span class=\"line\">    <span class=\"attribute\">margin</span>: <span class=\"number\">0</span> auto;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">@media</span> (<span class=\"attribute\">min-width</span>: <span class=\"number\">768px</span>) &#123;</span><br><span class=\"line\">    <span class=\"selector-class\">.card</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">background-color</span>: aquamarine;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>（2）栅格系统：预先将屏幕宽度分为12份，然后设定各元素在不同的屏幕宽度下应该占几份，例如：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;!DOCTYPE <span class=\"keyword\">html</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">html</span> <span class=\"attr\">lang</span>=<span class=\"string\">&quot;en&quot;</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">charset</span>=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">http-equiv</span>=<span class=\"string\">&quot;X-UA-Compatible&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;IE=edge&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;viewport&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">title</span>&gt;</span>Document<span class=\"tag\">&lt;/<span class=\"name\">title</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">link</span> <span class=\"attr\">rel</span>=<span class=\"string\">&quot;stylesheet&quot;</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/Web Application Lesson/static/css/responsive.css&quot;</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;container&quot;</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;row&quot;</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;col col-md-6 col-sm-12&quot;</span>&gt;</span>Username<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;col col-md-6 col-sm-12&quot;</span>&gt;</span>Password<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;col col-md-12 col-sm-12&quot;</span>&gt;</span>Self Introduction<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">html</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"selector-class\">.container</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: lightgray;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">80%</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">100vh</span>;</span><br><span class=\"line\">    <span class=\"attribute\">margin</span>: <span class=\"number\">0</span> auto;</span><br><span class=\"line\">    <span class=\"attribute\">padding</span>: <span class=\"number\">10px</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-class\">.col</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: lightsalmon;</span><br><span class=\"line\">    <span class=\"attribute\">border</span>: <span class=\"number\">1px</span> solid gray;</span><br><span class=\"line\">    <span class=\"attribute\">float</span>: left;</span><br><span class=\"line\">    <span class=\"attribute\">box-sizing</span>: border-box;</span><br><span class=\"line\">    <span class=\"attribute\">font-size</span>: <span class=\"number\">30px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">color</span>: white;</span><br><span class=\"line\">    <span class=\"attribute\">text-align</span>: center;</span><br><span class=\"line\">    <span class=\"attribute\">line-height</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/* 屏幕宽度大于等于768px时应用以下样式 */</span></span><br><span class=\"line\"><span class=\"keyword\">@media</span> (<span class=\"attribute\">min-width</span>: <span class=\"number\">768px</span>) &#123;</span><br><span class=\"line\">    <span class=\"selector-class\">.col-md-1</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"built_in\">calc</span>(<span class=\"number\">100%</span> / <span class=\"number\">12</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"selector-class\">.col-md-2</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"built_in\">calc</span>(<span class=\"number\">100%</span> / <span class=\"number\">6</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"selector-class\">.col-md-3</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"built_in\">calc</span>(<span class=\"number\">100%</span> / <span class=\"number\">4</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"selector-class\">.col-md-4</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"built_in\">calc</span>(<span class=\"number\">100%</span> / <span class=\"number\">3</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"selector-class\">.col-md-5</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"built_in\">calc</span>(<span class=\"number\">500%</span> / <span class=\"number\">12</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"selector-class\">.col-md-6</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"built_in\">calc</span>(<span class=\"number\">100%</span> / <span class=\"number\">2</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"selector-class\">.col-md-7</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"built_in\">calc</span>(<span class=\"number\">700%</span> / <span class=\"number\">12</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"selector-class\">.col-md-8</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"built_in\">calc</span>(<span class=\"number\">200%</span> / <span class=\"number\">3</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"selector-class\">.col-md-9</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"built_in\">calc</span>(<span class=\"number\">300%</span> / <span class=\"number\">4</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"selector-class\">.col-md-10</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"built_in\">calc</span>(<span class=\"number\">500%</span> / <span class=\"number\">6</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"selector-class\">.col-md-11</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"built_in\">calc</span>(<span class=\"number\">1100%</span> / <span class=\"number\">12</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"selector-class\">.col-md-12</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"number\">100%</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/* 屏幕宽度小于等于767px时应用以下样式 */</span></span><br><span class=\"line\"><span class=\"keyword\">@media</span> (<span class=\"attribute\">max-width</span>: <span class=\"number\">767px</span>) &#123;</span><br><span class=\"line\">    <span class=\"selector-class\">.col-sm-1</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"built_in\">calc</span>(<span class=\"number\">100%</span> / <span class=\"number\">12</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"selector-class\">.col-sm-2</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"built_in\">calc</span>(<span class=\"number\">100%</span> / <span class=\"number\">6</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"selector-class\">.col-sm-3</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"built_in\">calc</span>(<span class=\"number\">100%</span> / <span class=\"number\">4</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"selector-class\">.col-sm-4</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"built_in\">calc</span>(<span class=\"number\">100%</span> / <span class=\"number\">3</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"selector-class\">.col-sm-5</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"built_in\">calc</span>(<span class=\"number\">500%</span> / <span class=\"number\">12</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"selector-class\">.col-sm-6</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"built_in\">calc</span>(<span class=\"number\">100%</span> / <span class=\"number\">2</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"selector-class\">.col-sm-7</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"built_in\">calc</span>(<span class=\"number\">700%</span> / <span class=\"number\">12</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"selector-class\">.col-sm-8</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"built_in\">calc</span>(<span class=\"number\">200%</span> / <span class=\"number\">3</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"selector-class\">.col-sm-9</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"built_in\">calc</span>(<span class=\"number\">300%</span> / <span class=\"number\">4</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"selector-class\">.col-sm-10</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"built_in\">calc</span>(<span class=\"number\">500%</span> / <span class=\"number\">6</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"selector-class\">.col-sm-11</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"built_in\">calc</span>(<span class=\"number\">1100%</span> / <span class=\"number\">12</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"selector-class\">.col-sm-12</span> &#123;</span><br><span class=\"line\">        <span class=\"attribute\">width</span>: <span class=\"number\">100%</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>（3）Bootstrap<br>\n根据上述例子可以发现如果自己手动实现 CSS 代码会很长，因此可以直接使用 Bootstrap，首先前往官网下载：<a href=\"https://v5.bootcss.com/\">Bootstrap 官网</a>。然后在 <code>static</code> 中创建一个新文件夹 <code>third_party</code>（第三方），将下载好的 Bootstrap 放进来。在代码中引入 <code>bootstrap.min.css</code> 以及 <code>bootstrap.min.js</code> 后即可直接使用，例如：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;!DOCTYPE <span class=\"keyword\">html</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">html</span> <span class=\"attr\">lang</span>=<span class=\"string\">&quot;en&quot;</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">charset</span>=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">http-equiv</span>=<span class=\"string\">&quot;X-UA-Compatible&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;IE=edge&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;viewport&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">title</span>&gt;</span>Document<span class=\"tag\">&lt;/<span class=\"name\">title</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">link</span> <span class=\"attr\">rel</span>=<span class=\"string\">&quot;stylesheet&quot;</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/Web Application Lesson/static/css/responsive.css&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">link</span> <span class=\"attr\">rel</span>=<span class=\"string\">&quot;stylesheet&quot;</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/Web Application Lesson/static/third_party/bootstrap-5.1.3-dist/css/bootstrap.min.css&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">script</span> <span class=\"attr\">src</span>=<span class=\"string\">&quot;/Web Application Lesson/static/third_party/bootstrap-5.1.3-dist/js/bootstrap.min.js&quot;</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">script</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;container&quot;</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;row&quot;</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;col col-md-6 col-sm-12&quot;</span>&gt;</span>Username<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;col col-md-6 col-sm-12&quot;</span>&gt;</span>Password<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;col col-md-12 col-sm-12&quot;</span>&gt;</span>Self Introduction<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">html</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"selector-class\">.container</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: lightgray;</span><br><span class=\"line\">    <span class=\"attribute\">width</span>: <span class=\"number\">80%</span>;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">100vh</span>;</span><br><span class=\"line\">    <span class=\"attribute\">margin</span>: <span class=\"number\">0</span> auto;</span><br><span class=\"line\">    <span class=\"attribute\">padding</span>: <span class=\"number\">10px</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"selector-class\">.col</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">height</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">background-color</span>: lightsalmon;</span><br><span class=\"line\">    <span class=\"attribute\">border</span>: <span class=\"number\">1px</span> solid gray;</span><br><span class=\"line\">    <span class=\"attribute\">float</span>: left;</span><br><span class=\"line\">    <span class=\"attribute\">box-sizing</span>: border-box;</span><br><span class=\"line\">    <span class=\"attribute\">font-size</span>: <span class=\"number\">30px</span>;</span><br><span class=\"line\">    <span class=\"attribute\">color</span>: white;</span><br><span class=\"line\">    <span class=\"attribute\">text-align</span>: center;</span><br><span class=\"line\">    <span class=\"attribute\">line-height</span>: <span class=\"number\">100px</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Bootstrap 中一共定义了以下几种类型：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/0113bb40c0844e9c964211139a932ec5.png\" alt=\"\"></p>\n<p>此外，Bootstrap 还提供了很多设计好的组件，直接在官网中根据示例代码即可学习与使用，例如实现一个简单好看的表单如下图所示：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/2b377a8b03474e76ac18f359b7b5e332.png\" alt=\"\"></p>\n<p>代码如下：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;!DOCTYPE <span class=\"keyword\">html</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">html</span> <span class=\"attr\">lang</span>=<span class=\"string\">&quot;en&quot;</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">charset</span>=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">http-equiv</span>=<span class=\"string\">&quot;X-UA-Compatible&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;IE=edge&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;viewport&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">title</span>&gt;</span>Document<span class=\"tag\">&lt;/<span class=\"name\">title</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">link</span> <span class=\"attr\">rel</span>=<span class=\"string\">&quot;stylesheet&quot;</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/Web Application Lesson/static/third_party/bootstrap-5.1.3-dist/css/bootstrap.min.css&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">script</span> <span class=\"attr\">src</span>=<span class=\"string\">&quot;/Web Application Lesson/static/third_party/bootstrap-5.1.3-dist/js/bootstrap.min.js&quot;</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">script</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;container&quot;</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">form</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;row&quot;</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;col-md-6 col-xs-12&quot;</span>&gt;</span></span><br><span class=\"line\">                    <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;mb-3&quot;</span>&gt;</span></span><br><span class=\"line\">                        <span class=\"tag\">&lt;<span class=\"name\">label</span> <span class=\"attr\">for</span>=<span class=\"string\">&quot;inputUsername&quot;</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;form-label&quot;</span>&gt;</span>Username<span class=\"tag\">&lt;/<span class=\"name\">label</span>&gt;</span></span><br><span class=\"line\">                        <span class=\"tag\">&lt;<span class=\"name\">input</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;text&quot;</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;form-control&quot;</span> <span class=\"attr\">id</span>=<span class=\"string\">&quot;inputUsername&quot;</span> <span class=\"attr\">placeholder</span>=<span class=\"string\">&quot;Please input username&quot;</span>&gt;</span></span><br><span class=\"line\">                        <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">id</span>=<span class=\"string\">&quot;emailHelp&quot;</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;form-text&quot;</span>&gt;</span>We&#x27;ll never share your email with anyone else.<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">                    <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;col-md-6 col-xs-12&quot;</span>&gt;</span></span><br><span class=\"line\">                    <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;mb-3&quot;</span>&gt;</span></span><br><span class=\"line\">                        <span class=\"tag\">&lt;<span class=\"name\">label</span> <span class=\"attr\">for</span>=<span class=\"string\">&quot;inputPassword&quot;</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;form-label&quot;</span>&gt;</span>Password<span class=\"tag\">&lt;/<span class=\"name\">label</span>&gt;</span></span><br><span class=\"line\">                        <span class=\"tag\">&lt;<span class=\"name\">input</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;password&quot;</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;form-control&quot;</span> <span class=\"attr\">id</span>=<span class=\"string\">&quot;inputPassword&quot;</span></span></span><br><span class=\"line\"><span class=\"tag\">                            <span class=\"attr\">placeholder</span>=<span class=\"string\">&quot;Please input password&quot;</span>&gt;</span></span><br><span class=\"line\">                    <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;col-md-12 col-xs-12&quot;</span>&gt;</span></span><br><span class=\"line\">                    <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;mb-3&quot;</span>&gt;</span></span><br><span class=\"line\">                        <span class=\"tag\">&lt;<span class=\"name\">label</span> <span class=\"attr\">for</span>=<span class=\"string\">&quot;introTextarea&quot;</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;form-label&quot;</span>&gt;</span>Self introduction<span class=\"tag\">&lt;/<span class=\"name\">label</span>&gt;</span></span><br><span class=\"line\">                        <span class=\"tag\">&lt;<span class=\"name\">textarea</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;form-control&quot;</span> <span class=\"attr\">id</span>=<span class=\"string\">&quot;introTextarea&quot;</span> <span class=\"attr\">rows</span>=<span class=\"string\">&quot;3&quot;</span></span></span><br><span class=\"line\"><span class=\"tag\">                            <span class=\"attr\">placeholder</span>=<span class=\"string\">&quot;This person is very lazy, the introduction is nothing.&quot;</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">textarea</span>&gt;</span></span><br><span class=\"line\">                    <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;col-md-6 col-xs-12&quot;</span>&gt;</span></span><br><span class=\"line\">                    <span class=\"tag\">&lt;<span class=\"name\">button</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;button&quot;</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;btn btn-success&quot;</span> <span class=\"attr\">style</span>=<span class=\"string\">&quot;width: 100%; margin-top: 10px;&quot;</span>&gt;</span>Submit<span class=\"tag\">&lt;/<span class=\"name\">button</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;col-md-6 col-xs-12&quot;</span>&gt;</span></span><br><span class=\"line\">                    <span class=\"tag\">&lt;<span class=\"name\">button</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;button&quot;</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;btn btn-secondary&quot;</span></span></span><br><span class=\"line\"><span class=\"tag\">                        <span class=\"attr\">style</span>=<span class=\"string\">&quot;width: 100%; margin-top: 10px;&quot;</span>&gt;</span>Cancel<span class=\"tag\">&lt;/<span class=\"name\">button</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">form</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">html</span>&gt;</span></span><br></pre></td></tr></table></figure>\n",
            "tags": [
                "Web"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/28433.html",
            "url": "https://asanosaki.github.io/posts/28433.html",
            "title": "Web学习笔记-HTML",
            "date_published": "2022-11-08T04:05:00.000Z",
            "content_html": "<blockquote>\n<p>本文记录 HTML 的学习过程。<br>\nMDN 官方文档：<a href=\"https://developer.mozilla.org/zh-CN/\">MDN Web Docs</a>。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-VS-Code环境配置\">1. VS Code环境配置</h2>\n<p>（1）Live Server<br>\n由于一般写网站时都是部署在 Linux 上，该插件可以模拟一个终端，相当于模拟了一个真正的开发环境（后端）。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/b57b807db4454d4ab2786010dcdff287.png\" alt=\"\"></p>\n<p>（2）Auto Rename Tag<br>\n当修改 HTML 标签时，自动修改对应的标签对。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/3b4c0d64e3cf488aa4dbb1a79d9fb505.png\" alt=\"\"></p>\n<p>（3）自动格式化<br>\n在 <code>Setting-Text Editor-Formatting</code> 中勾选 <code>Format On Save</code>，这样保存代码的时候会自动格式化代码。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/8c850f7cf3334aba945de6f29ee8ad44.png\" alt=\"\"></p>\n<h2 id=\"2-HTML基础标签\">2. HTML基础标签</h2>\n<h3 id=\"2-1-HTML文件结构\">2.1 HTML文件结构</h3>\n<p>HTML 的所有标签为树形结构，一般都有一个开始标签和一个结束标签，开始标签和结束标签之间的标签就是子节点，同级的标签就是兄弟节点，例如：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;!DOCTYPE <span class=\"keyword\">html</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">html</span> <span class=\"attr\">lang</span>=<span class=\"string\">&quot;zh-CN&quot;</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">charset</span>=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;viewport&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">title</span>&gt;</span>Web Application Lesson<span class=\"tag\">&lt;/<span class=\"name\">title</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">h1</span>&gt;</span>First Class<span class=\"tag\">&lt;/<span class=\"name\">h1</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">html</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>（1）<code>&lt;html&gt;</code> 标签<br>\n表示一个HTML文档的根（顶级元素），所以它也被称为根元素。所有其他元素必须是此元素的后代。</p>\n<p>（2）<code>&lt;head&gt;</code> 标签：<br>\n规定文档相关的配置信息（元数据），包括文档的标题，引用的文档样式和脚本等。</p>\n<p>（3）<code>&lt;body&gt;</code> 标签：<br>\n表示文档的内容。<code>document.body</code> 属性提供了可以轻松访问文档的 <code>body</code> 元素的脚本。</p>\n<p>（4）<code>&lt;title&gt;</code> 标签：<br>\n定义文档的标题，显示在浏览器的标题栏或标签页上。它只应该包含文本，若是包含有标签，则它包含的任何标签都将被忽略。</p>\n<p>（5）<code>&lt;meta&gt;</code> 标签：<br>\n表示那些不能由其它 HTML 元相关（meta-related）元素（<code>&lt;base&gt;</code>、<code>&lt;link&gt;</code>、<code>&lt;script&gt;</code>、<code>&lt;style&gt;</code> 或 <code>&lt;title&gt;</code>）之一表示的任何元数据信息。</p>\n<p>常见属性：</p>\n<ul>\n<li><code>charset</code>：这个属性声明了文档的字符编码。如果使用了这个属性，其值必须是与 ASCII 大小写无关（ASCII case-insensitive）的 <code>utf-8</code>。</li>\n<li><code>name</code>：<code>name</code> 和 <code>content</code> 属性可以一起使用，以 <code>名 - 值</code> 对的方式给文档提供元数据，其中 <code>name</code> 作为元数据的名称，<code>content</code> 作为元数据的值。</li>\n</ul>\n<p>（6）<code>link</code> 标签：<br>\n规定了当前文档与外部资源的关系。该元素最常用于链接样式表，此外也可以被用来创建站点图标 <code>&lt;icon&gt;</code>，例如：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">link</span> <span class=\"attr\">rel</span>=<span class=\"string\">&quot;icon&quot;</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/Web Application Lesson/images/logo.png&quot;</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>（7）<code>&lt;!-- 多行注释 --&gt;</code>：<br>\nHTML 中只有多行注释，没有单行注释。</p>\n<p>综合示例如下：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;!DOCTYPE <span class=\"keyword\">html</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">html</span> <span class=\"attr\">lang</span>=<span class=\"string\">&quot;zh-CN&quot;</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">charset</span>=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;viewport&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;description&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;This is the description of web page.&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;keywords&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;This is the keywords when searching.&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">title</span>&gt;</span>Web Application Lesson<span class=\"tag\">&lt;/<span class=\"name\">title</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">link</span> <span class=\"attr\">rel</span>=<span class=\"string\">&quot;icon&quot;</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/Web Application Lesson/images/logo.png&quot;</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">h1</span>&gt;</span>First Class<span class=\"tag\">&lt;/<span class=\"name\">h1</span>&gt;</span></span><br><span class=\"line\">    <span class=\"comment\">&lt;!--</span></span><br><span class=\"line\"><span class=\"comment\">        This is first annotation sentence.</span></span><br><span class=\"line\"><span class=\"comment\">        This is second.</span></span><br><span class=\"line\"><span class=\"comment\">        ...</span></span><br><span class=\"line\"><span class=\"comment\">    --&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">html</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"2-2-文本标签\">2.2 文本标签</h3>\n<p>文本标签虽然很多，但大部分可看成是预定好样式的 <code>&lt;div&gt;</code> 和 <code>&lt;span&gt;</code>。</p>\n<p>（1）<code>&lt;div&gt;</code> 标签：<br>\n<code>&lt;div&gt;</code> 元素（或 HTML 文档分区元素）是一个通用型的流内容容器，在不使用 CSS 的情况下，其对内容或布局没有任何影响。其他块级标签例如：<code>&lt;h1&gt;, &lt;p&gt;, &lt;pre&gt;, &lt;ul&gt;, &lt;ol&gt;, &lt;table&gt;</code>。方便后续为某一块内容设置样式，且在 JS 中可以对各个 <code>div</code> 进行操作。即在逻辑上将某一块代码归为一类。</p>\n<p>（2）<code>&lt;span&gt;</code> 标签：<br>\n<code>&lt;span&gt;</code> 元素是短语内容的通用行内容器，并没有任何特殊语义。可以使用它来编组元素以达到某种样式意图（通过使用类 <code>class</code> 或者 <code>id</code> 属性），或者这些元素有着共同的属性，比如 <code>lang</code>。应该在没有其他合适的语义元素时才使用它。<code>&lt;span&gt;</code> 与 <code>&lt;div&gt;</code> 元素很相似，但 <code>&lt;div&gt;</code> 是一个<strong>块元素</strong>而 <code>&lt;span&gt;</code> 则是<strong>行内元素</strong>。其他内联标签例如：<code>&lt;i&gt;, &lt;b&gt;, &lt;del&gt;, &lt;ins&gt;, &lt;td&gt;, &lt;a&gt;</code>。</p>\n<p>（3）<code>&lt;h1&gt; - &lt;h6&gt;</code> 标签：<br>\nHTML 标题（Heading）元素呈现了六个不同的级别的标题，<code>&lt;h1&gt;</code> 级别最高，而 <code>&lt;h6&gt;</code> 级别最低。</p>\n<p>（4）<code>&lt;p&gt;</code> 标签：<br>\n<code>&lt;p&gt;</code> 元素（或者说 HTML 段落元素）表示文本的一个段落。该元素通常表现为一整块与相邻文本分离的文本，或以垂直的空白隔离或以首行缩进。另外，<code>&lt;p&gt;</code> 是块级元素。</p>\n<p>（5）<code>&lt;pre&gt;</code> 标签：<br>\n<code>&lt;pre&gt;</code> 元素表示预定义格式文本。在该元素中的文本通常按照原文件中的编排，以等宽字体的形式展现出来，文本中的空白符（比如空格和换行符）都会显示出来。（紧跟在 <code>&lt;pre&gt;</code> 开始标签后的换行符也会被省略）</p>\n<p>（6）<code>&lt;br&gt;</code> 标签：<br>\n<code>&lt;br&gt;</code> 元素在文本中生成一个换行（回车）符号。此元素在写诗和地址时很有用，这些地方的换行都非常重要。</p>\n<p>（7）<code>&lt;hr&gt;</code> 标签：<br>\n<code>&lt;hr&gt;</code> 元素表示段落级元素之间的主题转换（例如，一个故事中的场景的改变，或一个章节的主题的改变）。<br>\n在 HTML 的早期版本中，它是一个水平线。现在它仍能在可视化浏览器中表现为水平线，但目前被定义为语义上的，而不是表现层面上。所以如果想画一条横线，请使用适当的 <code>CSS</code> 样式来修饰。</p>\n<p>（8）<code>&lt;i&gt;</code> 标签：<br>\n<code>&lt;i&gt;</code> 元素用于表现因某些原因需要区分普通文本的一系列文本。例如技术术语、外文短语或是小说中人物的思想活动等，它的内容通常以斜体显示。</p>\n<p>（9）<code>&lt;b&gt;</code> 标签：<br>\nHTML 提醒注意（Bring Attention To）元素 <code>&lt;b&gt;</code> 用于吸引读者的注意到该元素的内容上（如果没有另加特别强调）。这个元素过去被认为是粗体（Boldface）元素，并且大多数浏览器仍然将文字显示为粗体。尽管如此，你不应将 <code>&lt;b&gt;</code> 元素用于显示粗体文字；替代方案是使用 <code>CSS</code> 中的 <code>font-weight</code> 属性来创建粗体文字。</p>\n<p>（10）<code>&lt;del&gt;</code> 标签：<br>\n<code>&lt;del&gt;</code> 元素表示一些被从文档中删除的文字内容，显示效果为在文字内容中间划一道横线。比如可以在需要显示修改记录或者源代码差异的情况使用这个标签。<code>&lt;ins&gt;</code> 标签的作用恰恰于此相反：表示文档中添加的内容。</p>\n<p>（11）<code>&lt;ins&gt;</code> 标签：<br>\n<code>&lt;ins&gt;</code> 元素定义已经被插入文档中的文本，显示效果为在文字内容底部划一道横线。</p>\n<h3 id=\"2-3-图片\">2.3 图片</h3>\n<p><code>&lt;img&gt;</code> 元素将一份图像嵌入文档。默认为行内元素，即 <code>display: inline</code>。</p>\n<p>（1）<code>src</code> 属性：<br>\n该属性是<strong>必须的</strong>，它包含了你想嵌入的图片的文件路径。</p>\n<p>（2）<code>alt</code> 属性：<br>\n该属性包含一条对图像的文本描述，这不是强制性的，但对可访问性而言，它十分有用。屏幕阅读器会将这些描述读给需要使用阅读器的使用者听，让他们知道图像的含义。或者如果由于某种原因无法加载图像，普通浏览器也会在页面上显示 <code>alt</code> 属性中的备用文本：例如，网络错误、内容被屏蔽或链接过期时。</p>\n<p>（3）<code>height</code> 属性：<br>\n图像的高度，在 HTML5 中的单位是 <code>CSS</code> 像素，在 HTML4 中既可以是像素也可以是百分比。可以只指定 <code>width</code> 和 <code>height</code> 中的一个值，浏览器会根据原始图像比例进行缩放。</p>\n<p>（4）<code>width</code> 属性：<br>\n图像的宽度，在 HTML5 中的单位是 <code>CSS</code> 像素，在 HTML4 中既可以是像素也可以是百分比。</p>\n<h3 id=\"2-4-音频与视频\">2.4 音频与视频</h3>\n<p>（1）<code>&lt;audio&gt;</code> 标签：<br>\n<code>&lt;audio&gt;</code> 元素用于在文档中嵌入音频内容。<code>&lt;audio&gt;</code> 元素可以包含一个或多个音频资源，这些音频资源可以使用 <code>src</code> 属性或者 <code>&lt;source&gt;</code> 元素来进行描述：浏览器将会选择最合适的一个来使用。也可以使用 <code>MediaStream</code> 将这个元素用于流式媒体。</p>\n<ul>\n<li>使用 <code>src</code> 属性播放：</li>\n</ul>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">audio</span> <span class=\"attr\">controls</span> <span class=\"attr\">src</span>=<span class=\"string\">&quot;/Web Application Lesson/audios/bgm.mp3&quot;</span>&gt;</span></span><br><span class=\"line\">    Your browser does not support this audio element.</span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">audio</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li><code>&lt;audio&gt;</code> 与多个 <code>&lt;source&gt;</code> 元素：<br>\n这个例子包含了多个 <code>&lt;source&gt;</code> 元素。如果能够播放的话，浏览器就会试图去加载第一个 <code>source</code> 元素；如果不行，那就退而求其次去加载第二个。</li>\n</ul>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">audio</span> <span class=\"attr\">controls</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">source</span> <span class=\"attr\">src</span>=<span class=\"string\">&quot;/Web Application Lesson/audios/bgm1.mp3&quot;</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;audio/mpeg&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">source</span> <span class=\"attr\">src</span>=<span class=\"string\">&quot;/Web Application Lesson/audios/bgm2.mp3&quot;</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;audio/mpeg&quot;</span>&gt;</span></span><br><span class=\"line\">    Your browser does not support this audio element.</span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">audio</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>（2）<code>&lt;video&gt;</code> 标签：<br>\n<code>&lt;video&gt;</code> 元素用于在 HTML 或者 XHTML 文档中嵌入媒体播放器，用于支持文档内的视频播放。你也可以将 <code>&lt;video&gt;</code> 标签用于音频内容，但是 <code>&lt;audio&gt;</code> 元素可能在用户体验上更合适。该标签的使用方法与 <code>&lt;audio&gt;</code> 相同。</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">video</span> <span class=\"attr\">controls</span> <span class=\"attr\">width</span>=<span class=\"string\">&quot;800&quot;</span> <span class=\"attr\">src</span>=<span class=\"string\">&quot;/Web Application Lesson/videos/mv.mp4&quot;</span>&gt;</span></span><br><span class=\"line\">    Your browser does not support embedded videos.</span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">video</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"2-5-超链接\">2.5 超链接</h3>\n<p><code>&lt;a&gt;</code> 元素（或称锚元素）可以通过它的 <code>href</code> 属性创建通向其他网页、文件、同一页面内的位置、电子邮件地址或任何其他 URL 的超链接。<code>&lt;a&gt;</code> 中的内容应该指明链接的意图。如果存在 <code>href</code> 属性，当 <code>&lt;a&gt;</code> 元素聚焦时按下回车键就会激活它。如果点击链接打开新标签页面需要加入属性：<code>target=&quot;_blank&quot;</code>。</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;https://www.baidu.com&quot;</span> <span class=\"attr\">target</span>=<span class=\"string\">&quot;_blank&quot;</span>&gt;</span>Baidu<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;https://www.acwing.com&quot;</span> <span class=\"attr\">target</span>=<span class=\"string\">&quot;_blank&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">img</span> <span class=\"attr\">height</span>=<span class=\"string\">&quot;200&quot;</span> <span class=\"attr\">src</span>=<span class=\"string\">&quot;/Web Application Lesson/images/logo.png&quot;</span> <span class=\"attr\">alt</span>=<span class=\"string\">&quot;logo&quot;</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"2-6-表单\">2.6 表单</h3>\n<p>（1）<code>&lt;form&gt;</code> 标签：<br>\n<code>&lt;form&gt;</code> 元素表示文档中的一个区域，此区域包含交互控件，用于向 Web 服务器提交信息。</p>\n<p>（2）<code>&lt;input&gt;</code> 标签：<br>\n<code>&lt;input&gt;</code> 元素表示一个用来填写内容的输入框，常见类型有：</p>\n<ul>\n<li><code>&lt;input type=&quot;text&quot;&gt;</code>：创建基础的单行文本框。</li>\n<li><code>&lt;input type=&quot;number&quot;&gt;</code>：用于让用户输入一个数字。其包括内置验证以拒绝非数字输入。浏览器可能会选择提供步进箭头，让用户可以使用鼠标增加和减少输入的值，或者只需用指尖敲击即可。</li>\n<li><code>&lt;input type=&quot;email&quot;&gt;</code>：带有 <code>email</code>（电子邮箱）类型标记的输入框元素（<code>&lt;input&gt;</code>）能够让用户输入或编辑一个电子邮箱地址，此外，如果指定了 <code>multiple</code> 属性，用户还可以输入多个电子邮箱地址。在表单提交前，输入框会自动验证输入值是否是一个或多个合法的电子邮箱地址（非空值且符合电子邮箱地址格式）<code>CSS</code> 伪标签 <code>:valid</code> 和 <code>:invalid</code> 能够在校验后自动应用。</li>\n<li><code>&lt;input type=&quot;password&quot;&gt;</code>：<code>&lt;input&gt;</code> 元素里有一种叫做 <code>password</code> 的值，给我们一个方法让用户更加安全的输入密码。这个元素是作为一行纯文本编辑器控件呈现的，其中文本被遮蔽以致于无法读取，通常通过用诸如星号（<code>*</code>）或点（<code>•</code>）等符号替换每个字符来实现。这个符号会根据用户的浏览器和操作系统来具体显示哪个。</li>\n<li><code>&lt;input type=&quot;radio&quot;&gt;</code>：<code>&lt;input&gt;</code> 的 <code>radio</code> 类型元素默认渲染为小型圆圈图表，填充即为激活，类似于复选框（checkbox）类型。单选按钮允许你选择单一的值来提交表单。<code>name</code> 属性相同的为一组 <code>radio</code>。</li>\n</ul>\n<p>常用属性有：</p>\n<ul>\n<li><code>name</code>：名称</li>\n<li><code>id</code>：唯一ID</li>\n<li><code>maxlength</code>：最大长度</li>\n<li><code>minlength</code>：最小长度</li>\n<li><code>required</code>：是否必填</li>\n<li><code>placeholder</code>：当表单控件为空时，控件中显示的内容</li>\n</ul>\n<p>综合示例如下：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">form</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">label</span> <span class=\"attr\">for</span>=<span class=\"string\">&quot;username&quot;</span>&gt;</span>账号：<span class=\"tag\">&lt;/<span class=\"name\">label</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">input</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;text&quot;</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;username&quot;</span> <span class=\"attr\">id</span>=<span class=\"string\">&quot;username&quot;</span> <span class=\"attr\">placeholder</span>=<span class=\"string\">&quot;Please input username&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">br</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">label</span> <span class=\"attr\">for</span>=<span class=\"string\">&quot;password&quot;</span>&gt;</span>密码：<span class=\"tag\">&lt;/<span class=\"name\">label</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">input</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;password&quot;</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;password&quot;</span> <span class=\"attr\">id</span>=<span class=\"string\">&quot;password&quot;</span> <span class=\"attr\">placeholder</span>=<span class=\"string\">&quot;Please input password&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">br</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">label</span> <span class=\"attr\">for</span>=<span class=\"string\">&quot;email&quot;</span>&gt;</span>邮箱：<span class=\"tag\">&lt;/<span class=\"name\">label</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">input</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;email&quot;</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;email&quot;</span> <span class=\"attr\">id</span>=<span class=\"string\">&quot;email&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">br</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">label</span> <span class=\"attr\">for</span>=<span class=\"string\">&quot;age&quot;</span>&gt;</span>年龄：<span class=\"tag\">&lt;/<span class=\"name\">label</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">input</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;number&quot;</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;age&quot;</span> <span class=\"attr\">id</span>=<span class=\"string\">&quot;age&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">br</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">label</span> <span class=\"attr\">for</span>=<span class=\"string\">&quot;cpp&quot;</span>&gt;</span>C++<span class=\"tag\">&lt;/<span class=\"name\">label</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">input</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;radio&quot;</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;lang&quot;</span> <span class=\"attr\">value</span>=<span class=\"string\">&quot;cpp&quot;</span> <span class=\"attr\">id</span>=<span class=\"string\">&quot;cpp&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"symbol\">&amp;nbsp;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">label</span> <span class=\"attr\">for</span>=<span class=\"string\">&quot;java&quot;</span>&gt;</span>Java<span class=\"tag\">&lt;/<span class=\"name\">label</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">input</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;radio&quot;</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;lang&quot;</span> <span class=\"attr\">value</span>=<span class=\"string\">&quot;java&quot;</span> <span class=\"attr\">id</span>=<span class=\"string\">&quot;java&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"symbol\">&amp;nbsp;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">label</span> <span class=\"attr\">for</span>=<span class=\"string\">&quot;py&quot;</span>&gt;</span>Python<span class=\"tag\">&lt;/<span class=\"name\">label</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">input</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;radio&quot;</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;lang&quot;</span> <span class=\"attr\">value</span>=<span class=\"string\">&quot;py&quot;</span> <span class=\"attr\">id</span>=<span class=\"string\">&quot;py&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">br</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">form</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>（3）<code>&lt;textarea&gt;</code> 标签：<br>\n<code>&lt;textarea&gt;</code> 元素表示一个多行纯文本编辑控件，当你希望用户输入一段相当长的、不限格式的文本，例如评论或反馈表单中的一段意见时，这很有用。参数 <code>rows</code> 指定初始的行数，<code>cols</code> 指定初始的列数。</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">label</span> <span class=\"attr\">for</span>=<span class=\"string\">&quot;feedback&quot;</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">label</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">textarea</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;feedback&quot;</span> <span class=\"attr\">id</span>=<span class=\"string\">&quot;feedback&quot;</span> <span class=\"attr\">cols</span>=<span class=\"string\">&quot;50&quot;</span> <span class=\"attr\">rows</span>=<span class=\"string\">&quot;10&quot;</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">textarea</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>（4）<code>&lt;select&gt;</code> 与 <code>&lt;option&gt;</code> 标签：<br>\n<code>&lt;select&gt;</code> 元素表示一个提供选项菜单的控件。</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">label</span> <span class=\"attr\">for</span>=<span class=\"string\">&quot;pet_select&quot;</span>&gt;</span>选择宠物：<span class=\"tag\">&lt;/<span class=\"name\">label</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">select</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;pet_select&quot;</span> <span class=\"attr\">id</span>=<span class=\"string\">&quot;pet_select&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">option</span> <span class=\"attr\">value</span>=<span class=\"string\">&quot;Cat&quot;</span>&gt;</span>Cat<span class=\"tag\">&lt;/<span class=\"name\">option</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">option</span> <span class=\"attr\">selected</span> <span class=\"attr\">value</span>=<span class=\"string\">&quot;Dog&quot;</span>&gt;</span>Dog<span class=\"tag\">&lt;/<span class=\"name\">option</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">option</span> <span class=\"attr\">value</span>=<span class=\"string\">&quot;Fish&quot;</span>&gt;</span>Fish<span class=\"tag\">&lt;/<span class=\"name\">option</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">option</span> <span class=\"attr\">value</span>=<span class=\"string\">&quot;Bird&quot;</span>&gt;</span>Bird<span class=\"tag\">&lt;/<span class=\"name\">option</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">select</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>（5）<code>&lt;button&gt;</code> 标签：<br>\n<code>&lt;button&gt;</code> 元素表示一个可点击的按钮，可以用在表单或文档其它需要使用简单标准按钮的地方。默认情况下，HTML 按钮的显示样式接近于 <code>user agent</code> 所在的宿主系统平台（用户操作系统）的按钮，但你可以使用 <code>CSS</code> 来改变按钮的样貌。</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">button</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;subimt&quot;</span>&gt;</span>Submit<span class=\"tag\">&lt;/<span class=\"name\">button</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"2-7-列表\">2.7 列表</h3>\n<p>（1）<code>&lt;ul&gt;</code> 与 <code>&lt;li&gt;</code> 标签：<br>\n<code>&lt;ul&gt;</code> 元素（或称 HTML 无序列表元素）表示一个内可含多个元素的无序列表或项目符号列表，通常渲染为一个用小点或者小圆圈表示的列表。</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">ul</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">li</span>&gt;</span>First item<span class=\"tag\">&lt;/<span class=\"name\">li</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">li</span>&gt;</span>Second item<span class=\"tag\">&lt;/<span class=\"name\">li</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">li</span>&gt;</span>Third item<span class=\"tag\">&lt;/<span class=\"name\">li</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">ul</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>（2）<code>&lt;ol&gt;</code> 与 <code>&lt;li&gt;</code> 标签：<br>\n<code>&lt;ol&gt;</code> 元素表示有序列表，通常渲染为一个带编号的列表。</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">ol</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">li</span>&gt;</span>First item<span class=\"tag\">&lt;/<span class=\"name\">li</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">li</span>&gt;</span>Second item<span class=\"tag\">&lt;/<span class=\"name\">li</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">li</span>&gt;</span>Third item<span class=\"tag\">&lt;/<span class=\"name\">li</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">ol</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>（3）<code>&lt;dl&gt;</code>、<code>&lt;dt&gt;</code> 与 <code>&lt;dd&gt;</code> 标签：<br>\n<code>&lt;dl&gt;</code> 元素（或 HTML 描述列表元素）是一个包含术语定义以及描述的列表，通常用于展示词汇表或者元数据（键-值对列表）。</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dl</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">dt</span>&gt;</span>Name<span class=\"tag\">&lt;/<span class=\"name\">dt</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">dd</span>&gt;</span>Godzilla<span class=\"tag\">&lt;/<span class=\"name\">dd</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">dt</span>&gt;</span>Born<span class=\"tag\">&lt;/<span class=\"name\">dt</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">dd</span>&gt;</span>1952<span class=\"tag\">&lt;/<span class=\"name\">dd</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">dt</span>&gt;</span>Birthplace<span class=\"tag\">&lt;/<span class=\"name\">dt</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">dd</span>&gt;</span>Japan<span class=\"tag\">&lt;/<span class=\"name\">dd</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">dt</span>&gt;</span>Color<span class=\"tag\">&lt;/<span class=\"name\">dt</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">dd</span>&gt;</span>Green<span class=\"tag\">&lt;/<span class=\"name\">dd</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">dd</span>&gt;</span>Orange<span class=\"tag\">&lt;/<span class=\"name\">dd</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dl</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"2-8-表格\">2.8 表格</h3>\n<p>（1）<code>&lt;table&gt;</code> 标签：<br>\n<code>table</code> 元素表示表格数据，即通过二维数据表表示的信息。</p>\n<p>（2）<code>&lt;thead&gt;</code> 标签：<br>\n<code>&lt;thead&gt;</code> 元素定义了一组定义表格的列头的行。</p>\n<p>（3）<code>&lt;tbody&gt;</code> 标签：<br>\n<code>&lt;tbody&gt;</code> 元素定义一组数据行。</p>\n<p>（4）<code>&lt;tr&gt;</code> 标签：<br>\n<code>&lt;tr&gt;</code> 元素定义表格中的行。同一行可同时出现 <code>&lt;td&gt;</code> 和 <code>&lt;th&gt;</code> 元素。</p>\n<p>（5）<code>&lt;th&gt;</code> 标签：<br>\n<code>&lt;th&gt;</code> 元素定义表格内的表头单元格。</p>\n<p>（6）<code>&lt;td&gt;</code> 标签：<br>\n<code>&lt;td&gt;</code> 元素定义了一个包含数据的表格单元格。</p>\n<p>（7）<code>&lt;caption&gt;</code>标签：<br>\n<code>&lt;caption&gt;</code> 元素（or HTML 表格标题元素）展示一个表格的标题，它常常作为 <code>&lt;table&gt;</code> 的第一个子元素出现，同时显示在表格内容的最前面，但是，它同样可以被 <code>CSS</code> 样式化，所以，它同样可以出现在相对于表格的任意位置。</p>\n<p>综合示例：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">table</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">caption</span>&gt;</span>My Table<span class=\"tag\">&lt;/<span class=\"name\">caption</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">thead</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">tr</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">th</span>&gt;</span>name<span class=\"tag\">&lt;/<span class=\"name\">th</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">th</span>&gt;</span>score<span class=\"tag\">&lt;/<span class=\"name\">th</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">tr</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">thead</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">tbody</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">tr</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">td</span>&gt;</span>Alice<span class=\"tag\">&lt;/<span class=\"name\">td</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">td</span>&gt;</span>80<span class=\"tag\">&lt;/<span class=\"name\">td</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">tr</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">tr</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">td</span>&gt;</span>Bob<span class=\"tag\">&lt;/<span class=\"name\">td</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">td</span>&gt;</span>70<span class=\"tag\">&lt;/<span class=\"name\">td</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">tbody</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">table</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"2-9-语义标签\">2.9 语义标签</h3>\n<p><img src=\"https://img-blog.csdnimg.cn/de39903b7e244acdb29cb13437cdfa34.png\" alt=\"\"></p>\n<p>（1）<code>&lt;header&gt;</code>：<br>\n<code>&lt;header&gt;</code> 元素用于展示介绍性内容，通常包含一组介绍性的或是辅助导航的实用元素。它可能包含一些标题元素，但也可能包含其他元素，比如 Logo、搜索框、作者名称，等等。</p>\n<p>（2）<code>&lt;nav&gt;</code>：<br>\n<code>&lt;nav&gt;</code> 元素表示页面的一部分，其目的是在当前文档或其他文档中提供导航链接。导航部分的常见示例是菜单，目录和索引。</p>\n<p>（3）<code>&lt;section&gt;</code>：<br>\n<code>&lt;section&gt;</code> 元素表示一个包含在 HTML 文档中的独立部分，它没有更具体的语义元素来表示，一般来说会有包含一个标题。</p>\n<p>（4）<code>&lt;figure&gt;</code>：<br>\n<code>&lt;figure&gt;</code> 元素代表一段独立的内容，经常与说明（caption）<code>&lt;figcaption&gt;</code> 配合使用，并且作为一个独立的引用单元。当它属于主内容流（main flow）时，它的位置独立于主体。这个标签经常是在主文中引用的图片，插图，表格，代码段等等，当这部分转移到附录中或者其他页面时不会影响到主体。</p>\n<p>（5）<code>&lt;figcaption&gt;</code>：<br>\n<code>&lt;figcaption&gt;</code> 元素是与其相关联的图片的说明/标题，用于描述其父节点 <code>&lt;figure&gt;</code> 元素里的其他数据。这意味着 <code>&lt;figcaption&gt;</code> 在 <code>&lt;figure&gt;</code> 块里是第一个或最后一个。同时 HTML Figcaption 元素是可选的；如果没有该元素，这个父节点的图片只是会没有说明/标题。</p>\n<p>（6）<code>&lt;article&gt;</code>：<br>\n<code>&lt;article&gt;</code> 元素表示文档、页面、应用或网站中的独立结构，其意在成为可独立分配的或可复用的结构，例如它可能是论坛帖子、杂志或新闻文章、博客、用户提交的评论、交互式组件，或者其他独立的内容项目。</p>\n<p>（7）<code>&lt;aside&gt;</code>：<br>\n<code>&lt;aside&gt;</code> 元素表示一个和其余页面内容几乎无关的部分，被认为是独立于该内容的一部分并且可以被单独的拆分出来而不会使整体受影响。其通常表现为侧边栏或者标注框（call-out boxes）。</p>\n<p>（8）<code>&lt;footer&gt;</code>：<br>\n<code>&lt;footer&gt;</code> 元素表示最近一个章节内容或者根节点（sectioning root）元素的页脚。一个页脚通常包含该章节作者、版权数据或者与文档相关的链接等信息。</p>\n<p>综合示例：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">header</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">h1</span>&gt;</span>Header<span class=\"tag\">&lt;/<span class=\"name\">h1</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">nav</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">h2</span>&gt;</span>Content<span class=\"tag\">&lt;/<span class=\"name\">h2</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">ul</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">li</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;https://www.baidu.com&quot;</span> <span class=\"attr\">target</span>=<span class=\"string\">&quot;_blank&quot;</span>&gt;</span>Baidu<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">li</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">li</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;https://www.bilibili.com&quot;</span> <span class=\"attr\">target</span>=<span class=\"string\">&quot;_blank&quot;</span>&gt;</span>Bilibili<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">li</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">li</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;https://www.acwing.com&quot;</span> <span class=\"attr\">target</span>=<span class=\"string\">&quot;_blank&quot;</span>&gt;</span>AcWing<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">li</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">ul</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">nav</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">header</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">hr</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">section</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">h2</span>&gt;</span>Image<span class=\"tag\">&lt;/<span class=\"name\">h2</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">figure</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">img</span> <span class=\"attr\">width</span>=<span class=\"string\">&quot;300&quot;</span> <span class=\"attr\">src</span>=<span class=\"string\">&quot;/Web Application Lesson/images/image1.png&quot;</span> <span class=\"attr\">alt</span>=<span class=\"string\">&quot;image1&quot;</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">figcaption</span>&gt;</span>image1<span class=\"tag\">&lt;/<span class=\"name\">figcaption</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">img</span> <span class=\"attr\">width</span>=<span class=\"string\">&quot;300&quot;</span> <span class=\"attr\">src</span>=<span class=\"string\">&quot;/Web Application Lesson/images/logo.png&quot;</span> <span class=\"attr\">alt</span>=<span class=\"string\">&quot;image2&quot;</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">figcaption</span>&gt;</span>image2<span class=\"tag\">&lt;/<span class=\"name\">figcaption</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">figure</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">section</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">hr</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">section</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">h2</span>&gt;</span>Article<span class=\"tag\">&lt;/<span class=\"name\">h2</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">article</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">h3</span>&gt;</span>Article1<span class=\"tag\">&lt;/<span class=\"name\">h3</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">p</span>&gt;</span>paragraph1<span class=\"tag\">&lt;/<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">p</span>&gt;</span>paragraph2<span class=\"tag\">&lt;/<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">article</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">article</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">h3</span>&gt;</span>Article2<span class=\"tag\">&lt;/<span class=\"name\">h3</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">p</span>&gt;</span>paragraph1<span class=\"tag\">&lt;/<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">p</span>&gt;</span>paragraph2<span class=\"tag\">&lt;/<span class=\"name\">p</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">article</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">section</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">hr</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">footer</span>&gt;</span></span><br><span class=\"line\">        <span class=\"symbol\">&amp;copy;</span>2018-2022 AsanoSaki 版权所有</span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">footer</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">body</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"2-10-特殊符号\">2.10 特殊符号</h3>\n<ul>\n<li><code>&amp;lt;</code>：&lt;，小于号或显示标记；</li>\n<li><code>&amp;gt;</code>：&gt;，大于号或显示标记；</li>\n<li><code>&amp;amp;</code>：&amp;，可用于显示其它特殊字符；</li>\n<li><code>&amp;quot;</code>：&quot;，引号；</li>\n<li><code>&amp;reg;</code>：®，已注册；</li>\n<li><code>&amp;copy;</code>：©，版权；</li>\n<li><code>&amp;trade;</code>：™，商标；</li>\n<li><code>&amp;nbsp;</code>：空格。</li>\n</ul>\n",
            "tags": [
                "Web"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/63179.html",
            "url": "https://asanosaki.github.io/posts/63179.html",
            "title": "Linux学习笔记-管道、环境变量与Docker",
            "date_published": "2022-09-28T07:14:00.000Z",
            "content_html": "<blockquote>\n<p>本文记录 Linux 的学习过程，内容为管道、环境变量与 Docker。<br>\nDocker 官网：<a href=\"https://hub.docker.com/\">Docker Hub</a>。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-管道\">1. 管道</h2>\n<p>（1）概念</p>\n<p>管道类似于文件重定向，可以将前一个命令的 <code>stdout</code> 重定向到下一个命令的 <code>stdin</code>。</p>\n<p>（2）要点</p>\n<ul>\n<li>管道命令仅处理 <code>stdout</code>，会忽略 <code>stderr</code>。</li>\n<li>管道右边的命令必须能接受 <code>stdin</code>。</li>\n<li>多个管道命令可以串联。</li>\n</ul>\n<p>（3）与文件重定向的区别</p>\n<ul>\n<li>文件重定向左边为命令，右边为文件。</li>\n<li>管道左右两边均为命令，左边有 <code>stdout</code>，右边有 <code>stdin</code>。</li>\n</ul>\n<p>（4）举例</p>\n<p>统计当前目录下所有 Python 文件的总行数，其中 <code>find</code>、<code>xargs</code>、<code>wc</code> 等命令可以参考：<a href=\"/posts/53725.html\">Linux学习笔记-命令、Tmux与Vim</a>。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">find . -name &#x27;*.py&#x27; | xargs cat | wc -l</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-环境变量\">2. 环境变量</h2>\n<p>（1）概念</p>\n<p>Linux 系统中会用很多环境变量来记录<strong>配置信息</strong>。<br>\n环境变量类似于全局变量，可以<strong>被各个进程访问到</strong>。我们可以通过修改环境变量来方便地修改系统配置。</p>\n<p>（2）查看</p>\n<p>列出当前环境下的所有环境变量：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">env  # 显示当前用户的变量</span><br><span class=\"line\">set  # 显示当前shell的变量，包括当前用户的变量</span><br><span class=\"line\">export  # 显示当前导出成用户变量的shell变量</span><br></pre></td></tr></table></figure>\n<p>输出某个环境变量的值：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo $PATH</span><br></pre></td></tr></table></figure>\n<p>（3）修改</p>\n<p>环境变量的定义、修改、删除操作可以参考<a href=\"/posts/31281.html\">Linux学习笔记-Shell</a>这一节的内容。</p>\n<p>为了将对环境变量的修改应用到未来所有环境下，可以将修改命令放到 <code>~/.bashrc</code> 文件中。修改完 <code>~/.bashrc</code> 文件后，需要执行 <code>source ~/.bashrc</code>，来将修改应用到当前的 <code>bash</code> 环境下。</p>\n<p>为何将修改命令放到 <code>~/.bashrc</code>，就可以确保修改会影响未来所有的环境呢？</p>\n<ul>\n<li>每次启动 <code>bash</code>，都会先执行 <code>~/.bashrc</code>。</li>\n<li>每次 <code>ssh</code> 登陆远程服务器，都会启动一个 <code>bash</code> 命令行给我们。</li>\n<li>每次 <code>tmux</code> 新开一个 <code>pane</code>，都会启动一个 <code>bash</code> 命令行给我们。</li>\n<li>所以未来所有新开的环境都会加载我们修改的内容。</li>\n</ul>\n<p>（4）常见环境变量</p>\n<ul>\n<li><code>HOME</code>：用户的家目录。</li>\n<li><code>PATH</code>：可执行文件（命令）的存储路径。路径与路径之间用 <code>:</code> 分隔。当某个可执行文件同时出现在多个路径中时，会选择从左到右数第一个路径中的执行。下列所有存储路径的环境变量，均采用从左到右的优先顺序。</li>\n<li><code>LD_LIBRARY_PATH</code>：用于指定动态链接库（<code>.so</code> 文件）的路径，其内容是以冒号分隔的路径列表。</li>\n<li><code>C_INCLUDE_PATH</code>：C 语言的头文件路径，内容是以冒号分隔的路径列表。</li>\n<li><code>CPLUS_INCLUDE_PATH</code>：CPP 的头文件路径，内容是以冒号分隔的路径列表。</li>\n<li><code>PYTHONPATH</code>：Python 导入包的路径，内容是以冒号分隔的路径列表。</li>\n<li><code>JAVA_HOME</code>：JDK 的安装目录。</li>\n<li><code>CLASSPATH</code>：存放 Java 导入类的路径，内容是以冒号分隔的路径列表。</li>\n</ul>\n<h2 id=\"3-Docker\">3. Docker</h2>\n<h3 id=\"3-1-Docker安装\">3.1 Docker安装</h3>\n<p>Ubuntu 系统 Docker 官网安装教程：<a href=\"https://docs.docker.com/engine/install/ubuntu/\">Docker Install Docs</a>。</p>\n<p>本文安装 Docker 所使用的 OS 版本为：Ubuntu 22.04 (LTS)。依次执行以下命令安装 Docker：</p>\n<p>（1）更新 <code>apt</code>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get update</span><br></pre></td></tr></table></figure>\n<p>（2）允许 <code>apt</code> 通过 HTTPS 使用存储库：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install \\</span><br><span class=\"line\">    ca-certificates \\</span><br><span class=\"line\">    curl \\</span><br><span class=\"line\">    gnupg \\</span><br><span class=\"line\">    lsb-release</span><br></pre></td></tr></table></figure>\n<p>（3）添加 Docker 的官方 GPG 密钥：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo mkdir -p /etc/apt/keyrings</span><br><span class=\"line\">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg</span><br></pre></td></tr></table></figure>\n<p>（4）设置 <code>repository</code>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo \\</span><br><span class=\"line\">  &quot;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\</span><br><span class=\"line\"><span class=\"meta prompt_\">  $</span><span class=\"language-bash\">(lsb_release -cs) stable<span class=\"string\">&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null</span></span></span><br></pre></td></tr></table></figure>\n<p>（5）安装 Docker Engine，首先更新 <code>apt</code>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get update</span><br></pre></td></tr></table></figure>\n<p>（6）安装 <code>Docker Engine</code>、<code>containerd</code>、<code>Docker Compose</code>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin</span><br></pre></td></tr></table></figure>\n<p>（7）检查版本：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker -v</span><br><span class=\"line\">docker --version</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-2-Docker教程\">3.2 Docker教程</h3>\n<p>（1）将当前用户添加到 <code>docker</code> 用户组</p>\n<p>为了避免每次使用 <code>docker</code> 命令都需要加上 <code>sudo</code> 权限，可以将当前用户加入安装中自动创建的 <code>docker</code> 用户组（可以参考<a href=\"https://docs.docker.com/engine/install/linux-postinstall/\">官方文档</a>）：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo usermod -aG docker $USER</span><br></pre></td></tr></table></figure>\n<p>执行完此操作后，需要退出服务器（即关闭系统），再重新登录回来，才可以省去 <code>sudo</code> 权限。</p>\n<p>（2）镜像（images）</p>\n<p>一个 Docker 中可以有很多镜像，镜像就相当于模板，每个镜像中又可以有很多容器。</p>\n<p>用相同镜像生成的容器环境都一样，如果 Docker 安装在云服务器上，那么每个容器也就相当于是一个独立的云服务器。</p>\n<p>迁移项目的时候即将容器先生成一个镜像，然后把镜像传到远程服务器上。</p>\n<ul>\n<li><code>docker pull ubuntu:20.04</code> 或 <code>docker pull ubuntu:latest</code>：拉取一个镜像。</li>\n<li><code>docker images</code>：列出本地所有镜像。</li>\n<li><code>docker save -o ubuntu_latest.tar ubuntu:latest</code>：将镜像 <code>ubuntu:latest</code> 导出到本地文件 <code>ubuntu_latest.tar</code> 中，导出后记得给文件加上可读权限：<code>chmod +r ubuntu_latest.tar</code>。</li>\n<li><code>docker image rm ubuntu:latest</code> 或 <code>docker rmi ubuntu:latest</code>：删除镜像 <code>ubuntu:latest</code>。</li>\n<li><code>docker [container] commit CONTAINER IMAGE_NAME:TAG</code>：创建某个 <code>container</code> 的镜像，<code>[]</code> 表示 <code>container</code> 为可选字段。</li>\n<li><code>docker load -i ubuntu_latest.tar</code>：将镜像 <code>ubuntu:latest</code> 从本地文件 <code>ubuntu_latest.tar</code> 中加载出来。</li>\n</ul>\n<p>（3）容器（container）</p>\n<ul>\n<li><code>docker [container] create -it ubuntu:latest</code>：利用镜像 <code>ubuntu:latest</code> 创建一个容器。</li>\n<li><code>docker ps -a</code>：查看本地的所有容器，<code>docker ps</code> 为查看运行中的容器。</li>\n<li><code>docker [container] start CONTAINER</code>：启动容器，<code>CONTAINER</code> 可以是 ID 或 NAMES。</li>\n<li><code>docker [container] stop CONTAINER</code>：停止容器。</li>\n<li><code>docker [container] restart CONTAINER</code>：重启容器。</li>\n<li><code>docker [contaienr] run -itd ubuntu:latest</code>：创建并启动一个容器，可以加上参数 <code>-p 20000:22</code> 表示将容器的22端口映射到本地的20000端口，因为本地的22端口已经被占用了，且如果是在云服务器安装 Docker 还需要修改云服务器安全组配置，把20000端口放行。</li>\n<li><code>docker [container] attach CONTAINER</code>：进入容器。\n<ul>\n<li>先按 <code>Ctrl+p</code>，再按 <code>Ctrl+q</code> 可以挂起容器，即退出但不关闭容器。</li>\n<li>按 <code>Ctrl+d</code> 可以退出并关闭容器。</li>\n</ul>\n</li>\n<li><code>docker [container] exec CONTAINER COMMAND</code>：在容器中执行 <code>COMMAND</code> 命令。</li>\n<li><code>docker [container] rm CONTAINER</code>：删除容器。</li>\n<li><code>docker container prune</code>：删除所有已停止的容器。</li>\n<li><code>docker export -o xxx.tar CONTAINER</code>：将容器导出到本地文件 <code>xxx.tar</code> 中。</li>\n<li><code>docker import xxx.tar image_name:tag</code>：将本地文件 <code>xxx.tar</code> 导入成镜像，并将镜像命名为 <code>image_name:tag</code>。</li>\n<li><code>docker export/import</code> 与 <code>docker save/load</code> 的区别：\n<ul>\n<li><code>export/import</code> 会丢弃历史记录和元数据信息，仅保存容器当时的快照状态。</li>\n<li><code>save/load</code> 会保存完整记录，体积更大。</li>\n</ul>\n</li>\n<li><code>docker top CONTAINER</code>：查看某个容器内的所有进程。</li>\n<li><code>docker stats</code>：查看所有容器的统计信息，包括 CPU、内存、存储、网络等信息。</li>\n<li><code>docker cp xxx CONTAINER:xxx</code> 或 <code>docker cp CONTAINER:xxx xxx</code>：在本地和容器间复制文件。</li>\n<li><code>docker rename NAMES1 NAMES2</code>：将 <code>NAMES1</code> 容器重命名为 <code>NAMES2</code>。</li>\n<li><code>docker update CONTAINER --memory 500MB</code>：修改容器限制，更多修改内容可以在官网查找。</li>\n<li>进入容器后输入 <code>passwd</code> 可以设置 <code>root</code> 密码。</li>\n</ul>\n<p>（4）云服务器配置示例</p>\n<p>首先在 AC Terminal 中操作：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scp /var/lib/acwing/docker/images/docker_lesson_1_0.tar server_name:  # 将镜像上传到自己租的云端服务器</span><br><span class=\"line\">ssh server_name  # 登录自己的云端服务器</span><br><span class=\"line\"></span><br><span class=\"line\">docker load -i docker_lesson_1_0.tar  # 将镜像加载到本地</span><br><span class=\"line\">docker run -p 20000:22 --name my_docker_server -itd docker_lesson:1.0  # 创建并运行docker_lesson:1.0镜像的容器</span><br><span class=\"line\"></span><br><span class=\"line\">docker attach my_docker_server  # 进入创建的docker容器</span><br><span class=\"line\">passwd  # 设置root密码</span><br></pre></td></tr></table></figure>\n<p>然后去云平台控制台中修改安全组配置，放行端口：20000。</p>\n<p>返回 AC Terminal，即可通过 SSH 登录自己的 Docker 容器：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh root@xxx.xxx.xxx.xxx -p 20000  # 将xxx.xxx.xxx.xxx替换成自己租的服务器的IP地址</span><br></pre></td></tr></table></figure>\n<p>创建 <code>acs</code> 用户：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">adduser acs  # 创建用户acs</span><br><span class=\"line\">usermod -aG sudo acs  # 给用户acs分配sudo权限</span><br></pre></td></tr></table></figure>\n<p>最后可以配置 Docker 容器的别名和免密登录。</p>\n<p>Tips：如果 <code>apt-get</code> 下载软件速度较慢，可以参考<a href=\"https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/\">清华大学开源软件镜像站</a>中的内容，修改软件源。</p>\n<p>（5）数据迁移</p>\n<p>如果想保留 Docker 数据库里的数据，需要用 <code>save</code> 和 <code>laod</code> 迁移。</p>\n<p>首先将容器打包成镜像：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker commit --author AsanoSaki --message &quot;Create the &lt;Image name&gt; by AsanoSaki&quot; &lt;Container name&gt; &lt;Image name&gt;:&lt;tag&gt;</span><br></pre></td></tr></table></figure>\n<p>导出镜像：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker save -o &lt;Image name&gt;.tar &lt;Image name&gt;:&lt;tag&gt;</span><br></pre></td></tr></table></figure>\n<p>将导出的镜像文件传到目标主机上，然后导入镜像：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker load -i &lt;Image name&gt;.tar</span><br></pre></td></tr></table></figure>\n<p>查看镜像：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker images</span><br></pre></td></tr></table></figure>\n<p>生成一个新的容器：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -d --name &lt;Container name&gt; -p 20000:22 &lt;Image name&gt;:&lt;tag&gt;</span><br></pre></td></tr></table></figure>\n",
            "tags": [
                "Linux"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/20905.html",
            "url": "https://asanosaki.github.io/posts/20905.html",
            "title": "Linux学习笔记-Thrift",
            "date_published": "2022-09-26T06:24:00.000Z",
            "content_html": "<blockquote>\n<p>本文记录 Linux 的学习过程，内容为 RPC 软件框架：Thrift。<br>\nThrift 官网：<a href=\"https://thrift.apache.org/\">Apache Thrift</a>。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-Thrift概述\">1. Thrift概述</h2>\n<h3 id=\"1-1-基本概念\">1.1 基本概念</h3>\n<p>Thrift 是一个 RPC（远程过程调用协议 Remote Procedure Call Protocol）软件框架，用来进行可扩展且跨语言的服务的开发。它结合了功能强大的软件堆栈和代码生成引擎，以构建在 C++、Java、Go、Python、PHP、Ruby、Erlang、Perl、Haskell、C#、Cocoa、JavaScript、Node.js、Smalltalk、OCaml 这些编程语言间无缝结合的、高效的服务。Thrift 允许定义一个简单的定义文件中的数据类型和服务接口，以作为输入文件，编译器生成代码用来方便地生成 RPC 客户端和服务器通信的无缝跨编程语言。</p>\n<h3 id=\"1-2-Thrift-IDL\">1.2 Thrift IDL</h3>\n<p>Thrift 采用接口定义语言 IDL（Interface Definition Language）来定义通用的服务接口，然后通过 Thrift 提供的编译器，可以将服务接口编译成不同语言编写的代码，通过这个方式来实现跨语言的功能。</p>\n<ul>\n<li>通过命令调用 Thrift 提供的编译器将服务接口编译成不同语言编写的代码。</li>\n<li>这些代码又分为服务端和客户端，将所在不同进程（或服务器）的功能连接起来。</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">thrift -r --gen &lt;language&gt; &lt;Thrift filename&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-3-如何创建Thrift服务？\">1.3 如何创建Thrift服务？</h3>\n<ol>\n<li>定义服务接口（存放接口的文件夹就是 Thrift 文件）。</li>\n<li>作为服务端的服务，需要生成 <code>server</code>。</li>\n<li>作为请求端的服务，需要生成 <code>client</code>。</li>\n</ol>\n<h3 id=\"1-4-实例讲解\">1.4 实例讲解</h3>\n<p>假设我们要实现一个游戏的匹配系统，这个游戏的功能可能运行在一个或多个服务器（进程）上，而 Thrift 就是将不同服务器不同语言的功能连接起来。</p>\n<p>游戏本体（假设用 Python 实现）、匹配系统（假设用 C++ 实现）、数据存储服务器这三个节点（功能）是完全独立的，既可以在同一个服务器上，也可以在不同服务器上。每一个节点就是一个进程，每个进程可以使用不同的语言来实现。</p>\n<p>游戏节点到匹配节点需要实现一条有向边（可以包含多个函数），表示向匹配系统添加和移除玩家 <code>add_user</code>、<code>remove_user</code>，因此游戏节点需要实现 <code>match_client</code> 端，表示可以调用匹配服务器的函数；匹配系统需要实现 <code>match_server</code> 端，表示可以让游戏节点的 <code>client</code> 端调用自身的函数。同时匹配系统还需实现 <code>save_client</code> 端，因为需要将数据传给服务器存储 <code>save_data</code>（假设数据存储服务器已实现 <code>save_server</code> 端）。</p>\n<h2 id=\"2-Thrift教程\">2. Thrift教程</h2>\n<p>首先创建一个游戏系统文件夹 <code>game</code>、匹配系统文件夹 <code>match_system</code>、保存各种接口的文件夹 <code>thrift</code>。</p>\n<h3 id=\"2-1-match-server框架\">2.1 match_server框架</h3>\n<p>在 <code>thrift</code> 文件夹中创建一个文件：<code>match.thrift</code>，内容如下：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">namespace cpp match_service</span><br><span class=\"line\"></span><br><span class=\"line\">struct User  /**定义结构体存储用户信息*/</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    1: i32 id,  /**i32表示int*/</span><br><span class=\"line\">    2: string name,</span><br><span class=\"line\">    3: i32 score  /**按照分值匹配*/</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">service Match</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    i32 add_user(1: User user, 2: string info),</span><br><span class=\"line\"></span><br><span class=\"line\">    i32 remove_user(1: User user, 2: string info)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>前往 Thrift 官网，点击 Tutorial，再点击 C++，即可看到如何通过这个接口生成一个 C++ 版本的服务器。命令如下：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">thrift -r --gen cpp tutorial.thrift</span><br></pre></td></tr></table></figure>\n<p>在 <code>match_system</code> 文件夹中创建一个文件夹 <code>src</code>，表示源文件。在 <code>src</code> 文件夹中输入以下命令：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">thrift -r --gen cpp ../../thrift/match.thrift</span><br></pre></td></tr></table></figure>\n<p>执行后会发现该目录下生成了一个 <code>gen-cpp</code> 的文件夹，为了后续方便操作，将文件夹改个名：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mv gen-cpp match_server</span><br></pre></td></tr></table></figure>\n<p>将自动实现好的文件移出来：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mv match_server/Match_server.skeleton.cpp main.cpp</span><br></pre></td></tr></table></figure>\n<p>由于该文件里的函数还没有进行逻辑实现，因此先在每个函数中加上 <code>return 0;</code> 后编译一遍，文件内容如下（可以使用 <code>gg=G</code> 进行格式化）：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// This autogenerated skeleton file illustrates how to build a server.</span></span><br><span class=\"line\"><span class=\"comment\">// You should copy it to another filename to avoid overwriting it.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;match_server/Match.h&quot;</span>  <span class=\"comment\">// 注意已经将该文件移出来了，因此头文件路径要改</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/protocol/TBinaryProtocol.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/server/TSimpleServer.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/transport/TServerSocket.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/transport/TBufferTransports.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift;</span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift::protocol;</span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift::transport;</span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift::server;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span>  ::match_service;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MatchHandler</span> : <span class=\"keyword\">virtual</span> <span class=\"keyword\">public</span> MatchIf &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span>:</span><br><span class=\"line\">        <span class=\"built_in\">MatchHandler</span>() &#123;</span><br><span class=\"line\">            <span class=\"comment\">// Your initialization goes here</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">int32_t</span> <span class=\"title\">add_user</span><span class=\"params\">(<span class=\"type\">const</span> User&amp; user, <span class=\"type\">const</span> std::string&amp; info)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">// Your implementation goes here</span></span><br><span class=\"line\">            <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;add_user\\n&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">int32_t</span> <span class=\"title\">remove_user</span><span class=\"params\">(<span class=\"type\">const</span> User&amp; user, <span class=\"type\">const</span> std::string&amp; info)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">// Your implementation goes here</span></span><br><span class=\"line\">            <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;remove_user\\n&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span> **argv)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> port = <span class=\"number\">9090</span>;</span><br><span class=\"line\">    ::<span class=\"function\">std::shared_ptr&lt;MatchHandler&gt; <span class=\"title\">handler</span><span class=\"params\">(<span class=\"keyword\">new</span> MatchHandler())</span></span>;</span><br><span class=\"line\">    ::<span class=\"function\">std::shared_ptr&lt;TProcessor&gt; <span class=\"title\">processor</span><span class=\"params\">(<span class=\"keyword\">new</span> MatchProcessor(handler))</span></span>;</span><br><span class=\"line\">    ::<span class=\"function\">std::shared_ptr&lt;TServerTransport&gt; <span class=\"title\">serverTransport</span><span class=\"params\">(<span class=\"keyword\">new</span> TServerSocket(port))</span></span>;</span><br><span class=\"line\">    ::<span class=\"function\">std::shared_ptr&lt;TTransportFactory&gt; <span class=\"title\">transportFactory</span><span class=\"params\">(<span class=\"keyword\">new</span> TBufferedTransportFactory())</span></span>;</span><br><span class=\"line\">    ::<span class=\"function\">std::shared_ptr&lt;TProtocolFactory&gt; <span class=\"title\">protocolFactory</span><span class=\"params\">(<span class=\"keyword\">new</span> TBinaryProtocolFactory())</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\">TSimpleServer <span class=\"title\">server</span><span class=\"params\">(processor, serverTransport, transportFactory, protocolFactory)</span></span>;</span><br><span class=\"line\">    server.<span class=\"built_in\">serve</span>();</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>接下来进行编译链接，链接的时候需要用到 Thrift 的动态链接库，需要加上 <code>-lthrift</code>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">g++ -c main.cpp match_server/*.cpp</span><br><span class=\"line\">g++ *.o -o main -lthrift</span><br></pre></td></tr></table></figure>\n<p>这时输入 <code>./main</code> 即可运行程序，但是此时什么内容都没有。Thrift 只是将接口实现好了，具体的业务逻辑没有实现。我们可以先将文件上传至 Git，上传的时候注意一般不将 <code>.o</code> 文件和可执行文件上传：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git add .</span><br><span class=\"line\">git restore --staged *.o</span><br><span class=\"line\">git restore --staged main</span><br><span class=\"line\">git status</span><br><span class=\"line\">git commit -m &quot;add match_server&quot;</span><br><span class=\"line\">git push</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-2-match-client框架与实现\">2.2 match_client框架与实现</h3>\n<p>首先同样在 <code>game</code> 文件夹中创建 <code>src</code> 文件夹，进入 <code>src</code> 文件夹后我们需要生成 Python 代码：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">thrift -r --gen py ../../thrift/match.thrift</span><br></pre></td></tr></table></figure>\n<p>生成后该目录下有个文件夹 <code>gen-py</code>，也就是生成了 Python 的服务器端，同样将其改个名：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mv gen-py match_client</span><br></pre></td></tr></table></figure>\n<p>创建文件 <code>client.py</code>，将官网中 Python 客户端的代码（前四行是为了将当前路径加入到 Python 的环境变量中，可以删掉）复制过来，并进行简单的修改：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> match_client.<span class=\"keyword\">match</span> <span class=\"keyword\">import</span> Match</span><br><span class=\"line\"><span class=\"keyword\">from</span> match_client.<span class=\"keyword\">match</span>.ttypes <span class=\"keyword\">import</span> User</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> thrift <span class=\"keyword\">import</span> Thrift</span><br><span class=\"line\"><span class=\"keyword\">from</span> thrift.transport <span class=\"keyword\">import</span> TSocket</span><br><span class=\"line\"><span class=\"keyword\">from</span> thrift.transport <span class=\"keyword\">import</span> TTransport</span><br><span class=\"line\"><span class=\"keyword\">from</span> thrift.protocol <span class=\"keyword\">import</span> TBinaryProtocol</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">main</span>():</span><br><span class=\"line\">    <span class=\"comment\"># Make socket</span></span><br><span class=\"line\">    transport = TSocket.TSocket(<span class=\"string\">&#x27;localhost&#x27;</span>, <span class=\"number\">9090</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Buffering is critical. Raw sockets are very slow</span></span><br><span class=\"line\">    transport = TTransport.TBufferedTransport(transport)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Wrap in a protocol</span></span><br><span class=\"line\">    protocol = TBinaryProtocol.TBinaryProtocol(transport)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Create a client to use the protocol encoder</span></span><br><span class=\"line\">    client = Match.Client(protocol)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Connect!</span></span><br><span class=\"line\">    transport.<span class=\"built_in\">open</span>()</span><br><span class=\"line\"></span><br><span class=\"line\">    user = User(<span class=\"number\">1</span>, <span class=\"string\">&#x27;yyj&#x27;</span>, <span class=\"number\">1500</span>)</span><br><span class=\"line\">    client.add_user(user, <span class=\"string\">&quot;&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Close!</span></span><br><span class=\"line\">    transport.close()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&quot;__main__&quot;</span>:</span><br><span class=\"line\">    main()</span><br></pre></td></tr></table></figure>\n<p>然后我们将 <code>match_system/src</code> 中的 <code>main</code> 执行后，再执行 <code>game/src</code> 中的 <code>client.py</code>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 client.py</span><br></pre></td></tr></table></figure>\n<p>可以看到 <code>main</code> 程序那边输出：<code>add_user</code>。说明我们的 <code>match_client</code> 端和 <code>match_server</code> 端已经初步实现了，此时更新一下 Git，注意 <code>.pyc</code> 文件也最好不要上传：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git add .</span><br><span class=\"line\">git restore --staged *.pyc</span><br><span class=\"line\">git status</span><br><span class=\"line\">git commit -m &quot;add match_client&quot;</span><br><span class=\"line\">git push</span><br></pre></td></tr></table></figure>\n<p>接着我们进行优化，从控制台输入用户信息，并指定是添加还是删除用户，修改后的 <code>client.py</code> 代码如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> match_client.<span class=\"keyword\">match</span> <span class=\"keyword\">import</span> Match</span><br><span class=\"line\"><span class=\"keyword\">from</span> match_client.<span class=\"keyword\">match</span>.ttypes <span class=\"keyword\">import</span> User</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> thrift <span class=\"keyword\">import</span> Thrift</span><br><span class=\"line\"><span class=\"keyword\">from</span> thrift.transport <span class=\"keyword\">import</span> TSocket</span><br><span class=\"line\"><span class=\"keyword\">from</span> thrift.transport <span class=\"keyword\">import</span> TTransport</span><br><span class=\"line\"><span class=\"keyword\">from</span> thrift.protocol <span class=\"keyword\">import</span> TBinaryProtocol</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> sys <span class=\"keyword\">import</span> stdin</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">operate</span>(<span class=\"params\">op, user_id, username, score</span>):</span><br><span class=\"line\">    <span class=\"comment\"># Make socket</span></span><br><span class=\"line\">    transport = TSocket.TSocket(<span class=\"string\">&#x27;localhost&#x27;</span>, <span class=\"number\">9090</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Buffering is critical. Raw sockets are very slow</span></span><br><span class=\"line\">    transport = TTransport.TBufferedTransport(transport)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Wrap in a protocol</span></span><br><span class=\"line\">    protocol = TBinaryProtocol.TBinaryProtocol(transport)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Create a client to use the protocol encoder</span></span><br><span class=\"line\">    client = Match.Client(protocol)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Connect!</span></span><br><span class=\"line\">    transport.<span class=\"built_in\">open</span>()</span><br><span class=\"line\"></span><br><span class=\"line\">    user = User(user_id, username, score)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> op == <span class=\"string\">&quot;add&quot;</span>:</span><br><span class=\"line\">        client.add_user(user, <span class=\"string\">&quot;&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">elif</span> op == <span class=\"string\">&quot;remove&quot;</span>:</span><br><span class=\"line\">        client.remove_user(user, <span class=\"string\">&quot;&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Close!</span></span><br><span class=\"line\">    transport.close()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">main</span>():</span><br><span class=\"line\">    <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> stdin:</span><br><span class=\"line\">        op, user_id, username, score = line.split(<span class=\"string\">&#x27; &#x27;</span>)</span><br><span class=\"line\">        operate(op, <span class=\"built_in\">int</span>(user_id), username, <span class=\"built_in\">int</span>(score))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&quot;__main__&quot;</span>:</span><br><span class=\"line\">    main()</span><br></pre></td></tr></table></figure>\n<p>这样我们的 <code>match_client</code> 端就算是完成了。</p>\n<h3 id=\"2-3-match-server-v2-0实现\">2.3 match_server_v2.0实现</h3>\n<p>由于 <code>server</code> 端一方面需要读入或者移出用户，另一方面还要不断地去匹配，因此需要有一个线程去不断添加用户进来，一个线程去进行匹配，匹配完后再将信息传给一个服务器，且这两个操作是完全独立的，有可能长时间没有用户添加进来，但是匹配系统能够匹配两个已经匹配了很久的人。因此在这里需要用到并行技术，C++ 多线程需要使用到 <code>&lt;thread&gt;</code> 头文件。</p>\n<p><strong>多线程相关知识点：</strong></p>\n<ul>\n<li>IP 和端口：如果把 IP 地址比作一间房子，端口就是出入这间房子的门。真正的房子只有几个门，但是一个 IP 地址的端口可以有65536个之多！端口是通过端口号来标记的，端口号只有整数，范围是从0到65535。同一个端口只能由一个进程来监听。所以我们一旦启动了一个服务，那么这个服务就不能在被另一个进程启动了。服务器的端口号要与客户端的端口号相同。</li>\n<li><code>&lt;thread&gt;</code> 库：C++ 中有一个 <code>thread</code> 的库，可以用来开线程。通过定义一个变量将函数名作为参数，就能开一个线程了，具体使用可以看后文代码。</li>\n<li>首先定义线程的操作：并行中经典的生产者和消费者模型。生产者、消费者是两个线程。本样例中的生产者：<code>add_user()</code>、<code>remove_user()</code>；消费者：匹配用户的功能。</li>\n<li>生产者和消费者之间需要一个媒介。这个媒介可以有很多种方法。比如：消费队列。很多语言都有自己实现的消费队列，也可以自己实现消费队列。实现消费队列，就需要用到一些锁（Mutex）。锁是并行编程的基本概念。</li>\n<li>互斥锁：在编程中，引入了对象互斥锁的概念，来保证共享数据操作的完整性。每个对象都对应于一个可称为“互斥锁”的标记，这个标记用来保证<strong>在任一时刻，只能有一个线程访问该对象</strong>。\n<ul>\n<li>锁有两个操作：一个 P 操作（上锁），一个 V 操作（解锁）。</li>\n<li>定义互斥锁：<code>mutex m</code>。锁一般使用信号量来实现的，<code>mutex</code> 其实就是一个信号量（它特殊也叫互斥量）。互斥量就是同一时间能够分给一个人，即 S = 1。信号量 S = 10 表示可以将信号量分给10个人来用。</li>\n<li>P 操作的主要动作是：<br>\n（1）S - 1；<br>\n（2）若 S - 1 后仍大于或等于0，则进程继续执行；<br>\n（3）若 S - 1 后小于0，则该进程被阻塞后放入等待该信号量的等待队列中，然后转进程调度。</li>\n<li>V 操作的主要动作是：<br>\n（1）S + 1；<br>\n（2）若 S + 1 后结果大于0，则进程继续执行；<br>\n（3）若 S + 1 后结果小于或等于0，则从该信号的等待队列中释放一个等待进程，然后再返回原进程继续执行或转进程调度。</li>\n<li>对于 P 和 V 都是原子操作，就是在执行 P 和 V 操作时，不会被插队，从而实现对共享变量操作的原子性。</li>\n<li>特殊：S = 1 表示互斥量，表示同一时间，信号量只能分配给一个线程。</li>\n</ul>\n</li>\n<li>多线程为啥要用锁？因为<strong>多线程可能共享一个内存空间</strong>，导致出现重复读取并修改的现象。</li>\n</ul>\n<p>我们将程序功能修改为傻瓜式匹配，只要匹配池中的玩家数大于等于2，那么就将前两名玩家进行匹配，修改后的 <code>match_server</code> 端 <code>main.cpp</code> 代码如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// This autogenerated skeleton file illustrates how to build a server.</span></span><br><span class=\"line\"><span class=\"comment\">// You should copy it to another filename to avoid overwriting it.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;match_server/Match.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/protocol/TBinaryProtocol.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/server/TSimpleServer.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/transport/TServerSocket.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/transport/TBufferTransports.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thread&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;mutex&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;condition_variable&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;queue&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;vector&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift;</span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift::protocol;</span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift::transport;</span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift::server;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::match_service;</span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">Task</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    User user;</span><br><span class=\"line\">    string type;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">MessageQueue</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    queue&lt;Task&gt; q;</span><br><span class=\"line\">    mutex m;</span><br><span class=\"line\">    condition_variable cv;</span><br><span class=\"line\">&#125;message_queue;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Pool</span>  <span class=\"comment\">// 玩家匹配池</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span>:</span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">save_result</span><span class=\"params\">(<span class=\"type\">int</span> a, <span class=\"type\">int</span> b)</span></span></span><br><span class=\"line\"><span class=\"function\">        </span>&#123;</span><br><span class=\"line\">            <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;Match result: %d %d\\n&quot;</span>, a, b);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">match</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\">        </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">while</span> (users.<span class=\"built_in\">size</span>() &gt; <span class=\"number\">1</span>)</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                <span class=\"keyword\">auto</span> a = users[<span class=\"number\">0</span>], b = users[<span class=\"number\">1</span>];</span><br><span class=\"line\">                users.<span class=\"built_in\">erase</span>(users.<span class=\"built_in\">begin</span>());</span><br><span class=\"line\">                users.<span class=\"built_in\">erase</span>(users.<span class=\"built_in\">begin</span>());</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"built_in\">save_result</span>(a.id, b.id);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">add</span><span class=\"params\">(User user)</span></span></span><br><span class=\"line\"><span class=\"function\">        </span>&#123;</span><br><span class=\"line\">            users.<span class=\"built_in\">push_back</span>(user);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">remove</span><span class=\"params\">(User user)</span></span></span><br><span class=\"line\"><span class=\"function\">        </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"type\">uint32_t</span> i = <span class=\"number\">0</span>; i &lt; users.<span class=\"built_in\">size</span>(); i++)</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (users[i].id == user.id)</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    users.<span class=\"built_in\">erase</span>(users.<span class=\"built_in\">begin</span>() + i);</span><br><span class=\"line\">                    <span class=\"keyword\">break</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span>:</span><br><span class=\"line\">        vector&lt;User&gt; users;</span><br><span class=\"line\">&#125;pool;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MatchHandler</span> : <span class=\"keyword\">virtual</span> <span class=\"keyword\">public</span> MatchIf &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span>:</span><br><span class=\"line\">        <span class=\"built_in\">MatchHandler</span>() &#123;</span><br><span class=\"line\">            <span class=\"comment\">// Your initialization goes here</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">int32_t</span> <span class=\"title\">add_user</span><span class=\"params\">(<span class=\"type\">const</span> User&amp; user, <span class=\"type\">const</span> std::string&amp; info)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">// Your implementation goes here</span></span><br><span class=\"line\">            <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;add_user\\n&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"function\">unique_lock&lt;mutex&gt; <span class=\"title\">lck</span><span class=\"params\">(message_queue.m)</span></span>;  <span class=\"comment\">// 当lck变量消失即函数结束后锁会自动释放</span></span><br><span class=\"line\">            message_queue.q.<span class=\"built_in\">push</span>(&#123; user, <span class=\"string\">&quot;add&quot;</span> &#125;);</span><br><span class=\"line\">            message_queue.cv.<span class=\"built_in\">notify_all</span>();  <span class=\"comment\">// 唤醒条件变量</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">int32_t</span> <span class=\"title\">remove_user</span><span class=\"params\">(<span class=\"type\">const</span> User&amp; user, <span class=\"type\">const</span> std::string&amp; info)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">// Your implementation goes here</span></span><br><span class=\"line\">            <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;remove_user\\n&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"function\">unique_lock&lt;mutex&gt; <span class=\"title\">lck</span><span class=\"params\">(message_queue.m)</span></span>;</span><br><span class=\"line\">            message_queue.q.<span class=\"built_in\">push</span>(&#123; user, <span class=\"string\">&quot;remove&quot;</span> &#125;);</span><br><span class=\"line\">            message_queue.cv.<span class=\"built_in\">notify_all</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">consume_task</span><span class=\"params\">()</span>  <span class=\"comment\">// 消费者模型</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (<span class=\"literal\">true</span>)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"function\">unique_lock&lt;mutex&gt; <span class=\"title\">lck</span><span class=\"params\">(message_queue.m)</span></span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (message_queue.q.<span class=\"built_in\">empty</span>())  <span class=\"comment\">// 如果消息队列为空则应该先阻塞，而不能一直循环</span></span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            message_queue.cv.<span class=\"built_in\">wait</span>(lck);  <span class=\"comment\">// 先将锁释放，然后卡住，直到在其他地方将这个条件变量唤醒</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">else</span></span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"keyword\">auto</span> task = message_queue.q.<span class=\"built_in\">front</span>();</span><br><span class=\"line\">            message_queue.q.<span class=\"built_in\">pop</span>();</span><br><span class=\"line\">            lck.<span class=\"built_in\">unlock</span>();  <span class=\"comment\">// 尽早解锁，若等处理完task再解锁就等待时间太长了</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\">// do task</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (task.type == <span class=\"string\">&quot;add&quot;</span>) pool.<span class=\"built_in\">add</span>(task.user);</span><br><span class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (task.type == <span class=\"string\">&quot;remove&quot;</span>) pool.<span class=\"built_in\">remove</span>(task.user);</span><br><span class=\"line\"></span><br><span class=\"line\">            pool.<span class=\"built_in\">match</span>();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span> **argv)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> port = <span class=\"number\">9090</span>;</span><br><span class=\"line\">    ::<span class=\"function\">std::shared_ptr&lt;MatchHandler&gt; <span class=\"title\">handler</span><span class=\"params\">(<span class=\"keyword\">new</span> MatchHandler())</span></span>;</span><br><span class=\"line\">    ::<span class=\"function\">std::shared_ptr&lt;TProcessor&gt; <span class=\"title\">processor</span><span class=\"params\">(<span class=\"keyword\">new</span> MatchProcessor(handler))</span></span>;</span><br><span class=\"line\">    ::<span class=\"function\">std::shared_ptr&lt;TServerTransport&gt; <span class=\"title\">serverTransport</span><span class=\"params\">(<span class=\"keyword\">new</span> TServerSocket(port))</span></span>;</span><br><span class=\"line\">    ::<span class=\"function\">std::shared_ptr&lt;TTransportFactory&gt; <span class=\"title\">transportFactory</span><span class=\"params\">(<span class=\"keyword\">new</span> TBufferedTransportFactory())</span></span>;</span><br><span class=\"line\">    ::<span class=\"function\">std::shared_ptr&lt;TProtocolFactory&gt; <span class=\"title\">protocolFactory</span><span class=\"params\">(<span class=\"keyword\">new</span> TBinaryProtocolFactory())</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\">TSimpleServer <span class=\"title\">server</span><span class=\"params\">(processor, serverTransport, transportFactory, protocolFactory)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;Start Match Server\\n&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\">thread <span class=\"title\">matching_thread</span><span class=\"params\">(consume_task)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    server.<span class=\"built_in\">serve</span>();</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>由于使用了线程库，因此编译的时候需要加上参数 <code>-pthread</code>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">g++ -c main.cpp</span><br><span class=\"line\">g++ *.o -o main -lthrift -pthread</span><br></pre></td></tr></table></figure>\n<p>此时可以打开 <code>match_server</code> 端和 <code>match_client</code> 端，然后在 <code>client</code> 端添加玩家看看 <code>server</code> 端的匹配结果。</p>\n<h3 id=\"2-4-save-client实现\">2.4 save_client实现</h3>\n<p>假设 <code>save_server</code> 端已经实现，在 <code>thrift</code> 文件夹中创建文件 <code>save.thrift</code>，内容如下：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">namespace cpp save_service</span><br><span class=\"line\"></span><br><span class=\"line\">service Save &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    /**</span><br><span class=\"line\">     * username: myserver的名称</span><br><span class=\"line\">     * password: myserver的密码的md5sum的前8位</span><br><span class=\"line\">     * 用户名密码验证成功会返回0，验证失败会返回1</span><br><span class=\"line\">     * 验证成功后，结果会被保存到myserver:homework/lesson_6/result.txt中</span><br><span class=\"line\">     */</span><br><span class=\"line\">    i32 save_data(1: string username, 2: string password, 3: i32 player1_id, 4: i32 player2_id)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到直接调用 <code>save_server</code> 端的接口函数 <code>save_data</code> 即可（该函数的实现我们不关心）。</p>\n<p>在 <code>match_system/src</code> 文件夹中输入以下指令生成接口的 C++ 实现，并重命名，然后需要将自动生成的服务端代码删去：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">thrift -r --gen cpp ../../thrift/save.thrift</span><br><span class=\"line\">mv gen-cpp save_client</span><br><span class=\"line\">cd save_client</span><br><span class=\"line\">rm Save_server.skeleton.cpp</span><br></pre></td></tr></table></figure>\n<p>接下来我们将 Thrift 官网 C++ 教程中的 Client 端代码抄下来并相应进行修改，修改后的 <code>main.cpp</code> 内容如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// This autogenerated skeleton file illustrates how to build a server.</span></span><br><span class=\"line\"><span class=\"comment\">// You should copy it to another filename to avoid overwriting it.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;match_server/Match.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;save_client/Save.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/protocol/TBinaryProtocol.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/server/TSimpleServer.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/transport/TServerSocket.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/transport/TBufferTransports.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/transport/TTransportUtils.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/transport/TSocket.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thread&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;mutex&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;condition_variable&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;queue&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;vector&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift;</span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift::protocol;</span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift::transport;</span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift::server;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::match_service;</span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::save_service;  <span class=\"comment\">// 注意加上save_client端的命名空间</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">Task</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    User user;</span><br><span class=\"line\">    string type;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">MessageQueue</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    queue&lt;Task&gt; q;</span><br><span class=\"line\">    mutex m;</span><br><span class=\"line\">    condition_variable cv;</span><br><span class=\"line\">&#125;message_queue;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Pool</span>  <span class=\"comment\">// 玩家匹配池</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span>:</span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">save_result</span><span class=\"params\">(<span class=\"type\">int</span> a, <span class=\"type\">int</span> b)</span></span></span><br><span class=\"line\"><span class=\"function\">        </span>&#123;</span><br><span class=\"line\">            <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;Match result: %d %d\\n&quot;</span>, a, b);</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"function\">std::shared_ptr&lt;TTransport&gt; <span class=\"title\">socket</span><span class=\"params\">(<span class=\"keyword\">new</span> TSocket(<span class=\"string\">&quot;123.57.47.211&quot;</span>, <span class=\"number\">9090</span>))</span></span>;</span><br><span class=\"line\">            <span class=\"function\">std::shared_ptr&lt;TTransport&gt; <span class=\"title\">transport</span><span class=\"params\">(<span class=\"keyword\">new</span> TBufferedTransport(socket))</span></span>;</span><br><span class=\"line\">            <span class=\"function\">std::shared_ptr&lt;TProtocol&gt; <span class=\"title\">protocol</span><span class=\"params\">(<span class=\"keyword\">new</span> TBinaryProtocol(transport))</span></span>;</span><br><span class=\"line\">            <span class=\"function\">SaveClient <span class=\"title\">client</span><span class=\"params\">(protocol)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                transport-&gt;<span class=\"built_in\">open</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"type\">int</span> res = client.<span class=\"built_in\">save_data</span>(<span class=\"string\">&quot;acs_2077&quot;</span>, <span class=\"string\">&quot;4503f06d&quot;</span>, a, b);</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (!res) <span class=\"built_in\">puts</span>(<span class=\"string\">&quot;Success&quot;</span>);</span><br><span class=\"line\">                <span class=\"keyword\">else</span> <span class=\"built_in\">puts</span>(<span class=\"string\">&quot;Failed&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                transport-&gt;<span class=\"built_in\">close</span>();</span><br><span class=\"line\">            &#125; <span class=\"built_in\">catch</span> (TException&amp; tx) &#123;</span><br><span class=\"line\">                cout &lt;&lt; <span class=\"string\">&quot;ERROR: &quot;</span> &lt;&lt; tx.<span class=\"built_in\">what</span>() &lt;&lt; endl;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">match</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\">        </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">while</span> (users.<span class=\"built_in\">size</span>() &gt; <span class=\"number\">1</span>)</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                <span class=\"keyword\">auto</span> a = users[<span class=\"number\">0</span>], b = users[<span class=\"number\">1</span>];</span><br><span class=\"line\">                users.<span class=\"built_in\">erase</span>(users.<span class=\"built_in\">begin</span>());</span><br><span class=\"line\">                users.<span class=\"built_in\">erase</span>(users.<span class=\"built_in\">begin</span>());</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"built_in\">save_result</span>(a.id, b.id);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">add</span><span class=\"params\">(User user)</span></span></span><br><span class=\"line\"><span class=\"function\">        </span>&#123;</span><br><span class=\"line\">            users.<span class=\"built_in\">push_back</span>(user);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">remove</span><span class=\"params\">(User user)</span></span></span><br><span class=\"line\"><span class=\"function\">        </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"type\">uint32_t</span> i = <span class=\"number\">0</span>; i &lt; users.<span class=\"built_in\">size</span>(); i++)</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (users[i].id == user.id)</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    users.<span class=\"built_in\">erase</span>(users.<span class=\"built_in\">begin</span>() + i);</span><br><span class=\"line\">                    <span class=\"keyword\">break</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span>:</span><br><span class=\"line\">        vector&lt;User&gt; users;</span><br><span class=\"line\">&#125;pool;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MatchHandler</span> : <span class=\"keyword\">virtual</span> <span class=\"keyword\">public</span> MatchIf &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span>:</span><br><span class=\"line\">        <span class=\"built_in\">MatchHandler</span>() &#123;</span><br><span class=\"line\">            <span class=\"comment\">// Your initialization goes here</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">int32_t</span> <span class=\"title\">add_user</span><span class=\"params\">(<span class=\"type\">const</span> User&amp; user, <span class=\"type\">const</span> std::string&amp; info)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">// Your implementation goes here</span></span><br><span class=\"line\">            <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;add_user\\n&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"function\">unique_lock&lt;mutex&gt; <span class=\"title\">lck</span><span class=\"params\">(message_queue.m)</span></span>;  <span class=\"comment\">// 当lck变量消失即函数结束后锁会自动释放</span></span><br><span class=\"line\">            message_queue.q.<span class=\"built_in\">push</span>(&#123; user, <span class=\"string\">&quot;add&quot;</span> &#125;);</span><br><span class=\"line\">            message_queue.cv.<span class=\"built_in\">notify_all</span>();  <span class=\"comment\">// 唤醒条件变量</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">int32_t</span> <span class=\"title\">remove_user</span><span class=\"params\">(<span class=\"type\">const</span> User&amp; user, <span class=\"type\">const</span> std::string&amp; info)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">// Your implementation goes here</span></span><br><span class=\"line\">            <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;remove_user\\n&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"function\">unique_lock&lt;mutex&gt; <span class=\"title\">lck</span><span class=\"params\">(message_queue.m)</span></span>;</span><br><span class=\"line\">            message_queue.q.<span class=\"built_in\">push</span>(&#123; user, <span class=\"string\">&quot;remove&quot;</span> &#125;);</span><br><span class=\"line\">            message_queue.cv.<span class=\"built_in\">notify_all</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">consume_task</span><span class=\"params\">()</span>  <span class=\"comment\">// 消费者模型</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (<span class=\"literal\">true</span>)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"function\">unique_lock&lt;mutex&gt; <span class=\"title\">lck</span><span class=\"params\">(message_queue.m)</span></span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (message_queue.q.<span class=\"built_in\">empty</span>())  <span class=\"comment\">// 如果消息队列为空则应该先阻塞，而不能一直循环</span></span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            message_queue.cv.<span class=\"built_in\">wait</span>(lck);  <span class=\"comment\">// 先将锁释放，然后卡住，直到在其他地方将这个条件变量唤醒</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">else</span></span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"keyword\">auto</span> task = message_queue.q.<span class=\"built_in\">front</span>();</span><br><span class=\"line\">            message_queue.q.<span class=\"built_in\">pop</span>();</span><br><span class=\"line\">            lck.<span class=\"built_in\">unlock</span>();  <span class=\"comment\">// 尽早解锁，若等处理完task再解锁就等待时间太长了</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\">// do task</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (task.type == <span class=\"string\">&quot;add&quot;</span>) pool.<span class=\"built_in\">add</span>(task.user);</span><br><span class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (task.type == <span class=\"string\">&quot;remove&quot;</span>) pool.<span class=\"built_in\">remove</span>(task.user);</span><br><span class=\"line\"></span><br><span class=\"line\">            pool.<span class=\"built_in\">match</span>();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span> **argv)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> port = <span class=\"number\">9090</span>;</span><br><span class=\"line\">    ::<span class=\"function\">std::shared_ptr&lt;MatchHandler&gt; <span class=\"title\">handler</span><span class=\"params\">(<span class=\"keyword\">new</span> MatchHandler())</span></span>;</span><br><span class=\"line\">    ::<span class=\"function\">std::shared_ptr&lt;TProcessor&gt; <span class=\"title\">processor</span><span class=\"params\">(<span class=\"keyword\">new</span> MatchProcessor(handler))</span></span>;</span><br><span class=\"line\">    ::<span class=\"function\">std::shared_ptr&lt;TServerTransport&gt; <span class=\"title\">serverTransport</span><span class=\"params\">(<span class=\"keyword\">new</span> TServerSocket(port))</span></span>;</span><br><span class=\"line\">    ::<span class=\"function\">std::shared_ptr&lt;TTransportFactory&gt; <span class=\"title\">transportFactory</span><span class=\"params\">(<span class=\"keyword\">new</span> TBufferedTransportFactory())</span></span>;</span><br><span class=\"line\">    ::<span class=\"function\">std::shared_ptr&lt;TProtocolFactory&gt; <span class=\"title\">protocolFactory</span><span class=\"params\">(<span class=\"keyword\">new</span> TBinaryProtocolFactory())</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\">TSimpleServer <span class=\"title\">server</span><span class=\"params\">(processor, serverTransport, transportFactory, protocolFactory)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;Start Match Server\\n&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\">thread <span class=\"title\">matching_thread</span><span class=\"params\">(consume_task)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    server.<span class=\"built_in\">serve</span>();</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-5-match-server-v3-0实现\">2.5 match_server_v3.0实现</h3>\n<p>通过修改匹配函数 <code>match()</code> 实现将分差小于等于50的玩家进行匹配，修改后的代码如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// This autogenerated skeleton file illustrates how to build a server.</span></span><br><span class=\"line\"><span class=\"comment\">// You should copy it to another filename to avoid overwriting it.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;match_server/Match.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;save_client/Save.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/protocol/TBinaryProtocol.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/server/TSimpleServer.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/transport/TServerSocket.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/transport/TBufferTransports.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/transport/TTransportUtils.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/transport/TSocket.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thread&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;mutex&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;condition_variable&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;queue&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;vector&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;unistd.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift;</span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift::protocol;</span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift::transport;</span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift::server;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::match_service;</span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::save_service;  <span class=\"comment\">// 注意加上save_client端的命名空间</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">Task</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    User user;</span><br><span class=\"line\">    string type;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">MessageQueue</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    queue&lt;Task&gt; q;</span><br><span class=\"line\">    mutex m;</span><br><span class=\"line\">    condition_variable cv;</span><br><span class=\"line\">&#125;message_queue;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Pool</span>  <span class=\"comment\">// 玩家匹配池</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span>:</span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">save_result</span><span class=\"params\">(<span class=\"type\">int</span> a, <span class=\"type\">int</span> b)</span></span></span><br><span class=\"line\"><span class=\"function\">        </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">// do save task</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">match</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\">        </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">while</span> (users.<span class=\"built_in\">size</span>() &gt; <span class=\"number\">1</span>)</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                <span class=\"built_in\">sort</span>(users.<span class=\"built_in\">begin</span>(), users.<span class=\"built_in\">end</span>(), [&amp;](User &amp;a, User &amp;b)&#123;</span><br><span class=\"line\">                        <span class=\"keyword\">return</span> a.score &lt; b.score;</span><br><span class=\"line\">                        &#125;);</span><br><span class=\"line\">                <span class=\"type\">bool</span> flag = <span class=\"literal\">true</span>;  <span class=\"comment\">// 防止玩家之间分数差距都很大导致死循环</span></span><br><span class=\"line\">                <span class=\"keyword\">for</span> (<span class=\"type\">uint32_t</span> i = <span class=\"number\">1</span>; i &lt; users.<span class=\"built_in\">size</span>(); i++)</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    <span class=\"keyword\">auto</span> a = users[i - <span class=\"number\">1</span>], b = users[i];</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (b.score - a.score &lt;= <span class=\"number\">50</span>)</span><br><span class=\"line\">                    &#123;</span><br><span class=\"line\">                        <span class=\"built_in\">save_result</span>(a.id, b.id);</span><br><span class=\"line\">                        users.<span class=\"built_in\">erase</span>(users.<span class=\"built_in\">begin</span>() + i - <span class=\"number\">1</span>, users.<span class=\"built_in\">begin</span>() + i + <span class=\"number\">1</span>);</span><br><span class=\"line\">                        flag = <span class=\"literal\">false</span>;</span><br><span class=\"line\">                        <span class=\"keyword\">break</span>;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (flag) <span class=\"keyword\">break</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">add</span><span class=\"params\">(User user)</span></span></span><br><span class=\"line\"><span class=\"function\">        </span>&#123;</span><br><span class=\"line\">            users.<span class=\"built_in\">push_back</span>(user);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">remove</span><span class=\"params\">(User user)</span></span></span><br><span class=\"line\"><span class=\"function\">        </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"type\">uint32_t</span> i = <span class=\"number\">0</span>; i &lt; users.<span class=\"built_in\">size</span>(); i++)</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (users[i].id == user.id)</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    users.<span class=\"built_in\">erase</span>(users.<span class=\"built_in\">begin</span>() + i);</span><br><span class=\"line\">                    <span class=\"keyword\">break</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span>:</span><br><span class=\"line\">        vector&lt;User&gt; users;</span><br><span class=\"line\">&#125;pool;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MatchHandler</span> : <span class=\"keyword\">virtual</span> <span class=\"keyword\">public</span> MatchIf &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span>:</span><br><span class=\"line\">        <span class=\"built_in\">MatchHandler</span>() &#123;</span><br><span class=\"line\">            <span class=\"comment\">// Your initialization goes here</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">int32_t</span> <span class=\"title\">add_user</span><span class=\"params\">(<span class=\"type\">const</span> User&amp; user, <span class=\"type\">const</span> std::string&amp; info)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">// Your implementation goes here</span></span><br><span class=\"line\">            <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;add_user\\n&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"function\">unique_lock&lt;mutex&gt; <span class=\"title\">lck</span><span class=\"params\">(message_queue.m)</span></span>;  <span class=\"comment\">// 当lck变量消失即函数结束后锁会自动释放</span></span><br><span class=\"line\">            message_queue.q.<span class=\"built_in\">push</span>(&#123; user, <span class=\"string\">&quot;add&quot;</span> &#125;);</span><br><span class=\"line\">            message_queue.cv.<span class=\"built_in\">notify_all</span>();  <span class=\"comment\">// 唤醒条件变量</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">int32_t</span> <span class=\"title\">remove_user</span><span class=\"params\">(<span class=\"type\">const</span> User&amp; user, <span class=\"type\">const</span> std::string&amp; info)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">// Your implementation goes here</span></span><br><span class=\"line\">            <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;remove_user\\n&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"function\">unique_lock&lt;mutex&gt; <span class=\"title\">lck</span><span class=\"params\">(message_queue.m)</span></span>;</span><br><span class=\"line\">            message_queue.q.<span class=\"built_in\">push</span>(&#123; user, <span class=\"string\">&quot;remove&quot;</span> &#125;);</span><br><span class=\"line\">            message_queue.cv.<span class=\"built_in\">notify_all</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">consume_task</span><span class=\"params\">()</span>  <span class=\"comment\">// 消费者模型</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (<span class=\"literal\">true</span>)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"function\">unique_lock&lt;mutex&gt; <span class=\"title\">lck</span><span class=\"params\">(message_queue.m)</span></span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (message_queue.q.<span class=\"built_in\">empty</span>())  <span class=\"comment\">// 如果消息队列为空则应该先阻塞，而不能一直循环</span></span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"comment\">// message_queue.cv.wait(lck);  // 先将锁释放，然后卡住，直到在其他地方将这个条件变量唤醒</span></span><br><span class=\"line\">            lck.<span class=\"built_in\">unlock</span>();</span><br><span class=\"line\">            pool.<span class=\"built_in\">match</span>();</span><br><span class=\"line\">            <span class=\"built_in\">sleep</span>(<span class=\"number\">1</span>);  <span class=\"comment\">// 每秒匹配一次</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">else</span></span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"keyword\">auto</span> task = message_queue.q.<span class=\"built_in\">front</span>();</span><br><span class=\"line\">            message_queue.q.<span class=\"built_in\">pop</span>();</span><br><span class=\"line\">            lck.<span class=\"built_in\">unlock</span>();  <span class=\"comment\">// 尽早解锁，若等处理完task再解锁就等待时间太长了</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\">// do task</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (task.type == <span class=\"string\">&quot;add&quot;</span>) pool.<span class=\"built_in\">add</span>(task.user);</span><br><span class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (task.type == <span class=\"string\">&quot;remove&quot;</span>) pool.<span class=\"built_in\">remove</span>(task.user);</span><br><span class=\"line\"></span><br><span class=\"line\">            pool.<span class=\"built_in\">match</span>();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span> **argv)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> port = <span class=\"number\">9090</span>;</span><br><span class=\"line\">    ::<span class=\"function\">std::shared_ptr&lt;MatchHandler&gt; <span class=\"title\">handler</span><span class=\"params\">(<span class=\"keyword\">new</span> MatchHandler())</span></span>;</span><br><span class=\"line\">    ::<span class=\"function\">std::shared_ptr&lt;TProcessor&gt; <span class=\"title\">processor</span><span class=\"params\">(<span class=\"keyword\">new</span> MatchProcessor(handler))</span></span>;</span><br><span class=\"line\">    ::<span class=\"function\">std::shared_ptr&lt;TServerTransport&gt; <span class=\"title\">serverTransport</span><span class=\"params\">(<span class=\"keyword\">new</span> TServerSocket(port))</span></span>;</span><br><span class=\"line\">    ::<span class=\"function\">std::shared_ptr&lt;TTransportFactory&gt; <span class=\"title\">transportFactory</span><span class=\"params\">(<span class=\"keyword\">new</span> TBufferedTransportFactory())</span></span>;</span><br><span class=\"line\">    ::<span class=\"function\">std::shared_ptr&lt;TProtocolFactory&gt; <span class=\"title\">protocolFactory</span><span class=\"params\">(<span class=\"keyword\">new</span> TBinaryProtocolFactory())</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\">TSimpleServer <span class=\"title\">server</span><span class=\"params\">(processor, serverTransport, transportFactory, protocolFactory)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;Start Match Server\\n&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\">thread <span class=\"title\">matching_thread</span><span class=\"params\">(consume_task)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    server.<span class=\"built_in\">serve</span>();</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-6-match-server-v4-0实现\">2.6 match_server_v4.0实现</h3>\n<p>通过 Thrift 官网 C++ 教程下的 Server 端代码可以将 <code>match_server</code> 改为多线程，修改后的 <code>main.cpp</code> 代码如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// This autogenerated skeleton file illustrates how to build a server.</span></span><br><span class=\"line\"><span class=\"comment\">// You should copy it to another filename to avoid overwriting it.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;match_server/Match.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;save_client/Save.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/concurrency/ThreadManager.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/concurrency/ThreadFactory.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/protocol/TBinaryProtocol.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/server/TSimpleServer.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/server/TThreadedServer.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/transport/TServerSocket.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/transport/TBufferTransports.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/transport/TTransportUtils.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/transport/TSocket.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/TToString.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thread&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;mutex&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;condition_variable&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;queue&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;vector&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;unistd.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift;</span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift::protocol;</span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift::transport;</span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift::server;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::match_service;</span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::save_service;  <span class=\"comment\">// 注意加上save_client端的命名空间</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">Task</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    User user;</span><br><span class=\"line\">    string type;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">MessageQueue</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    queue&lt;Task&gt; q;</span><br><span class=\"line\">    mutex m;</span><br><span class=\"line\">    condition_variable cv;</span><br><span class=\"line\">&#125;message_queue;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Pool</span>  <span class=\"comment\">// 玩家匹配池</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span>:</span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">save_result</span><span class=\"params\">(<span class=\"type\">int</span> a, <span class=\"type\">int</span> b)</span></span></span><br><span class=\"line\"><span class=\"function\">        </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">// do save task</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">match</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\">        </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">// do match task</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">add</span><span class=\"params\">(User user)</span></span></span><br><span class=\"line\"><span class=\"function\">        </span>&#123;</span><br><span class=\"line\">            users.<span class=\"built_in\">push_back</span>(user);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">remove</span><span class=\"params\">(User user)</span></span></span><br><span class=\"line\"><span class=\"function\">        </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"type\">uint32_t</span> i = <span class=\"number\">0</span>; i &lt; users.<span class=\"built_in\">size</span>(); i++)</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (users[i].id == user.id)</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    users.<span class=\"built_in\">erase</span>(users.<span class=\"built_in\">begin</span>() + i);</span><br><span class=\"line\">                    <span class=\"keyword\">break</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span>:</span><br><span class=\"line\">        vector&lt;User&gt; users;</span><br><span class=\"line\">&#125;pool;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MatchHandler</span> : <span class=\"keyword\">virtual</span> <span class=\"keyword\">public</span> MatchIf &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span>:</span><br><span class=\"line\">        <span class=\"built_in\">MatchHandler</span>() &#123;</span><br><span class=\"line\">            <span class=\"comment\">// Your initialization goes here</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">int32_t</span> <span class=\"title\">add_user</span><span class=\"params\">(<span class=\"type\">const</span> User&amp; user, <span class=\"type\">const</span> std::string&amp; info)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">// Your implementation goes here</span></span><br><span class=\"line\">            <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;add_user\\n&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"function\">unique_lock&lt;mutex&gt; <span class=\"title\">lck</span><span class=\"params\">(message_queue.m)</span></span>;  <span class=\"comment\">// 当lck变量消失即函数结束后锁会自动释放</span></span><br><span class=\"line\">            message_queue.q.<span class=\"built_in\">push</span>(&#123; user, <span class=\"string\">&quot;add&quot;</span> &#125;);</span><br><span class=\"line\">            message_queue.cv.<span class=\"built_in\">notify_all</span>();  <span class=\"comment\">// 唤醒条件变量</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">int32_t</span> <span class=\"title\">remove_user</span><span class=\"params\">(<span class=\"type\">const</span> User&amp; user, <span class=\"type\">const</span> std::string&amp; info)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">// Your implementation goes here</span></span><br><span class=\"line\">            <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;remove_user\\n&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"function\">unique_lock&lt;mutex&gt; <span class=\"title\">lck</span><span class=\"params\">(message_queue.m)</span></span>;</span><br><span class=\"line\">            message_queue.q.<span class=\"built_in\">push</span>(&#123; user, <span class=\"string\">&quot;remove&quot;</span> &#125;);</span><br><span class=\"line\">            message_queue.cv.<span class=\"built_in\">notify_all</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MatchCloneFactory</span> : <span class=\"keyword\">virtual</span> <span class=\"keyword\">public</span> MatchIfFactory &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span>:</span><br><span class=\"line\">        ~<span class=\"built_in\">MatchCloneFactory</span>() <span class=\"keyword\">override</span> = <span class=\"keyword\">default</span>;</span><br><span class=\"line\">        <span class=\"function\">MatchIf* <span class=\"title\">getHandler</span><span class=\"params\">(<span class=\"type\">const</span> ::apache::thrift::TConnectionInfo&amp; connInfo)</span> <span class=\"keyword\">override</span></span></span><br><span class=\"line\"><span class=\"function\">        </span>&#123;</span><br><span class=\"line\">            std::shared_ptr&lt;TSocket&gt; sock = std::<span class=\"built_in\">dynamic_pointer_cast</span>&lt;TSocket&gt;(connInfo.transport);</span><br><span class=\"line\">            <span class=\"comment\">/*cout &lt;&lt; &quot;Incoming connection\\n&quot;;</span></span><br><span class=\"line\"><span class=\"comment\">            cout &lt;&lt; &quot;\\tSocketInfo: &quot;  &lt;&lt; sock-&gt;getSocketInfo() &lt;&lt; &quot;\\n&quot;;</span></span><br><span class=\"line\"><span class=\"comment\">            cout &lt;&lt; &quot;\\tPeerHost: &quot;    &lt;&lt; sock-&gt;getPeerHost() &lt;&lt; &quot;\\n&quot;;</span></span><br><span class=\"line\"><span class=\"comment\">            cout &lt;&lt; &quot;\\tPeerAddress: &quot; &lt;&lt; sock-&gt;getPeerAddress() &lt;&lt; &quot;\\n&quot;;</span></span><br><span class=\"line\"><span class=\"comment\">            cout &lt;&lt; &quot;\\tPeerPort: &quot;    &lt;&lt; sock-&gt;getPeerPort() &lt;&lt; &quot;\\n&quot;;*/</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> MatchHandler;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">releaseHandler</span><span class=\"params\">(MatchIf* handler)</span> <span class=\"keyword\">override</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">delete</span> handler;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">consume_task</span><span class=\"params\">()</span>  <span class=\"comment\">// 消费者模型</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (<span class=\"literal\">true</span>)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"function\">unique_lock&lt;mutex&gt; <span class=\"title\">lck</span><span class=\"params\">(message_queue.m)</span></span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (message_queue.q.<span class=\"built_in\">empty</span>())  <span class=\"comment\">// 如果消息队列为空则应该先阻塞，而不能一直循环</span></span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"comment\">// message_queue.cv.wait(lck);  // 先将锁释放，然后卡住，直到在其他地方将这个条件变量唤醒</span></span><br><span class=\"line\">            lck.<span class=\"built_in\">unlock</span>();</span><br><span class=\"line\">            pool.<span class=\"built_in\">match</span>();</span><br><span class=\"line\">            <span class=\"built_in\">sleep</span>(<span class=\"number\">1</span>);  <span class=\"comment\">// 每秒匹配一次</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">else</span></span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"keyword\">auto</span> task = message_queue.q.<span class=\"built_in\">front</span>();</span><br><span class=\"line\">            message_queue.q.<span class=\"built_in\">pop</span>();</span><br><span class=\"line\">            lck.<span class=\"built_in\">unlock</span>();  <span class=\"comment\">// 尽早解锁，若等处理完task再解锁就等待时间太长了</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\">// do task</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (task.type == <span class=\"string\">&quot;add&quot;</span>) pool.<span class=\"built_in\">add</span>(task.user);</span><br><span class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (task.type == <span class=\"string\">&quot;remove&quot;</span>) pool.<span class=\"built_in\">remove</span>(task.user);</span><br><span class=\"line\"></span><br><span class=\"line\">            pool.<span class=\"built_in\">match</span>();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span> **argv)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"function\">TThreadedServer <span class=\"title\">server</span><span class=\"params\">(</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">            std::make_shared&lt;MatchProcessorFactory&gt;(std::make_shared&lt;MatchCloneFactory&gt;()),</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">            std::make_shared&lt;TServerSocket&gt;(<span class=\"number\">9090</span>), <span class=\"comment\">//port</span></span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">            std::make_shared&lt;TBufferedTransportFactory&gt;(),</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">            std::make_shared&lt;TBinaryProtocolFactory&gt;())</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;Start Match Server\\n&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\">thread <span class=\"title\">matching_thread</span><span class=\"params\">(consume_task)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    server.<span class=\"built_in\">serve</span>();</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-7-match-server-v5-0实现\">2.7 match_server_v5.0实现</h3>\n<p>通过对匹配机制的修改，实现玩家每等待一秒钟，匹配的分数区间扩大50分，修改后的 <code>main.cpp</code> 代码如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br><span class=\"line\">211</span><br><span class=\"line\">212</span><br><span class=\"line\">213</span><br><span class=\"line\">214</span><br><span class=\"line\">215</span><br><span class=\"line\">216</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// This autogenerated skeleton file illustrates how to build a server.</span></span><br><span class=\"line\"><span class=\"comment\">// You should copy it to another filename to avoid overwriting it.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;match_server/Match.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;save_client/Save.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/concurrency/ThreadManager.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/concurrency/ThreadFactory.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/protocol/TBinaryProtocol.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/server/TSimpleServer.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/server/TThreadedServer.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/transport/TServerSocket.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/transport/TBufferTransports.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/transport/TTransportUtils.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/transport/TSocket.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thrift/TToString.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thread&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;mutex&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;condition_variable&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;queue&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;vector&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;unistd.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift;</span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift::protocol;</span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift::transport;</span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::apache::thrift::server;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::match_service;</span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> ::save_service;  <span class=\"comment\">// 注意加上save_client端的命名空间</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">Task</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    User user;</span><br><span class=\"line\">    string type;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">MessageQueue</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    queue&lt;Task&gt; q;</span><br><span class=\"line\">    mutex m;</span><br><span class=\"line\">    condition_variable cv;</span><br><span class=\"line\">&#125;message_queue;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Pool</span>  <span class=\"comment\">// 玩家匹配池</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span>:</span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">save_result</span><span class=\"params\">(<span class=\"type\">int</span> a, <span class=\"type\">int</span> b)</span></span></span><br><span class=\"line\"><span class=\"function\">        </span>&#123;</span><br><span class=\"line\">            <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;Match result: %d %d\\n&quot;</span>, a, b);</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"function\">std::shared_ptr&lt;TTransport&gt; <span class=\"title\">socket</span><span class=\"params\">(<span class=\"keyword\">new</span> TSocket(<span class=\"string\">&quot;123.57.47.211&quot;</span>, <span class=\"number\">9090</span>))</span></span>;</span><br><span class=\"line\">            <span class=\"function\">std::shared_ptr&lt;TTransport&gt; <span class=\"title\">transport</span><span class=\"params\">(<span class=\"keyword\">new</span> TBufferedTransport(socket))</span></span>;</span><br><span class=\"line\">            <span class=\"function\">std::shared_ptr&lt;TProtocol&gt; <span class=\"title\">protocol</span><span class=\"params\">(<span class=\"keyword\">new</span> TBinaryProtocol(transport))</span></span>;</span><br><span class=\"line\">            <span class=\"function\">SaveClient <span class=\"title\">client</span><span class=\"params\">(protocol)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                transport-&gt;<span class=\"built_in\">open</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"type\">int</span> res = client.<span class=\"built_in\">save_data</span>(<span class=\"string\">&quot;acs_2077&quot;</span>, <span class=\"string\">&quot;4503f06d&quot;</span>, a, b);</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (!res) <span class=\"built_in\">puts</span>(<span class=\"string\">&quot;Success&quot;</span>);</span><br><span class=\"line\">                <span class=\"keyword\">else</span> <span class=\"built_in\">puts</span>(<span class=\"string\">&quot;Failed&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                transport-&gt;<span class=\"built_in\">close</span>();</span><br><span class=\"line\">            &#125; <span class=\"built_in\">catch</span> (TException&amp; tx) &#123;</span><br><span class=\"line\">                cout &lt;&lt; <span class=\"string\">&quot;ERROR: &quot;</span> &lt;&lt; tx.<span class=\"built_in\">what</span>() &lt;&lt; endl;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">bool</span> <span class=\"title\">check_match</span><span class=\"params\">(<span class=\"type\">uint32_t</span> i, <span class=\"type\">uint32_t</span> j)</span></span></span><br><span class=\"line\"><span class=\"function\">        </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">auto</span> a = users[i], b = users[j];</span><br><span class=\"line\">            <span class=\"type\">int</span> dt = <span class=\"built_in\">abs</span>(a.score - b.score);</span><br><span class=\"line\">            <span class=\"type\">int</span> a_max_dif = wt[i] * <span class=\"number\">50</span>, b_max_dif = wt[j] * <span class=\"number\">50</span>;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> dt &lt;= a_max_dif &amp;&amp; dt &lt;= b_max_dif;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">match</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\">        </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"type\">uint32_t</span> i = <span class=\"number\">0</span>; i &lt; wt.<span class=\"built_in\">size</span>(); i++)</span><br><span class=\"line\">                wt[i]++;  <span class=\"comment\">// 表示等待秒数+1</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">while</span> (users.<span class=\"built_in\">size</span>() &gt; <span class=\"number\">1</span>)</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                <span class=\"type\">bool</span> flag = <span class=\"literal\">true</span>;  <span class=\"comment\">// 防止玩家之间分数差距都很大导致死循环</span></span><br><span class=\"line\">                <span class=\"keyword\">for</span> (<span class=\"type\">uint32_t</span> i = <span class=\"number\">0</span>; i &lt; users.<span class=\"built_in\">size</span>(); i++)</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    <span class=\"keyword\">for</span> (<span class=\"type\">uint32_t</span> j = i + <span class=\"number\">1</span>; j &lt; users.<span class=\"built_in\">size</span>(); j++)</span><br><span class=\"line\">                        <span class=\"keyword\">if</span> (<span class=\"built_in\">check_match</span>(i, j))</span><br><span class=\"line\">                        &#123;</span><br><span class=\"line\">                            <span class=\"keyword\">auto</span> a = users[i], b = users[j];</span><br><span class=\"line\">                            users.<span class=\"built_in\">erase</span>(users.<span class=\"built_in\">begin</span>() + j);  <span class=\"comment\">// 先删后面的再删前面的</span></span><br><span class=\"line\">                            users.<span class=\"built_in\">erase</span>(users.<span class=\"built_in\">begin</span>() + i);</span><br><span class=\"line\">                            wt.<span class=\"built_in\">erase</span>(wt.<span class=\"built_in\">begin</span>() + j);</span><br><span class=\"line\">                            wt.<span class=\"built_in\">erase</span>(wt.<span class=\"built_in\">begin</span>() + i);</span><br><span class=\"line\">                            <span class=\"built_in\">save_result</span>(a.id, b.id);</span><br><span class=\"line\">                            flag = <span class=\"literal\">false</span>;</span><br><span class=\"line\">                            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (!flag) <span class=\"keyword\">break</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (flag) <span class=\"keyword\">break</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">add</span><span class=\"params\">(User user)</span></span></span><br><span class=\"line\"><span class=\"function\">        </span>&#123;</span><br><span class=\"line\">            users.<span class=\"built_in\">push_back</span>(user);</span><br><span class=\"line\">            wt.<span class=\"built_in\">push_back</span>(<span class=\"number\">0</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">remove</span><span class=\"params\">(User user)</span></span></span><br><span class=\"line\"><span class=\"function\">        </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"type\">uint32_t</span> i = <span class=\"number\">0</span>; i &lt; users.<span class=\"built_in\">size</span>(); i++)</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (users[i].id == user.id)</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    users.<span class=\"built_in\">erase</span>(users.<span class=\"built_in\">begin</span>() + i);</span><br><span class=\"line\">                    wt.<span class=\"built_in\">erase</span>(wt.<span class=\"built_in\">begin</span>() + i);</span><br><span class=\"line\">                    <span class=\"keyword\">break</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span>:</span><br><span class=\"line\">        vector&lt;User&gt; users;</span><br><span class=\"line\">        vector&lt;<span class=\"type\">int</span>&gt; wt;  <span class=\"comment\">// 表示玩家的waiting time</span></span><br><span class=\"line\">&#125;pool;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MatchHandler</span> : <span class=\"keyword\">virtual</span> <span class=\"keyword\">public</span> MatchIf &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span>:</span><br><span class=\"line\">        <span class=\"built_in\">MatchHandler</span>() &#123;</span><br><span class=\"line\">            <span class=\"comment\">// Your initialization goes here</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">int32_t</span> <span class=\"title\">add_user</span><span class=\"params\">(<span class=\"type\">const</span> User&amp; user, <span class=\"type\">const</span> std::string&amp; info)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">// Your implementation goes here</span></span><br><span class=\"line\">            <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;add_user\\n&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"function\">unique_lock&lt;mutex&gt; <span class=\"title\">lck</span><span class=\"params\">(message_queue.m)</span></span>;  <span class=\"comment\">// 当lck变量消失即函数结束后锁会自动释放</span></span><br><span class=\"line\">            message_queue.q.<span class=\"built_in\">push</span>(&#123; user, <span class=\"string\">&quot;add&quot;</span> &#125;);</span><br><span class=\"line\">            message_queue.cv.<span class=\"built_in\">notify_all</span>();  <span class=\"comment\">// 唤醒条件变量</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">int32_t</span> <span class=\"title\">remove_user</span><span class=\"params\">(<span class=\"type\">const</span> User&amp; user, <span class=\"type\">const</span> std::string&amp; info)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">// Your implementation goes here</span></span><br><span class=\"line\">            <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;remove_user\\n&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"function\">unique_lock&lt;mutex&gt; <span class=\"title\">lck</span><span class=\"params\">(message_queue.m)</span></span>;</span><br><span class=\"line\">            message_queue.q.<span class=\"built_in\">push</span>(&#123; user, <span class=\"string\">&quot;remove&quot;</span> &#125;);</span><br><span class=\"line\">            message_queue.cv.<span class=\"built_in\">notify_all</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MatchCloneFactory</span> : <span class=\"keyword\">virtual</span> <span class=\"keyword\">public</span> MatchIfFactory &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span>:</span><br><span class=\"line\">        ~<span class=\"built_in\">MatchCloneFactory</span>() <span class=\"keyword\">override</span> = <span class=\"keyword\">default</span>;</span><br><span class=\"line\">        <span class=\"function\">MatchIf* <span class=\"title\">getHandler</span><span class=\"params\">(<span class=\"type\">const</span> ::apache::thrift::TConnectionInfo&amp; connInfo)</span> <span class=\"keyword\">override</span></span></span><br><span class=\"line\"><span class=\"function\">        </span>&#123;</span><br><span class=\"line\">            std::shared_ptr&lt;TSocket&gt; sock = std::<span class=\"built_in\">dynamic_pointer_cast</span>&lt;TSocket&gt;(connInfo.transport);</span><br><span class=\"line\">            <span class=\"comment\">/*cout &lt;&lt; &quot;Incoming connection\\n&quot;;</span></span><br><span class=\"line\"><span class=\"comment\">            cout &lt;&lt; &quot;\\tSocketInfo: &quot;  &lt;&lt; sock-&gt;getSocketInfo() &lt;&lt; &quot;\\n&quot;;</span></span><br><span class=\"line\"><span class=\"comment\">            cout &lt;&lt; &quot;\\tPeerHost: &quot;    &lt;&lt; sock-&gt;getPeerHost() &lt;&lt; &quot;\\n&quot;;</span></span><br><span class=\"line\"><span class=\"comment\">            cout &lt;&lt; &quot;\\tPeerAddress: &quot; &lt;&lt; sock-&gt;getPeerAddress() &lt;&lt; &quot;\\n&quot;;</span></span><br><span class=\"line\"><span class=\"comment\">            cout &lt;&lt; &quot;\\tPeerPort: &quot;    &lt;&lt; sock-&gt;getPeerPort() &lt;&lt; &quot;\\n&quot;;*/</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> MatchHandler;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">releaseHandler</span><span class=\"params\">(MatchIf* handler)</span> <span class=\"keyword\">override</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">delete</span> handler;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">consume_task</span><span class=\"params\">()</span>  <span class=\"comment\">// 消费者模型</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (<span class=\"literal\">true</span>)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"function\">unique_lock&lt;mutex&gt; <span class=\"title\">lck</span><span class=\"params\">(message_queue.m)</span></span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (message_queue.q.<span class=\"built_in\">empty</span>())  <span class=\"comment\">// 如果消息队列为空则应该先阻塞，而不能一直循环</span></span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"comment\">// message_queue.cv.wait(lck);  // 先将锁释放，然后卡住，直到在其他地方将这个条件变量唤醒</span></span><br><span class=\"line\">            lck.<span class=\"built_in\">unlock</span>();</span><br><span class=\"line\">            pool.<span class=\"built_in\">match</span>();</span><br><span class=\"line\">            <span class=\"built_in\">sleep</span>(<span class=\"number\">1</span>);  <span class=\"comment\">// 每秒匹配一次</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">else</span></span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"keyword\">auto</span> task = message_queue.q.<span class=\"built_in\">front</span>();</span><br><span class=\"line\">            message_queue.q.<span class=\"built_in\">pop</span>();</span><br><span class=\"line\">            lck.<span class=\"built_in\">unlock</span>();  <span class=\"comment\">// 尽早解锁，若等处理完task再解锁就等待时间太长了</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\">// do task</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (task.type == <span class=\"string\">&quot;add&quot;</span>) pool.<span class=\"built_in\">add</span>(task.user);</span><br><span class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (task.type == <span class=\"string\">&quot;remove&quot;</span>) pool.<span class=\"built_in\">remove</span>(task.user);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span> **argv)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"function\">TThreadedServer <span class=\"title\">server</span><span class=\"params\">(</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">            std::make_shared&lt;MatchProcessorFactory&gt;(std::make_shared&lt;MatchCloneFactory&gt;()),</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">            std::make_shared&lt;TServerSocket&gt;(<span class=\"number\">9090</span>), <span class=\"comment\">//port</span></span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">            std::make_shared&lt;TBufferedTransportFactory&gt;(),</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">            std::make_shared&lt;TBinaryProtocolFactory&gt;())</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;Start Match Server\\n&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\">thread <span class=\"title\">matching_thread</span><span class=\"params\">(consume_task)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    server.<span class=\"built_in\">serve</span>();</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n",
            "tags": [
                "Linux"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/20491.html",
            "url": "https://asanosaki.github.io/posts/20491.html",
            "title": "Linux学习笔记-SSH与Git",
            "date_published": "2022-04-30T05:56:00.000Z",
            "content_html": "<blockquote>\n<p>本文记录 Linux 的学习过程，内容为 SSH 与 Git。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-SSH\">1. SSH</h2>\n<h3 id=\"1-1-SSH登录\">1.1 SSH登录</h3>\n<p>（1）基本用法：</p>\n<p>远程登录服务器：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh user@hostname</span><br><span class=\"line\"></span><br><span class=\"line\">user: 用户名</span><br><span class=\"line\">hostname: IP地址或域名</span><br></pre></td></tr></table></figure>\n<p>第一次登录时会提示：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">The authenticity of host &#x27;123.57.47.211 (123.57.47.211)&#x27; can&#x27;t be established.</span><br><span class=\"line\">ECDSA key fingerprint is SHA256:iy237yysfCe013/l+kpDGfEG9xxHxm0dnxnAbJTPpG8.</span><br><span class=\"line\">Are you sure you want to continue connecting (yes/no/[fingerprint])?</span><br></pre></td></tr></table></figure>\n<p>输入 <code>yes</code>，然后回车即可。这样会将该服务器的信息记录在 <code>~/.ssh/known_hosts</code> 文件中。然后输入密码即可登录到远程服务器中。</p>\n<p>默认登录端口号为 <code>22</code>。如果想登录某一特定端口可以加参数 <code>-p</code>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh user@hostname -p 22</span><br></pre></td></tr></table></figure>\n<p>（2）配置文件：</p>\n<p>创建文件 <code>~/.ssh/config</code>。</p>\n<p>然后在文件中输入：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Host myserver1</span><br><span class=\"line\">    HostName IP地址或域名</span><br><span class=\"line\">    User 用户名</span><br><span class=\"line\"></span><br><span class=\"line\">Host myserver2</span><br><span class=\"line\">    HostName IP地址或域名</span><br><span class=\"line\">    User 用户名</span><br></pre></td></tr></table></figure>\n<p>例如：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Host myserver1</span><br><span class=\"line\">    HostName 123.57.47.211</span><br><span class=\"line\">    User acs_2077</span><br></pre></td></tr></table></figure>\n<p>之后再使用服务器时，可以直接使用别名 <code>myserver1</code>、<code>myserver2</code>。</p>\n<p>（3）配置公钥免密登录：</p>\n<p>创建密钥：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh-keygen</span><br></pre></td></tr></table></figure>\n<p>然后一直回车即可，执行结束后，在 <code>~/.ssh/</code> 目录下会多两个文件：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">id_rsa：私钥</span><br><span class=\"line\">id_rsa.pub：公钥</span><br></pre></td></tr></table></figure>\n<p>之后想免密码登录哪个服务器，就将公钥传给哪个服务器即可。</p>\n<p>例如，想免密登录 <code>myserver</code> 服务器。则将公钥中的内容，复制到 <code>myserver</code> 中的 <code>~/.ssh/authorized_keys</code> 文件里即可。</p>\n<p>也可以使用如下命令一键添加公钥：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh-copy-id myserver</span><br></pre></td></tr></table></figure>\n<p>（4）执行命令：</p>\n<p>命令格式：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh user@hostname command</span><br></pre></td></tr></table></figure>\n<p>例如：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh user@hostname ls -a</span><br></pre></td></tr></table></figure>\n<p>或者：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">单引号中的<span class=\"variable\">$i</span>可以求值</span></span><br><span class=\"line\">ssh myserver &#x27;for ((i = 0; i &lt; 10; i ++ )) do echo $i; done&#x27;</span><br></pre></td></tr></table></figure>\n<p>或者：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">双引号中的<span class=\"variable\">$i</span>不可以求值</span></span><br><span class=\"line\">ssh myserver &quot;for ((i = 0; i &lt; 10; i ++ )) do echo $i; done&quot;</span><br></pre></td></tr></table></figure>\n<h2 id=\"1-2-SCP远程拷贝文件\">1.2 SCP远程拷贝文件</h2>\n<p>命令格式：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scp source destination</span><br></pre></td></tr></table></figure>\n<p>功能：将 <code>source</code> 路径下的文件复制到 <code>destination</code> 中。</p>\n<p>一次复制多个文件：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scp source1 source2 destination</span><br></pre></td></tr></table></figure>\n<p>复制文件夹（将本地家目录中的 <code>tmp</code> 文件夹复制到 <code>myserver</code> 服务器中的 <code>/home/acs/</code> 目录下）：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scp -r ~/tmp myserver:/home/acs/</span><br></pre></td></tr></table></figure>\n<p>将本地家目录中的 <code>tmp</code> 文件夹复制到 <code>myserver</code> 服务器中的 <code>~/homework/</code> 目录下：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scp -r ~/tmp myserver:homework/</span><br></pre></td></tr></table></figure>\n<p>将 <code>myserver</code> 服务器中的 <code>~/homework/</code> 文件夹复制到本地的当前路径下：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scp -r myserver:homework .</span><br></pre></td></tr></table></figure>\n<p>指定服务器的端口号：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scp -P 22 source1 source2 destination</span><br></pre></td></tr></table></figure>\n<p>注意：<code>scp</code> 的 <code>-r -P</code> 等参数尽量加在 <code>source</code> 和 <code>destination</code> 之前。</p>\n<p>使用 <code>scp</code> 配置其他服务器的 <code>vim</code> 和 <code>tmux</code>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scp ~/.vimrc ~/.tmux.conf myserver:</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-Git\">2. Git</h2>\n<h3 id=\"2-1-Git基本概念\">2.1 Git基本概念</h3>\n<ul>\n<li>工作区：仓库的目录。工作区是独立于各个分支的。</li>\n<li>暂存区：数据暂时存放的区域，类似于工作区写入版本库前的缓存区。暂存区是独立于各个分支的。</li>\n<li>版本库：存放所有已经提交到本地仓库的代码版本</li>\n<li>版本结构：树结构，树中每个节点代表一个代码版本。</li>\n</ul>\n<h3 id=\"2-2-Git常用命令\">2.2 Git常用命令</h3>\n<ul>\n<li><code>git config --global user.name xxx</code>：设置全局用户名，信息记录在 <code>~/.gitconfig</code> 文件中。</li>\n<li><code>git config --global user.email xxx@xxx.com</code>：设置全局邮箱地址，信息记录在 <code>~/.gitconfig</code> 文件中。</li>\n<li><code>git init</code>：将当前目录配置成 Git 仓库，信息记录在隐藏的 <code>.git</code> 文件夹中。</li>\n<li><code>git add XX</code>：将 XX 文件添加到暂存区。\n<ul>\n<li><code>git add .</code>：将所有待加入暂存区的文件加入暂存区。</li>\n</ul>\n</li>\n<li><code>git rm --cached XX</code>：将文件从仓库索引目录中删掉。</li>\n<li><code>git commit -m &quot;给自己看的备注信息&quot;</code>：将暂存区的内容提交到当前分支。</li>\n<li><code>git status</code>：查看仓库状态。</li>\n<li><code>git diff XX</code>：查看 XX 文件相对于暂存区修改了哪些内容。</li>\n<li><code>git log</code>：查看当前分支的所有版本。\n<ul>\n<li><code>git log --pretty=oneline</code>：每个版本用一行显示。</li>\n</ul>\n</li>\n<li><code>git reflog</code>：查看HEAD指针的移动历史（包括被回滚的版本）。</li>\n<li><code>git reset --hard HEAD^</code> 或 <code>git reset --hard HEAD~</code>：将代码库回滚到上一个版本。\n<ul>\n<li><code>git reset --hard HEAD^^</code>：往上回滚两次，以此类推。</li>\n<li><code>git reset --hard HEAD~100</code>：往上回滚100个版本。</li>\n<li><code>git reset --hard 版本号</code>：回滚到某一特定版本。</li>\n</ul>\n</li>\n<li><code>git checkout -- XX</code> 或 <code>git restore XX</code>：将 XX 文件尚未加入暂存区的修改全部撤销。</li>\n<li><code>git restore --staged XX</code>：将 XX 文件从暂存区撤出，不会更改文件的内容。</li>\n<li><code>git remote add origin git@git.acwing.com:xxx/XXX.git</code>：将本地仓库关联到远程仓库。</li>\n<li><code>git remote -v</code>：查看当前 Git 仓库有没有关联远程仓库，如果已经有关联则会显示具体远程仓库路径，如果没有返回，说明没有关联任何远程仓库。</li>\n<li><code>git remote rm XXX</code>：解除与远程仓库的关联，例如：<code>git remote rm origin</code>。</li>\n<li><code>git push -u (第一次需要-u以后不需要)</code>：将当前分支推送到远程仓库。\n<ul>\n<li><code>git push origin branch_name</code>：将本地的某个分支推送到远程仓库。</li>\n</ul>\n</li>\n<li><code>git clone git@git.acwing.com:xxx/XXX.git</code>：将远程仓库 XXX 下载到当前目录下。</li>\n<li><code>git checkout -b branch_name</code>：创建并切换到 <code>branch_name</code> 这个分支。</li>\n<li><code>git branch</code>：查看所有分支和当前所处分支。</li>\n<li><code>git checkout branch_name</code>：切换到 <code>branch_name</code> 这个分支。</li>\n<li><code>git merge branch_name</code>：将分支 <code>branch_name</code> 合并到当前分支上。</li>\n<li><code>git branch -d branch_name</code>：删除本地仓库的 <code>branch_name</code> 分支。</li>\n<li><code>git branch branch_name</code>：创建新分支 <code>branch_name</code>。</li>\n<li><code>git push --set-upstream origin branch_name</code>：设置本地的 <code>branch_name</code> 分支对应远程仓库的 <code>branch_name</code> 分支。</li>\n<li><code>git push -d origin branch_name</code>：删除远程仓库的 <code>branch_name</code> 分支。</li>\n<li><code>git pull</code>：将远程仓库的当前分支与本地仓库的当前分支合并。\n<ul>\n<li><code>git pull origin branch_name</code>：将远程仓库的 <code>branch_name</code> 分支与本地仓库的当前分支合并。</li>\n</ul>\n</li>\n<li><code>git branch --set-upstream-to=origin/branch_name1 branch_name2</code>：将远程的 <code>branch_name1</code> 分支与本地的 <code>branch_name2</code> 分支对应。</li>\n<li><code>git checkout -t origin/branch_name</code>：将远程的 <code>branch_name</code> 分支拉取到本地。</li>\n<li><code>git stash</code>：将工作区和暂存区中尚未提交的修改存入栈中。</li>\n<li><code>git stash apply</code>：将栈顶存储的修改恢复到当前分支，但不删除栈顶元素。</li>\n<li><code>git stash drop</code>：删除栈顶存储的修改。</li>\n<li><code>git stash pop</code>：将栈顶存储的修改恢复到当前分支，同时删除栈顶元素。</li>\n<li><code>git stash list</code>：查看栈中所有元素。</li>\n</ul>\n",
            "tags": [
                "Linux"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/31281.html",
            "url": "https://asanosaki.github.io/posts/31281.html",
            "title": "Linux学习笔记-Shell",
            "date_published": "2022-03-17T01:39:00.000Z",
            "content_html": "<blockquote>\n<p>本文记录 Linux 的学习过程，内容为 Shell 命令语言。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-Shell-概论\">1. Shell 概论</h2>\n<p>Shell 是我们通过命令行与操作系统沟通的语言。</p>\n<p>Shell 脚本可以直接在命令行中执行，也可以将一套逻辑组织成一个文件，方便复用。</p>\n<p>Linux 中常见的 Shell 脚本有很多种，常见的有：</p>\n<ul>\n<li>Bourne Shell（<code>/usr/bin/sh</code> 或 <code>/bin/sh</code>）</li>\n<li>Bourne Again Shell（<code>/bin/bash</code>）</li>\n<li>C Shell（<code>/usr/bin/csh</code>）</li>\n<li>K Shell（<code>/usr/bin/ksh</code>）</li>\n<li>zsh</li>\n<li>…</li>\n</ul>\n<p>Linux 系统中一般默认使用 bash，所以接下来讲解 bash 中的语法。</p>\n<p>文件开头需要写 <code>#! /bin/bash</code>，指明 bash 为脚本解释器。</p>\n<p>新建一个 <code>test.sh</code> 文件，内容如下：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">! /bin/bash</span></span><br><span class=\"line\">echo &quot;Hello World!&quot;</span><br></pre></td></tr></table></figure>\n<p>运行方式：</p>\n<p>（1）作为可执行文件</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">acs@9e0ebfcd82d7:~$ chmod +x test.sh  # 使脚本具有可执行权限</span><br><span class=\"line\">acs@9e0ebfcd82d7:~$ ./test.sh  # 当前路径下执行</span><br><span class=\"line\">Hello World!  # 脚本输出</span><br><span class=\"line\">acs@9e0ebfcd82d7:~$ /home/acs/test.sh  # 绝对路径下执行</span><br><span class=\"line\">Hello World!  # 脚本输出</span><br><span class=\"line\">acs@9e0ebfcd82d7:~$ ~/test.sh  # 家目录路径下执行</span><br><span class=\"line\">Hello World!  # 脚本输出</span><br></pre></td></tr></table></figure>\n<p>（2）用解释器执行</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">acs@9e0ebfcd82d7:~$ bash test.sh</span><br><span class=\"line\">Hello World!  # 脚本输出</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-注释\">2. 注释</h2>\n<p>（1）单行注释</p>\n<p>每行中 <code>#</code> 之后的内容均是注释：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">这是一行注释</span></span><br><span class=\"line\"></span><br><span class=\"line\">echo &#x27;Hello World&#x27;  #  这也是注释</span><br></pre></td></tr></table></figure>\n<p>（2）多行注释</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:&lt;&lt;EOF</span><br><span class=\"line\">第一行注释</span><br><span class=\"line\">第二行注释</span><br><span class=\"line\">第三行注释</span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure>\n<p>其中 <code>EOF</code> 可以换成其它任意字符串，例如：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:&lt;&lt;abc</span><br><span class=\"line\">第一行注释</span><br><span class=\"line\">第二行注释</span><br><span class=\"line\">第三行注释</span><br><span class=\"line\">abc</span><br><span class=\"line\"></span><br><span class=\"line\">:&lt;&lt;orz</span><br><span class=\"line\">第一行注释</span><br><span class=\"line\">第二行注释</span><br><span class=\"line\">第三行注释</span><br><span class=\"line\">orz</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-变量\">3. 变量</h2>\n<h3 id=\"3-1-定义变量\">3.1 定义变量</h3>\n<p>定义变量不需要加 <code>$</code> 符号，例如：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">name1=&#x27;yxc&#x27;  # 单引号定义字符串</span><br><span class=\"line\">name2=&quot;yxc&quot;  # 双引号定义字符串</span><br><span class=\"line\">name3=yxc    # 也可以不加引号，同样表示字符串</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-2-使用变量\">3.2 使用变量</h3>\n<p>使用变量，需要加上 <code>$</code> 符号，或者 <code>$&#123;&#125;</code> 符号。花括号是可选的，主要为了帮助解释器识别变量边界。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">name=yxc</span><br><span class=\"line\">echo $name  # 输出yxc</span><br><span class=\"line\">echo $&#123;name&#125;  # 输出yxc</span><br><span class=\"line\">echo $&#123;name&#125;acwing  # 输出yxcacwing</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-3-只读变量\">3.3 只读变量</h3>\n<p>使用 <code>readonly</code> 或者 <code>declare</code> 可以将变量变为只读。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">name=yxc</span><br><span class=\"line\">readonly name</span><br><span class=\"line\">declare -r name  # 两种写法均可</span><br><span class=\"line\"></span><br><span class=\"line\">name=abc  # 会报错，因为此时name只读</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-4-删除变量\">3.4 删除变量</h3>\n<p><code>unset</code> 可以删除变量。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">name=yxc</span><br><span class=\"line\">unset name</span><br><span class=\"line\">echo $name  # 输出空行</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-5-变量类型\">3.5 变量类型</h3>\n<ul>\n<li>自定义变量（局部变量）：子进程不能访问的变量。</li>\n<li>环境变量（全局变量）：子进程可以访问的变量。</li>\n</ul>\n<p>自定义变量改成环境变量：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">acs@9e0ebfcd82d7:~$ name=AsanoSaki  # 定义变量</span><br><span class=\"line\">acs@9e0ebfcd82d7:~$ export name  # 第一种方法</span><br><span class=\"line\">acs@9e0ebfcd82d7:~$ declare -x name  # 第二种方法</span><br></pre></td></tr></table></figure>\n<p>环境变量改为自定义变量：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">acs@9e0ebfcd82d7:~$ export name=AsanoSaki  # 定义环境变量</span><br><span class=\"line\">acs@9e0ebfcd82d7:~$ declare +x name  # 改为自定义变量</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-6-字符串\">3.6 字符串</h3>\n<p>字符串可以用单引号，也可以用双引号，也可以不用引号，不用引号与双引号是一样的。</p>\n<p>单引号与双引号的区别：</p>\n<ul>\n<li>单引号中的内容会原样输出，不会执行、不会取变量。</li>\n<li>双引号中的内容可以执行、可以取变量。</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">name=yxc  # 不用引号</span><br><span class=\"line\">echo &#x27;hello, $name \\&quot;hh\\&quot;&#x27;  # 单引号字符串，输出 hello, $name \\&quot;hh\\&quot;</span><br><span class=\"line\">echo &quot;hello, $name \\&quot;hh\\&quot;&quot;  # 双引号字符串，输出 hello, yxc &quot;hh&quot;</span><br></pre></td></tr></table></figure>\n<p>获取字符串长度：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">name=&quot;yxc&quot;</span><br><span class=\"line\">echo $&#123;#name&#125;  # 输出3</span><br></pre></td></tr></table></figure>\n<p>提取子串：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">name=&quot;hello, yxc&quot;</span><br><span class=\"line\">echo $&#123;name:0:5&#125;  # 提取从0开始的5个字符</span><br></pre></td></tr></table></figure>\n<h2 id=\"4-默认变量\">4. 默认变量</h2>\n<h3 id=\"4-1-文件参数变量\">4.1 文件参数变量</h3>\n<p>在执行 Shell 脚本时，可以向脚本传递参数。<code>$1</code> 是第一个参数，<code>$2</code> 是第二个参数，以此类推。特殊的，<code>$0</code> 是文件名（包含路径）。例如：</p>\n<p>创建文件 <code>test.sh</code>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">! /bin/bash</span></span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;文件名：&quot;$0</span><br><span class=\"line\">echo &quot;第一个参数：&quot;$1</span><br><span class=\"line\">echo &quot;第二个参数：&quot;$2</span><br><span class=\"line\">echo &quot;第三个参数：&quot;$3</span><br><span class=\"line\">echo &quot;第四个参数：&quot;$4</span><br></pre></td></tr></table></figure>\n<p>然后执行该脚本：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">acs@9e0ebfcd82d7:~$ chmod +x test.sh </span><br><span class=\"line\">acs@9e0ebfcd82d7:~$ ./test.sh 1 2 3 4</span><br><span class=\"line\"></span><br><span class=\"line\">文件名：./test.sh</span><br><span class=\"line\">第一个参数：1</span><br><span class=\"line\">第二个参数：2</span><br><span class=\"line\">第三个参数：3</span><br><span class=\"line\">第四个参数：4</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-2-其它参数相关变量\">4.2 其它参数相关变量</h3>\n<ul>\n<li><code>$#</code>：代表文件传入的参数个数，如上例中值为4。</li>\n<li><code>$*</code>：由所有参数构成的用空格隔开的字符串，如上例中值为 <code>&quot;$1 $2 $3 $4&quot;</code>。</li>\n<li><code>$@</code>：每个参数分别用双引号括起来的字符串，如上例中值为 <code>&quot;$1&quot; &quot;$2&quot; &quot;$3&quot; &quot;$4&quot;</code>。</li>\n<li><code>$$</code>：脚本当前运行的进程 ID。</li>\n<li><code>$?</code>：上一条命令的退出状态（注意不是 <code>stdout</code>，而是 <code>exit code</code>）。0表示正常退出，其他值表示错误。</li>\n<li><code>$(command)</code>：返回 <code>command</code> 这条命令的 <code>stdout</code>（可嵌套）</li>\n<li><strong>`command`</strong>：返回 <code>command</code> 这条命令的 <code>stdout</code>（不可嵌套）。注意是 <code>~</code> 下面的那个点号。</li>\n</ul>\n<h2 id=\"5-数组\">5. 数组</h2>\n<p>数组中可以存放多个<strong>不同类型</strong>的值，只支持一维数组，初始化时不需要指明数组大小，数组下标从0开始。</p>\n<h3 id=\"5-1-数组定义\">5.1 数组定义</h3>\n<p>数组用小括号表示，元素之间用空格隔开。例如：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">array=(1 abc &quot;def&quot; AsanoSaki)</span><br></pre></td></tr></table></figure>\n<p>也可以直接定义数组中某个元素的值：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">array[0]=1</span><br><span class=\"line\">array[1]=abc</span><br><span class=\"line\">array[2]=&quot;def&quot;</span><br><span class=\"line\">array[3]=yxc</span><br></pre></td></tr></table></figure>\n<h3 id=\"5-2-读取数组中元素的值\">5.2 读取数组中元素的值</h3>\n<p>格式：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$</span><span class=\"language-bash\">&#123;array[index]&#125;</span></span><br></pre></td></tr></table></figure>\n<p>例如：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">array=(1 abc &quot;def&quot; AsanoSaki)</span><br><span class=\"line\">echo $&#123;array[0]&#125;</span><br><span class=\"line\">echo $&#123;array[1]&#125;</span><br><span class=\"line\">echo $&#123;array[2]&#125;</span><br><span class=\"line\">echo $&#123;array[3]&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"5-3-读取整个数组\">5.3 读取整个数组</h3>\n<p>格式：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$</span><span class=\"language-bash\">&#123;array[@]&#125;  <span class=\"comment\"># 第一种写法</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">$</span><span class=\"language-bash\">&#123;array[*]&#125;  <span class=\"comment\"># 第二种写法</span></span></span><br></pre></td></tr></table></figure>\n<p>例如：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">array=(1 abc &quot;def&quot; AsanoSaki)</span><br><span class=\"line\"></span><br><span class=\"line\">echo $&#123;array[@]&#125;  # 第一种写法</span><br><span class=\"line\">echo $&#123;array[*]&#125;  # 第二种写法</span><br></pre></td></tr></table></figure>\n<h3 id=\"5-4-数组长度\">5.4 数组长度</h3>\n<p>类似于字符串：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$</span><span class=\"language-bash\">&#123;<span class=\"comment\">#array[@]&#125;  # 第一种写法</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">$</span><span class=\"language-bash\">&#123;<span class=\"comment\">#array[*]&#125;  # 第二种写法</span></span></span><br></pre></td></tr></table></figure>\n<p>例如：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">array=(1 abc &quot;def&quot; AsanoSaki)</span><br><span class=\"line\"></span><br><span class=\"line\">echo $&#123;#array[@]&#125;  # 第一种写法</span><br><span class=\"line\">echo $&#123;#array[*]&#125;  # 第二种写法</span><br></pre></td></tr></table></figure>\n<h2 id=\"6-expr命令\">6. expr命令</h2>\n<p><code>expr</code> 命令用于求表达式的值，格式为：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">expr 表达式</span><br></pre></td></tr></table></figure>\n<p>表达式说明：</p>\n<ul>\n<li>用空格隔开每一项。</li>\n<li>用反斜杠放在 Shell 特定的字符前面（发现表达式运行错误时，可以试试转义）。</li>\n<li>对包含空格和其他特殊字符的字符串要用引号括起来。</li>\n<li><code>expr</code> 会在 <code>stdout</code> 中输出结果。如果为逻辑关系表达式，则若结果为真，<code>stdout</code> 为1，否则为0。</li>\n<li><code>expr</code> 的 <code>exit code</code>：如果为逻辑关系表达式，则若结果为真，<code>exit code</code> 为0，否则为1。</li>\n</ul>\n<h3 id=\"6-1-字符串表达式\">6.1 字符串表达式</h3>\n<ul>\n<li><code>length STRING</code>：返回 <code>STRING</code> 的长度。</li>\n<li><code>index STRING CHARSET</code>：<code>CHARSET</code> 中任意单个字符在 <code>STRING</code> 中最前面的字符位置，下标从1开始。如果在 <code>STRING</code> 中完全不存在 <code>CHARSET</code> 中的字符，则返回0。</li>\n<li><code>substr STRING POSITION LENGTH</code>：返回 <code>STRING</code> 字符串中从 <code>POSITION</code> 开始，长度最大为 <code>LENGTH</code> 的子串。如果 <code>POSITION</code> 或 <code>LENGTH</code> 为负数、0或非数值，则返回空字符串。</li>\n</ul>\n<p>示例：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">str=&quot;Hello World!&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">echo `expr length &quot;$str&quot;`  # ``不是单引号，表示执行该命令，输出12</span><br><span class=\"line\">echo `expr index &quot;$str&quot; aWd`  # 输出7，下标从1开始</span><br><span class=\"line\">echo `expr substr &quot;$str&quot; 2 3`  # 输出 ell，下标从1开始</span><br></pre></td></tr></table></figure>\n<h3 id=\"6-2-整数表达式\">6.2 整数表达式</h3>\n<p><code>expr</code> 支持普通的算术操作，算术表达式优先级低于字符串表达式，高于逻辑关系表达式。</p>\n<ul>\n<li><code>+ -</code>：加减运算。两端参数会转换为整数，如果转换失败则报错。</li>\n<li><code>* / %</code>：乘、除与取模运算。两端参数会转换为整数，如果转换失败则报错。注意 <code>*</code> 需要转义。</li>\n<li><code>()</code>：可以改变优先级，但需要<strong>用反斜杠转义</strong>。</li>\n</ul>\n<p>示例：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=3</span><br><span class=\"line\">b=4</span><br><span class=\"line\"></span><br><span class=\"line\">echo `expr $a + $b`  # 输出7</span><br><span class=\"line\">echo `expr $a - $b`  # 输出-1</span><br><span class=\"line\">echo `expr $a \\* $b`  # 输出12，*需要转义</span><br><span class=\"line\">echo `expr $a / $b`  # 输出0，整除</span><br><span class=\"line\">echo `expr $a % $b` # 输出3</span><br><span class=\"line\">echo `expr \\( $a + 1 \\) \\* \\( $b + 1 \\)`  # 输出20，值为(a + 1) * (b + 1)</span><br></pre></td></tr></table></figure>\n<h3 id=\"6-3-逻辑关系表达式\">6.3 逻辑关系表达式</h3>\n<ul>\n<li><code>|</code>：如果第一个参数非空且非0，则返回第一个参数的值，否则返回第二个参数的值，但要求第二个参数的值也是非空或非0，否则返回0。如果第一个参数是非空或非0时，不会计算第二个参数。</li>\n<li><code>&amp;</code>：如果两个参数都非空且非0，则返回第一个参数，否则返回0。如果第一个参为0或为空，则不会计算第二个参数。</li>\n<li><code>&lt; &lt;= = == != &gt;= &gt;</code>：比较两端的参数，如果为 <code>true</code>，则返回1，否则返回0。<code>==</code> 是 <code>=</code> 的同义词。<code>expr</code> 首先尝试将两端参数转换为整数，并做算术比较，如果转换失败，则按字符集排序规则做字符比较。</li>\n<li><code>()</code>：可以改变优先级，但需要<strong>用反斜杠转义</strong>。</li>\n</ul>\n<p>示例：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=3</span><br><span class=\"line\">b=4</span><br><span class=\"line\"></span><br><span class=\"line\">echo `expr $a \\&gt; $b`  # 输出0，&gt;需要转义</span><br><span class=\"line\">echo `expr $a &#x27;&lt;&#x27; $b`  # 输出1，也可以将特殊字符用引号引起来</span><br><span class=\"line\">echo `expr $a &#x27;&gt;=&#x27; $b`  # 输出0</span><br><span class=\"line\">echo `expr $a \\&lt;\\= $b`  # 输出1</span><br><span class=\"line\"></span><br><span class=\"line\">c=0</span><br><span class=\"line\">d=5</span><br><span class=\"line\"></span><br><span class=\"line\">echo `expr $c \\&amp; $d`  # 输出0</span><br><span class=\"line\">echo `expr $a \\&amp; $b`  # 输出3</span><br><span class=\"line\">echo `expr $c \\| $d`  # 输出5</span><br><span class=\"line\">echo `expr $a \\| $b`  # 输出3</span><br></pre></td></tr></table></figure>\n<h2 id=\"7-read命令\">7. read命令</h2>\n<p><code>read</code> 命令用于从标准输入中读取单行数据。当读到文件结束符时，<code>exit code</code> 为1，否则为0。</p>\n<p>参数说明：</p>\n<ul>\n<li><code>-p</code>：后面可以接提示信息。</li>\n<li><code>-t</code>：后面跟秒数，定义输入字符的等待时间，超过等待时间后会自动忽略此命令。</li>\n</ul>\n<p>实例：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">acs@9e0ebfcd82d7:~$ read name  # 读入name的值</span><br><span class=\"line\">GitHub AsanoSaki  # 标准输入</span><br><span class=\"line\">acs@9e0ebfcd82d7:~$ echo $name  # 输出name的值</span><br><span class=\"line\">GitHub AsanoSaki  #标准输出</span><br><span class=\"line\">acs@9e0ebfcd82d7:~$ read -p &quot;Please input your name: &quot; -t 30 name  # 读入name的值，等待时间30秒</span><br><span class=\"line\">Please input your name: GitHub AsanoSaki  # 标准输入</span><br><span class=\"line\">acs@9e0ebfcd82d7:~$ echo $name  # 输出name的值</span><br><span class=\"line\">GitHub AsanoSaki  # 标准输出</span><br></pre></td></tr></table></figure>\n<h2 id=\"8-echo命令\">8. echo命令</h2>\n<p><code>echo</code> 用于输出字符串。命令格式：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo STRING</span><br></pre></td></tr></table></figure>\n<p>（1）显示普通字符串</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo &quot;Hello AC Terminal&quot;</span><br><span class=\"line\">echo Hello AC Terminal  # 引号可以省略</span><br></pre></td></tr></table></figure>\n<p>（2）显示转义字符</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo &quot;\\&quot;Hello AC Terminal\\&quot;&quot;  # 注意只能使用双引号，如果使用单引号，则不转义</span><br><span class=\"line\">echo \\&quot;Hello AC Terminal\\&quot;  # 也可以省略双引号</span><br></pre></td></tr></table></figure>\n<p>（3）显示变量</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">name=AsanoSaki</span><br><span class=\"line\">echo &quot;My name is $name&quot;  # 输出 My name is AsanoSaki</span><br></pre></td></tr></table></figure>\n<p>（4）显示换行</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo -e &quot;Hi\\n&quot;  # -e 开启转义</span><br><span class=\"line\">echo &quot;AsanoSaki&quot;</span><br></pre></td></tr></table></figure>\n<p>输出结果：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Hi</span><br><span class=\"line\"></span><br><span class=\"line\">AsanoSaki</span><br></pre></td></tr></table></figure>\n<p>（5）显示不换行</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo -e &quot;Hi \\c&quot; # -e 开启转义 \\c 不换行</span><br><span class=\"line\">echo &quot;AsanoSaki&quot;</span><br></pre></td></tr></table></figure>\n<p>输出结果：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Hi AsanoSaki</span><br></pre></td></tr></table></figure>\n<p>（6）显示结果定向至文件</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo &quot;Hello World&quot; &gt; output.txt  # 将内容以覆盖的方式输出到output.txt中</span><br></pre></td></tr></table></figure>\n<p>（7）原样输出字符串，不进行转义或取变量（用单引号）</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">name=acwing</span><br><span class=\"line\">echo &#x27;$name\\&quot;&#x27;</span><br></pre></td></tr></table></figure>\n<p>输出结果：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$</span><span class=\"language-bash\">name\\&quot;</span></span><br></pre></td></tr></table></figure>\n<p>（8）显示命令的执行结果</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo `date`</span><br></pre></td></tr></table></figure>\n<p>输出结果：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Wed Sep 1 11:45:33 CST 2021</span><br></pre></td></tr></table></figure>\n<h2 id=\"9-printf命令\">9. printf命令</h2>\n<p><code>printf</code> 命令用于格式化输出，类似于 C/C++ 中的 <code>printf</code> 函数。默认不会在字符串末尾添加换行符。</p>\n<p>命令格式：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">printf format-string [arguments...]</span><br></pre></td></tr></table></figure>\n<p>脚本内容：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">printf &quot;%10d.\\n&quot; 123  # 占10位，右对齐</span><br><span class=\"line\">printf &quot;%-10.2f.\\n&quot; 123.123321  # 占10位，保留2位小数，左对齐</span><br><span class=\"line\">printf &quot;My name is %s\\n&quot; &quot;AsanoSaki&quot;  # 格式化输出字符串</span><br><span class=\"line\">printf &quot;%d * %d = %d\\n&quot; 2 3 `expr 2 \\* 3`  # 表达式的值作为参数</span><br></pre></td></tr></table></figure>\n<p>输出结果：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">       123.</span><br><span class=\"line\">123.12    .</span><br><span class=\"line\">My name is AsanoSaki</span><br><span class=\"line\">2 * 3 = 6</span><br></pre></td></tr></table></figure>\n<h2 id=\"10-test命令与判断符号\">10. test命令与判断符号[]</h2>\n<h3 id=\"10-1-逻辑运算符\">10.1 逻辑运算符</h3>\n<ul>\n<li><code>&amp;&amp;</code> 表示与，<code>||</code> 表示或。</li>\n<li>二者具有短路原则：\n<ul>\n<li><code>expr1 &amp;&amp; expr2</code>：当 <code>expr1</code> 为假时，直接忽略 <code>expr2</code>。</li>\n<li><code>expr1 || expr2</code>：当 <code>expr1</code> 为真时，直接忽略 <code>expr2</code>。</li>\n</ul>\n</li>\n<li>表达式的 <code>exit code</code> 为0，表示真；为非零，表示假（与 C/C++ 中的定义相反）。</li>\n</ul>\n<h3 id=\"10-2-test命令\">10.2 test命令</h3>\n<p>在命令行中输入 <code>man test</code>，可以查看 <code>test</code> 命令的用法。</p>\n<p><code>test</code> 命令用于判断文件类型，以及对变量做比较。</p>\n<p><code>test</code> 命令用 <code>exit code</code> 返回结果，而不是使用 <code>stdout</code>。0表示真，非0表示假。</p>\n<p>例如：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">test 2 -lt 3  # 为真，返回值为0</span><br><span class=\"line\">echo $?  # 输出上个命令的返回值，输出0</span><br><span class=\"line\">acs@9e0ebfcd82d7:~$ ls  # 列出当前目录下的所有文件</span><br><span class=\"line\">homework  output.txt  test.sh  tmp</span><br><span class=\"line\">acs@9e0ebfcd82d7:~$ test -e test.sh &amp;&amp; echo &quot;exist&quot; || echo &quot;Not exist&quot;</span><br><span class=\"line\">exist  # test.sh 文件存在</span><br><span class=\"line\">acs@9e0ebfcd82d7:~$ test -e test2.sh &amp;&amp; echo &quot;exist&quot; || echo &quot;Not exist&quot;</span><br><span class=\"line\">Not exist  # testh2.sh 文件不存在</span><br></pre></td></tr></table></figure>\n<p>（1）文件<strong>类型</strong>判断</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">test -e filename  # 判断文件是否存在</span><br></pre></td></tr></table></figure>\n<p>其它参数如下：</p>\n<ul>\n<li><code>-e</code>：文件是否存在。</li>\n<li><code>-f</code>：是否为文件。</li>\n<li><code>-d</code>：是否为目录。</li>\n</ul>\n<p>（2）文件<strong>权限</strong>判断</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">test -r filename  # 判断文件是否可读</span><br></pre></td></tr></table></figure>\n<p>其它参数如下：</p>\n<ul>\n<li><code>-r</code>：文件是否可读。</li>\n<li><code>-w</code>：文件是否可写。</li>\n<li><code>-x</code>：文件是否可执行。</li>\n<li><code>-s</code>：是否为非空文件。</li>\n</ul>\n<p>（3）整数间比较</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">test $a -eq $b  # a是否等于b</span><br></pre></td></tr></table></figure>\n<p>其它参数如下：</p>\n<ul>\n<li><code>-eq</code>：<code>a</code> 是否等于 <code>b</code>。</li>\n<li><code>-ne</code>：<code>a</code> 是否不等于 <code>b</code>。</li>\n<li><code>-gt</code>：<code>a</code> 是否大于 <code>b</code>。</li>\n<li><code>-lt</code>：<code>a</code> 是否小于 <code>b</code>。</li>\n<li><code>-ge</code>：<code>a</code> 是否大于等于 <code>b</code>。</li>\n<li><code>-le</code>：<code>a</code> 是否小于等于 <code>b</code>。</li>\n</ul>\n<p>（4）字符串比较</p>\n<ul>\n<li><code>test -z STRING</code>：判断 <code>STRING</code> 是否为空，如果为空，则返回 <code>true</code>。</li>\n<li><code>test -n STRING</code>：判断 <code>STRING</code> 是否非空，如果非空，则返回 <code>true</code>（<code>-n</code> 可以省略）。</li>\n<li><code>test str1 == str2</code>：判断 <code>str1</code> 是否等于 <code>str2</code>。</li>\n<li><code>test str1 != str2</code>：判断 <code>str1</code> 是否不等于 <code>str2</code>。</li>\n</ul>\n<p>（5）多重条件判定</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">test -r filename -a -x filename</span><br></pre></td></tr></table></figure>\n<p>其它参数如下：</p>\n<ul>\n<li><code>-a</code>：两条件是否同时成立。</li>\n<li><code>-o</code>：两条件是否至少一个成立。</li>\n<li><code>!</code>：取反，如 <code>test ! -x file</code>，当 <code>file</code> 不可执行时，返回 <code>true</code>。</li>\n</ul>\n<h3 id=\"10-3-判断符号\">10.3 判断符号[]</h3>\n<p><code>[]</code> 与 <code>test</code> 用法几乎一模一样，更常用于 <code>if</code> 语句中。另外 <code>[[]]</code> 是 <code>[]</code> 的加强版，支持的特性更多。</p>\n<p>例如：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[ 2 -lt 3 ]  # 为真，返回值为0</span><br><span class=\"line\">echo $?  # 输出上个命令的返回值，输出0</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">acs@9e0ebfcd82d7:~$ ls  # 列出当前目录下的所有文件</span><br><span class=\"line\">homework  output.txt  test.sh  tmp</span><br><span class=\"line\">acs@9e0ebfcd82d7:~$ [ -e test.sh ] &amp;&amp; echo &quot;exist&quot; || echo &quot;Not exist&quot;</span><br><span class=\"line\">exist  # test.sh 文件存在</span><br><span class=\"line\">acs@9e0ebfcd82d7:~$ [ -e test2.sh ] &amp;&amp; echo &quot;exist&quot; || echo &quot;Not exist&quot;</span><br><span class=\"line\">Not exist  # testh2.sh 文件不存在</span><br></pre></td></tr></table></figure>\n<p>注意：</p>\n<ul>\n<li><code>[]</code> 内的每一项都要用空格隔开。</li>\n<li><code>[]</code> 内的变量，最好用双引号括起来。</li>\n<li><code>[]</code> 内的常数，最好用单或双引号括起来。</li>\n</ul>\n<p>例如：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">name=&quot;acwing AsanoSaki&quot;</span><br><span class=\"line\">[ $name == &quot;acwing AsanoSaki&quot; ]  # 错误，等价于 [ acwing AsanoSaki == &quot;acwing AsanoSaki&quot; ]</span><br><span class=\"line\">[ &quot;$name&quot; == &quot;acwing AsanoSaki&quot; ]  # 正确</span><br></pre></td></tr></table></figure>\n<h2 id=\"11-判断语句\">11. 判断语句</h2>\n<h3 id=\"11-1-if…then形式\">11.1 if…then形式</h3>\n<p>类似于 C/C++ 中的 <code>if-else</code> 语句。</p>\n<p>（1）单层 <code>if</code>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">if condition</span><br><span class=\"line\">then</span><br><span class=\"line\">    语句1</span><br><span class=\"line\">    语句2</span><br><span class=\"line\">    ...</span><br><span class=\"line\">fi</span><br></pre></td></tr></table></figure>\n<p>示例：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=3</span><br><span class=\"line\">b=4</span><br><span class=\"line\"></span><br><span class=\"line\">if [ &quot;$a&quot; -lt &quot;$b&quot; ] &amp;&amp; [ &quot;$a&quot; -gt 2 ]</span><br><span class=\"line\">then</span><br><span class=\"line\">    echo $&#123;a&#125;在范围内</span><br><span class=\"line\">fi</span><br></pre></td></tr></table></figure>\n<p>输出结果：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">3在范围内</span><br></pre></td></tr></table></figure>\n<p>（2）单层 <code>if-else</code>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">if condition</span><br><span class=\"line\">then</span><br><span class=\"line\">    语句1</span><br><span class=\"line\">    语句2</span><br><span class=\"line\">    ...</span><br><span class=\"line\">else</span><br><span class=\"line\">    语句1</span><br><span class=\"line\">    语句2</span><br><span class=\"line\">    ...</span><br><span class=\"line\">fi</span><br></pre></td></tr></table></figure>\n<p>示例：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=3</span><br><span class=\"line\">b=4</span><br><span class=\"line\"></span><br><span class=\"line\">if ! [ &quot;$a&quot; -lt &quot;$b&quot; ]</span><br><span class=\"line\">then</span><br><span class=\"line\">    echo $&#123;a&#125;不小于$&#123;b&#125;</span><br><span class=\"line\">else</span><br><span class=\"line\">    echo $&#123;a&#125;小于$&#123;b&#125;</span><br><span class=\"line\">fi</span><br></pre></td></tr></table></figure>\n<p>输出结果：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">3小于4</span><br></pre></td></tr></table></figure>\n<p>（3）多层 <code>if-elif-elif-else</code>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">if condition</span><br><span class=\"line\">then</span><br><span class=\"line\">    语句1</span><br><span class=\"line\">    语句2</span><br><span class=\"line\">    ...</span><br><span class=\"line\">elif condition</span><br><span class=\"line\">then</span><br><span class=\"line\">    语句1</span><br><span class=\"line\">    语句2</span><br><span class=\"line\">    ...</span><br><span class=\"line\">elif condition</span><br><span class=\"line\">then</span><br><span class=\"line\">    语句1</span><br><span class=\"line\">    语句2</span><br><span class=\"line\">else</span><br><span class=\"line\">    语句1</span><br><span class=\"line\">    语句2</span><br><span class=\"line\">    ...</span><br><span class=\"line\">fi</span><br></pre></td></tr></table></figure>\n<p>示例：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=4</span><br><span class=\"line\"></span><br><span class=\"line\">if [ $a -eq 1 ]</span><br><span class=\"line\">then</span><br><span class=\"line\">    echo $&#123;a&#125;等于1</span><br><span class=\"line\">elif [ $a -eq 2 ]</span><br><span class=\"line\">then</span><br><span class=\"line\">    echo $&#123;a&#125;等于2</span><br><span class=\"line\">elif [ $a -eq 3 ]</span><br><span class=\"line\">then</span><br><span class=\"line\">    echo $&#123;a&#125;等于3</span><br><span class=\"line\">else</span><br><span class=\"line\">    echo 其他</span><br><span class=\"line\">fi</span><br></pre></td></tr></table></figure>\n<p>输出结果：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">其他</span><br></pre></td></tr></table></figure>\n<h3 id=\"11-2-case…esac形式\">11.2 case…esac形式</h3>\n<p>类似于 C/C++ 中的 <code>switch</code> 语句。</p>\n<p>命令格式：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">case $变量名称 in</span><br><span class=\"line\">    值1)</span><br><span class=\"line\">        语句1</span><br><span class=\"line\">        语句2</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        ;;  # 类似于C/C++中的break</span><br><span class=\"line\">    值2)</span><br><span class=\"line\">        语句1</span><br><span class=\"line\">        语句2</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        ;;</span><br><span class=\"line\">    *)  # 类似于C/C++中的default</span><br><span class=\"line\">        语句1</span><br><span class=\"line\">        语句2</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        ;;</span><br><span class=\"line\">esac</span><br></pre></td></tr></table></figure>\n<p>示例：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=4</span><br><span class=\"line\"></span><br><span class=\"line\">case $a in</span><br><span class=\"line\">    1)</span><br><span class=\"line\">        echo $&#123;a&#125;等于1</span><br><span class=\"line\">        ;;</span><br><span class=\"line\">    2)</span><br><span class=\"line\">        echo $&#123;a&#125;等于2</span><br><span class=\"line\">        ;;</span><br><span class=\"line\">    3)</span><br><span class=\"line\">        echo $&#123;a&#125;等于3</span><br><span class=\"line\">        ;;</span><br><span class=\"line\">    *)</span><br><span class=\"line\">        echo 其他</span><br><span class=\"line\">        ;;</span><br><span class=\"line\">esac</span><br></pre></td></tr></table></figure>\n<p>输出结果：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">其他</span><br></pre></td></tr></table></figure>\n<h2 id=\"12-循环语句\">12. 循环语句</h2>\n<h3 id=\"12-1-for…in…do…done\">12.1 for…in…do…done</h3>\n<p>命令格式：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">for var in val1 val2 val3</span><br><span class=\"line\">do</span><br><span class=\"line\">    语句1</span><br><span class=\"line\">    语句2</span><br><span class=\"line\">    ...</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n<p>示例一，输出 <code>a 2 cc</code>，每个元素一行：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">for i in a 2 cc</span><br><span class=\"line\">do</span><br><span class=\"line\">    echo $i</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n<p>示例二，输出当前路径下的所有文件名，每个文件名一行：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">for file in `ls`</span><br><span class=\"line\">do</span><br><span class=\"line\">    echo $file</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n<p>示例三，输出1到10：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">for i in $(seq 1 10)</span><br><span class=\"line\">do</span><br><span class=\"line\">    echo $i</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n<p>示例四，使用 <code>&#123;1..10&#125;</code> 或者 <code>&#123;a..z&#125;</code>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">for i in &#123;a..z&#125;</span><br><span class=\"line\">do</span><br><span class=\"line\">    echo $i</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n<h3 id=\"12-2-for-…-…-…-do…done\">12.2 for ((…;…;…)) do…done</h3>\n<p>命令格式：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">for ((expression; condition; expression))</span><br><span class=\"line\">do</span><br><span class=\"line\">    语句1</span><br><span class=\"line\">    语句2</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n<p>示例，输出1到10，每个数占一行：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">for ((i=1; i&lt;=10; i++))</span><br><span class=\"line\">do</span><br><span class=\"line\">    echo $i</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n<h3 id=\"12-3-while…do…done\">12.3 while…do…done</h3>\n<p><code>while...do...done</code>：当条件为假时结束。</p>\n<p>命令格式：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">while condition</span><br><span class=\"line\">do</span><br><span class=\"line\">    语句1</span><br><span class=\"line\">    语句2</span><br><span class=\"line\">    ...</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n<p>示例，文件结束符为 <code>Ctrl+d</code>，输入文件结束符后 <code>read</code> 指令返回 <code>false</code>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">while read name</span><br><span class=\"line\">do</span><br><span class=\"line\">    echo $name</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n<h3 id=\"12-4-until…do…done\">12.4 until…do…done</h3>\n<p><code>until...do...done</code>：当条件为真时结束。</p>\n<p>命令格式：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">until condition</span><br><span class=\"line\">do</span><br><span class=\"line\">    语句1</span><br><span class=\"line\">    语句2</span><br><span class=\"line\">    ...</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n<p>示例，当用户输入 <code>yes</code> 或者 <code>YES</code> 时结束，否则一直等待读入：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">until [ &quot;$&#123;word&#125;&quot; == &quot;yes&quot; ] || [ &quot;$&#123;word&#125;&quot; == &quot;YES&quot; ]</span><br><span class=\"line\">do</span><br><span class=\"line\">    read -p &quot;Please input yes/YES to stop this program: &quot; word</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n<h3 id=\"12-5-break\">12.5 break</h3>\n<p><code>break</code>：跳出当前一层循环，注意与 C/C++ 不同的是：<code>break</code> 不能跳出 <code>case</code> 语句。</p>\n<p>示例，每读入非 EOF 的字符串，会输出一遍1到7。该程序可以输入 <code>Ctrl+d</code> 文件结束符来结束，也可以直接用 <code>Ctrl+c</code> 杀掉该进程：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">while read name</span><br><span class=\"line\">do</span><br><span class=\"line\">    for ((i=1;i&lt;=10;i++))</span><br><span class=\"line\">    do</span><br><span class=\"line\">        case $i in</span><br><span class=\"line\">            8)</span><br><span class=\"line\">                break</span><br><span class=\"line\">                ;;</span><br><span class=\"line\">            *)</span><br><span class=\"line\">                echo $i</span><br><span class=\"line\">                ;;</span><br><span class=\"line\">        esac</span><br><span class=\"line\">    done</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n<h3 id=\"12-6-continue\">12.6 continue</h3>\n<p><code>continue</code>：跳出当前循环。</p>\n<p>示例，输出1到10中的所有奇数：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">for ((i=1;i&lt;=10;i++))</span><br><span class=\"line\">do</span><br><span class=\"line\">    if [ `expr $i % 2` -eq 0 ]</span><br><span class=\"line\">    then</span><br><span class=\"line\">        continue</span><br><span class=\"line\">    fi</span><br><span class=\"line\">    echo $i</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n<p>直接关闭进程的方式：</p>\n<ol>\n<li>使用 <code>top</code> 命令找到进程的 <code>PID</code>。</li>\n<li>输入 <code>kill -9 PID</code> 即可关掉此进程。</li>\n</ol>\n<h2 id=\"13-函数\">13. 函数</h2>\n<p><code>bash</code> 中的函数类似于 <code>C/C++</code> 中的函数，但 <code>return</code> 的返回值与 C/C++ 不同，返回的是 <code>exit code</code>，取值为 <code>[0, 255]</code>，0表示正常结束。</p>\n<p>如果想获取函数的输出结果，可以通过 <code>echo</code> 输出到 <code>stdout</code> 中，然后通过 <code>$(function_name)</code> 来获取 <code>stdout</code> 中的结果。</p>\n<p>函数的 <code>return</code> 值可以通过 <code>$?</code> 来获取。</p>\n<p>命令格式：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[function] func_name() &#123;  # function关键字可以省略</span><br><span class=\"line\">    语句1</span><br><span class=\"line\">    语句2</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>（1）不获取 <code>return</code> 值和 <code>stdout</code> 值</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func() &#123;</span><br><span class=\"line\">    name=AsanoSaki</span><br><span class=\"line\">    echo &quot;Hello $name&quot;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">AsanoSaki</span><br></pre></td></tr></table></figure>\n<p>输出结果：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Hello yxc</span><br></pre></td></tr></table></figure>\n<p>（2）获取 <code>return</code> 值和 <code>stdout</code> 值（不写 <code>return</code> 时，默认 <code>return 0</code>）</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func() &#123;</span><br><span class=\"line\">    name=AsanoSaki</span><br><span class=\"line\">    echo &quot;Hello $name&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">    return 123</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">output=$(func)</span><br><span class=\"line\">ret=$?</span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;output = $output&quot;</span><br><span class=\"line\">echo &quot;return = $ret&quot;</span><br></pre></td></tr></table></figure>\n<p>输出结果：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">output = Hello AsanoSaki</span><br><span class=\"line\">return = 123</span><br></pre></td></tr></table></figure>\n<p>（3）函数的输入参数</p>\n<p>在函数内，<code>$1</code> 表示第一个输入参数，<code>$2</code> 表示第二个输入参数，依此类推。</p>\n<p>注意：函数内的 <code>$0</code> 仍然是文件名，而不是函数名。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func() &#123;  # 递归计算 $1 + ($1 - 1) + ($1 - 2) + ... + 0</span><br><span class=\"line\">    word=&quot;&quot;</span><br><span class=\"line\">    while [ &quot;$&#123;word&#125;&quot; != &#x27;y&#x27; ] &amp;&amp; [ &quot;$&#123;word&#125;&quot; != &#x27;n&#x27; ]</span><br><span class=\"line\">    do</span><br><span class=\"line\">        read -p &quot;要进入func($1)函数吗？请输入y/n：&quot; word</span><br><span class=\"line\">    done</span><br><span class=\"line\"></span><br><span class=\"line\">    if [ &quot;$word&quot; == &#x27;n&#x27; ]</span><br><span class=\"line\">    then</span><br><span class=\"line\">        echo 0</span><br><span class=\"line\">        return 0</span><br><span class=\"line\">    fi  </span><br><span class=\"line\"></span><br><span class=\"line\">    if [ $1 -le 0 ] </span><br><span class=\"line\">    then</span><br><span class=\"line\">        echo 0</span><br><span class=\"line\">        return 0</span><br><span class=\"line\">    fi  </span><br><span class=\"line\"></span><br><span class=\"line\">    sum=$(func $(expr $1 - 1))</span><br><span class=\"line\">    echo $(expr $sum + $1)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">echo $(func 10)</span><br></pre></td></tr></table></figure>\n<p>输出结果：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">55</span><br></pre></td></tr></table></figure>\n<p>（4）函数内的局部变量</p>\n<p>可以在函数内定义局部变量，作用范围仅在当前函数内。可以在递归函数中定义局部变量。</p>\n<p>命令格式：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">local 变量名=变量值</span><br></pre></td></tr></table></figure>\n<p>例如：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">! /bin/bash</span></span><br><span class=\"line\"></span><br><span class=\"line\">func() &#123;</span><br><span class=\"line\">    local name=AsanoSaki</span><br><span class=\"line\">    echo $name</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func</span><br><span class=\"line\"></span><br><span class=\"line\">echo $name</span><br></pre></td></tr></table></figure>\n<p>输出结果：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AsanoSaki</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>第一行为函数内的 <code>name</code> 变量，第二行为函数外调用 <code>name</code> 变量，会发现此时该变量不存在。</p>\n<h2 id=\"14-exit命令\">14. exit命令</h2>\n<p><code>exit</code> 命令用来退出当前 Shell 进程，并返回一个退出状态；使用 <code>$?</code> 可以接收这个退出状态。</p>\n<p><code>exit</code> 命令可以接受一个整数值作为参数，代表退出状态。如果不指定，默认状态值是0。</p>\n<p><code>exit</code> 退出状态只能是一个介于0到255之间的整数，其中只有0表示成功，其它值都表示失败。</p>\n<p>示例，创建脚本 <code>test.sh</code>，内容如下：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">! /bin/bash</span></span><br><span class=\"line\"></span><br><span class=\"line\">if [ $# -ne 1 ]  # 如果传入参数个数等于1，则正常退出；否则非正常退出。</span><br><span class=\"line\">then</span><br><span class=\"line\">    echo &quot;arguments not valid&quot;</span><br><span class=\"line\">    exit 1</span><br><span class=\"line\">else</span><br><span class=\"line\">    echo &quot;arguments valid&quot;</span><br><span class=\"line\">    exit 0</span><br><span class=\"line\">fi</span><br></pre></td></tr></table></figure>\n<p>执行该脚本：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">acs@9e0ebfcd82d7:~$ chmod +x test.sh </span><br><span class=\"line\">acs@9e0ebfcd82d7:~$ ./test.sh acwing</span><br><span class=\"line\">arguments valid</span><br><span class=\"line\">acs@9e0ebfcd82d7:~$ echo $?  # 传入一个参数，则正常退出，exit code为0</span><br><span class=\"line\">0</span><br><span class=\"line\">acs@9e0ebfcd82d7:~$ ./test.sh </span><br><span class=\"line\">arguments not valid</span><br><span class=\"line\">acs@9e0ebfcd82d7:~$ echo $?  # 传入参数个数不是1，则非正常退出，exit code为1</span><br><span class=\"line\">1</span><br></pre></td></tr></table></figure>\n<h2 id=\"15-文件重定向\">15. 文件重定向</h2>\n<p>每个进程默认打开3个文件描述符：</p>\n<ul>\n<li><code>stdin</code>：标准输入，从命令行读取数据，文件描述符为0。</li>\n<li><code>stdout</code>：标准输出，向命令行输出数据，文件描述符为1。</li>\n<li><code>stderr</code>：标准错误输出，向命令行输出数据，文件描述符为2。</li>\n</ul>\n<p>可以用文件重定向将这三个文件重定向到其他文件中。</p>\n<p>重定向命令列表：</p>\n<ul>\n<li><code>command &gt; file</code>：将 <code>stdout</code> 重定向到 <code>file</code> 中。</li>\n<li><code>command &lt; file</code>：将 <code>stdin</code> 重定向到 <code>file</code> 中。</li>\n<li><code>command &gt;&gt; file</code>：将 <code>stdout</code> 以追加方式重定向到 <code>file</code> 中。</li>\n<li><code>command n&gt; file</code>：将文件描述符 <code>n</code> 重定向到 <code>file</code> 中。</li>\n<li><code>command n&gt;&gt; file</code>：将文件描述符 <code>n</code> 以追加方式重定向到 <code>file</code> 中。</li>\n</ul>\n<p>输入和输出重定向：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo -e &quot;Hello \\c&quot; &gt; output.txt  # 将stdout重定向到output.txt中</span><br><span class=\"line\">echo &quot;World&quot; &gt;&gt; output.txt  # 将字符串追加到output.txt中</span><br><span class=\"line\"></span><br><span class=\"line\">read str &lt; output.txt  # 从output.txt中读取字符串</span><br><span class=\"line\"></span><br><span class=\"line\">echo $str  # 输出结果：Hello World</span><br></pre></td></tr></table></figure>\n<p>同时重定向 <code>stdin</code> 和 <code>stdout</code>：</p>\n<p>创建 bash 脚本：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">! /bin/bash</span></span><br><span class=\"line\"></span><br><span class=\"line\">read a</span><br><span class=\"line\">read b</span><br><span class=\"line\"></span><br><span class=\"line\">echo $(expr &quot;$a&quot; + &quot;$b&quot;)</span><br></pre></td></tr></table></figure>\n<p>创建 <code>input.txt</code>，里面的内容为：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td></tr></table></figure>\n<p>执行命令：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">acs@9e0ebfcd82d7:~$ chmod +x test.sh  # 添加可执行权限</span><br><span class=\"line\">acs@9e0ebfcd82d7:~$ ./test.sh &lt; input.txt &gt; output.txt  # 从input.txt中读取内容，将输出写入output.txt中</span><br><span class=\"line\">acs@9e0ebfcd82d7:~$ cat output.txt  # 查看output.txt中的内容</span><br><span class=\"line\">7</span><br></pre></td></tr></table></figure>\n<h2 id=\"16-引入外部脚本\">16. 引入外部脚本</h2>\n<p>类似于 C/C++ 中的 <code>include</code> 操作，bash 也可以引入其他文件中的代码。</p>\n<p>语法格式：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">. filename  # 注意点和文件名之间有一个空格</span><br><span class=\"line\"></span><br><span class=\"line\">或</span><br><span class=\"line\"></span><br><span class=\"line\">source filename</span><br></pre></td></tr></table></figure>\n<p>示例，创建 <code>test1.sh</code>，内容为：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">! /bin/bash</span></span><br><span class=\"line\"></span><br><span class=\"line\">name=AsanoSaki  # 定义变量name</span><br></pre></td></tr></table></figure>\n<p>然后创建 <code>test2.sh</code>，内容为：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">! /bin/bash</span></span><br><span class=\"line\"></span><br><span class=\"line\">source test1.sh  # 或 . test1.sh</span><br><span class=\"line\"></span><br><span class=\"line\">echo My name is: $name  # 可以使用test1.sh中的变量</span><br></pre></td></tr></table></figure>\n<p>执行命令：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">acs@9e0ebfcd82d7:~$ chmod +x test2.sh </span><br><span class=\"line\">acs@9e0ebfcd82d7:~$ ./test2.sh </span><br><span class=\"line\">My name is: AsanoSaki</span><br></pre></td></tr></table></figure>\n",
            "tags": [
                "Linux"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/53725.html",
            "url": "https://asanosaki.github.io/posts/53725.html",
            "title": "Linux学习笔记-命令、Tmux与Vim",
            "date_published": "2022-03-15T10:06:00.000Z",
            "content_html": "<blockquote>\n<p>本文记录 Linux 的学习过程，内容为 Linux 常用文件管理命令、Tmux、Vim。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-常用命令\">1. 常用命令</h2>\n<p>Linux 中描述路径有两种方式（假设当前用户的目录为 <code>AsanoSaki</code>）：</p>\n<ol>\n<li>绝对路径：从根目录（即 <code>/</code>）开始描述，例如：<code>/home/AsanoSaki/main.cpp</code>。</li>\n<li>相对路径：从当前的路径开始描述，例如：<code>AsanoSaki/main.cpp</code>（当前在 <code>home</code> 中）。</li>\n</ol>\n<p>绝对路径的开头一定是 <code>/</code>，相对路径开头一定不是 <code>/</code>。</p>\n<ul>\n<li><code>.</code> 表示当前目录，<code>..</code> 表示上一目录，假如当前在 <code>AsanoSaki</code> 目录下，则路径 <code>../AsanoSaki/./../AsanoSaki</code> 表示同一路径。</li>\n<li><code>~/</code> 表示家目录，等价于 <code>/home/AsanoSaki</code>。</li>\n</ul>\n<h3 id=\"1-1-常用文件管理命令\">1.1 常用文件管理命令</h3>\n<ul>\n<li><code>ctrl+c</code>：取消命令，并且换行。如当前有一个程序正在运行且一直无法停止，则可以使用该操作将当前正在运行的程序中止。另一个作用是中断当前正在输入的这一行，直接跳到下一行重新输入。</li>\n<li><code>ctrl+u</code>：清空本行命令。</li>\n<li><code>tab</code>：可以补全命令和文件名，如果补全不了快速按两下 <code>tab</code> 键，可以显示备选选项。</li>\n<li><code>ls</code>：列出当前目录下所有文件，蓝色的是文件夹，白色的是普通文件，绿色的是可执行文件。</li>\n<li><code>pwd</code>：显示当前路径。</li>\n<li><code>cd XXX</code>：进入 <code>XXX</code> 目录下，<code>cd ..</code> 表示返回上层目录，<code>cd -</code> 表示返回上一个待过的目录。<code>cd</code> 后面既可以用相对路径也可以用绝对路径，不加目录则默认返回家目录。</li>\n<li><code>cp XXX YYY</code>：将 <code>XXX</code> 文件复制成 <code>YYY</code>，<code>XXX</code> 和 <code>YYY</code> 可以是一个路径，比如将目录 <code>a</code> 中的文件 <code>tmp.txt</code> 复制到目录 <code>b</code> 中：<code>cp a/tmp.txt b</code>。如果想顺带将复制后的文件重命名则可以写成：<code>cp a/tmp.txt b/tmp2.txt</code>。如果想把目录 <code>a</code> 整个复制到目录 <code>b</code> 下则可以写：<code>cp a b -r</code>。</li>\n<li><code>mkdir XXX</code>：创建目录 <code>XXX</code>。如在当前目录下创建文件夹 <code>a</code>：<code>mkdir a</code>。使用绝对路径在目录 <code>a</code> 下创建文件夹 <code>b</code>：<code>mkdir /home/AsanoSaki/a/b</code>。直接创建 <code>a</code> 里有 <code>b</code>，<code>b</code> 里有 <code>c</code> 的目录：<code>mkdir a/b/c -p</code>。</li>\n<li><code>rm XXX</code>：删除普通文件。<code>rm XXX -r</code>：删除文件夹。删除多个文件：<code>rm tmp1.txt tmp2.txt</code>。删除当前目录下的所有 <code>txt</code> 文件：<code>rm *.txt</code>。删除 <code>a</code> 中的所有文件：<code>rm a/*</code>。</li>\n<li><code>mv XXX YYY</code>：将 <code>XXX</code> 文件移动（剪切）到 <code>YYY</code>，和 <code>cp</code> 命令一样，<code>XXX</code> 和 <code>YYY</code> 可以是一个路径，重命名也是用这个命令。</li>\n<li><code>touch XXX</code>：创建一个文件。</li>\n<li><code>cat XXX</code>：展示文件 <code>XXX</code> 中的内容。</li>\n</ul>\n<h3 id=\"1-2-其它常用命令\">1.2 其它常用命令</h3>\n<p>（1）系统状况</p>\n<ul>\n<li><code>top</code>：查看所有进程的信息（Linux 的任务管理器）。\n<ul>\n<li>打开后，输入 <code>M</code>：按使用内存排序。</li>\n<li>打开后，输入 <code>P</code>：按使用 CPU 排序。</li>\n<li>打开后，输入 <code>q</code>：退出。</li>\n</ul>\n</li>\n<li><code>df -h</code>：查看硬盘使用情况。</li>\n<li><code>free -h</code>：查看内存使用情况。</li>\n<li><code>du -sh</code>：查看当前目录占用的硬盘空间。</li>\n<li><code>ps aux</code>：查看所有进程。</li>\n<li><code>kill -9 pid</code>：杀死编号为 <code>pid</code> 的进程。\n<ul>\n<li>传递某个具体的信号：<code>kill -s SIGTERM pid</code>。</li>\n</ul>\n</li>\n<li><code>netstat -nt</code>：查看所有网络连接。</li>\n<li><code>w</code>：列出当前登陆的用户。</li>\n<li><code>ping www.baidu.com</code>：检查是否连网。</li>\n</ul>\n<p>（2）文件权限</p>\n<ul>\n<li><code>chmod</code>：修改文件权限\n<ul>\n<li><code>chmod +x xxx</code>：给 <code>xxx</code> 添加可执行权限。</li>\n<li><code>chmod -x xxx</code>：去掉 <code>xxx</code> 的可执行权限。</li>\n<li><code>chmod 777 xxx</code>：将 <code>xxx</code> 的权限改成 <code>777</code>（三个数字按顺序分别表示 <code>Owner</code>、<code>Group</code>、<code>Other Users</code>，每个数字的二进制例如7的二进制为111，表示具有 <code>rwx</code> 权限，某一位为0表示没有该权限）。</li>\n<li><code>chmod 777 xxx -R</code>：递归修改整个文件夹的权限。</li>\n</ul>\n</li>\n</ul>\n<p>（3）文件检索</p>\n<ul>\n<li><code>find /path/to/directory/ -name '*.py'</code>：搜索某个文件路径下的所有 <code>*.py</code> 文件。</li>\n<li><code>grep xxx</code>：从 <code>stdin</code> 中读入若干行数据，如果某行中包含 <code>xxx</code>，则输出该行；否则忽略该行。</li>\n<li><code>wc</code>：统计行数、单词数、字节数。\n<ul>\n<li>既可以从 <code>stdin</code> 中直接读入内容；也可以在命令行参数中传入文件名列表。</li>\n<li><code>wc -l</code>：统计行数。</li>\n<li><code>wc -w</code>：统计单词数。</li>\n<li><code>wc -c</code>：统计字节数。</li>\n</ul>\n</li>\n<li><code>tree</code>：展示当前目录的文件结构。\n<ul>\n<li><code>tree /path/to/directory/</code>：展示某个目录的文件结构。</li>\n<li><code>tree -a</code>：展示隐藏文件。</li>\n</ul>\n</li>\n<li><code>ag xxx</code>：搜索当前目录下的所有文件，检索 <code>xxx</code> 字符串。</li>\n<li><code>cut</code>：分割一行内容。\n<ul>\n<li>从 <code>stdin</code> 中读入多行数据。</li>\n<li><code>echo $PATH | cut -d ':' -f 3,5</code>：输出 <code>PATH</code> 用 <code>:</code> 分割后第3、5列数据。</li>\n<li><code>echo $PATH | cut -d ':' -f 3-5</code>：输出 <code>PATH</code> 用 <code>:</code> 分割后第3-5列数据。</li>\n<li><code>echo $PATH | cut -c 3,5</code>：输出 <code>PATH</code> 的第3、5个字符。</li>\n<li><code>echo $PATH | cut -c 3-5</code>：输出 <code>PATH</code> 的第3-5个字符。</li>\n</ul>\n</li>\n<li><code>sort</code>：将每行内容按字典序排序。\n<ul>\n<li>可以从 <code>stdin</code> 中读取多行数据。</li>\n<li>可以从命令行参数中读取文件名列表。</li>\n</ul>\n</li>\n<li><code>xargs</code>：将 <code>stdin</code> 中的数据用空格或回车分割成命令行参数。\n<ul>\n<li><code>find . -name '*.py' | xargs cat | wc -l</code>：统计当前目录下所有 Python 文件的总行数</li>\n</ul>\n</li>\n</ul>\n<p>（4）查看文件内容</p>\n<ul>\n<li><code>more</code>：浏览文件内容。\n<ul>\n<li>回车：下一行。</li>\n<li>空格：下一页。</li>\n<li><code>b</code>：上一页。</li>\n<li><code>q</code>：退出。</li>\n</ul>\n</li>\n<li><code>less</code>：与 <code>more</code> 类似，功能更全。\n<ul>\n<li>回车：下一行。</li>\n<li><code>y</code>：上一行。</li>\n<li><code>Page Down</code>：下一页。</li>\n<li><code>Page Up</code>：上一页。</li>\n<li><code>q</code>：退出。</li>\n</ul>\n</li>\n<li><code>head -3 xxx</code>：展示 <code>xxx</code> 的前3行内容。\n<ul>\n<li>同时支持从 <code>stdin</code> 读入内容。</li>\n</ul>\n</li>\n<li><code>tail -3 xxx</code>：展示 <code>xxx</code> 末尾3行内容。\n<ul>\n<li>同时支持从 <code>stdin</code> 读入内容。</li>\n</ul>\n</li>\n</ul>\n<p>（5）用户相关</p>\n<ul>\n<li><code>history</code>：展示当前用户的历史操作。内容存放在 <code>~/.bash_history</code> 中。</li>\n</ul>\n<p>（6）工具</p>\n<ul>\n<li><code>md5sum</code>：计算 <code>md5</code> 哈希值。\n<ul>\n<li>可以从 <code>stdin</code> 读入内容。</li>\n<li>也可以在命令行参数中传入文件名列表。</li>\n</ul>\n</li>\n<li><code>time command</code>：统计 <code>command</code> 命令的执行时间。</li>\n<li><code>ipython3</code>：交互式 Python3 环境。可以当做计算器，或者批量管理文件。\n<ul>\n<li><code>! echo &quot;Hello World&quot;</code>：<code>!</code> 表示执行 <code>shell</code> 脚本。</li>\n</ul>\n</li>\n<li><code>watch -n 0.1 command</code>：每0.1秒执行一次 <code>command</code> 命令。</li>\n<li><code>tar</code>：压缩文件。\n<ul>\n<li><code>tar -zcvf xxx.tar.gz /path/to/file/*</code>：压缩。</li>\n<li><code>tar -zxvf xxx.tar.gz</code>：解压缩。</li>\n</ul>\n</li>\n<li><code>diff xxx yyy</code>：查找文件 <code>xxx</code> 与 <code>yyy</code> 的不同点。</li>\n</ul>\n<p>（7）安装软件</p>\n<ul>\n<li><code>sudo command</code>：以 <code>root</code> 身份执行 <code>command</code> 命令。</li>\n<li><code>apt-get install xxx</code>：安装软件。</li>\n<li><code>pip install xxx --user --upgrade</code>：安装 Python 包。</li>\n</ul>\n<h2 id=\"2-Tmux与Vim\">2. Tmux与Vim</h2>\n<h3 id=\"2-1-Tmux\">2.1 Tmux</h3>\n<p>（1）功能</p>\n<ul>\n<li>分屏。</li>\n<li>允许断开 Terminal 连接后，继续运行进程。</li>\n</ul>\n<p>（2）结构</p>\n<p>一个 Tmux 可以包含多个 <code>session</code>，一个 <code>session</code> 可以包含多个 <code>window</code>，一个 <code>window</code> 可以包含多个 <code>pane</code>。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">实例：</span><br><span class=\"line\">    tmux:</span><br><span class=\"line\">        session 0:</span><br><span class=\"line\">            window 0:</span><br><span class=\"line\">                pane 0</span><br><span class=\"line\">                pane 1</span><br><span class=\"line\">                pane 2</span><br><span class=\"line\">                ...</span><br><span class=\"line\">            window 1</span><br><span class=\"line\">            window 2</span><br><span class=\"line\">            ...</span><br><span class=\"line\">        session 1</span><br><span class=\"line\">        session 2</span><br><span class=\"line\">        ...</span><br></pre></td></tr></table></figure>\n<p>（3）常用操作</p>\n<ul>\n<li><code>tmux [-u]</code>：新建一个 <code>session</code>，其中包含一个 <code>window</code>，<code>window</code> 中包含一个 <code>pane</code>，<code>pane</code> 里打开了一个 shell 对话框，<code>-u</code> 参数可以在 Tmux 中显示中文内容。</li>\n<li>按下 <code>Ctrl + a</code> 后手指松开，然后按 <code>%</code>：将当前 <code>pane</code> 左右平分成两个 <code>pane</code>。</li>\n<li>按下 <code>Ctrl + a</code> 后手指松开，然后按 <code>&quot;</code>（注意是双引号）：将当前 <code>pane</code> 上下平分成两个 <code>pane</code>。</li>\n<li><code>Ctrl + d</code>：关闭当前 <code>pane</code>；如果当前 <code>window</code> 的所有 <code>pane</code> 均已关闭，则自动关闭 <code>window</code>；如果当前 <code>session</code> 的所有 <code>window</code> 均已关闭，则自动关闭 <code>session</code>。</li>\n<li>鼠标点击可以选则 <code>pane</code>。</li>\n<li>按下 <code>Ctrl + a</code> 后手指松开，然后按方向键：选择相邻的 <code>pane</code>。</li>\n<li>鼠标拖动 <code>pane</code> 之间的分割线，可以调整分割线的位置。</li>\n<li>按住 <code>Ctrl + a</code> 的同时按方向键，可以调整 <code>pane</code> 之间分割线的位置。</li>\n<li>按下 <code>Ctrl + a</code> 后手指松开，然后按 <code>z</code>：将当前 <code>pane</code> 全屏/取消全屏。</li>\n<li>按下 <code>Ctrl + a</code> 后手指松开，然后按 <code>d</code>：挂起当前 <code>session</code>。</li>\n<li><code>tmux a</code>：打开之前挂起的 <code>session</code>。</li>\n<li>按下 <code>Ctrl + a</code> 后手指松开，然后按 <code>s</code>：选择其它 <code>session</code>：\n<ul>\n<li>方向键上：选择上一项；</li>\n<li>方向键下：选择下一项；</li>\n<li>方向键右：展开当前项；</li>\n<li>方向键左：闭合当前项。</li>\n</ul>\n</li>\n<li>按下 <code>Ctrl + a</code> 后手指松开，然后按 <code>c</code>：在当前 <code>session</code> 中创建一个新的 <code>window</code>。</li>\n<li>按下 <code>Ctrl + a</code> 后手指松开，然后按 <code>w</code>：选择其他 <code>window</code>，操作方法与选择 <code>session</code> 完全相同。</li>\n<li>按下 <code>Ctrl + a</code> 后手指松开，然后按 <code>PageUp/PageDown</code>：翻阅当前 <code>pane</code> 内的内容。注意第一次唤醒该操作时只能按 <code>PageUp</code>。</li>\n<li>鼠标滚轮：翻阅当前 <code>pane</code> 内的内容。</li>\n<li>在 <code>tmux</code> 中选中文本时，需要按住 <code>shift</code> 键。（仅支持 Windows 和 Linux，不支持 Mac，不过该操作并不是必须的，因此影响不大）</li>\n<li><code>tmux</code> 中复制/粘贴文本的通用方式：\n<ol>\n<li>按下 <code>Ctrl + a</code> 后松开手指，然后按 <code>[</code>。</li>\n<li>用鼠标选中文本，被选中的文本会被自动复制到 <code>tmux</code> 的剪贴板。</li>\n<li>按下 <code>Ctrl + a</code> 后松开手指，然后按 <code>]</code>，会将剪贴板中的内容粘贴到光标处。</li>\n</ol>\n</li>\n</ul>\n<p>注意：Tmux 的配置文件为 <code>~/.tmux.conf</code>，默认 Tmux 前缀快捷键是 <code>Ctrl + b</code>！本文是已经修改过了配置文件后的操作说明，不过基本上操作逻辑都是一样的。</p>\n<h3 id=\"2-2-Vim\">2.2 Vim</h3>\n<p>（1）功能</p>\n<ul>\n<li>命令行模式下的文本编辑器。</li>\n<li>根据文件扩展名自动判别编程语言。支持代码缩进、代码高亮等功能。</li>\n<li>使用方式：<code>vim &lt;filename&gt;</code>。\n<ul>\n<li>如果已有该文件，则打开它。</li>\n<li>如果没有该文件，则打开个一个新的文件，并命名为 <code>filename</code>。</li>\n</ul>\n</li>\n</ul>\n<p>（2）模式</p>\n<ul>\n<li>一般命令模式\n<ul>\n<li>默认模式。命令输入方式：类似于打游戏放技能，按不同字符，即可进行不同操作。可以复制、粘贴、删除文本等。</li>\n</ul>\n</li>\n<li>编辑模式\n<ul>\n<li>在一般命令模式里按下 <code>i</code>，会进入编辑模式。</li>\n<li>按下 <code>ESC</code> 会退出编辑模式，返回到一般命令模式。</li>\n</ul>\n</li>\n<li>命令行模式\n<ul>\n<li>在一般命令模式里按下 <code>:/?</code> 三个字符中的任意一个，会进入命令行模式。命令行在最下面。</li>\n<li>可以查找、替换、保存、退出、配置编辑器等。</li>\n</ul>\n</li>\n</ul>\n<p>（3）操作</p>\n<ul>\n<li><code>i</code>：进入编辑模式。</li>\n<li><code>ESC</code>：进入一般命令模式。</li>\n<li><code>h</code> 或左方向键：光标向左移动一个字符。</li>\n<li><code>j</code> 或下方向键：光标向下移动一个字符。</li>\n<li><code>k</code> 或上方向键：光标向上移动一个字符。</li>\n<li><code>l</code> 或右方向键：光标向右移动一个字符。</li>\n<li><code>n&lt;Space&gt;</code>：<code>n</code> 表示数字，按下数字后再按空格，光标会向右移动 <code>n</code> 个字符。</li>\n<li><code>0</code> 或 <code>Home</code>：光标移动到本行开头。</li>\n<li><code>$</code> 或 <code>End</code>：光标移动到本行末尾。</li>\n<li><code>G</code>：光标移动到最后一行。</li>\n<li><code>:n</code> 或 <code>nG</code>：<code>n</code> 为数字，光标移动到第 <code>n</code> 行。</li>\n<li><code>gg</code>：光标移动到第一行，相当于 <code>1G</code>。</li>\n<li><code>n&lt;Enter&gt;</code>：<code>n</code> 为数字，光标向下移动 <code>n</code> 行。</li>\n<li><code>/word</code>：向光标之下寻找第一个值为 <code>word</code> 的字符串。</li>\n<li><code>?word</code>：向光标之上寻找第一个值为 <code>word</code> 的字符串。</li>\n<li><code>n</code>：重复前一个查找操作。</li>\n<li><code>N</code>：反向重复前一个查找操作。</li>\n<li><code>:n1,n2s/word1/word2/g</code>：<code>n1</code> 与 <code>n2</code> 为数字，在第 <code>n1</code> 行与 <code>n2</code> 行之间寻找 <code>word1</code> 这个字符串，并将该字符串替换为 <code>word2</code>。</li>\n<li><code>:1,$s/word1/word2/g</code>：将全文的 <code>word1</code> 替换为 <code>word2</code>。</li>\n<li><code>:1,$s/word1/word2/gc</code>：将全文的 <code>word1</code> 替换为 <code>word2</code>，且在替换前要求用户确认。</li>\n<li><code>v</code>：选中文本，连续按两次 <code>ESC</code> 取消选中。</li>\n<li><code>d</code>：删除选中的文本。</li>\n<li><code>dd</code>：删除当前行（其实是剪切）。</li>\n<li><code>ggdG</code>：删除全部内容。</li>\n<li><code>y</code>：复制选中的文本。</li>\n<li><code>yy</code>：复制当前行。</li>\n<li><code>ggyG</code>：复制全部内容。</li>\n<li><code>p</code>：将复制的数据在光标的下一行/下一个位置粘贴。</li>\n<li><code>u</code>：撤销。</li>\n<li><code>Ctrl + r</code>：取消撤销。</li>\n<li><code>&gt;</code>：将选中的文本整体向右缩进一次。</li>\n<li><code>&lt;</code>：将选中的文本整体向左缩进一次。</li>\n<li><code>:w</code>：保存。</li>\n<li><code>:w!</code>：强制保存。</li>\n<li><code>:q</code>：退出。</li>\n<li><code>:q!</code>：强制退出。</li>\n<li><code>:wq</code>：保存并退出。</li>\n<li><code>:set paste</code>：设置成粘贴模式，取消代码自动缩进。</li>\n<li><code>:set nopaste</code>：取消粘贴模式，开启代码自动缩进。</li>\n<li><code>:set nu</code>：显示行号。</li>\n<li><code>:set nonu</code>：隐藏行号。</li>\n<li><code>gg=G</code>：将全文代码格式化。</li>\n<li><code>:noh</code>：关闭查找关键词高亮。</li>\n<li><code>Ctrl + q</code>：当 Vim 卡死时，可以取消当前正在执行的命令。</li>\n</ul>\n<p>异常处理：<br>\n每次用 Vim 编辑文件时，会自动创建一个 <code>&lt;filename&gt;.swp</code> 的临时文件。<br>\n如果打开某个文件时，该文件的 <code>swp</code> 文件已存在，则会报错。此时解决办法有两种：</p>\n<ul>\n<li>找到正在打开该文件的程序，并退出。</li>\n<li>直接删掉该 <code>swp</code> 文件即可。</li>\n</ul>\n<p>Vim 的配置文件在 <code>~/.vimrc</code> 中。</p>\n",
            "tags": [
                "Linux"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/45632.html",
            "url": "https://asanosaki.github.io/posts/45632.html",
            "title": "VS Code实用插件推荐与使用教程",
            "date_published": "2022-01-25T09:57:00.000Z",
            "content_html": "<blockquote>\n<p>推荐一些 VS Code 基础插件，例如：简体中文、C++、背景、主题等。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-Chinese-Simplified\">1. Chinese (Simplified)</h2>\n<p><img src=\"https://img-blog.csdnimg.cn/9150917c32384c1181be7d7c2b42c84f.png\" alt=\"\"></p>\n<p>简体中文插件，不用多说了，上来第一个先装这个。</p>\n<h2 id=\"2-C-C\">2. C/C++</h2>\n<p><img src=\"https://img-blog.csdnimg.cn/eeb8dd629d2a40efb56b014e4af2e09c.png\" alt=\"\"></p>\n<p>需要编写调试运行 C/C++ 文件所需的插件，安装完成后在工作空间的顶层文件夹中新建一个 <code>.vscode</code> 文件夹，新建两个文件名字分别为 <code>tasks.json</code> 和 <code>launch.json</code>。</p>\n<p>其中，<code>tasks.json</code> 文件内容固定如下：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;version&quot;</span>: <span class=\"string\">&quot;2.0.0&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;tasks&quot;</span>: [&#123;</span><br><span class=\"line\">            <span class=\"string\">&quot;label&quot;</span>: <span class=\"string\">&quot;g++&quot;</span>,</span><br><span class=\"line\">            <span class=\"string\">&quot;command&quot;</span>: <span class=\"string\">&quot;g++&quot;</span>,</span><br><span class=\"line\">            <span class=\"string\">&quot;args&quot;</span>: [</span><br><span class=\"line\">                <span class=\"string\">&quot;-g&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;$&#123;file&#125;&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;-o&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;$&#123;fileDirname&#125;/$&#123;fileBasenameNoExtension&#125;.exe&quot;</span></span><br><span class=\"line\">            ],</span><br><span class=\"line\">            <span class=\"string\">&quot;problemMatcher&quot;</span>: &#123;</span><br><span class=\"line\">                <span class=\"string\">&quot;owner&quot;</span>: <span class=\"string\">&quot;cpp&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;fileLocation&quot;</span>: [</span><br><span class=\"line\">                    <span class=\"string\">&quot;relative&quot;</span>,</span><br><span class=\"line\">                    <span class=\"string\">&quot;$&#123;workspaceRoot&#125;&quot;</span></span><br><span class=\"line\">                ],</span><br><span class=\"line\">                <span class=\"string\">&quot;pattern&quot;</span>: &#123;</span><br><span class=\"line\">                    <span class=\"string\">&quot;regexp&quot;</span>: <span class=\"string\">&quot;^(.*):(\\\\d+):(\\\\d+):\\\\s+(warning|error):\\\\s+(.*)$&quot;</span>,</span><br><span class=\"line\">                    <span class=\"string\">&quot;file&quot;</span>: <span class=\"number\">1</span>,</span><br><span class=\"line\">                    <span class=\"string\">&quot;line&quot;</span>: <span class=\"number\">2</span>,</span><br><span class=\"line\">                    <span class=\"string\">&quot;column&quot;</span>: <span class=\"number\">3</span>,</span><br><span class=\"line\">                    <span class=\"string\">&quot;severity&quot;</span>: <span class=\"number\">4</span>,</span><br><span class=\"line\">                    <span class=\"string\">&quot;message&quot;</span>: <span class=\"number\">5</span></span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            <span class=\"string\">&quot;group&quot;</span>: &#123;</span><br><span class=\"line\">                <span class=\"string\">&quot;kind&quot;</span>: <span class=\"string\">&quot;build&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;isDefault&quot;</span>: <span class=\"literal\">true</span></span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            <span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">            &quot;presentation&quot;: &#123; </span></span><br><span class=\"line\"><span class=\"comment\">                &quot;panel&quot;: &quot;new&quot; //默认为“shared“表示共享，改成new之后每个进程创建新的端口</span></span><br><span class=\"line\"><span class=\"comment\">            &#125;</span></span><br><span class=\"line\"><span class=\"comment\">            */</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><code>launch.json</code> 文件内容如下，注意 <code>&quot;miDebuggerPath&quot;</code> 的路径需要选择自己电脑上安装的 MinGW 的 <code>gdb.exe</code> 路径：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;version&quot;</span>: <span class=\"string\">&quot;0.2.0&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;configurations&quot;</span>: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"string\">&quot;name&quot;</span>: <span class=\"string\">&quot;(gdb) Launch&quot;</span>, <span class=\"comment\">// 配置名称，将会在启动配置的下拉菜单中显示</span></span><br><span class=\"line\">            <span class=\"string\">&quot;type&quot;</span>: <span class=\"string\">&quot;cppdbg&quot;</span>, <span class=\"comment\">// 配置类型，这里只能为cppdbg</span></span><br><span class=\"line\">            <span class=\"string\">&quot;request&quot;</span>: <span class=\"string\">&quot;launch&quot;</span>, <span class=\"comment\">// 请求配置类型，可以为launch（启动）或attach（附加）</span></span><br><span class=\"line\">            <span class=\"string\">&quot;program&quot;</span>: <span class=\"string\">&quot;$&#123;fileDirname&#125;\\\\$&#123;fileBasenameNoExtension&#125;.exe&quot;</span>, <span class=\"comment\">// 将要进行调试的程序的路径</span></span><br><span class=\"line\">            <span class=\"string\">&quot;args&quot;</span>: [], <span class=\"comment\">// 程序调试时传递给程序的命令行参数，一般设为空即可</span></span><br><span class=\"line\">            <span class=\"string\">&quot;stopAtEntry&quot;</span>: <span class=\"literal\">false</span>, <span class=\"comment\">// 设为true时程序将暂停在程序入口处，一般设置为false</span></span><br><span class=\"line\">            <span class=\"string\">&quot;cwd&quot;</span>: <span class=\"string\">&quot;$&#123;fileDirname&#125;&quot;</span>, <span class=\"comment\">// 调试程序时的工作目录，一般为$&#123;fileDirname&#125;即代码所在目录</span></span><br><span class=\"line\">            <span class=\"string\">&quot;environment&quot;</span>: [],</span><br><span class=\"line\">            <span class=\"string\">&quot;externalConsole&quot;</span>: <span class=\"literal\">false</span>, <span class=\"comment\">// 调试时是否显示控制台窗口，一般设置为false显示控制台</span></span><br><span class=\"line\">            <span class=\"string\">&quot;MIMode&quot;</span>: <span class=\"string\">&quot;gdb&quot;</span>,</span><br><span class=\"line\">            <span class=\"string\">&quot;miDebuggerPath&quot;</span>: <span class=\"string\">&quot;D:\\\\MinGW\\\\mingw32\\\\bin\\\\gdb.exe&quot;</span>, <span class=\"comment\">// miDebugger的路径，注意这里要与MinGw的路径对应</span></span><br><span class=\"line\">            <span class=\"string\">&quot;setupCommands&quot;</span>: [</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    <span class=\"string\">&quot;description&quot;</span>: <span class=\"string\">&quot;为 gdb 启用整齐打印&quot;</span>,</span><br><span class=\"line\">                    <span class=\"string\">&quot;text&quot;</span>: <span class=\"string\">&quot;-enable-pretty-printing&quot;</span>,</span><br><span class=\"line\">                    <span class=\"string\">&quot;ignoreFailures&quot;</span>: <span class=\"literal\">true</span></span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            ],</span><br><span class=\"line\">            <span class=\"string\">&quot;preLaunchTask&quot;</span>: <span class=\"string\">&quot;g++&quot;</span> <span class=\"comment\">// 调试会话开始前执行的任务，一般为编译程序，c++为g++, c为gcc</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>如果使用 TDM-GCC 那么 <code>launch.json</code> 文件的内容如下：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;version&quot;</span>: <span class=\"string\">&quot;0.2.0&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;configurations&quot;</span>: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"string\">&quot;name&quot;</span>: <span class=\"string\">&quot;(gdb) Launch&quot;</span>, <span class=\"comment\">// 配置名称，将会在启动配置的下拉菜单中显示</span></span><br><span class=\"line\">            <span class=\"string\">&quot;type&quot;</span>: <span class=\"string\">&quot;cppdbg&quot;</span>, <span class=\"comment\">// 配置类型，这里只能为cppdbg</span></span><br><span class=\"line\">            <span class=\"string\">&quot;request&quot;</span>: <span class=\"string\">&quot;launch&quot;</span>, <span class=\"comment\">// 请求配置类型，可以为launch（启动）或attach（附加）</span></span><br><span class=\"line\">            <span class=\"string\">&quot;program&quot;</span>: <span class=\"string\">&quot;$&#123;fileDirname&#125;\\\\$&#123;fileBasenameNoExtension&#125;.exe&quot;</span>, <span class=\"comment\">// 将要进行调试的程序的路径</span></span><br><span class=\"line\">            <span class=\"string\">&quot;args&quot;</span>: [], <span class=\"comment\">// 程序调试时传递给程序的命令行参数，一般设为空即可</span></span><br><span class=\"line\">            <span class=\"string\">&quot;stopAtEntry&quot;</span>: <span class=\"literal\">false</span>, <span class=\"comment\">// 设为true时程序将暂停在程序入口处，一般设置为false</span></span><br><span class=\"line\">            <span class=\"string\">&quot;cwd&quot;</span>: <span class=\"string\">&quot;$&#123;fileDirname&#125;&quot;</span>, <span class=\"comment\">// 调试程序时的工作目录，一般为$&#123;fileDirname&#125;即代码所在目录</span></span><br><span class=\"line\">            <span class=\"string\">&quot;environment&quot;</span>: [],</span><br><span class=\"line\">            <span class=\"string\">&quot;externalConsole&quot;</span>: <span class=\"literal\">false</span>, <span class=\"comment\">// 调试时是否显示控制台窗口，一般设置为false显示控制台</span></span><br><span class=\"line\">            <span class=\"string\">&quot;MIMode&quot;</span>: <span class=\"string\">&quot;gdb&quot;</span>,</span><br><span class=\"line\">            <span class=\"string\">&quot;miDebuggerPath&quot;</span>: <span class=\"string\">&quot;D:\\\\TDM-GCC-64\\\\bin\\\\gdb64.exe&quot;</span>, <span class=\"comment\">// miDebugger的路径，注意这里要与TDM-GCC的路径对应</span></span><br><span class=\"line\">            <span class=\"string\">&quot;setupCommands&quot;</span>: [</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    <span class=\"string\">&quot;description&quot;</span>: <span class=\"string\">&quot;为 gdb 启用整齐打印&quot;</span>,</span><br><span class=\"line\">                    <span class=\"string\">&quot;text&quot;</span>: <span class=\"string\">&quot;-enable-pretty-printing&quot;</span>,</span><br><span class=\"line\">                    <span class=\"string\">&quot;ignoreFailures&quot;</span>: <span class=\"literal\">true</span></span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            ],</span><br><span class=\"line\">            <span class=\"string\">&quot;preLaunchTask&quot;</span>: <span class=\"string\">&quot;g++&quot;</span> <span class=\"comment\">// 调试会话开始前执行的任务，一般为编译程序，c++为g++, c为gcc</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>配置完成后编写 C++ 代码即可成功编译运行啦！（注意源文件的路径中不能有任何中文的文件夹）</p>\n<p><img src=\"https://img-blog.csdnimg.cn/1470380190074eb29c67a32d28146a3b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5p-D5q2M,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n<h2 id=\"3-Markdown-All-in-One\">3. Markdown All in One</h2>\n<p><img src=\"https://img-blog.csdnimg.cn/e837043a7e9544f1815b47f1b8ec7c32.png\" alt=\"\"></p>\n<p>安装了该插件后即可编写 Markdown 文件（文件后缀名为 <code>.md</code>），效果如下：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/a6dd759c0246462fa325de60623f1634.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5p-D5q2M,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n<h2 id=\"4-MASM-TASM\">4. MASM/TASM</h2>\n<p><img src=\"https://img-blog.csdnimg.cn/950e7718945f4fd18e2b8fa7da71b459.png\" alt=\"\"></p>\n<p>编写汇编语言代码必备插件，效果如下：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/8d641116a6ff49d1923e82cba097200e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5p-D5q2M,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n<h2 id=\"5-background\">5. background</h2>\n<p><img src=\"https://img-blog.csdnimg.cn/3eb3bb11746340e4bbbec9307bbe818d.png\" alt=\"\"></p>\n<p>更换背景的插件，效果如前文中的图片所示，安装完成后点击&quot;文件&quot;-“首选项”-“设置”-“扩展”，打开 <code>Plugin background config. background 插件配置</code>，点击&quot;在 <code>settings.json</code> 中编辑&quot;，添加一段配置代码（注意得添加在大括号内）：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//background 的相关配置</span></span><br><span class=\"line\">   <span class=\"string\">&quot;update.enableWindowsBackgroundUpdates&quot;</span>: <span class=\"literal\">true</span>,</span><br><span class=\"line\">   <span class=\"string\">&quot;background.customImages&quot;</span>: [</span><br><span class=\"line\">       <span class=\"string\">&quot;file:///E:/Microsoft VS Code/image/background.png&quot;</span><span class=\"comment\">//图片地址</span></span><br><span class=\"line\">   ],</span><br><span class=\"line\">   <span class=\"string\">&quot;background.style&quot;</span>: &#123;</span><br><span class=\"line\">       <span class=\"string\">&quot;content&quot;</span>:<span class=\"string\">&quot;&#x27;&#x27;&quot;</span>,</span><br><span class=\"line\">       <span class=\"string\">&quot;pointer-events&quot;</span>:<span class=\"string\">&quot;none&quot;</span>,</span><br><span class=\"line\">       <span class=\"string\">&quot;position&quot;</span>:<span class=\"string\">&quot;absolute&quot;</span>,<span class=\"comment\">//图片位置</span></span><br><span class=\"line\">       <span class=\"string\">&quot;width&quot;</span>:<span class=\"string\">&quot;100%&quot;</span>,</span><br><span class=\"line\">       <span class=\"string\">&quot;height&quot;</span>:<span class=\"string\">&quot;100%&quot;</span>,</span><br><span class=\"line\">       <span class=\"string\">&quot;z-index&quot;</span>:<span class=\"string\">&quot;99999&quot;</span>,</span><br><span class=\"line\">       <span class=\"string\">&quot;background.repeat&quot;</span>:<span class=\"string\">&quot;no-repeat&quot;</span>,</span><br><span class=\"line\">       <span class=\"string\">&quot;background-size&quot;</span>:<span class=\"string\">&quot;30%,30%&quot;</span>,<span class=\"comment\">//图片大小</span></span><br><span class=\"line\">       <span class=\"string\">&quot;opacity&quot;</span>:<span class=\"number\">0.2</span> <span class=\"comment\">//透明度</span></span><br><span class=\"line\">   &#125;,</span><br><span class=\"line\">   <span class=\"string\">&quot;background.useFront&quot;</span>: <span class=\"literal\">true</span>,</span><br><span class=\"line\">   <span class=\"string\">&quot;background.useDefault&quot;</span>: <span class=\"literal\">false</span>,<span class=\"comment\">//是否使用默认图片</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"6-Atom-One-Dark-Theme\">6. Atom One Dark Theme</h2>\n<p><img src=\"https://img-blog.csdnimg.cn/cc73b0a9e5384a1ba17909ee0e97a468.png\" alt=\"\"></p>\n<p>最经典的一款黑色皮肤，强烈推荐！效果参考前文所示。</p>\n<h2 id=\"7-Python\">7. Python</h2>\n<p><img src=\"https://img-blog.csdnimg.cn/c22acc2b4075499c951d49edb099a67c.png\" alt=\"\"></p>\n<p>Python 这个插件必装！别问为啥！因为它是微软 VS Code 开发团队自己开发的，亲儿子的级别。虽然 VS Code 不安装任何插件也能高亮 Python 代码，但该件提供的功能远不止如此，还有很多强大的功能。</p>\n<p>注意：如果装有 <code>Code Runner</code> 插件，运行 Python 代码时可能会出现中文乱码问题，其实 Python 运行不需要 <code>Code Runner</code> 插件。</p>\n<h2 id=\"8-Python-Extension-Pack\">8. Python Extension Pack</h2>\n<p><img src=\"https://img-blog.csdnimg.cn/f067a0e59b544ddd86b79d0a9e22eecd.png\" alt=\"\"></p>\n<p>这是一个 Python 扩展包，它依赖于以下扩展包：</p>\n<ul>\n<li><code>Python</code>：高亮、调试（多线程、远程）、智能提示、代码格式化、重构、单元测试、代码片段、数据科学（使用 Jupyter）、PySpark 等等。</li>\n<li><code>Jinja</code>：对 Visual Studio 代码的 Jinja 模板语言支持。</li>\n<li><code>Django</code>：为有期限的完美主义者提供了漂亮的语法和限定范围的片段。</li>\n<li><code>Visual Studio IntelliCode</code>：在 Visual Studio 代码中为 Python 开发人员提供人工智能辅助的生产力功能，并基于对代码的理解和机器学习提供见解。</li>\n<li><code>Python Environment Manager</code>：提供从一个地方查看和管理所有 Python 环境和包的能力。</li>\n<li><code>Python Docstring Generator</code>：基于多个可选模板模式，为类和方法快速插入带有上下文推断参数的 Python 注释块。</li>\n<li><code>Python Indent</code>：在 Visual Studio 代码中更正 Python 缩进。</li>\n<li><code>Jupyter</code>：为 Python 语言提供 Jupyter Notebook 支持，用于数据科学、科学计算和机器学习。</li>\n</ul>\n",
            "tags": [
                "Others"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/11764.html",
            "url": "https://asanosaki.github.io/posts/11764.html",
            "title": "Linux与Windows下Vim配置方案推荐",
            "date_published": "2021-11-21T02:15:00.000Z",
            "content_html": "<blockquote>\n<p><s>Vim 真香！！！不会吧不会还有人在用 IDE 吧（bushi）</s></p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"1-前言\">1. 前言</h2>\n<p>可能很多萌新程序员会问为什么很多大佬写代码时用的都是 Vim 而不是自己熟悉的 Visual Studio、VS Code、IDEA、PyCharm、CLion 等这些 IDE 呢？用 Vim 编辑有什么优点呢？</p>\n<ul>\n<li>Vim 对硬件需求小：如果只是编写一个相对来说不是那么庞大的程序的话，用 IDE 有种“大材小用”的感觉，启动、编译运行相比 Vim 来说都满了很多，占用内存资源也很大，因此平常自己写写代码不是搞大型开发的话用 Vim 绝对会感觉又快又方便；</li>\n<li>Vim 自由度高：无论什么 IDE，那终究还是别人搭建好的编辑平台，你永远驯服不了，而对于 Vim，从头到尾的元素都是可以由自己 DIY 的，就像是自己专属的开发环境，里面的元素都是自己最喜欢且最熟悉的；</li>\n<li>Vim 光标移动效率高：对于新手而言，肯定会觉得 Vim 很难用，效率很低，那是因为还不熟悉 Vim 的整套光标移动快捷键，一旦熟练了快捷键后，你会发现用鼠标移动光标是一件特别慢还特别麻烦的事情，用 Vim 后完全可以脱离鼠标，让光标移动的比鼠标操作灵活很多（毫不夸张）；</li>\n<li>Vim 编辑效率高：同样的，Vim 有着一整套编辑操作的快捷键组合，且可以扩展各种插件，编辑代码的功能方面绝不逊于各大 IDE。</li>\n</ul>\n<p>那么如何配置一个属于自己的 Vim 环境呢？本文就来介绍一下 <code>.vimrc</code> 文件的配置（Windows 操作系统环境下）。</p>\n<h2 id=\"2-Windows下-vimrc文件的配置\">2. Windows下.vimrc文件的配置</h2>\n<p>在 Windows 操作系统环境下，用户可以进入自己的用户根目录（<code>C:\\Users\\你的用户id</code>）下，新建一个文本文件，在其中输入以下内容：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br><span class=\"line\">211</span><br><span class=\"line\">212</span><br><span class=\"line\">213</span><br><span class=\"line\">214</span><br><span class=\"line\">215</span><br><span class=\"line\">216</span><br><span class=\"line\">217</span><br><span class=\"line\">218</span><br><span class=\"line\">219</span><br><span class=\"line\">220</span><br><span class=\"line\">221</span><br><span class=\"line\">222</span><br><span class=\"line\">223</span><br><span class=\"line\">224</span><br><span class=\"line\">225</span><br><span class=\"line\">226</span><br><span class=\"line\">227</span><br><span class=\"line\">228</span><br><span class=\"line\">229</span><br><span class=\"line\">230</span><br><span class=\"line\">231</span><br><span class=\"line\">232</span><br><span class=\"line\">233</span><br><span class=\"line\">234</span><br><span class=\"line\">235</span><br><span class=\"line\">236</span><br><span class=\"line\">237</span><br><span class=\"line\">238</span><br><span class=\"line\">239</span><br><span class=\"line\">240</span><br><span class=\"line\">241</span><br><span class=\"line\">242</span><br><span class=\"line\">243</span><br><span class=\"line\">244</span><br><span class=\"line\">245</span><br><span class=\"line\">246</span><br><span class=\"line\">247</span><br><span class=\"line\">248</span><br><span class=\"line\">249</span><br><span class=\"line\">250</span><br><span class=\"line\">251</span><br><span class=\"line\">252</span><br><span class=\"line\">253</span><br><span class=\"line\">254</span><br><span class=\"line\">255</span><br><span class=\"line\">256</span><br><span class=\"line\">257</span><br><span class=\"line\">258</span><br><span class=\"line\">259</span><br><span class=\"line\">260</span><br><span class=\"line\">261</span><br><span class=\"line\">262</span><br><span class=\"line\">263</span><br><span class=\"line\">264</span><br><span class=\"line\">265</span><br><span class=\"line\">266</span><br><span class=\"line\">267</span><br><span class=\"line\">268</span><br><span class=\"line\">269</span><br><span class=\"line\">270</span><br><span class=\"line\">271</span><br><span class=\"line\">272</span><br><span class=\"line\">273</span><br><span class=\"line\">274</span><br><span class=\"line\">275</span><br><span class=\"line\">276</span><br><span class=\"line\">277</span><br><span class=\"line\">278</span><br><span class=\"line\">279</span><br><span class=\"line\">280</span><br><span class=\"line\">281</span><br><span class=\"line\">282</span><br><span class=\"line\">283</span><br><span class=\"line\">284</span><br><span class=\"line\">285</span><br><span class=\"line\">286</span><br><span class=\"line\">287</span><br><span class=\"line\">288</span><br><span class=\"line\">289</span><br><span class=\"line\">290</span><br><span class=\"line\">291</span><br><span class=\"line\">292</span><br><span class=\"line\">293</span><br><span class=\"line\">294</span><br><span class=\"line\">295</span><br><span class=\"line\">296</span><br><span class=\"line\">297</span><br><span class=\"line\">298</span><br><span class=\"line\">299</span><br><span class=\"line\">300</span><br><span class=\"line\">301</span><br><span class=\"line\">302</span><br><span class=\"line\">303</span><br><span class=\"line\">304</span><br><span class=\"line\">305</span><br><span class=\"line\">306</span><br><span class=\"line\">307</span><br><span class=\"line\">308</span><br><span class=\"line\">309</span><br><span class=\"line\">310</span><br><span class=\"line\">311</span><br><span class=\"line\">312</span><br><span class=\"line\">313</span><br><span class=\"line\">314</span><br><span class=\"line\">315</span><br><span class=\"line\">316</span><br><span class=\"line\">317</span><br><span class=\"line\">318</span><br><span class=\"line\">319</span><br><span class=\"line\">320</span><br><span class=\"line\">321</span><br><span class=\"line\">322</span><br><span class=\"line\">323</span><br><span class=\"line\">324</span><br><span class=\"line\">325</span><br><span class=\"line\">326</span><br><span class=\"line\">327</span><br><span class=\"line\">328</span><br><span class=\"line\">329</span><br><span class=\"line\">330</span><br><span class=\"line\">331</span><br><span class=\"line\">332</span><br><span class=\"line\">333</span><br><span class=\"line\">334</span><br><span class=\"line\">335</span><br><span class=\"line\">336</span><br><span class=\"line\">337</span><br><span class=\"line\">338</span><br><span class=\"line\">339</span><br><span class=\"line\">340</span><br><span class=\"line\">341</span><br><span class=\"line\">342</span><br><span class=\"line\">343</span><br><span class=\"line\">344</span><br><span class=\"line\">345</span><br><span class=\"line\">346</span><br><span class=\"line\">347</span><br><span class=\"line\">348</span><br><span class=\"line\">349</span><br><span class=\"line\">350</span><br><span class=\"line\">351</span><br><span class=\"line\">352</span><br><span class=\"line\">353</span><br><span class=\"line\">354</span><br><span class=\"line\">355</span><br><span class=\"line\">356</span><br><span class=\"line\">357</span><br><span class=\"line\">358</span><br><span class=\"line\">359</span><br><span class=\"line\">360</span><br><span class=\"line\">361</span><br><span class=\"line\">362</span><br><span class=\"line\">363</span><br><span class=\"line\">364</span><br><span class=\"line\">365</span><br><span class=\"line\">366</span><br><span class=\"line\">367</span><br><span class=\"line\">368</span><br><span class=\"line\">369</span><br><span class=\"line\">370</span><br><span class=\"line\">371</span><br><span class=\"line\">372</span><br><span class=\"line\">373</span><br><span class=\"line\">374</span><br><span class=\"line\">375</span><br><span class=\"line\">376</span><br><span class=\"line\">377</span><br><span class=\"line\">378</span><br><span class=\"line\">379</span><br><span class=\"line\">380</span><br><span class=\"line\">381</span><br><span class=\"line\">382</span><br><span class=\"line\">383</span><br><span class=\"line\">384</span><br><span class=\"line\">385</span><br><span class=\"line\">386</span><br><span class=\"line\">387</span><br><span class=\"line\">388</span><br><span class=\"line\">389</span><br><span class=\"line\">390</span><br><span class=\"line\">391</span><br><span class=\"line\">392</span><br><span class=\"line\">393</span><br><span class=\"line\">394</span><br><span class=\"line\">395</span><br><span class=\"line\">396</span><br><span class=\"line\">397</span><br><span class=\"line\">398</span><br><span class=\"line\">399</span><br><span class=\"line\">400</span><br><span class=\"line\">401</span><br><span class=\"line\">402</span><br><span class=\"line\">403</span><br><span class=\"line\">404</span><br><span class=\"line\">405</span><br><span class=\"line\">406</span><br><span class=\"line\">407</span><br><span class=\"line\">408</span><br><span class=\"line\">409</span><br><span class=\"line\">410</span><br><span class=\"line\">411</span><br><span class=\"line\">412</span><br><span class=\"line\">413</span><br><span class=\"line\">414</span><br><span class=\"line\">415</span><br><span class=\"line\">416</span><br><span class=\"line\">417</span><br><span class=\"line\">418</span><br><span class=\"line\">419</span><br><span class=\"line\">420</span><br><span class=\"line\">421</span><br><span class=\"line\">422</span><br><span class=\"line\">423</span><br><span class=\"line\">424</span><br><span class=\"line\">425</span><br><span class=\"line\">426</span><br><span class=\"line\">427</span><br><span class=\"line\">428</span><br><span class=\"line\">429</span><br><span class=\"line\">430</span><br><span class=\"line\">431</span><br><span class=\"line\">432</span><br><span class=\"line\">433</span><br><span class=\"line\">434</span><br><span class=\"line\">435</span><br><span class=\"line\">436</span><br><span class=\"line\">437</span><br><span class=\"line\">438</span><br><span class=\"line\">439</span><br><span class=\"line\">440</span><br><span class=\"line\">441</span><br><span class=\"line\">442</span><br><span class=\"line\">443</span><br><span class=\"line\">444</span><br><span class=\"line\">445</span><br><span class=\"line\">446</span><br><span class=\"line\">447</span><br><span class=\"line\">448</span><br><span class=\"line\">449</span><br><span class=\"line\">450</span><br><span class=\"line\">451</span><br><span class=\"line\">452</span><br><span class=\"line\">453</span><br><span class=\"line\">454</span><br><span class=\"line\">455</span><br><span class=\"line\">456</span><br><span class=\"line\">457</span><br><span class=\"line\">458</span><br><span class=\"line\">459</span><br><span class=\"line\">460</span><br><span class=\"line\">461</span><br><span class=\"line\">462</span><br><span class=\"line\">463</span><br><span class=\"line\">464</span><br><span class=\"line\">465</span><br><span class=\"line\">466</span><br><span class=\"line\">467</span><br><span class=\"line\">468</span><br><span class=\"line\">469</span><br><span class=\"line\">470</span><br><span class=\"line\">471</span><br><span class=\"line\">472</span><br><span class=\"line\">473</span><br><span class=\"line\">474</span><br><span class=\"line\">475</span><br><span class=\"line\">476</span><br><span class=\"line\">477</span><br><span class=\"line\">478</span><br><span class=\"line\">479</span><br><span class=\"line\">480</span><br><span class=\"line\">481</span><br><span class=\"line\">482</span><br><span class=\"line\">483</span><br><span class=\"line\">484</span><br><span class=\"line\">485</span><br><span class=\"line\">486</span><br><span class=\"line\">487</span><br><span class=\"line\">488</span><br><span class=\"line\">489</span><br><span class=\"line\">490</span><br><span class=\"line\">491</span><br><span class=\"line\">492</span><br><span class=\"line\">493</span><br><span class=\"line\">494</span><br><span class=\"line\">495</span><br><span class=\"line\">496</span><br><span class=\"line\">497</span><br><span class=\"line\">498</span><br><span class=\"line\">499</span><br><span class=\"line\">500</span><br><span class=\"line\">501</span><br><span class=\"line\">502</span><br><span class=\"line\">503</span><br><span class=\"line\">504</span><br><span class=\"line\">505</span><br><span class=\"line\">506</span><br><span class=\"line\">507</span><br><span class=\"line\">508</span><br><span class=\"line\">509</span><br><span class=\"line\">510</span><br><span class=\"line\">511</span><br><span class=\"line\">512</span><br><span class=\"line\">513</span><br><span class=\"line\">514</span><br><span class=\"line\">515</span><br><span class=\"line\">516</span><br><span class=\"line\">517</span><br><span class=\"line\">518</span><br><span class=\"line\">519</span><br><span class=\"line\">520</span><br><span class=\"line\">521</span><br><span class=\"line\">522</span><br><span class=\"line\">523</span><br><span class=\"line\">524</span><br><span class=\"line\">525</span><br><span class=\"line\">526</span><br><span class=\"line\">527</span><br><span class=\"line\">528</span><br><span class=\"line\">529</span><br><span class=\"line\">530</span><br><span class=\"line\">531</span><br><span class=\"line\">532</span><br><span class=\"line\">533</span><br><span class=\"line\">534</span><br><span class=\"line\">535</span><br><span class=\"line\">536</span><br><span class=\"line\">537</span><br><span class=\"line\">538</span><br><span class=\"line\">539</span><br><span class=\"line\">540</span><br><span class=\"line\">541</span><br><span class=\"line\">542</span><br><span class=\"line\">543</span><br><span class=\"line\">544</span><br><span class=\"line\">545</span><br><span class=\"line\">546</span><br><span class=\"line\">547</span><br><span class=\"line\">548</span><br><span class=\"line\">549</span><br><span class=\"line\">550</span><br><span class=\"line\">551</span><br><span class=\"line\">552</span><br><span class=\"line\">553</span><br><span class=\"line\">554</span><br><span class=\"line\">555</span><br><span class=\"line\">556</span><br><span class=\"line\">557</span><br><span class=\"line\">558</span><br><span class=\"line\">559</span><br><span class=\"line\">560</span><br><span class=\"line\">561</span><br><span class=\"line\">562</span><br><span class=\"line\">563</span><br><span class=\"line\">564</span><br><span class=\"line\">565</span><br><span class=\"line\">566</span><br><span class=\"line\">567</span><br><span class=\"line\">568</span><br><span class=\"line\">569</span><br><span class=\"line\">570</span><br><span class=\"line\">571</span><br><span class=\"line\">572</span><br><span class=\"line\">573</span><br><span class=\"line\">574</span><br><span class=\"line\">575</span><br><span class=\"line\">576</span><br><span class=\"line\">577</span><br><span class=\"line\">578</span><br><span class=\"line\">579</span><br><span class=\"line\">580</span><br><span class=\"line\">581</span><br><span class=\"line\">582</span><br><span class=\"line\">583</span><br><span class=\"line\">584</span><br><span class=\"line\">585</span><br><span class=\"line\">586</span><br><span class=\"line\">587</span><br><span class=\"line\">588</span><br><span class=\"line\">589</span><br><span class=\"line\">590</span><br><span class=\"line\">591</span><br><span class=\"line\">592</span><br><span class=\"line\">593</span><br><span class=\"line\">594</span><br><span class=\"line\">595</span><br><span class=\"line\">596</span><br><span class=\"line\">597</span><br><span class=\"line\">598</span><br><span class=\"line\">599</span><br><span class=\"line\">600</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 显示相关  </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;设置默认保存路径</span><br><span class=\"line\">exec &#x27;cd &#x27; . fnameescape(&#x27;E:\\Vim\\code&#x27;)</span><br><span class=\"line\"></span><br><span class=\"line\">set shortmess=atI   &quot; 启动的时候不显示那个援助乌干达儿童的提示  </span><br><span class=\"line\"></span><br><span class=\"line\">winpos 250 100          &quot; 设定窗口位置  </span><br><span class=\"line\"></span><br><span class=\"line\">set lines=35 columns=120    &quot; 设定窗口大小  </span><br><span class=\"line\"></span><br><span class=\"line\">set nu              &quot; 显示行号  </span><br><span class=\"line\"></span><br><span class=\"line\">set go=             &quot; 不要图形按钮  </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;color asmanian2     &quot; 设置背景主题  </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;set guifont=Courier_New:h15:cANSI   &quot; 设置字体  </span><br><span class=\"line\">&quot;set guifont=Bitstream_Vera_Sans_Mono:h200:cANSI</span><br><span class=\"line\">&quot;set guifont=幼圆:h18:cGB2312</span><br><span class=\"line\">set guifont=Consolas:h16:cANSI</span><br><span class=\"line\"></span><br><span class=\"line\">syntax on           &quot; 语法高亮  </span><br><span class=\"line\"></span><br><span class=\"line\">autocmd InsertLeave * se nocul  &quot; 用浅色高亮当前行  </span><br><span class=\"line\"></span><br><span class=\"line\">autocmd InsertEnter * se cul    &quot; 用浅色高亮当前行  </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;set ruler           &quot; 显示标尺  </span><br><span class=\"line\"></span><br><span class=\"line\">set showcmd         &quot; 输入的命令显示出来，看的清楚些  </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;set cmdheight=1     &quot; 命令行（在状态行下）的高度，设置为1  </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;set whichwrap+=&lt;,&gt;,h,l   &quot; 允许backspace和光标键跨越行边界(不建议)  </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;set scrolloff=3     &quot; 光标移动到buffer的顶部和底部时保持3行距离  </span><br><span class=\"line\"></span><br><span class=\"line\">set novisualbell    &quot; 不要闪烁(不明白)  </span><br><span class=\"line\"></span><br><span class=\"line\">set statusline=%F%m%r%h%w\\ [FORMAT=%&#123;&amp;ff&#125;]\\ [TYPE=%Y]\\ [POS=%l,%v][%p%%]\\ %&#123;strftime(\\&quot;%d/%m/%y\\ -\\ %H:%M\\&quot;)&#125;   &quot;状态行显示的内容  </span><br><span class=\"line\"></span><br><span class=\"line\">set laststatus=1    &quot; 启动显示状态行(1),总是显示状态行(2)  </span><br><span class=\"line\"></span><br><span class=\"line\">set foldenable      &quot; 允许折叠  </span><br><span class=\"line\"></span><br><span class=\"line\">set foldmethod=manual   &quot; 手动折叠  </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;set background=dark &quot;背景使用黑色 </span><br><span class=\"line\"></span><br><span class=\"line\">set nocompatible  &quot;去掉讨厌的有关vi一致性模式，避免以前版本的一些bug和局限  </span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 显示中文帮助</span><br><span class=\"line\"></span><br><span class=\"line\">if version &gt;= 603</span><br><span class=\"line\"></span><br><span class=\"line\">    set helplang=cn</span><br><span class=\"line\"></span><br><span class=\"line\">    set encoding=utf-8</span><br><span class=\"line\"></span><br><span class=\"line\">endif</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 设置配色方案</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;colorscheme morning</span><br><span class=\"line\">&quot;colorscheme elflord</span><br><span class=\"line\">&quot;colorscheme evening</span><br><span class=\"line\">&quot;colorscheme ron</span><br><span class=\"line\">&quot;colorscheme koehler</span><br><span class=\"line\">&quot;colorscheme murphy</span><br><span class=\"line\">&quot;colorscheme pablo</span><br><span class=\"line\">&quot;colorscheme torte</span><br><span class=\"line\">&quot;colorscheme zellner</span><br><span class=\"line\">colorscheme desert</span><br><span class=\"line\">&quot;colorscheme peachpuff</span><br><span class=\"line\">&quot;colorscheme darkblue</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;字体 </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;if (has(&quot;gui_running&quot;)) </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;   set guifont=Bitstream\\ Vera\\ Sans\\ Mono\\ 10 </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;endif </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\">set fencs=utf-8,ucs-bom,shift-jis,gb18030,gbk,gb2312,cp936</span><br><span class=\"line\"></span><br><span class=\"line\">set termencoding=utf-8</span><br><span class=\"line\"></span><br><span class=\"line\">set encoding=utf-8</span><br><span class=\"line\"></span><br><span class=\"line\">set fileencodings=ucs-bom,utf-8,cp936</span><br><span class=\"line\"></span><br><span class=\"line\">set fileencoding=utf-8</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;&quot;&quot;&quot;&quot;新文件标题&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;新建.c,.h,.sh,.java文件，自动插入文件头 </span><br><span class=\"line\"></span><br><span class=\"line\">autocmd BufNewFile *.cpp,*.[ch],*.sh,*.java exec &quot;:call SetTitle()&quot; </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;&quot;定义函数SetTitle，自动插入文件头 </span><br><span class=\"line\"></span><br><span class=\"line\">func SetTitle() </span><br><span class=\"line\"></span><br><span class=\"line\">    &quot;如果文件类型为.sh文件 </span><br><span class=\"line\"></span><br><span class=\"line\">    if &amp;filetype == &#x27;sh&#x27; </span><br><span class=\"line\"></span><br><span class=\"line\">        call setline(1,&quot;\\#########################################################################&quot;) </span><br><span class=\"line\"></span><br><span class=\"line\">        call append(line(&quot;.&quot;), &quot;\\# File Name: &quot;.expand(&quot;%&quot;)) </span><br><span class=\"line\"></span><br><span class=\"line\">        call append(line(&quot;.&quot;)+1, &quot;\\# Author: ma6174&quot;) </span><br><span class=\"line\"></span><br><span class=\"line\">        call append(line(&quot;.&quot;)+2, &quot;\\# mail: ma6174@163.com&quot;) </span><br><span class=\"line\"></span><br><span class=\"line\">        call append(line(&quot;.&quot;)+3, &quot;\\# Created Time: &quot;.strftime(&quot;%c&quot;)) </span><br><span class=\"line\"></span><br><span class=\"line\">        call append(line(&quot;.&quot;)+4, &quot;\\#########################################################################&quot;) </span><br><span class=\"line\"></span><br><span class=\"line\">        call append(line(&quot;.&quot;)+5, &quot;\\#!/bin/bash&quot;) </span><br><span class=\"line\"></span><br><span class=\"line\">        call append(line(&quot;.&quot;)+6, &quot;&quot;) </span><br><span class=\"line\"></span><br><span class=\"line\">    else </span><br><span class=\"line\"></span><br><span class=\"line\">        call setline(1, &quot;/*************************************************************************&quot;) </span><br><span class=\"line\"></span><br><span class=\"line\">        call append(line(&quot;.&quot;), &quot;    &gt; File Name: &quot;.expand(&quot;%&quot;)) </span><br><span class=\"line\"></span><br><span class=\"line\">        call append(line(&quot;.&quot;)+1, &quot;    &gt; Author: ma6174&quot;) </span><br><span class=\"line\"></span><br><span class=\"line\">        call append(line(&quot;.&quot;)+2, &quot;    &gt; Mail: ma6174@163.com &quot;) </span><br><span class=\"line\"></span><br><span class=\"line\">        call append(line(&quot;.&quot;)+3, &quot;    &gt; Created Time: &quot;.strftime(&quot;%c&quot;)) </span><br><span class=\"line\"></span><br><span class=\"line\">        call append(line(&quot;.&quot;)+4, &quot; ************************************************************************/&quot;) </span><br><span class=\"line\"></span><br><span class=\"line\">        call append(line(&quot;.&quot;)+5, &quot;&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    endif</span><br><span class=\"line\"></span><br><span class=\"line\">    if &amp;filetype == &#x27;cpp&#x27;</span><br><span class=\"line\"></span><br><span class=\"line\">        call append(line(&quot;.&quot;)+6, &quot;#include&lt;iostream&gt;&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">        call append(line(&quot;.&quot;)+7, &quot;using namespace std;&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">        call append(line(&quot;.&quot;)+8, &quot;&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    endif</span><br><span class=\"line\"></span><br><span class=\"line\">    if &amp;filetype == &#x27;c&#x27;</span><br><span class=\"line\"></span><br><span class=\"line\">        call append(line(&quot;.&quot;)+6, &quot;#include&lt;stdio.h&gt;&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">        call append(line(&quot;.&quot;)+7, &quot;&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    endif</span><br><span class=\"line\"></span><br><span class=\"line\">    &quot;新建文件后，自动定位到文件末尾</span><br><span class=\"line\"></span><br><span class=\"line\">    autocmd BufNewFile * normal G</span><br><span class=\"line\"></span><br><span class=\"line\">endfunc </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;键盘命令</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">nmap &lt;leader&gt;w :w!&lt;cr&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">nmap &lt;leader&gt;f :find&lt;cr&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 映射全选+复制 ctrl+a</span><br><span class=\"line\"></span><br><span class=\"line\">map &lt;C-A&gt; ggVGY</span><br><span class=\"line\"></span><br><span class=\"line\">map! &lt;C-A&gt; &lt;Esc&gt;ggVGY</span><br><span class=\"line\"></span><br><span class=\"line\">map &lt;F12&gt; gg=G</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 选中状态下 Ctrl+c 复制</span><br><span class=\"line\"></span><br><span class=\"line\">vmap &lt;C-c&gt; &quot;+y</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;去空行  </span><br><span class=\"line\"></span><br><span class=\"line\">nnoremap &lt;F2&gt; :g/^\\s*$/d&lt;CR&gt; </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;比较文件  </span><br><span class=\"line\"></span><br><span class=\"line\">nnoremap &lt;C-F2&gt; :vert diffsplit </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;新建标签  </span><br><span class=\"line\"></span><br><span class=\"line\">map &lt;M-F2&gt; :tabnew&lt;CR&gt;  </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;列出当前目录文件  </span><br><span class=\"line\"></span><br><span class=\"line\">map &lt;F3&gt; :tabnew .&lt;CR&gt;  </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;打开树状文件目录  </span><br><span class=\"line\"></span><br><span class=\"line\">map &lt;C-F3&gt; \\be  </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;C，C++ 按F5编译运行</span><br><span class=\"line\"></span><br><span class=\"line\">map &lt;F5&gt; :call CompileRunGcc()&lt;CR&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">func! CompileRunGcc()</span><br><span class=\"line\"></span><br><span class=\"line\">    exec &quot;w&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">    if &amp;filetype == &#x27;c&#x27;</span><br><span class=\"line\"></span><br><span class=\"line\">        exec &quot;!g++ % -o %&lt;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">        exec &quot;! %&lt;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">    elseif &amp;filetype == &#x27;cpp&#x27;</span><br><span class=\"line\"></span><br><span class=\"line\">        exec &quot;!g++ % -o %&lt;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">        exec &quot;! %&lt;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">    elseif &amp;filetype == &#x27;java&#x27; </span><br><span class=\"line\"></span><br><span class=\"line\">        exec &quot;!javac %&quot; </span><br><span class=\"line\"></span><br><span class=\"line\">        exec &quot;!java %&lt;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">    elseif &amp;filetype == &#x27;sh&#x27;</span><br><span class=\"line\"></span><br><span class=\"line\">        :!%</span><br><span class=\"line\"></span><br><span class=\"line\">    endif</span><br><span class=\"line\"></span><br><span class=\"line\">endfunc</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;C,C++的调试</span><br><span class=\"line\"></span><br><span class=\"line\">map &lt;F8&gt; :call Rungdb()&lt;CR&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">func! Rungdb()</span><br><span class=\"line\"></span><br><span class=\"line\">    exec &quot;w&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">    exec &quot;!g++ % -g -o %&lt;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">    exec &quot;!gdb %&lt;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">endfunc</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;&quot;实用设置</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 设置当文件被改动时自动载入</span><br><span class=\"line\"></span><br><span class=\"line\">set autoread</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; quickfix模式</span><br><span class=\"line\"></span><br><span class=\"line\">autocmd FileType c,cpp map &lt;buffer&gt; &lt;leader&gt;&lt;space&gt; :w&lt;cr&gt;:make&lt;cr&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;代码补全 </span><br><span class=\"line\"></span><br><span class=\"line\">set completeopt=preview,menu </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;允许插件  </span><br><span class=\"line\"></span><br><span class=\"line\">filetype plugin on</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;共享剪贴板  </span><br><span class=\"line\"></span><br><span class=\"line\">set clipboard+=unnamed </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;从不备份  </span><br><span class=\"line\"></span><br><span class=\"line\">set nobackup</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;make 运行</span><br><span class=\"line\"></span><br><span class=\"line\">:set makeprg=g++\\ -Wall\\ \\ %</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;自动保存</span><br><span class=\"line\"></span><br><span class=\"line\">set autowrite</span><br><span class=\"line\"></span><br><span class=\"line\">set ruler                   &quot; 打开状态栏标尺</span><br><span class=\"line\"></span><br><span class=\"line\">set cursorline              &quot; 突出显示当前行</span><br><span class=\"line\"></span><br><span class=\"line\">set magic                   &quot; 设置魔术</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;set guioptions-=T           &quot; 隐藏工具栏</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;set guioptions-=m           &quot; 隐藏菜单栏</span><br><span class=\"line\"></span><br><span class=\"line\">set statusline=\\ %&lt;%F[%1*%M%*%n%R%H]%=\\ %y\\ %0(%&#123;&amp;fileformat&#125;\\ %&#123;&amp;encoding&#125;\\ %c:%l/%L%)\\</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 设置在状态行显示的信息</span><br><span class=\"line\"></span><br><span class=\"line\">set foldcolumn=0</span><br><span class=\"line\"></span><br><span class=\"line\">set foldmethod=indent </span><br><span class=\"line\"></span><br><span class=\"line\">set foldlevel=3 </span><br><span class=\"line\"></span><br><span class=\"line\">set foldenable              &quot; 开始折叠</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 不要使用vi的键盘模式，而是vim自己的</span><br><span class=\"line\"></span><br><span class=\"line\">set nocompatible</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 语法高亮</span><br><span class=\"line\"></span><br><span class=\"line\">set syntax=on</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 去掉输入错误的提示声音</span><br><span class=\"line\"></span><br><span class=\"line\">set noeb</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 在处理未保存或只读文件的时候，弹出确认</span><br><span class=\"line\"></span><br><span class=\"line\">set confirm</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 自动缩进</span><br><span class=\"line\"></span><br><span class=\"line\">set autoindent</span><br><span class=\"line\"></span><br><span class=\"line\">set cindent</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; Tab键的宽度</span><br><span class=\"line\"></span><br><span class=\"line\">set tabstop=4</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 统一缩进为4</span><br><span class=\"line\"></span><br><span class=\"line\">set softtabstop=4</span><br><span class=\"line\"></span><br><span class=\"line\">set shiftwidth=4</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 不要用空格代替制表符</span><br><span class=\"line\"></span><br><span class=\"line\">set noexpandtab</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 在行和段开始处使用制表符</span><br><span class=\"line\"></span><br><span class=\"line\">set smarttab</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 显示行号</span><br><span class=\"line\"></span><br><span class=\"line\">set number</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 历史记录数</span><br><span class=\"line\"></span><br><span class=\"line\">set history=1000</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;禁止生成临时文件</span><br><span class=\"line\"></span><br><span class=\"line\">set nobackup</span><br><span class=\"line\"></span><br><span class=\"line\">set noswapfile</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;搜索忽略大小写</span><br><span class=\"line\"></span><br><span class=\"line\">set ignorecase</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;搜索逐字符高亮</span><br><span class=\"line\"></span><br><span class=\"line\">set hlsearch</span><br><span class=\"line\"></span><br><span class=\"line\">set incsearch</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;行内替换</span><br><span class=\"line\"></span><br><span class=\"line\">set gdefault</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;编码设置</span><br><span class=\"line\"></span><br><span class=\"line\">set enc=utf-8</span><br><span class=\"line\"></span><br><span class=\"line\">set fencs=utf-8,ucs-bom,shift-jis,gb18030,gbk,gb2312,cp936</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;语言设置</span><br><span class=\"line\"></span><br><span class=\"line\">set langmenu=zh_CN.UTF-8</span><br><span class=\"line\"></span><br><span class=\"line\">set helplang=cn</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 我的状态行显示的内容（包括文件类型和解码）</span><br><span class=\"line\"></span><br><span class=\"line\">set statusline=%F%m%r%h%w\\ [FORMAT=%&#123;&amp;ff&#125;]\\ [TYPE=%Y]\\ [POS=%l,%v][%p%%]\\ %&#123;strftime(\\&quot;%d/%m/%y\\ -\\ %H:%M\\&quot;)&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">set statusline=[%F]%y%r%m%*%=[Line:%l/%L,Column:%c][%p%%]</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 总是显示状态行</span><br><span class=\"line\"></span><br><span class=\"line\">set laststatus=2</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 命令行（在状态行下）的高度，默认为1，这里是2</span><br><span class=\"line\"></span><br><span class=\"line\">set cmdheight=2</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 侦测文件类型</span><br><span class=\"line\"></span><br><span class=\"line\">filetype on</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 载入文件类型插件</span><br><span class=\"line\"></span><br><span class=\"line\">filetype plugin on</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 为特定文件类型载入相关缩进文件</span><br><span class=\"line\"></span><br><span class=\"line\">filetype indent on</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 保存全局变量</span><br><span class=\"line\"></span><br><span class=\"line\">set viminfo+=!</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 带有如下符号的单词不要被换行分割</span><br><span class=\"line\"></span><br><span class=\"line\">set iskeyword+=_,$,@,%,#,-</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 字符间插入的像素行数目</span><br><span class=\"line\"></span><br><span class=\"line\">set linespace=0</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 增强模式中的命令行自动完成操作</span><br><span class=\"line\"></span><br><span class=\"line\">set wildmenu</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 使回格键（backspace）正常处理indent, eol, start等</span><br><span class=\"line\"></span><br><span class=\"line\">set backspace=2</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 允许backspace和光标键跨越行边界</span><br><span class=\"line\"></span><br><span class=\"line\">set whichwrap+=&lt;,&gt;,h,l</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 可以在buffer的任何地方使用鼠标（类似office中在工作区双击鼠标定位）</span><br><span class=\"line\"></span><br><span class=\"line\">set mouse=a</span><br><span class=\"line\"></span><br><span class=\"line\">set selection=exclusive</span><br><span class=\"line\"></span><br><span class=\"line\">set selectmode=mouse,key</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 通过使用: commands命令，告诉我们文件的哪一行被改变过</span><br><span class=\"line\"></span><br><span class=\"line\">set report=0</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 在被分割的窗口间显示空白，便于阅读</span><br><span class=\"line\"></span><br><span class=\"line\">set fillchars=vert:\\ ,stl:\\ ,stlnc:\\</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 高亮显示匹配的括号</span><br><span class=\"line\"></span><br><span class=\"line\">set showmatch</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 匹配括号高亮的时间（单位是十分之一秒）</span><br><span class=\"line\"></span><br><span class=\"line\">set matchtime=1</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 光标移动到buffer的顶部和底部时保持3行距离</span><br><span class=\"line\"></span><br><span class=\"line\">set scrolloff=3</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 为C程序提供自动缩进</span><br><span class=\"line\"></span><br><span class=\"line\">set smartindent</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 高亮显示普通txt文件（需要txt.vim脚本）</span><br><span class=\"line\"></span><br><span class=\"line\">au BufRead,BufNewFile *  setfiletype txt</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;自动补全</span><br><span class=\"line\"></span><br><span class=\"line\">:inoremap ( ()&lt;ESC&gt;i</span><br><span class=\"line\"></span><br><span class=\"line\">:inoremap ) &lt;c-r&gt;=ClosePair(&#x27;)&#x27;)&lt;CR&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">:inoremap &#123; &#123;&lt;CR&gt;&#125;&lt;ESC&gt;O</span><br><span class=\"line\">&quot;:inoremap &#123; &#123;&#125;&lt;ESC&gt;i</span><br><span class=\"line\"></span><br><span class=\"line\">:inoremap &#125; &lt;c-r&gt;=ClosePair(&#x27;&#125;&#x27;)&lt;CR&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">:inoremap [ []&lt;ESC&gt;i</span><br><span class=\"line\"></span><br><span class=\"line\">:inoremap ] &lt;c-r&gt;=ClosePair(&#x27;]&#x27;)&lt;CR&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">:inoremap &quot; &quot;&quot;&lt;ESC&gt;i</span><br><span class=\"line\"></span><br><span class=\"line\">:inoremap &#x27; &#x27;&#x27;&lt;ESC&gt;i</span><br><span class=\"line\"></span><br><span class=\"line\">function! ClosePair(char)</span><br><span class=\"line\"></span><br><span class=\"line\">    if getline(&#x27;.&#x27;)[col(&#x27;.&#x27;) - 1] == a:char</span><br><span class=\"line\"></span><br><span class=\"line\">        return &quot;\\&lt;Right&gt;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">    else</span><br><span class=\"line\"></span><br><span class=\"line\">        return a:char</span><br><span class=\"line\"></span><br><span class=\"line\">    endif</span><br><span class=\"line\"></span><br><span class=\"line\">endfunction</span><br><span class=\"line\"></span><br><span class=\"line\">filetype plugin indent on </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;打开文件类型检测, 加了这句才可以用智能补全</span><br><span class=\"line\"></span><br><span class=\"line\">set completeopt=longest,menu</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; CTags的设定  </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">let Tlist_Sort_Type = &quot;name&quot;    &quot; 按照名称排序  </span><br><span class=\"line\"></span><br><span class=\"line\">let Tlist_Use_Right_Window = 1  &quot; 在右侧显示窗口  </span><br><span class=\"line\"></span><br><span class=\"line\">let Tlist_Compart_Format = 1    &quot; 压缩方式  </span><br><span class=\"line\"></span><br><span class=\"line\">let Tlist_Exist_OnlyWindow = 1  &quot; 如果只有一个buffer，kill窗口也kill掉buffer  </span><br><span class=\"line\"></span><br><span class=\"line\">let Tlist_File_Fold_Auto_Close = 0  &quot; 不要关闭其他文件的tags  </span><br><span class=\"line\"></span><br><span class=\"line\">let Tlist_Enable_Fold_Column = 0    &quot; 不要显示折叠树  </span><br><span class=\"line\"></span><br><span class=\"line\">autocmd FileType java set tags+=D:\\tools\\java\\tags  </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;autocmd FileType h,cpp,cc,c set tags+=D:\\tools\\cpp\\tags  </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;let Tlist_Show_One_File=1            &quot;不同时显示多个文件的tag，只显示当前文件的</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;设置tags  </span><br><span class=\"line\"></span><br><span class=\"line\">set tags=tags  </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;set autochdir </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;其他东东</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;默认打开Taglist </span><br><span class=\"line\"></span><br><span class=\"line\">let Tlist_Auto_Open=1 </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot; </span><br><span class=\"line\"></span><br><span class=\"line\">&quot; Tag list (ctags) </span><br><span class=\"line\"></span><br><span class=\"line\">&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot; </span><br><span class=\"line\"></span><br><span class=\"line\">let Tlist_Ctags_Cmd = &#x27;/usr/bin/ctags&#x27; </span><br><span class=\"line\"></span><br><span class=\"line\">let Tlist_Show_One_File = 1 &quot;不同时显示多个文件的tag，只显示当前文件的 </span><br><span class=\"line\"></span><br><span class=\"line\">let Tlist_Exit_OnlyWindow = 1 &quot;如果taglist窗口是最后一个窗口，则退出vim </span><br><span class=\"line\"></span><br><span class=\"line\">let Tlist_Use_Right_Window = 1 &quot;在右侧窗口中显示taglist窗口</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; minibufexpl插件的一般设置</span><br><span class=\"line\"></span><br><span class=\"line\">let g:miniBufExplMapWindowNavVim = 1</span><br><span class=\"line\"></span><br><span class=\"line\">let g:miniBufExplMapWindowNavArrows = 1</span><br><span class=\"line\"></span><br><span class=\"line\">let g:miniBufExplMapCTabSwitchBufs = 1</span><br><span class=\"line\">let g:miniBufExplModSelTarget = 1</span><br></pre></td></tr></table></figure>\n<p>然后保存，将文件名改成 <code>.vimrc</code> ，这时候再打开 Vim 看一下效果，如下图所示：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/c803dceaf09d4465889c7ff6c02d5cb1.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5p-D5q2M,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"\"></p>\n<p>配置文件中的每一行代码的功能在边上都有相应的注释（<code>&quot;</code> 之后的内容即为注释），要打开/关闭某一项功能只需在代码的最前面加上/删去 <code>&quot;</code> 即可。<br>\n对于主题，个人喜好的是 <code>elflord</code> 和 <code>desert</code>，字体的话 <code>Consolas</code> yyds！剩下的元素大家可以根据自己的喜好进行调整。<br>\n对于 <code>F5</code> 自动编译功能，如果使用的是 Linux 环境，那么需要将以下红框部分的代码修改为 <code>exec &quot;! ./%&lt;&quot;</code></p>\n<p><img src=\"https://img-blog.csdnimg.cn/9410a5f8b22b4cb89db98f12c14a8cb8.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5p-D5q2M,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"\"></p>\n<h2 id=\"3-Linux下-vimrc文件的配置\">3. Linux下.vimrc文件的配置</h2>\n<h3 id=\"3-1-Onedark主题配置\">3.1 Onedark主题配置</h3>\n<p>本人较为喜欢的主题为 Onedark，因此在本文中介绍一下 Linux 环境下怎么配置 Onedark 主题。</p>\n<p>首先在 <code>~/.vim</code> 文件夹中创建两个文件夹：<code>colors</code> 和 <code>autoload</code>，如果已经有了那么跳过此步。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/.vim</span><br><span class=\"line\">mkdir colors autoload</span><br></pre></td></tr></table></figure>\n<p>然后将<a href=\"https://gitee.com/dglxlcl/onedark.vim\">Onedark主题</a>中的东西下载至 <code>.vim</code> 文件夹中，可以直接使用命令行：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https://gitee.com/dglxlcl/onedark.vim.git</span><br></pre></td></tr></table></figure>\n<p>下载完成后在 <code>.vim</code> 文件夹中有个 <code>onedark.vim</code> 文件夹，将 <code>onedark.vim/colors</code> 文件夹中的 <code>onedark.vim</code> 文件复制到 <code>~/.vim/colors</code> 中，将 <code>onedark.vim/autoload</code> 文件夹中的 <code>onedark.vim</code> 文件复制到 <code>~/.vim/autoload</code> 中：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cp onedark.vim/colors/onedark.vim ~/.vim/colors</span><br><span class=\"line\">cp onedark.vim/autoload/onedark.vim ~/.vim/autoload</span><br></pre></td></tr></table></figure>\n<p>然后修改 <code>.vimrc</code> 文件，将原本的主题设置注释掉，添加一行 <code>colorscheme onedark</code>，至此配置就完成了。</p>\n<h3 id=\"3-2-Linux环境下-vimrc文件推荐\">3.2 Linux环境下.vimrc文件推荐</h3>\n<p>以下为本人目前正在使用的一套 Vim 配置，配置代码如下：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br><span class=\"line\">211</span><br><span class=\"line\">212</span><br><span class=\"line\">213</span><br><span class=\"line\">214</span><br><span class=\"line\">215</span><br><span class=\"line\">216</span><br><span class=\"line\">217</span><br><span class=\"line\">218</span><br><span class=\"line\">219</span><br><span class=\"line\">220</span><br><span class=\"line\">221</span><br><span class=\"line\">222</span><br><span class=\"line\">223</span><br><span class=\"line\">224</span><br><span class=\"line\">225</span><br><span class=\"line\">226</span><br><span class=\"line\">227</span><br><span class=\"line\">228</span><br><span class=\"line\">229</span><br><span class=\"line\">230</span><br><span class=\"line\">231</span><br><span class=\"line\">232</span><br><span class=\"line\">233</span><br><span class=\"line\">234</span><br><span class=\"line\">235</span><br><span class=\"line\">236</span><br><span class=\"line\">237</span><br><span class=\"line\">238</span><br><span class=\"line\">239</span><br><span class=\"line\">240</span><br><span class=\"line\">241</span><br><span class=\"line\">242</span><br><span class=\"line\">243</span><br><span class=\"line\">244</span><br><span class=\"line\">245</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot; An example for a vimrc file.</span><br><span class=\"line\">&quot;</span><br><span class=\"line\">&quot; To use it, copy it to</span><br><span class=\"line\">&quot;     for Unix and OS/2:  ~/.vimrc</span><br><span class=\"line\">&quot;             for Amiga:  s:.vimrc</span><br><span class=\"line\">&quot;  for MS-DOS and Win32:  $VIM\\_vimrc</span><br><span class=\"line\">&quot;           for OpenVMS:  sys$login:.vimrc</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; When started as &quot;evim&quot;, evim.vim will already have done these settings.</span><br><span class=\"line\">if v:progname =~? &quot;evim&quot;</span><br><span class=\"line\">  finish</span><br><span class=\"line\">endif</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; Use Vim settings, rather then Vi settings (much better!).</span><br><span class=\"line\">&quot; This must be first, because it changes other options as a side effect.</span><br><span class=\"line\">set nocompatible</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; allow backspacing over everything in insert mode</span><br><span class=\"line\">set backspace=indent,eol,start</span><br><span class=\"line\"></span><br><span class=\"line\">if has(&quot;vms&quot;)</span><br><span class=\"line\">  set nobackup          &quot; do not keep a backup file, use versions instead</span><br><span class=\"line\">else</span><br><span class=\"line\">  set backup            &quot; keep a backup file</span><br><span class=\"line\">endif</span><br><span class=\"line\">set history=50          &quot; keep 50 lines of command line history</span><br><span class=\"line\">set ruler               &quot; show the cursor position all the time</span><br><span class=\"line\">set showcmd             &quot; display incomplete commands</span><br><span class=\"line\">set incsearch           &quot; do incremental searching</span><br><span class=\"line\">&quot;==========================================================================</span><br><span class=\"line\">&quot;My Setting-sunshanlu</span><br><span class=\"line\">&quot;==========================================================================</span><br><span class=\"line\">vmap &lt;leader&gt;y :w! /tmp/vitmp&lt;CR&gt;</span><br><span class=\"line\">nmap &lt;leader&gt;p :r! cat /tmp/vitmp&lt;CR&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;语法高亮</span><br><span class=\"line\">syntax enable</span><br><span class=\"line\">syntax on</span><br><span class=\"line\">&quot;显示行号</span><br><span class=\"line\">set nu</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;修改默认注释颜色</span><br><span class=\"line\">&quot;hi Comment ctermfg=DarkCyan</span><br><span class=\"line\">&quot;允许退格键删除</span><br><span class=\"line\">&quot;set backspace=2</span><br><span class=\"line\">&quot;启用鼠标</span><br><span class=\"line\">set mouse=a</span><br><span class=\"line\">set selection=exclusive</span><br><span class=\"line\">set selectmode=mouse,key</span><br><span class=\"line\">&quot;按C语言格式缩进</span><br><span class=\"line\">set cindent</span><br><span class=\"line\">set autoindent</span><br><span class=\"line\">set smartindent</span><br><span class=\"line\">set shiftwidth=4</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 允许在有未保存的修改时切换缓冲区</span><br><span class=\"line\">&quot;set hidden</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 设置无备份文件</span><br><span class=\"line\">set writebackup</span><br><span class=\"line\">set nobackup</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;显示括号匹配</span><br><span class=\"line\">set showmatch</span><br><span class=\"line\">&quot;括号匹配显示时间为1(单位是十分之一秒)</span><br><span class=\"line\">set matchtime=5</span><br><span class=\"line\">&quot;显示当前的行号列号：</span><br><span class=\"line\">set ruler</span><br><span class=\"line\">&quot;在状态栏显示正在输入的命令</span><br><span class=\"line\">set showcmd</span><br><span class=\"line\"></span><br><span class=\"line\">set foldmethod=syntax</span><br><span class=\"line\">&quot;默认情况下不折叠</span><br><span class=\"line\">set foldlevel=100</span><br><span class=\"line\">&quot; 开启状态栏信息</span><br><span class=\"line\">set laststatus=2</span><br><span class=\"line\">&quot; 命令行的高度，默认为1，这里设为2</span><br><span class=\"line\">set cmdheight=2</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 显示Tab符，使用一高亮竖线代替</span><br><span class=\"line\">set list</span><br><span class=\"line\">&quot;set listchars=tab:\\|\\ ,</span><br><span class=\"line\">set listchars=tab:&gt;-,trail:-</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">&quot;侦测文件类型</span><br><span class=\"line\">filetype on</span><br><span class=\"line\">&quot;载入文件类型插件</span><br><span class=\"line\">filetype plugin on</span><br><span class=\"line\">&quot;为特定文件类型载入相关缩进文件</span><br><span class=\"line\">filetype indent on</span><br><span class=\"line\">&quot; 启用自动补全</span><br><span class=\"line\">filetype plugin indent on </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">&quot;设置编码自动识别, 中文引号显示</span><br><span class=\"line\">filetype on &quot;打开文件类型检测</span><br><span class=\"line\">&quot;set fileencodings=euc-cn,ucs-bom,utf-8,cp936,gb2312,gb18030,gbk,big5,euc-jp,euc-kr,latin1</span><br><span class=\"line\">set fileencodings=utf-8,gb2312,gbk,gb18030</span><br><span class=\"line\">&quot;这个用能很给劲，不管encoding是什么编码，都能将文本显示汉字</span><br><span class=\"line\">&quot;set termencoding=gb2312</span><br><span class=\"line\">set termencoding=utf-8</span><br><span class=\"line\">&quot;新建文件使用的编码</span><br><span class=\"line\">set fileencoding=utf-8</span><br><span class=\"line\">&quot;set fileencoding=gb2312</span><br><span class=\"line\">&quot;用于显示的编码，仅仅是显示</span><br><span class=\"line\">set encoding=utf-8</span><br><span class=\"line\">&quot;set encoding=utf-8</span><br><span class=\"line\">&quot;set encoding=euc-cn</span><br><span class=\"line\">&quot;set encoding=gbk</span><br><span class=\"line\">&quot;set encoding=gb2312</span><br><span class=\"line\">&quot;set ambiwidth=double</span><br><span class=\"line\">set fileformat=unix</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">&quot;设置高亮搜索</span><br><span class=\"line\">set hlsearch</span><br><span class=\"line\">&quot;在搜索时，输入的词句的逐字符高亮</span><br><span class=\"line\">set incsearch</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 着色模式</span><br><span class=\"line\">set t_Co=256</span><br><span class=\"line\">&quot;colorscheme wombat256mod</span><br><span class=\"line\">&quot;colorscheme gardener</span><br><span class=\"line\">&quot;colorscheme elflord</span><br><span class=\"line\">&quot;colorscheme desert</span><br><span class=\"line\">&quot;colorscheme evening</span><br><span class=\"line\">&quot;colorscheme darkblue</span><br><span class=\"line\">&quot;colorscheme torte</span><br><span class=\"line\">&quot;colorscheme default</span><br><span class=\"line\">colorscheme onedark</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 字体 &amp;&amp; 字号</span><br><span class=\"line\">set guifont=Monaco:h10</span><br><span class=\"line\">&quot;set guifont=Consolas:h10</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; :LoadTemplate       根据文件后缀自动加载模板</span><br><span class=\"line\">&quot;let g:template_path=&#x27;/home/ruchee/.vim/template/&#x27;</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; :AuthorInfoDetect   自动添加作者、时间等信息，本质是NERD_commenter &amp;&amp; authorinfo的结合</span><br><span class=\"line\">&quot;&quot;let g:vimrc_author=&#x27;sunshanlu&#x27;</span><br><span class=\"line\">&quot;&quot;let g:vimrc_email=&#x27;sunshanlu@baidu.com&#x27;</span><br><span class=\"line\">&quot;&quot;let g:vimrc_homepage=&#x27;http://www.sunshanlu.com&#x27;</span><br><span class=\"line\">&quot;</span><br><span class=\"line\">&quot;</span><br><span class=\"line\">&quot; Ctrl + E            一步加载语法模板和作者、时间信息</span><br><span class=\"line\">&quot;&quot;map &lt;c-e&gt; &lt;ESC&gt;:AuthorInfoDetect&lt;CR&gt;&lt;ESC&gt;Gi</span><br><span class=\"line\">&quot;&quot;imap &lt;c-e&gt; &lt;ESC&gt;:AuthorInfoDetect&lt;CR&gt;&lt;ESC&gt;Gi</span><br><span class=\"line\">&quot;&quot;vmap &lt;c-e&gt; &lt;ESC&gt;:AuthorInfoDetect&lt;CR&gt;&lt;ESC&gt;Gi</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">&quot; ======= 引号 &amp;&amp; 括号自动匹配 ======= &quot;</span><br><span class=\"line\">&quot;</span><br><span class=\"line\">:inoremap ( ()&lt;ESC&gt;i</span><br><span class=\"line\"></span><br><span class=\"line\">&quot;:inoremap ) &lt;c-r&gt;=ClosePair(&#x27;)&#x27;)&lt;CR&gt;</span><br><span class=\"line\">&quot;</span><br><span class=\"line\">:inoremap &#123; &#123;&#125;&lt;ESC&gt;i</span><br><span class=\"line\">&quot;</span><br><span class=\"line\">&quot;:inoremap &#125; &lt;c-r&gt;=ClosePair(&#x27;&#125;&#x27;)&lt;CR&gt;</span><br><span class=\"line\">&quot;</span><br><span class=\"line\">:inoremap [ []&lt;ESC&gt;i</span><br><span class=\"line\">&quot;</span><br><span class=\"line\">&quot;:inoremap ] &lt;c-r&gt;=ClosePair(&#x27;]&#x27;)&lt;CR&gt;</span><br><span class=\"line\">&quot;</span><br><span class=\"line\">&quot;:inoremap &lt; &lt;&gt;&lt;ESC&gt;i</span><br><span class=\"line\">&quot;</span><br><span class=\"line\">&quot;:inoremap &gt; &lt;c-r&gt;=ClosePair(&#x27;&gt;&#x27;)&lt;CR&gt;</span><br><span class=\"line\">&quot;</span><br><span class=\"line\">:inoremap &quot; &quot;&quot;&lt;ESC&gt;i</span><br><span class=\"line\">&quot;</span><br><span class=\"line\">:inoremap &#x27; &#x27;&#x27;&lt;ESC&gt;i</span><br><span class=\"line\">&quot;</span><br><span class=\"line\">&quot;:inoremap ` ``&lt;ESC&gt;i</span><br><span class=\"line\">&quot;</span><br><span class=\"line\">&quot;:inoremap * **&lt;ESC&gt;i</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 每行超过80个的字符用下划线标示</span><br><span class=\"line\">&quot;&quot;au BufRead,BufNewFile *.s,*.asm,*.h,*.c,*.cpp,*.java,*.cs,*.lisp,*.el,*.erl,*.tex,*.sh,*.lua,*.pl,*.php,*.tpl,*.py,*.rb,*.erb,*.vim,*.js,*.jade,*.coffee,*.css,*.xml,*.html,*.shtml,*.xhtml Underlined /.\\%81v/</span><br><span class=\"line\">&quot;</span><br><span class=\"line\">&quot;</span><br><span class=\"line\">&quot; For Win32 GUI: remove &#x27;t&#x27; flag from &#x27;guioptions&#x27;: no tearoff menu entries</span><br><span class=\"line\">&quot; let &amp;guioptions = substitute(&amp;guioptions, &quot;t&quot;, &quot;&quot;, &quot;g&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; Don&#x27;t use Ex mode, use Q for formatting</span><br><span class=\"line\">map Q gq</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; This is an alternative that also works in block mode, but the deleted</span><br><span class=\"line\">&quot; text is lost and it only works for putting the current register.</span><br><span class=\"line\">&quot;vnoremap p &quot;_dp</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; Switch syntax highlighting on, when the terminal has colors</span><br><span class=\"line\">&quot; Also switch on highlighting the last used search pattern.</span><br><span class=\"line\">if &amp;t_Co &gt; 2 || has(&quot;gui_running&quot;)</span><br><span class=\"line\">  syntax on</span><br><span class=\"line\">  set hlsearch</span><br><span class=\"line\">endif</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; Only do this part when compiled with support for autocommands.</span><br><span class=\"line\">if has(&quot;autocmd&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">  &quot; Enable file type detection.</span><br><span class=\"line\">  &quot; Use the default filetype settings, so that mail gets &#x27;tw&#x27; set to 72,</span><br><span class=\"line\">  &quot; &#x27;cindent&#x27; is on in C files, etc.</span><br><span class=\"line\">  &quot; Also load indent files, to automatically do language-dependent indenting.</span><br><span class=\"line\">  filetype plugin indent on</span><br><span class=\"line\"></span><br><span class=\"line\">  &quot; Put these in an autocmd group, so that we can delete them easily.</span><br><span class=\"line\">  augroup vimrcEx</span><br><span class=\"line\">  au!</span><br><span class=\"line\"></span><br><span class=\"line\">  &quot; For all text files set &#x27;textwidth&#x27; to 80 characters.</span><br><span class=\"line\">  autocmd FileType text setlocal textwidth=80</span><br><span class=\"line\"></span><br><span class=\"line\">  &quot; When editing a file, always jump to the last known cursor position.</span><br><span class=\"line\">  &quot; Don&#x27;t do it when the position is invalid or when inside an event handler</span><br><span class=\"line\">  &quot; (happens when dropping a file on gvim).</span><br><span class=\"line\">  autocmd BufReadPost *</span><br><span class=\"line\">    \\ if line(&quot;&#x27;\\&quot;&quot;) &gt; 0 &amp;&amp; line(&quot;&#x27;\\&quot;&quot;) &lt;= line(&quot;$&quot;) |</span><br><span class=\"line\">    \\   exe &quot;normal g`\\&quot;&quot; |</span><br><span class=\"line\">    \\ endif</span><br><span class=\"line\"></span><br><span class=\"line\">  augroup END</span><br><span class=\"line\"></span><br><span class=\"line\">else</span><br><span class=\"line\"></span><br><span class=\"line\">  set autoindent                &quot; always set autoindenting on</span><br><span class=\"line\"></span><br><span class=\"line\">endif &quot; has(&quot;autocmd&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 增加鼠标行高亮</span><br><span class=\"line\">set cursorline</span><br><span class=\"line\">&quot;hi CursorLine  cterm=NONE   ctermbg=darkgrey ctermfg=lightblue</span><br><span class=\"line\">&quot;hi CursorLine  cterm=NONE   ctermbg=darkmagenta ctermfg=lightyellow</span><br><span class=\"line\">hi CursorLine  cterm=NONE   ctermbg=darkgrey ctermfg=lightcyan</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 设置tab是四个空格</span><br><span class=\"line\">set ts=4</span><br><span class=\"line\">set expandtab</span><br><span class=\"line\"></span><br><span class=\"line\">&quot; 主要给Tlist使用</span><br><span class=\"line\">let Tlist_Exit_OnlyWindow = 1</span><br><span class=\"line\">let Tlist_Auto_Open = 1</span><br></pre></td></tr></table></figure>",
            "tags": [
                "Others"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/11062.html",
            "url": "https://asanosaki.github.io/posts/11062.html",
            "title": "NodeJS的安装及配置",
            "date_published": "2021-10-13T03:38:00.000Z",
            "content_html": "<blockquote>\n<p>Node.js 是一个基于 Chrome V8 引擎的 JavaScript 运行环境。本文介绍如何安装与配置Node.js。<br>\n使用教程见官方文档：<a href=\"https://nodejs.org/zh-cn/docs/\">Node.js Docs</a>。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"一、官网下载及安装NodeJS\">一、官网下载及安装NodeJS</h2>\n<p>官网下载地址：<a href=\"https://nodejs.org/en/download/\">Node.js Download</a></p>\n<p>本文的安装路径为：<code>E:\\NodeJS</code>。</p>\n<p>安装完成后打开命令行窗口校验版本：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">node -v</span><br></pre></td></tr></table></figure>\n<p>测试 <code>npm</code> 是否安装成功，由于新版的NodeJS已经集成了 <code>npm</code>，所以之前 <code>npm</code> 也一并安装好了，同样可以使用命令行窗口校验：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm -v</span><br></pre></td></tr></table></figure>\n<h2 id=\"二、环境变量配置\">二、环境变量配置</h2>\n<p>在 <code>E:\\NodeJS</code> 目录下新建两个文件夹：<code>node_global</code> 和 <code>node_cache</code>：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/c781cdc825714e9585c9cfdb05628a38.png\" alt=\"\"></p>\n<p>长按 <code>Shift+鼠标右键</code>，选择打开 PowerShell 窗口或者打开 Git Bash：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/2ad041f533684b389018a91a76ab5c55.png\" alt=\"\"></p>\n<p>输入以下命令：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm config set prefix &quot;E:\\NodeJS\\node_global&quot;</span><br><span class=\"line\">npm config set cache &quot;E:\\NodeJS\\node_cache&quot;</span><br></pre></td></tr></table></figure>\n<p>接着配置环境变量，右键&quot;我的电脑&quot;-“属性”-“高级系统设置”，点击&quot;高级&quot;选项卡，选择&quot;环境变量&quot;进行配置。在&quot;系统变量&quot;下点击&quot;新建&quot;，变量名和路径如下图所示：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/d023c5e811634ab9991e549854c30dde.png\" alt=\"\"></p>\n<p>在&quot;用户变量&quot;的 <code>Path</code> 变量上点击&quot;编辑&quot;，更改 <code>npm</code> 的默认路径，如下图所示：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/403a8a345cd64379800881204213a3a6.png\" alt=\"\"></p>\n<p><img src=\"https://img-blog.csdnimg.cn/d619a11d117847db9235b59f085882f3.png\" alt=\"\"></p>\n<p>配置完成后，在命令行窗口中安装相关环境（若安装出错可用管理员身份打开 <code>cmd</code> 窗口）</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install express -g</span><br><span class=\"line\">npm install jade -g</span><br><span class=\"line\">npm install mysql -g</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://img-blog.csdnimg.cn/735917a5a318431aa197f5747303210a.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5p-D5q2M,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"\"></p>\n",
            "tags": [
                "Others"
            ]
        },
        {
            "id": "https://asanosaki.github.io/posts/46520.html",
            "url": "https://asanosaki.github.io/posts/46520.html",
            "title": "算法竞赛C++ STL详解",
            "date_published": "2021-10-05T09:40:00.000Z",
            "content_html": "<blockquote>\n<p>本文介绍了什么是 STL 以及如何使用 STL 更高效<s>偷懒</s>地解题。本篇文章将会长期更新，欢迎大家一起监督学习。</p>\n</blockquote>\n<span id=\"more\"></span>\n<h2 id=\"一、STL概念\">一、STL概念</h2>\n<p>STL（<code>Standard Template Library</code>，标准模板库），是惠普实验室开发的一系列软件的统称。现主要出现在 C++ 中，STL 从广义上分为：容器（<code>Container</code>）、算法（<code>Algorithm</code>）和迭代器（<code>Iterator</code>）。STL 几乎所有的代码都采用了<strong>模板类或者模板函数</strong>，这相比传统的由函数和类组成的库来说提供了更好的代码重用机会。</p>\n<h2 id=\"二、STL六大组件\">二、STL六大组件</h2>\n<p>STL 提供了六大组件，彼此之间可以组合套用，这六大组件分别是容器、算法、迭代器、仿函数、适配器、空间配置器。其中，在算法竞赛中用到最多的为<strong>容器、算法与迭代器</strong>。</p>\n<ul>\n<li>容器（<code>Container</code>）：STL 容器为各种<strong>数据结构</strong>，如 <code>vector</code>、<code>stack</code>、<code>queue</code>、<code>map</code>、<code>set</code> 等，用来存放数据，从实现角度来看，STL 容器是一种 <code>class template</code>。</li>\n<li>算法（<code>Algorithm</code>）：STL 的算法多数定义在 <code>&lt;algorithm&gt;</code> 头文件中，其中包括了各种常用的算法，如 <code>sort</code>、<code>find</code>、<code>copy</code>、<code>reverse</code> 等，从实现角度来看，STL 算法是一种 <code>function template</code>。</li>\n<li>迭代器（<code>Iterator</code>）：STL 迭代器扮演了容器与算法之间的胶合剂，共有五种类型，从实现角度来看，迭代器是一种将 <code>opetator*</code>、<code>opetator-&gt;</code>、<code>operator++</code> 等指针相关操作予以重载的 <code>class template</code>。所有 STL 容器都附带有自己专属的迭代器，只有容器的设计者才知道如何遍历自己的元素。</li>\n<li>仿函数（<code>Functor</code>）：行为类似函数，可作为算法的某种策略，从实现角度来看，仿函数是一种重载了 <code>operator()</code> 的 <code>class</code> 或者 <code>class template</code>。</li>\n<li>适配器（<code>Adaptor</code>）：一种用来修饰容器或仿函数或迭代器接口的东西。</li>\n<li>空间配置器（<code>Allocator</code>）：负责空间的配置与管理。从实现角度来看，配置器是一个实现了动态空间配置、空间管理、空间释放的 <code>class template</code>。</li>\n</ul>\n<h2 id=\"三、STL容器\">三、STL容器</h2>\n<p>相信很多人学习 STL 就是为了在比赛中能够更好地<s>装B</s>运用各种数据结构和算法，提高解题速度。确实，使用 STL 中的容器能够不需要自己手写定义各种数据结构，使用 STL 中的算法能够不需要自己手写实现各种基本算法，因此本部分对于算法巨巨们是最为重要的一部分，那么 STL 容器究竟有哪些呢？在做题中该如何使用呢？</p>\n<h3 id=\"3-1-vector\">3.1 vector</h3>\n<p><code>vector</code> 又称<strong>变长数组</strong>，定义在 <code>&lt;vector&gt;</code> 头文件中，<code>vector</code> 容器是<strong>动态空间</strong>，随着元素的加入，它的内部机制会自动扩充空间以容纳新的元素。因此 <code>vector</code> 的运用对于内存的合理利用与运用的灵活性有很大的帮助。</p>\n<ul>\n<li><code>vector</code> 的定义方式：</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vector&lt;<span class=\"type\">int</span>&gt; v;  <span class=\"comment\">// 定义一个vector，其中的元素为int类型</span></span><br><span class=\"line\">vector&lt;<span class=\"type\">int</span>&gt; v[N];  <span class=\"comment\">// 定义一个vector数组，其中有N个vector</span></span><br><span class=\"line\"><span class=\"function\">vector&lt;<span class=\"type\">int</span>&gt; <span class=\"title\">v</span><span class=\"params\">(len)</span></span>;  <span class=\"comment\">// 定义一个长度为len的vector</span></span><br><span class=\"line\"><span class=\"function\">vector&lt;<span class=\"type\">int</span>&gt; <span class=\"title\">v</span><span class=\"params\">(len, x)</span></span>;  <span class=\"comment\">// 定义一个长度为len的vector，初始化每个元素为x</span></span><br><span class=\"line\"><span class=\"function\">vector&lt;<span class=\"type\">int</span>&gt; <span class=\"title\">v2</span><span class=\"params\">(v1)</span></span>;  <span class=\"comment\">// 用v1给v2赋值，v1的类型为vector</span></span><br><span class=\"line\"><span class=\"function\">vector&lt;<span class=\"type\">int</span>&gt; <span class=\"title\">v2</span><span class=\"params\">(v1.begin(), v1.begin() + <span class=\"number\">3</span>)</span></span>;  <span class=\"comment\">// 将v1中第0~2三个元素赋值给v2</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li><code>vector</code> 的常用内置函数：</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// vector中的常用内置函数</span></span><br><span class=\"line\">vector&lt;<span class=\"type\">int</span>&gt; v = &#123; <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span> &#125;;  <span class=\"comment\">// 初始化vector，v:&#123;1, 2, 3&#125;</span></span><br><span class=\"line\">vector&lt;<span class=\"type\">int</span>&gt;::iterator it = v.<span class=\"built_in\">begin</span>();  <span class=\"comment\">// 定义vector的迭代器，指向begin()</span></span><br><span class=\"line\"></span><br><span class=\"line\">v.<span class=\"built_in\">push_back</span>(<span class=\"number\">4</span>);  <span class=\"comment\">// 在vector的尾部插入元素4，v:&#123;1, 2, 3, 4&#125;</span></span><br><span class=\"line\">v.<span class=\"built_in\">pop_back</span>();  <span class=\"comment\">// 删除vector的最后一个元素，v:&#123;1, 2, 3&#125;</span></span><br><span class=\"line\"><span class=\"comment\">// 注意使用lower_bound()与upper_bound()函数时vector必须是有序的，upper_bound()在&lt;algorithm&gt;中</span></span><br><span class=\"line\"><span class=\"built_in\">lower_bound</span>(v.<span class=\"built_in\">begin</span>(), v.<span class=\"built_in\">end</span>(), <span class=\"number\">2</span>);  <span class=\"comment\">// 返回第一个大于等于2的元素的迭代器v.begin() + 1，若不存在则返回v.end()</span></span><br><span class=\"line\"><span class=\"built_in\">upper_bound</span>(v.<span class=\"built_in\">begin</span>(), v.<span class=\"built_in\">end</span>(), <span class=\"number\">2</span>);  <span class=\"comment\">// 返回第一个大于2的元素的迭代器v.begin() + 2，若不存在则返回v.end()</span></span><br><span class=\"line\">v.<span class=\"built_in\">size</span>();  <span class=\"comment\">// 返回vector中元素的个数</span></span><br><span class=\"line\">v.<span class=\"built_in\">empty</span>();  <span class=\"comment\">// 返回vector是否为空，若为空则返回true否则返回false</span></span><br><span class=\"line\">v.<span class=\"built_in\">front</span>();  <span class=\"comment\">// 返回vector中的第一个元素</span></span><br><span class=\"line\">v.<span class=\"built_in\">back</span>();  <span class=\"comment\">// 返回vector中的最后一个元素</span></span><br><span class=\"line\">v.<span class=\"built_in\">begin</span>();  <span class=\"comment\">// 返回vector第一个元素的迭代器</span></span><br><span class=\"line\">v.<span class=\"built_in\">end</span>();  <span class=\"comment\">// 返回vector最后一个元素后一个位置的迭代器</span></span><br><span class=\"line\">v.<span class=\"built_in\">clear</span>();  <span class=\"comment\">// 清空vector</span></span><br><span class=\"line\">v.<span class=\"built_in\">erase</span>(v.<span class=\"built_in\">begin</span>());  <span class=\"comment\">// 删除迭代器it所指向的元素，即删除第一个元素</span></span><br><span class=\"line\">v.<span class=\"built_in\">erase</span>(v.<span class=\"built_in\">begin</span>(), v.<span class=\"built_in\">begin</span>() + <span class=\"number\">2</span>);  <span class=\"comment\">// 删除区间[v.begin(), v.begin() + 2)的所有元素</span></span><br><span class=\"line\">v.<span class=\"built_in\">insert</span>(v.<span class=\"built_in\">begin</span>(), <span class=\"number\">1</span>);  <span class=\"comment\">// 在迭代器it所指向的位置前插入元素1，返回插入元素的迭代器</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 根据下标进行遍历</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; v.<span class=\"built_in\">size</span>(); i++)</span><br><span class=\"line\">    cout &lt;&lt; v[i] &lt;&lt; <span class=\"string\">&#x27; &#x27;</span>;</span><br><span class=\"line\"><span class=\"comment\">// 使用迭代器遍历</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (vector&lt;<span class=\"type\">int</span>&gt;::iterator it = v.<span class=\"built_in\">begin</span>(); it != v.<span class=\"built_in\">end</span>(); it++)</span><br><span class=\"line\">    cout &lt;&lt; *it &lt;&lt; <span class=\"string\">&#x27; &#x27;</span>;</span><br><span class=\"line\"><span class=\"comment\">// for_each遍历(C++11)</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> x : v)</span><br><span class=\"line\">    cout &lt;&lt; x &lt;&lt; <span class=\"string\">&#x27; &#x27;</span>;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-2-stack\">3.2 stack</h3>\n<p><code>stack</code> 又称<strong>栈</strong>，是一种<strong>后进先出</strong>（Last In First Out，LIFO）的数据结构，定义在 <code>&lt;stack&gt;</code> 头文件中，<code>stack</code> 容器允许新增元素、移除元素、取得栈顶元素，但是除了最顶端以外，没有任何方法可以存取 <code>stack</code> 的其它元素，换言之，<code>stack</code> <strong>不允许有遍历行为</strong>。</p>\n<ul>\n<li><code>stack</code> 的定义方式：</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">stack&lt;<span class=\"type\">int</span>&gt; stk;  <span class=\"comment\">// 定义一个stack，其中元素的类型为int</span></span><br><span class=\"line\">stack&lt;<span class=\"type\">int</span>&gt; stk[N];  <span class=\"comment\">// 定义一个stack数组，其中有N个stack</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li><code>stack</code> 的常用内置函数：</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// stack中的常用内置函数</span></span><br><span class=\"line\">stack&lt;<span class=\"type\">int</span>&gt; stk;</span><br><span class=\"line\">stk.<span class=\"built_in\">push</span>(x);  <span class=\"comment\">// 在stack中插入元素x</span></span><br><span class=\"line\">stk.<span class=\"built_in\">pop</span>();  <span class=\"comment\">// 弹出stack的栈顶元素</span></span><br><span class=\"line\">stk.<span class=\"built_in\">top</span>();  <span class=\"comment\">// 返回stack的栈顶元素</span></span><br><span class=\"line\">stk.<span class=\"built_in\">size</span>();  <span class=\"comment\">// 返回stack中元素的个数</span></span><br><span class=\"line\">stk.<span class=\"built_in\">empty</span>();  <span class=\"comment\">// 返回stack是否为空，若为空则返回true否则返回false</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"3-3-string\">3.3 string</h3>\n<p><code>string</code> 又称<strong>字符串</strong>，定义在 <code>&lt;string&gt;</code> 头文件中。C 风格的字符串（以空字符结尾的字符数组）太过复杂难于掌握，因此 C++ 标准库定义了一种 <code>string</code> 类。<code>string</code> 和 <code>vector&lt;char&gt;</code> 在数据结构、内存管理等方面都是相同的。但是，<code>vector&lt;char&gt;</code> 只是单纯的一个“<code>char</code> 元素的容器”，而 <code>string</code> 不仅是一个“<code>char</code> 元素的容器”，它还扩展了一些针对字符串的操作，例如 <code>string</code> 可以使用 <code>c_str()</code> 函数转换为 C 风格的字符串，<code>vector</code> 中并未对输入输出流操作符进行重载，因此无法直接对 <code>vector&lt;char&gt;</code> 进行 <code>cin</code> 或者 <code>cout</code> 这样的操作，但是 <code>string</code> 可以，且 <code>vector&lt;char&gt;</code> 并不能直接实现字符串的拼接，但是 <code>string</code> 可以，<code>string</code> 中重载了 <code>+, +=</code> 运算符。</p>\n<ul>\n<li><code>string</code> 的定义方式：</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">string str;  <span class=\"comment\">// 定义一个空的字符串</span></span><br><span class=\"line\">string str[N];  <span class=\"comment\">// 定义一个string数组，其中有N个string</span></span><br><span class=\"line\"><span class=\"function\">string <span class=\"title\">str</span><span class=\"params\">(<span class=\"number\">5</span>, <span class=\"string\">&#x27;a&#x27;</span>)</span></span>;  <span class=\"comment\">// 使用5个字符&#x27;a&#x27;初始化</span></span><br><span class=\"line\"><span class=\"function\">string <span class=\"title\">str</span><span class=\"params\">(<span class=\"string\">&quot;abc&quot;</span>)</span></span>;  <span class=\"comment\">// 使用字符串初始化</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li><code>string</code> 的常用内置函数：</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// string中的常用内置函数</span></span><br><span class=\"line\"><span class=\"function\">string <span class=\"title\">str</span><span class=\"params\">(<span class=\"string\">&quot;abcabc&quot;</span>)</span></span>;</span><br><span class=\"line\">str.<span class=\"built_in\">push_back</span>(<span class=\"string\">&#x27;d&#x27;</span>);  <span class=\"comment\">// 在string尾部插入字符，&quot;abcabcd&quot;</span></span><br><span class=\"line\">str.<span class=\"built_in\">pop_back</span>();  <span class=\"comment\">// 删除string尾部的字符，&quot;abcabc&quot;</span></span><br><span class=\"line\">str.<span class=\"built_in\">length</span>();  <span class=\"comment\">// 返回string中字符的个数</span></span><br><span class=\"line\">str.<span class=\"built_in\">size</span>();  <span class=\"comment\">// 作用与length()相同</span></span><br><span class=\"line\">str.<span class=\"built_in\">empty</span>();  <span class=\"comment\">// 返回string是否为空，若为空返回true否则返回false</span></span><br><span class=\"line\">str.<span class=\"built_in\">substr</span>(<span class=\"number\">1</span>);  <span class=\"comment\">// 返回string中从下标为1开始至末尾的子串，&quot;bcabc&quot;</span></span><br><span class=\"line\">str.<span class=\"built_in\">substr</span>(<span class=\"number\">0</span>, <span class=\"number\">2</span>);  <span class=\"comment\">// 返回string中从下标为0开始长度为2的子串，&quot;ab&quot;</span></span><br><span class=\"line\">str.<span class=\"built_in\">insert</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"string\">&#x27;x&#x27;</span>);  <span class=\"comment\">// 在下标为1的字符前插入2个字符&#x27;x&#x27;，&quot;axxbcabc&quot;</span></span><br><span class=\"line\">str.<span class=\"built_in\">insert</span>(<span class=\"number\">1</span>, <span class=\"string\">&quot;yy&quot;</span>);  <span class=\"comment\">// 在下标为1的字符前插入字符串&quot;yy&quot;，&quot;ayyxxbcabc&quot;</span></span><br><span class=\"line\">str.<span class=\"built_in\">erase</span>(<span class=\"number\">1</span>, <span class=\"number\">4</span>);  <span class=\"comment\">// 删除从位置1开始的4个字符，&quot;abcabc&quot;</span></span><br><span class=\"line\">str.<span class=\"built_in\">find</span>(<span class=\"string\">&#x27;b&#x27;</span>);  <span class=\"comment\">// 返回字符&#x27;b&#x27;在string中第一次出现的位置，返回1，若不存在则返回-1</span></span><br><span class=\"line\">str.<span class=\"built_in\">find</span>(<span class=\"string\">&#x27;b&#x27;</span>, <span class=\"number\">2</span>);  <span class=\"comment\">// 返回从位置2开始字符&#x27;b&#x27;在string中第一次出现的位置，返回4</span></span><br><span class=\"line\">str.<span class=\"built_in\">find</span>(<span class=\"string\">&quot;bc&quot;</span>);  <span class=\"comment\">// 同上，返回字符串第一次出现的位置，返回1，若不存在则返回-1</span></span><br><span class=\"line\">str.<span class=\"built_in\">find</span>(<span class=\"string\">&quot;bc&quot;</span>, <span class=\"number\">2</span>);  <span class=\"comment\">// 返回4</span></span><br><span class=\"line\">str.<span class=\"built_in\">rfind</span>(<span class=\"string\">&#x27;b&#x27;</span>);  <span class=\"comment\">// 反向查找，原理同上，返回4，若不存在则返回-1</span></span><br><span class=\"line\">str.<span class=\"built_in\">rfind</span>(<span class=\"string\">&#x27;b&#x27;</span>, <span class=\"number\">3</span>);  <span class=\"comment\">// 返回1</span></span><br><span class=\"line\">str.<span class=\"built_in\">rfind</span>(<span class=\"string\">&quot;bc&quot;</span>);  <span class=\"comment\">// 返回4，若不存在则返回-1</span></span><br><span class=\"line\">str.<span class=\"built_in\">rfind</span>(<span class=\"string\">&quot;bc&quot;</span>, <span class=\"number\">3</span>);  <span class=\"comment\">// 返回1</span></span><br><span class=\"line\"><span class=\"built_in\">stoi</span>(str);  <span class=\"comment\">// 返回str的整数形式</span></span><br><span class=\"line\"><span class=\"built_in\">to_string</span>(value);  <span class=\"comment\">// 返回value的字符串形式，value为整型、浮点型等</span></span><br><span class=\"line\">str[<span class=\"number\">0</span>];  <span class=\"comment\">// 用下标访问string中的字符</span></span><br><span class=\"line\">cout &lt;&lt; (str == str) &lt;&lt; endl;  <span class=\"comment\">// string可比较大小，按字典序</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li><code>string</code> 的 <code>erase()</code> 与 <code>remove()</code> 函数的用法：</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// string中erase()与remove()的用法</span></span><br><span class=\"line\">string str1, str2, str3, str4, str5;</span><br><span class=\"line\">str1 = str2 = str3 = str4 = str5 = <span class=\"string\">&quot;I love AcWing! It&#x27;s very funny!&quot;</span>;</span><br><span class=\"line\">str1.<span class=\"built_in\">erase</span>(<span class=\"number\">15</span>);  <span class=\"comment\">// 删除[15,end())的所有元素，&quot;I love AcWing!&quot;</span></span><br><span class=\"line\">str2.<span class=\"built_in\">erase</span>(<span class=\"number\">6</span>, <span class=\"number\">11</span>);  <span class=\"comment\">// 从第6个元素(包括)开始往后删除11个元素，&quot;I love&#x27;s very funny!&quot;</span></span><br><span class=\"line\">str3.<span class=\"built_in\">erase</span>(str3.<span class=\"built_in\">begin</span>() + <span class=\"number\">2</span>);  <span class=\"comment\">// 删除迭代器所指的元素，&quot;I ove AcWing! It&#x27;s very funny!&quot;</span></span><br><span class=\"line\">str4.<span class=\"built_in\">erase</span>(str4.<span class=\"built_in\">begin</span>() + <span class=\"number\">7</span>, str4.<span class=\"built_in\">end</span>() - <span class=\"number\">11</span>);  <span class=\"comment\">// 删除[str4.begin()+7,str4.end()-11)的所有元素，&quot;I love very funny!&quot;</span></span><br><span class=\"line\">str5.<span class=\"built_in\">erase</span>(<span class=\"built_in\">remove</span>(str5.<span class=\"built_in\">begin</span>(), str5.<span class=\"built_in\">end</span>(), <span class=\"string\">&#x27;n&#x27;</span>), str5.<span class=\"built_in\">end</span>());  <span class=\"comment\">// 删除[str5.begin(),str5.end())中所有字符&#x27;n&#x27;，&quot;I love AcWig! It&#x27;s very fuy!&quot;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"3-4-queue-priority-queue\">3.4 queue/priority_queue</h3>\n<p><code>queue</code> 又称<strong>队列</strong>，是一种<strong>先进先出</strong>（First In First Out，FIFO）的数据结构，定义在 <code>&lt;queue&gt;</code> 头文件中，<code>queue</code> 容器允许从一端（称为<strong>队尾</strong>）新增元素（入队），从另一端（称为<strong>队头</strong>）移除元素（出队）。</p>\n<p><code>priority_queue</code> 又称<strong>优先队列</strong>，同样定义在 <code>&lt;queue&gt;</code> 头文件中，与 <code>queue</code> 不同的地方在于我们可以自定义其中数据的优先级，优先级高的排在队列前面，优先出队。<code>priority_queue</code> 具有 <code>queue</code> 的所有特性，包括基本操作，只是在这基础上添加了内部的一个排序，它的本质是用<strong>堆</strong>实现的，因此可分为<strong>小根堆</strong>与<strong>大根堆</strong>，<strong>小根堆</strong>中较小的元素排在前面，<strong>大根堆</strong>中较大的元素排在前面。（创建 <code>priority_queue</code> 时<strong>默认是大根堆！</strong>）</p>\n<ul>\n<li><code>queue/priority_queue</code> 的定义方式：</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">queue&lt;<span class=\"type\">int</span>&gt; que;  <span class=\"comment\">// 定义一个queue，其中元素的类型为int</span></span><br><span class=\"line\">queue&lt;<span class=\"type\">int</span>&gt; que[N];  <span class=\"comment\">// 定义一个queue数组，其中有N个queue</span></span><br><span class=\"line\">priority_queue&lt;<span class=\"type\">int</span>&gt; bigHeap;  <span class=\"comment\">// 定义一个大根堆</span></span><br><span class=\"line\">priority_queue&lt;<span class=\"type\">int</span>, vector&lt;<span class=\"type\">int</span>&gt;, greater&lt;<span class=\"type\">int</span>&gt; &gt; smallHeap;  <span class=\"comment\">// 定义一个小根堆</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li><code>queue/priority_queue</code> 的常用内置函数：</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// queue/priority_queue中的常用内置函数</span></span><br><span class=\"line\">queue&lt;<span class=\"type\">int</span>&gt; que;</span><br><span class=\"line\">priority_queue&lt;<span class=\"type\">int</span>&gt; bigHeap;</span><br><span class=\"line\">que.<span class=\"built_in\">push</span>(x);  <span class=\"comment\">// 在queue的队尾插入元素x</span></span><br><span class=\"line\">que.<span class=\"built_in\">pop</span>();  <span class=\"comment\">// 出队queue的队头元素</span></span><br><span class=\"line\">que.<span class=\"built_in\">front</span>();  <span class=\"comment\">// 返回queue的队头元素</span></span><br><span class=\"line\">que.<span class=\"built_in\">back</span>();  <span class=\"comment\">// 返回queue的队尾元素</span></span><br><span class=\"line\">que.<span class=\"built_in\">size</span>();  <span class=\"comment\">// 返回queue中元素的个数</span></span><br><span class=\"line\">que.<span class=\"built_in\">empty</span>();  <span class=\"comment\">// 返回queue是否为空，若为空则返回true否则返回false</span></span><br><span class=\"line\">bigHeap.<span class=\"built_in\">top</span>();  <span class=\"comment\">// 返回priority_queue的队头元素</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"3-5-deque\">3.5 deque</h3>\n<p><code>deque</code> 又称<strong>双端队列</strong>，定义在 <code>&lt;deque&gt;</code> 头文件中，<code>vector</code> 容器是单向开口的连续内存空间，<code>deque</code> 则是一种<strong>双向开口的连续线性空间</strong>。所谓的双向开口，意思是可以在头尾两端分别做元素的插入和删除操作，当然，<code>vector</code>也可以在头尾两端插入元素，但是在其头部进行插入操作效率很低。<code>deque</code> 和 <code>vector</code> 最大的差异一是在于 <code>deque</code> 允许使用常数项时间在头部进行元素的插入和删除操作，二是在于 <code>deque</code> 没有容量的概念，因为它是动态的以分段连续空间组合而成，随时可以增加一段新的空间并链接起来。</p>\n<ul>\n<li><code>deque</code> 的定义方式：</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">deque&lt;<span class=\"type\">int</span>&gt; deq;  <span class=\"comment\">// 定义一个deque，其中的元素为int类型</span></span><br><span class=\"line\">deque&lt;<span class=\"type\">int</span>&gt; deq[N];  <span class=\"comment\">// 定义一个deque数组，其中有N个deque</span></span><br><span class=\"line\"><span class=\"function\">deque&lt;<span class=\"type\">int</span>&gt; <span class=\"title\">deq</span><span class=\"params\">(len)</span></span>;  <span class=\"comment\">// 定义一个长度为len的deque</span></span><br><span class=\"line\"><span class=\"function\">deque&lt;<span class=\"type\">int</span>&gt; <span class=\"title\">deq</span><span class=\"params\">(len, x)</span></span>;  <span class=\"comment\">// 定义一个长度为len的deque，初始化每个元素为x</span></span><br><span class=\"line\"><span class=\"function\">deque&lt;<span class=\"type\">int</span>&gt; <span class=\"title\">deq2</span><span class=\"params\">(deq1)</span></span>;  <span class=\"comment\">// 用deq1给v2赋值，deq2的类型为deque</span></span><br><span class=\"line\"><span class=\"function\">deque&lt;<span class=\"type\">int</span>&gt; <span class=\"title\">deq2</span><span class=\"params\">(deq1.begin(), deq1.begin() + <span class=\"number\">3</span>)</span></span>;  <span class=\"comment\">// 将deq1中第0~2三个元素赋值给deq2</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li><code>deque</code> 的常用内置函数：</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//deque中的常用内置函数</span></span><br><span class=\"line\">deque&lt;<span class=\"type\">int</span>&gt; deq = &#123; <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span> &#125;;  <span class=\"comment\">// 初始化vector，v:&#123;1, 2, 3&#125;</span></span><br><span class=\"line\">deque&lt;<span class=\"type\">int</span>&gt;::iterator it = deq.<span class=\"built_in\">begin</span>();  <span class=\"comment\">// 定义vector的迭代器，指向begin()</span></span><br><span class=\"line\"></span><br><span class=\"line\">deq.<span class=\"built_in\">push_back</span>(<span class=\"number\">4</span>);  <span class=\"comment\">// 在deque的尾部插入元素4，v:&#123;1, 2, 3, 4&#125;</span></span><br><span class=\"line\">deq.<span class=\"built_in\">pop_back</span>();  <span class=\"comment\">// 删除deque的尾部元素，v:&#123;1, 2, 3&#125;</span></span><br><span class=\"line\">deq.<span class=\"built_in\">push_front</span>(<span class=\"number\">4</span>);  <span class=\"comment\">// 在deque的头部插入元素4，v:&#123;4, 1, 2, 3&#125;</span></span><br><span class=\"line\">deq.<span class=\"built_in\">pop_front</span>();  <span class=\"comment\">// 删除deque的头部元素，v:&#123;1, 2, 3&#125;</span></span><br><span class=\"line\">deq.<span class=\"built_in\">size</span>();  <span class=\"comment\">// 返回deque中元素的个数</span></span><br><span class=\"line\">deq.<span class=\"built_in\">empty</span>();  <span class=\"comment\">// 返回deque是否为空，若为空则返回true否则返回false</span></span><br><span class=\"line\">deq.<span class=\"built_in\">front</span>();  <span class=\"comment\">// 返回deque中的第一个元素</span></span><br><span class=\"line\">deq.<span class=\"built_in\">back</span>();  <span class=\"comment\">// 返回deque中的最后一个元素</span></span><br><span class=\"line\">deq.<span class=\"built_in\">begin</span>();  <span class=\"comment\">// 返回deque第一个元素的迭代器</span></span><br><span class=\"line\">deq.<span class=\"built_in\">end</span>();  <span class=\"comment\">// 返回deque最后一个元素后一个位置的迭代器</span></span><br><span class=\"line\">deq.<span class=\"built_in\">clear</span>();  <span class=\"comment\">// 清空deque</span></span><br><span class=\"line\">deq.<span class=\"built_in\">erase</span>(deq.<span class=\"built_in\">begin</span>());  <span class=\"comment\">// 删除迭代器it所指向的元素，即删除第一个元素</span></span><br><span class=\"line\">deq.<span class=\"built_in\">erase</span>(deq.<span class=\"built_in\">begin</span>(), deq.<span class=\"built_in\">begin</span>() + <span class=\"number\">2</span>);  <span class=\"comment\">// 删除区间[v.begin(), v.begin() + 2)的所有元素</span></span><br><span class=\"line\">deq.<span class=\"built_in\">insert</span>(deq.<span class=\"built_in\">begin</span>(), <span class=\"number\">1</span>);  <span class=\"comment\">// 在迭代器it所指向的位置前插入元素1，返回插入元素的迭代器</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 根据下标进行遍历</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; deq.<span class=\"built_in\">size</span>(); i++)</span><br><span class=\"line\">    cout &lt;&lt; deq[i] &lt;&lt; <span class=\"string\">&#x27; &#x27;</span>;</span><br><span class=\"line\"><span class=\"comment\">// 使用迭代器遍历</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (deque&lt;<span class=\"type\">int</span>&gt;::iterator it = deq.<span class=\"built_in\">begin</span>(); it != deq.<span class=\"built_in\">end</span>(); it++)</span><br><span class=\"line\">    cout &lt;&lt; *it &lt;&lt; <span class=\"string\">&#x27; &#x27;</span>;</span><br><span class=\"line\"><span class=\"comment\">// for_each遍历(C++11)</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> x : deq)</span><br><span class=\"line\">    cout &lt;&lt; x &lt;&lt; <span class=\"string\">&#x27; &#x27;</span>;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-6-map-multimap\">3.6 map/multimap</h3>\n<p><code>map/multimap</code> 又称<strong>映射</strong>，定义在 <code>&lt;map&gt;</code> 头文件中，<code>map</code> 和 <code>multimap</code> 的底层实现机制都是红黑树。<code>map</code> 的功能是<strong>能够将任意类型的元素映射到另一个任意类型的元素上</strong>，并且所有的元素都会根据元素的键值自动排序。<code>map</code> 所有的元素都是 <code>pair</code>，同时拥有<strong>键值</strong>和<strong>实值</strong>（即 <code>(key, value)</code> 对），<code>key</code> 被视为<strong>键值</strong>，<code>value</code> 被视为<strong>实值</strong>，<code>map</code> 不允许两个元素有相同的键值。<code>multimap</code> 和 <code>map</code> 的操作类似，唯一区别是 <code>multimap</code> 的键值允许重复。</p>\n<ul>\n<li><code>map/multimap</code> 的定义方式：</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">map&lt;string, <span class=\"type\">int</span>&gt; mp;  <span class=\"comment\">// 定义一个将string映射成int的map</span></span><br><span class=\"line\">map&lt;string, <span class=\"type\">int</span>&gt; mp[N];  <span class=\"comment\">// 定义一个map数组，其中有N个map</span></span><br><span class=\"line\">multimap&lt;string, <span class=\"type\">int</span>&gt; mulmp;  <span class=\"comment\">// 定义一个将string映射成int的multimap</span></span><br><span class=\"line\">multimap&lt;string, <span class=\"type\">int</span>&gt; mulmp[N];  <span class=\"comment\">// 定义一个multimap数组，其中有N个multimap</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li><code>map/multimap</code> 的常用内置函数：</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// map/multimap中的常用内置函数</span></span><br><span class=\"line\">map&lt;string, <span class=\"type\">int</span>&gt; mp;</span><br><span class=\"line\">mp[<span class=\"string\">&quot;abc&quot;</span>] = <span class=\"number\">3</span>;  <span class=\"comment\">// 将&quot;abc&quot;映射到3</span></span><br><span class=\"line\">mp[<span class=\"string\">&quot;ab&quot;</span>]++;  <span class=\"comment\">// 将&quot;ab&quot;所映射的整数++</span></span><br><span class=\"line\">mp.<span class=\"built_in\">insert</span>(<span class=\"built_in\">make_pair</span>(<span class=\"string\">&quot;cd&quot;</span>, <span class=\"number\">2</span>));  <span class=\"comment\">// 插入元素</span></span><br><span class=\"line\">mp.<span class=\"built_in\">insert</span>(&#123; <span class=\"string\">&quot;ef&quot;</span>, <span class=\"number\">5</span> &#125;);  <span class=\"comment\">// 同上</span></span><br><span class=\"line\">mp.<span class=\"built_in\">size</span>();  <span class=\"comment\">// 返回map中元素的个数</span></span><br><span class=\"line\">mp.<span class=\"built_in\">empty</span>();  <span class=\"comment\">// 返回map是否为空，若为空返回true否则返回false</span></span><br><span class=\"line\">mp.<span class=\"built_in\">clear</span>();  <span class=\"comment\">// 清空map</span></span><br><span class=\"line\">mp.<span class=\"built_in\">erase</span>(<span class=\"string\">&quot;ef&quot;</span>);  <span class=\"comment\">// 清除元素&#123;&quot;ef&quot;, 5&#125;</span></span><br><span class=\"line\">mp[<span class=\"string\">&quot;abc&quot;</span>];  <span class=\"comment\">// 返回&quot;abc&quot;映射的值</span></span><br><span class=\"line\">mp.<span class=\"built_in\">begin</span>();  <span class=\"comment\">// 返回map第一个元素的迭代器</span></span><br><span class=\"line\">mp.<span class=\"built_in\">end</span>();  <span class=\"comment\">// 返回map最后一个元素后一个位置的迭代器</span></span><br><span class=\"line\">mp.<span class=\"built_in\">find</span>(<span class=\"string\">&quot;ab&quot;</span>);  <span class=\"comment\">// 返回第一个键值为&quot;ab&quot;的迭代器，若不存在则返回mp.end()</span></span><br><span class=\"line\">mp.<span class=\"built_in\">find</span>(&#123; <span class=\"string\">&quot;abc&quot;</span>, <span class=\"number\">3</span> &#125;);  <span class=\"comment\">// 返回元素&#123;&quot;abc&quot;, 3&#125;的迭代器，若不存在则返回mp.end()</span></span><br><span class=\"line\">mp.<span class=\"built_in\">count</span>(<span class=\"string\">&quot;abc&quot;</span>);  <span class=\"comment\">// 返回第一个键值为&quot;abc&quot;的元素数量1，由于map元素不能重复因此count返回值只有0或1</span></span><br><span class=\"line\">mp.<span class=\"built_in\">count</span>(&#123; <span class=\"string\">&quot;abc&quot;</span>, <span class=\"number\">2</span> &#125;);  <span class=\"comment\">// 返回第一个键值为&quot;abc&quot;的元素数量1，注意和find不一样，count只判断第一个键值</span></span><br><span class=\"line\">mp.<span class=\"built_in\">lower_bound</span>(<span class=\"string\">&quot;abc&quot;</span>);  <span class=\"comment\">// 返回第一个键值大于等于&quot;abc&quot;的元素的迭代器，&#123;&quot;abc&quot;, 3&#125;</span></span><br><span class=\"line\">mp.<span class=\"built_in\">upper_bound</span>(<span class=\"string\">&quot;abc&quot;</span>);  <span class=\"comment\">// 返回第一个键值大于&quot;abc&quot;的元素的迭代器，&#123;&quot;cd&quot;, 2&#125;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 使用迭代器遍历</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (map&lt;string, <span class=\"type\">int</span>&gt;::iterator it = mp.<span class=\"built_in\">begin</span>(); it != mp.<span class=\"built_in\">end</span>(); it++)</span><br><span class=\"line\">    cout &lt;&lt; (*it).first &lt;&lt; <span class=\"string\">&#x27; &#x27;</span> &lt;&lt; (*it).second &lt;&lt; endl;</span><br><span class=\"line\"><span class=\"comment\">// for_each遍历(C++11)</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> x : mp)</span><br><span class=\"line\">    cout &lt;&lt; x.first &lt;&lt; <span class=\"string\">&#x27; &#x27;</span> &lt;&lt; x.second &lt;&lt; endl;</span><br><span class=\"line\"><span class=\"comment\">// 扩展推断范围的for_each遍历(C++17)</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> &amp;[k, v] : mp)</span><br><span class=\"line\">    cout &lt;&lt; k &lt;&lt; <span class=\"string\">&#x27; &#x27;</span> &lt;&lt; v &lt;&lt; endl;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-7-set-multiset\">3.7 set/multiset</h3>\n<p><code>set/multiset</code> 又称<strong>集合</strong>，定义在 <code>&lt;set&gt;</code> 头文件中。<code>set</code> 的特性是所有元素都会根据元素的键值自动被排序，<code>set</code> 的元素不像 <code>map</code> 那样可以同时拥有键值和实值，<code>set</code> 的元素既是键值又是实值，<code>set</code> 不允许两个元素有相同的键值，因此总结来说就是 <code>set</code> 中的元素是<strong>有序且不重复的</strong>。<code>multiset</code> 的特性和用法和 <code>set</code> 完全相同，唯一的区别在于 <code>multiset</code> 允许有重复元素，<code>set</code> 和 <code>multiset</code> 的底层实现都是红黑树。</p>\n<ul>\n<li><code>set/multiset</code> 的定义方式：</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set&lt;<span class=\"type\">int</span>&gt; st;  <span class=\"comment\">// 定义一个set，其中的元素类型为int</span></span><br><span class=\"line\">set&lt;<span class=\"type\">int</span>&gt; st[N];  <span class=\"comment\">// 定义一个set数组，其中有N个set</span></span><br><span class=\"line\">multiset&lt;<span class=\"type\">int</span>&gt; mulst;  <span class=\"comment\">// 定义一个multiset</span></span><br><span class=\"line\">multiset&lt;<span class=\"type\">int</span>&gt; mulst[N];  <span class=\"comment\">// 定义一个multiset数组，其中有N个multiset</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li><code>set/multiset</code> 的常用内置函数：</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// set/multiset中的常用内置函数</span></span><br><span class=\"line\">set&lt;<span class=\"type\">int</span>&gt; st;</span><br><span class=\"line\">st.<span class=\"built_in\">insert</span>(<span class=\"number\">5</span>);  <span class=\"comment\">// 插入元素5</span></span><br><span class=\"line\">st.<span class=\"built_in\">insert</span>(<span class=\"number\">6</span>);  <span class=\"comment\">// 同上</span></span><br><span class=\"line\">st.<span class=\"built_in\">insert</span>(<span class=\"number\">7</span>);  <span class=\"comment\">// 同上</span></span><br><span class=\"line\">st.<span class=\"built_in\">size</span>();  <span class=\"comment\">// 返回set中元素的个数</span></span><br><span class=\"line\">st.<span class=\"built_in\">empty</span>();  <span class=\"comment\">// 返回set是否为空，若为空返回true否则返回false</span></span><br><span class=\"line\">st.<span class=\"built_in\">erase</span>(<span class=\"number\">6</span>);  <span class=\"comment\">// 清除元素6</span></span><br><span class=\"line\">st.<span class=\"built_in\">begin</span>();  <span class=\"comment\">// 返回set第一个元素的迭代器</span></span><br><span class=\"line\">st.<span class=\"built_in\">end</span>();  <span class=\"comment\">// 返回set最后一个元素后一个位置的迭代器</span></span><br><span class=\"line\">st.<span class=\"built_in\">clear</span>();  <span class=\"comment\">// 清空set</span></span><br><span class=\"line\">st.<span class=\"built_in\">find</span>(<span class=\"number\">5</span>);  <span class=\"comment\">// 返回元素5的迭代器，若不存在则返回st.end()</span></span><br><span class=\"line\">st.<span class=\"built_in\">count</span>(<span class=\"number\">5</span>);  <span class=\"comment\">// 返回元素5的个数1，由于set元素不会重复，因此count返回值只有0或1</span></span><br><span class=\"line\">st.<span class=\"built_in\">lower_bound</span>(<span class=\"number\">5</span>);  <span class=\"comment\">// 返回第一个键值大于等于5的元素的迭代器，返回元素5的迭代器</span></span><br><span class=\"line\">st.<span class=\"built_in\">upper_bound</span>(<span class=\"number\">5</span>);  <span class=\"comment\">// 返回第一个键值大于5的元素的迭代器，返回元素7的迭代器</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 使用迭代器遍历</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (set&lt;<span class=\"type\">int</span>&gt;::iterator it = st.<span class=\"built_in\">begin</span>(); it != st.<span class=\"built_in\">end</span>(); it++)</span><br><span class=\"line\">    cout &lt;&lt; (*it) &lt;&lt; <span class=\"string\">&#x27; &#x27;</span>;</span><br><span class=\"line\"><span class=\"comment\">// for_each遍历(C++11)</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> x : st)</span><br><span class=\"line\">    cout &lt;&lt; x &lt;&lt; <span class=\"string\">&#x27; &#x27;</span>;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-8-unordered-map-unordered-set\">3.8 unordered_map/unordered_set</h3>\n<p><code>unordered_map/unordered_set</code> 分别定义在 <code>&lt;unordered_map&gt;</code> 与 <code>&lt;unordered_set&gt;</code> 头文件中，内部采用的是 <code>hash</code> 表结构，拥有快速检索的功能。与 <code>map/set</code> 相比最大的区别在于 <code>unordered_map/unordered_set</code> 中的元素是<strong>无序</strong>的，增删改查的时间复杂度为 <code>O(1)</code>（<code>map/set</code> 增删改查的时间复杂度为 <code>O(logn)</code>），但是不支持 <code>lower_bound()/upper_bound()</code> 函数。</p>\n<ul>\n<li><code>unordered_map/unordered_set</code> 的定义方式：</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">unordered_set&lt;<span class=\"type\">int</span>&gt; st;  <span class=\"comment\">// 定义一个unordered_set，其中的元素类型为int</span></span><br><span class=\"line\">unordered_set&lt;<span class=\"type\">int</span>&gt; st[N];  <span class=\"comment\">// 定义一个unordered_set数组，其中有N个unordered_set</span></span><br><span class=\"line\">unordered_map&lt;<span class=\"type\">int</span>, <span class=\"type\">int</span>&gt; mp;  <span class=\"comment\">// 定义一个unordered_map</span></span><br><span class=\"line\">unordered_map&lt;<span class=\"type\">int</span>, <span class=\"type\">int</span>&gt; mp[N];  <span class=\"comment\">// 定义一个unordered_map数组，其中有N个unordered_map</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li><code>unordered_map/unordered_set</code> 的常用内置函数：</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// unordered_map/unordered_set中的常用内置函数</span></span><br><span class=\"line\">unordered_set&lt;<span class=\"type\">int</span>&gt; st;</span><br><span class=\"line\">unordered_map&lt;<span class=\"type\">int</span>, <span class=\"type\">int</span>&gt; mp;</span><br><span class=\"line\">st.<span class=\"built_in\">insert</span>(<span class=\"number\">5</span>);  <span class=\"comment\">// 插入元素5</span></span><br><span class=\"line\">st.<span class=\"built_in\">insert</span>(<span class=\"number\">6</span>);  <span class=\"comment\">// 同上</span></span><br><span class=\"line\">st.<span class=\"built_in\">insert</span>(<span class=\"number\">7</span>);  <span class=\"comment\">// 同上</span></span><br><span class=\"line\">st.<span class=\"built_in\">size</span>();  <span class=\"comment\">// 返回unordered_set中元素的个数</span></span><br><span class=\"line\">st.<span class=\"built_in\">empty</span>();  <span class=\"comment\">// 返回unordered_set是否为空，若为空返回true否则返回false</span></span><br><span class=\"line\">st.<span class=\"built_in\">erase</span>(<span class=\"number\">6</span>);  <span class=\"comment\">// 清除元素6</span></span><br><span class=\"line\">st.<span class=\"built_in\">find</span>(<span class=\"number\">5</span>);  <span class=\"comment\">// 返回元素5的迭代器，若不存在则返回st.end()</span></span><br><span class=\"line\">st.<span class=\"built_in\">count</span>(<span class=\"number\">5</span>);  <span class=\"comment\">// 返回元素5的个数，由于unordered_set元素不会重复，因此count返回值只有0或1</span></span><br><span class=\"line\">st.<span class=\"built_in\">begin</span>();  <span class=\"comment\">// 返回unordered_set第一个元素的迭代器</span></span><br><span class=\"line\">st.<span class=\"built_in\">end</span>();  <span class=\"comment\">// 返回unordered_set最后一个元素后一个位置的迭代器</span></span><br><span class=\"line\">st.<span class=\"built_in\">clear</span>();  <span class=\"comment\">// 清空unordered_set</span></span><br><span class=\"line\">mp.<span class=\"built_in\">insert</span>(<span class=\"built_in\">make_pair</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>));  <span class=\"comment\">// 插入元素&#123;1, 2&#125;</span></span><br><span class=\"line\">mp.<span class=\"built_in\">insert</span>(&#123; <span class=\"number\">3</span>, <span class=\"number\">4</span> &#125;);  <span class=\"comment\">// 同上</span></span><br><span class=\"line\">mp.<span class=\"built_in\">size</span>();  <span class=\"comment\">// 返回unordered_map中元素的个数</span></span><br><span class=\"line\">mp.<span class=\"built_in\">empty</span>();  <span class=\"comment\">// 返回unordered_map是否为空，若为空返回true否则返回false</span></span><br><span class=\"line\">mp.<span class=\"built_in\">erase</span>(<span class=\"number\">3</span>);  <span class=\"comment\">// 清除元素&#123;3, 4&#125;</span></span><br><span class=\"line\">mp.<span class=\"built_in\">find</span>(<span class=\"number\">1</span>);  <span class=\"comment\">// 返回第一个键值为1的迭代器，若不存在则返回mp.end()</span></span><br><span class=\"line\">mp.<span class=\"built_in\">count</span>(<span class=\"number\">1</span>);  <span class=\"comment\">// 返回第一个键值为1的元素数量，由于unordered_map元素不能重复因此count返回值只有0或1</span></span><br><span class=\"line\">mp.<span class=\"built_in\">begin</span>();  <span class=\"comment\">// 返回unordered_map第一个元素的迭代器</span></span><br><span class=\"line\">mp.<span class=\"built_in\">end</span>();  <span class=\"comment\">// 返回unordered_map最后一个元素后一个位置的迭代器</span></span><br><span class=\"line\">mp.<span class=\"built_in\">clear</span>();  <span class=\"comment\">// 清空unordered_map</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 使用迭代器遍历</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (unordered_set&lt;<span class=\"type\">int</span>&gt;::iterator it = st.<span class=\"built_in\">begin</span>(); it != st.<span class=\"built_in\">end</span>(); it++)</span><br><span class=\"line\">    cout &lt;&lt; (*it) &lt;&lt; <span class=\"string\">&#x27; &#x27;</span>;</span><br><span class=\"line\"><span class=\"comment\">// for_each遍历(C++11)</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> x : st)</span><br><span class=\"line\">    cout &lt;&lt; x &lt;&lt; <span class=\"string\">&#x27; &#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 使用迭代器遍历</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (unordered_map&lt;<span class=\"type\">int</span>, <span class=\"type\">int</span>&gt;::iterator it = mp.<span class=\"built_in\">begin</span>(); it != mp.<span class=\"built_in\">end</span>(); it++)</span><br><span class=\"line\">    cout &lt;&lt; (*it).first &lt;&lt; <span class=\"string\">&#x27; &#x27;</span> &lt;&lt; (*it).second &lt;&lt; endl;</span><br><span class=\"line\"><span class=\"comment\">// for_each遍历(C++11)</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> x : mp)</span><br><span class=\"line\">    cout &lt;&lt; x.first &lt;&lt; <span class=\"string\">&#x27; &#x27;</span> &lt;&lt; x.second &lt;&lt; endl;</span><br><span class=\"line\"><span class=\"comment\">// 扩展推断范围的for_each遍历(C++17)</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> &amp;[k, v] : mp)</span><br><span class=\"line\">    cout &lt;&lt; k &lt;&lt; <span class=\"string\">&#x27; &#x27;</span> &lt;&lt; v &lt;&lt; endl;</span><br></pre></td></tr></table></figure>\n<h2 id=\"四、STL算法\">四、STL算法</h2>\n<p>C++ 标准库定义了一组<strong>泛型算法</strong>，之所以称为泛型指的是它们可以操作在多种容器上，<strong>不但可以作用于标准库类型，还可以用在内置数组类型甚至其它类型的序列上</strong>。泛型算法定义在 <code>&lt;algorithm&gt;</code> 头文件中，标准库还定义了一组<strong>泛化的算术算法</strong>（Generalized Numeric Algorithm），定义在 <code>&lt;numeric&gt;</code> 头文件中。使用方法如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;algorithm&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;numeric&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 使用STL容器时将数组指针改为迭代器即可</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> a[<span class=\"number\">5</span>] = &#123; <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span> &#125;;</span><br><span class=\"line\">    <span class=\"type\">int</span> b[<span class=\"number\">5</span>] = &#123; <span class=\"number\">0</span> &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 排序算法</span></span><br><span class=\"line\">    <span class=\"built_in\">sort</span>(a, a + <span class=\"number\">5</span>);  <span class=\"comment\">// 将区间[0, 5)内元素按字典序从小到大排序</span></span><br><span class=\"line\">    <span class=\"built_in\">sort</span>(a, a + <span class=\"number\">5</span>, <span class=\"built_in\">greater</span>&lt;<span class=\"type\">int</span>&gt;());  <span class=\"comment\">// 将区间[0, 5)内元素按字典序从大到小排序</span></span><br><span class=\"line\">    <span class=\"built_in\">reverse</span>(a, a + <span class=\"number\">5</span>);  <span class=\"comment\">// 将区间[0, 5)内元素翻转</span></span><br><span class=\"line\">    <span class=\"built_in\">nth_element</span>(a, a + <span class=\"number\">3</span>, a + <span class=\"number\">5</span>);  <span class=\"comment\">// 将区间[0, 5)中第a + 3个数归位，即将第3大的元素放到正确的位置上，该元素前后的元素不一定有序</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 查找与统计算法</span></span><br><span class=\"line\">    <span class=\"built_in\">find</span>(a, a + <span class=\"number\">5</span>, <span class=\"number\">3</span>);  <span class=\"comment\">// 在区间[0, 5)内查找等于3的元素，返回迭代器，若不存在则返回end()</span></span><br><span class=\"line\">    <span class=\"built_in\">binary_search</span>(a, a + <span class=\"number\">5</span>, <span class=\"number\">2</span>);  <span class=\"comment\">// 二分查找区间[0, 5)内是否存在元素2，若存在返回true否则返回false</span></span><br><span class=\"line\">    <span class=\"built_in\">count</span>(a, a + <span class=\"number\">5</span>, <span class=\"number\">3</span>);  <span class=\"comment\">// 返回区间[0, 5)内元素3的个数</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 可变序列算法</span></span><br><span class=\"line\">    <span class=\"built_in\">copy</span>(a, a + <span class=\"number\">2</span>, a + <span class=\"number\">3</span>);  <span class=\"comment\">// 将区间[0, 2)的元素复制到以a+3开始的区间，即[3, 5)</span></span><br><span class=\"line\">    <span class=\"built_in\">replace</span>(a, a + <span class=\"number\">5</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>);  <span class=\"comment\">// 将区间[0, 5)内等于3的元素替换为4</span></span><br><span class=\"line\">    <span class=\"built_in\">fill</span>(a, a + <span class=\"number\">5</span>, <span class=\"number\">1</span>);  <span class=\"comment\">// 将1写入区间[0, 5)中(初始化数组函数)</span></span><br><span class=\"line\">    <span class=\"built_in\">unique</span>(a, a + <span class=\"number\">5</span>);  <span class=\"comment\">// 将相邻元素间的重复元素全部移动至末端，返回去重之后数组最后一个元素之后的地址</span></span><br><span class=\"line\">    <span class=\"built_in\">remove</span>(a, a + <span class=\"number\">5</span>, <span class=\"number\">3</span>);  <span class=\"comment\">// 将区间[0, 5)中的元素3移至末端，返回新数组最后一个元素之后的地址</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 排列算法</span></span><br><span class=\"line\">    <span class=\"built_in\">next_permutation</span>(a, a + <span class=\"number\">5</span>);  <span class=\"comment\">// 产生下一个排列&#123; 1, 2, 3, 5, 4 &#125;</span></span><br><span class=\"line\">    <span class=\"built_in\">prev_permutation</span>(a, a + <span class=\"number\">5</span>);  <span class=\"comment\">// 产生上一个排列&#123; 1, 2, 3, 4, 5 &#125;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 前缀和算法</span></span><br><span class=\"line\">    <span class=\"built_in\">partial_sum</span>(a, a + <span class=\"number\">5</span>, b);  <span class=\"comment\">// 计算数组a在区间[0, 5)内的前缀和并将结果保存至数组b中，b = &#123; 1, 3, 6, 10, 15 &#125;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 差分算法(感谢willem248同学的补充)</span></span><br><span class=\"line\">    <span class=\"built_in\">adjacent_difference</span>(a, a + <span class=\"number\">5</span>, b);  <span class=\"comment\">// 计算数组a区间[0, 5)内的差分并将结果保存至数组b中，b = &#123; 1, 1, 1, 1, 1 &#125;</span></span><br><span class=\"line\">    <span class=\"built_in\">adjacent_difference</span>(a, a + <span class=\"number\">5</span>, b, <span class=\"built_in\">plus</span>&lt;<span class=\"type\">int</span>&gt;());  <span class=\"comment\">// 计算相邻两元素的和，b = &#123; 1, 3, 5, 7, 9 &#125;</span></span><br><span class=\"line\">    <span class=\"built_in\">adjacent_difference</span>(a, a + <span class=\"number\">5</span>, b, <span class=\"built_in\">multiplies</span>&lt;<span class=\"type\">int</span>&gt;());  <span class=\"comment\">// 计算相邻两元素的乘积，b = &#123; 1, 2, 6, 12, 20 &#125;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>",
            "tags": [
                "C++"
            ]
        }
    ]
}